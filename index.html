<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-05T00:00:00Z">2025-06-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time Hyper-Scaling with KV Cache Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity
  Analysis Between Alignment and Fine-tuning <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hsiung.cc/llm-similarity-risk/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases
  in Preference Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Bharadwaj, Chaitanya Malaviya, Nitish Joshi, Mark Yatskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models serve as proxies for human preference judgements in alignment
and evaluation, yet they exhibit systematic miscalibration, prioritizing
superficial patterns over substantive qualities. This bias manifests as
overreliance on features like length, structure, and style, leading to issues
like reward hacking and unreliable evaluations. Evidence suggests these biases
originate in artifacts in human training data. In this work, we systematically
investigate the relationship between training data biases and preference model
miscalibration across five idiosyncratic features of language model
generations: length, structure, jargon, sycophancy and vagueness. Using
controlled counterfactual pairs, we first quantify the extent to which
preference models favor responses with magnified biases (skew), finding this
preference occurs in >60% of instances, and model preferences show high
miscalibration (~40%) compared to human preferences. Notably, bias features
only show mild negative correlations to human preference labels (mean r_human =
-0.12) but show moderately strong positive correlations with labels from a
strong reward model (mean r_model = +0.36), suggesting that models may overrely
on spurious cues. To mitigate these issues, we propose a simple post-training
method based on counterfactual data augmentation (CDA) using synthesized
contrastive examples. Finetuning models with CDA reduces average miscalibration
from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%,
while maintaining overall RewardBench performance, showing that targeted
debiasing is effective for building reliable preference models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at
  https://github.com/anirudhb123/preference-model-biases</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search Arena: Analyzing Search-Augmented LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code: https://github.com/lmarena/search-arena. Dataset:
  https://huggingface.co/datasets/lmarena-ai/search-arena-24k</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinetics: Rethinking Test-Time Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
for realizing the full potential of test-time scaling because, unlike training,
where parameter scaling saturates, test-time accuracy continues to improve
through increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Hour-Scale Video Training for Long Video-Language
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent long-form video-language understanding benchmarks have driven progress
in video large multimodal models (Video-LMMs). However, the scarcity of
well-annotated long videos has left the training of hour-long Video-LLMs
underexplored. To close this gap, we present VideoMarathon, a large-scale
hour-long video instruction-following dataset. This dataset includes around
9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60
minutes per video. Specifically, it contains 3.3M high-quality QA pairs,
spanning six fundamental topics: temporality, spatiality, object, action,
scene, and event. Compared to existing video instruction datasets,
VideoMarathon significantly extends training video durations up to 1 hour, and
supports 22 diverse tasks requiring both short- and long-term video
comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and
efficient Video-LMM for hour-scale video-language modeling. It enables
hour-long video training and inference at 1-FPS sampling by leveraging a memory
augmentation module, which adaptively integrates user question-relevant and
spatiotemporal-informative semantics from a cached full video context. In our
experiments, Hour-LLaVA achieves the best performance on multiple long
video-language benchmarks, demonstrating the high quality of the VideoMarathon
dataset and the superiority of the Hour-LLaVA model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://videomarathon.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Data Efficiency for LLM Reinforcement Fine-tuning Through
  Difficulty-targeted Online Data Selection and Rollout Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has become an effective approach for fine-tuning
large language models (LLMs), particularly to enhance their reasoning
capabilities. However, RL fine-tuning remains highly resource-intensive, and
existing work has largely overlooked the problem of data efficiency. In this
paper, we propose two techniques to improve data efficiency in LLM RL
fine-tuning: difficulty-targeted online data selection and rollout replay. We
introduce the notion of adaptive difficulty to guide online data selection,
prioritizing questions of moderate difficulty that are more likely to yield
informative learning signals. To estimate adaptive difficulty efficiently, we
develop an attention-based framework that requires rollouts for only a small
reference set of questions. The adaptive difficulty of the remaining questions
is then estimated based on their similarity to this set. To further reduce
rollout cost, we introduce a rollout replay mechanism that reuses recent
rollouts, lowering per-step computation while maintaining stable updates.
Extensive experiments across 6 LLM-dataset combinations show that our method
reduces RL fine-tuning time by 25% to 65% to reach the same level of
performance as the original GRPO algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Entropic Unlearning: A Primal-Dual Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks
across diverse LLM architectures demonstrate that our approach consistently
matches or exceeds state-of-the-art baselines, effectively removing targeted
information while preserving downstream utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niv Eckhaus, Uri Berger, Gabriel Stanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are used predominantly in synchronous communication, where a human user
and a model communicate in alternating turns. In contrast, many real-world
settings are inherently asynchronous. For example, in group chats, online team
meetings, or social games, there is no inherent notion of turns; therefore, the
decision of when to speak forms a crucial part of the participant's decision
making. In this work, we develop an adaptive asynchronous LLM-agent which, in
addition to determining what to say, also decides when to say it. To evaluate
our agent, we collect a unique dataset of online Mafia games, including both
human participants, as well as our asynchronous agent. Overall, our agent
performs on par with human players, both in game performance, as well as in its
ability to blend in with the other human players. Our analysis shows that the
agent's behavior in deciding when to speak closely mirrors human patterns,
although differences emerge in message content. We release all our data and
code to support and encourage further research for more realistic asynchronous
communication between LLM agents. This work paves the way for integration of
LLMs into realistic human group settings, from assistance in team discussions
to educational and professional environments where complex social dynamics must
be navigated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProRefine: Inference-time <span class="highlight-title">Prompt</span> Refinement with Textual Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Pandita, Tharindu Cyril Weerasooriya, Ankit Parag Shah, Christopher M. Homan, Wei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic workflows, where multiple AI agents collaborate to accomplish complex
tasks like reasoning or planning, are becoming increasingly prevalent. However,
these workflows often suffer from error propagation and sub-optimal
performance, largely due to poorly designed prompts that fail to effectively
guide individual agents. This is a critical problem because it limits the
reliability and scalability of these powerful systems. We introduce ProRefine,
an innovative inference-time prompt optimization method that leverages textual
feedback from large language models (LLMs) to address this challenge. ProRefine
dynamically refines prompts for multi-step reasoning tasks without additional
training or ground truth labels. Evaluated on five benchmark mathematical
reasoning datasets, ProRefine significantly surpasses zero-shot
Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only
boosts accuracy but also allows smaller models to match the performance of
larger ones, highlighting its potential for efficient and scalable AI
deployment, and democratizing access to high-performing AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Micro-Act: Mitigate Knowledge Conflict in Question Answering via
  Actionable Self-Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge
Conflicts, where retrieved external knowledge contradicts the inherent,
parametric knowledge of large language models (LLMs). It adversely affects
performance on downstream tasks such as question answering (QA). Existing
approaches often attempt to mitigate conflicts by directly comparing two
knowledge sources in a side-by-side manner, but this can overwhelm LLMs with
extraneous or lengthy contexts, ultimately hindering their ability to identify
and mitigate inconsistencies. To address this issue, we propose Micro-Act a
framework with a hierarchical action space that automatically perceives context
complexity and adaptively decomposes each knowledge source into a sequence of
fine-grained comparisons. These comparisons are represented as actionable
steps, enabling reasoning beyond the superficial context. Through extensive
experiments on five benchmark datasets, Micro-Act consistently achieves
significant increase in QA accuracy over state-of-the-art baselines across all
5 datasets and 3 conflict types, especially in temporal and semantic types
where all baselines fail significantly. More importantly, Micro-Act exhibits
robust performance on non-conflict questions simultaneously, highlighting its
practical value in real-world RAG applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ron Eliav, Arie Cattan, Eran Hirsch, Shahaf Bassan, Elias Stengel-Eskin, Mohit Bansal, Ido Dagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common approach to hallucination detection casts it as a natural language
inference (NLI) task, often using LLMs to classify whether the generated text
is entailed by corresponding reference texts. Since entailment classification
is a complex reasoning task, one would expect that LLMs could benefit from
generating an explicit reasoning process, as in CoT reasoning or the explicit
``thinking'' of recent reasoning models. In this work, we propose that guiding
such models to perform a systematic and comprehensive reasoning process -- one
that both decomposes the text into smaller facts and also finds evidence in the
source for each fact -- allows models to execute much finer-grained and
accurate entailment decisions, leading to increased performance. To that end,
we define a 3-step reasoning process, consisting of (i) claim decomposition,
(ii) sub-claim attribution and entailment classification, and (iii) aggregated
classification, showing that such guided reasoning indeed yields improved
hallucination detection. Following this reasoning framework, we introduce an
analysis scheme, consisting of several metrics that measure the quality of the
intermediate reasoning steps, which provided additional empirical evidence for
the improved quality of our guided reasoning scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Unified System of Representation for Continuity and
  Discontinuity in Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ratna Kandala, Prakash Mondal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syntactic discontinuity is a grammatical phenomenon in which a constituent is
split into more than one part because of the insertion of an element which is
not part of the constituent. This is observed in many languages across the
world such as Turkish, Russian, Japanese, Warlpiri, Navajo, Hopi, Dyirbal,
Yidiny etc. Different formalisms/frameworks in current linguistic theory
approach the problem of discontinuous structures in different ways. Each
framework/formalism has widely been viewed as an independent and non-converging
system of analysis. In this paper, we propose a unified system of
representation for both continuity and discontinuity in structures of natural
languages by taking into account three formalisms, in particular, Phrase
Structure Grammar (PSG) for its widely used notion of constituency, Dependency
Grammar (DG) for its head-dependent relations, and Categorial Grammar (CG) for
its focus on functor-argument relations. We attempt to show that discontinuous
expressions as well as continuous structures can be analysed through a unified
mathematical derivation incorporating the representations of linguistic
structure in these three grammar formalisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MesaNet: Sequence Modeling by Locally Optimal Test-Time Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, Rif A. Saurous, Guillaume Lajoie, Charlotte Frenkel, Razvan Pascanu, Blaise Agüera y Arcas, João Sacramento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence modeling is currently dominated by causal transformer architectures
that use softmax self-attention. Although widely adopted, transformers require
scaling memory and compute linearly during inference. A recent stream of work
linearized the softmax operation, resulting in powerful recurrent neural
network (RNN) models with constant memory and compute costs such as DeltaNet,
Mamba or xLSTM. These models can be unified by noting that their recurrent
layer dynamics can all be derived from an in-context regression objective,
approximately optimized through an online learning rule. Here, we join this
line of work and introduce a numerically stable, chunkwise parallelizable
version of the recently proposed Mesa layer (von Oswald et al., 2024), and
study it in language modeling at the billion-parameter scale. This layer again
stems from an in-context loss, but which is now minimized to optimality at
every time point using a fast conjugate gradient solver. Through an extensive
suite of experiments, we show that optimal test-time training enables reaching
lower language modeling perplexity and higher downstream benchmark performance
than previous RNNs, especially on tasks requiring long context understanding.
This performance gain comes at the cost of additional flops spent during
inference time. Our results are therefore intriguingly related to recent trends
of increasing test-time compute to improve performance -- here by spending
compute to solve sequential optimization problems within the neural network
itself.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagonal Batching Unlocks Parallelism in Recurrent Memory <span class="highlight-title">Transformer</span>s
  for Long Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danil Sivtsov, Ivan Rodkin, Gleb Kuzmin, Yuri Kuratov, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models struggle with long-context inference due to their
quadratic time and linear memory complexity. Recurrent Memory Transformers
(RMTs) offer a solution by reducing the asymptotic cost to linear time and
constant memory usage. However, their memory update mechanism leads to
sequential execution, causing a performance bottleneck.
  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism
across segments in RMTs while preserving exact recurrence. This approach
eliminates the sequential constraint, enabling efficient GPU inference even for
single long-context inputs without complex batching and pipelining techniques.
Because the technique is purely a run-time computation reordering, existing RMT
models adopt it with no retraining.
  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup
over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential
RMT implementation on 131,072-token sequences. By removing sequential
bottleneck, Diagonal Batching reduces inference cost and latency, thereby
strengthening RMTs as a practical solution for real-world, long-context
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Low-Resource Morphological Inflection via <span class="highlight-title">Self-Supervised</span>
  Objectives <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Wiemerslage, Katharina von der Wense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised objectives have driven major advances in NLP by leveraging
large-scale unlabeled data, but such resources are scarce for many of the
world's languages. Surprisingly, they have not been explored much for
character-level tasks, where smaller amounts of data have the potential to be
beneficial. We investigate the effectiveness of self-supervised auxiliary tasks
for morphological inflection -- a character-level task highly relevant for
language documentation -- in extremely low-resource settings, training
encoder-decoder transformers for 19 languages and 13 auxiliary objectives.
Autoencoding yields the best performance when unlabeled data is very limited,
while character masked language modeling (CMLM) becomes more effective as data
availability increases. Though objectives with stronger inductive biases
influence model predictions intuitively, they rarely outperform standard CMLM.
However, sampling masks based on known morpheme boundaries consistently
improves performance, highlighting a promising direction for low-resource
morphological modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Hu, Hongbo Bo, Jun Hong, Xiaowei Liu, Weiru Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) often suffer from degree bias in node
classification tasks, where prediction performance varies across nodes with
different degrees. Several approaches, which adopt Graph Contrastive Learning
(GCL), have been proposed to mitigate this bias. However, the limited number of
positive pairs and the equal weighting of all positives and negatives in GCL
still lead to low-degree nodes acquiring insufficient and noisy information.
This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to
mitigate degree bias. It adds more positive pairs by leveraging node labels and
adaptively weights positive and negative pairs based on their learning
hardness. In addition, we develop an experimental framework named SHARP to
extend HAR to a broader range of scenarios. Both our theoretical analysis and
experiments validate the effectiveness of SHARP. The experimental results
across four datasets show that SHARP achieves better performance against
baselines at both global and degree levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-First Search: Self-Guided Exploration of the Solution Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Herr, Tim Rocktäschel, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable improvements in
reasoning and planning through increased test-time compute, often by framing
problem-solving as a search process. While methods like Monte Carlo Tree Search
(MCTS) have proven effective in some domains, their reliance on fixed
exploration hyperparameters limits their adaptability across tasks of varying
difficulty, rendering them impractical or expensive in certain settings. In
this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM
Self-Guided Search} method that removes the need for pre-defined search
strategies by empowering the LLM to autonomously control the search process via
self-guided exploration. Rather than relying on external heuristics or
hardcoded policies, the LLM evaluates whether to pursue the current search path
or explore alternative branches based on its internal scoring mechanisms. This
enables more flexible and context-sensitive reasoning without requiring manual
tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku
against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth
First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which
have been used to achieve SotA results on a range of challenging reasoning
tasks. We found that LFS (1) performs better on more challenging tasks without
additional tuning, (2) is more computationally efficient compared to the other
methods, especially when powered by a stronger model, (3) scales better with
stronger models, due to its LLM-First design, and (4) scales better with
increased compute budget. Our code is publicly available at
\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages, 2 figures, 2 tables, 36 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Common Pile v0.1: An 8TB <span class="highlight-title">Dataset</span> of Public Domain and Openly
  Licensed Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are typically trained on enormous quantities of
unlicensed text, a practice that has led to scrutiny due to possible
intellectual property infringement and ethical concerns. Training LLMs on
openly licensed text presents a first step towards addressing these issues, but
prior data collection efforts have yielded datasets too small or low-quality to
produce performant LLMs. To address this gap, we collect, curate, and release
the Common Pile v0.1, an eight terabyte collection of openly licensed text
designed for LLM pretraining. The Common Pile comprises content from 30 sources
that span diverse domains including research papers, code, books,
encyclopedias, educational materials, audio transcripts, and more. Crucially,
we validate our efforts by training two 7 billion parameter LLMs on text from
the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion
tokens respectively. Both models attain competitive performance to LLMs trained
on unlicensed text with similar computational budgets, such as Llama 1 and 2
7B. In addition to releasing the Common Pile v0.1 itself, we also release the
code used in its creation as well as the training mixture and checkpoints for
the Comma v0.1 models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RELIC: Evaluating Compositional Instruction Following via Language
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jackson Petty, Michael Y. Hu, Wentao Wang, Shauli Ravfogel, William Merrill, Tal Linzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly expected to perform tasks based
only on a specification of the task provided in context, without examples of
inputs and outputs; this ability is referred to as instruction following. We
introduce the Recognition of Languages In-Context (RELIC) framework to evaluate
instruction following using language recognition: the task of determining if a
string is generated by formal grammar. Unlike many standard evaluations of
LLMs' ability to use their context, this task requires composing together a
large number of instructions (grammar productions) retrieved from the context.
Because the languages are synthetic, the task can be increased in complexity as
LLMs' skills improve, and new instances can be automatically generated,
mitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and
find that their accuracy can be reliably predicted from the complexity of the
grammar and the individual example strings, and that even the most advanced
LLMs currently available show near-chance performance on more complex grammars
and samples, in line with theoretical expectations. We also use RELIC to
diagnose how LLMs attempt to solve increasingly difficult reasoning tasks,
finding that as the complexity of the language recognition task increases,
models switch to relying on shallow heuristics instead of following complex
instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual reasoning: an analysis of in-context emergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Miller, Bernhard Schölkopf, Siyuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale neural language models (LMs) exhibit remarkable performance in
in-context learning: the ability to learn and reason the input context on the
fly without parameter update. This work studies in-context counterfactual
reasoning in language models, that is, to predict the consequences of changes
under hypothetical scenarios. We focus on studying a well-defined synthetic
setup: a linear regression task that requires noise abduction, where accurate
prediction is based on inferring and copying the contextual noise from factual
observations. We show that language models are capable of counterfactual
reasoning in this controlled setup and provide insights that counterfactual
reasoning for a broad class of functions can be reduced to a transformation on
in-context observations; we find self-attention, model depth, and data
diversity in pre-training drive performance in Transformers. More
interestingly, our findings extend beyond regression tasks and show that
Transformers can perform noise abduction on sequential data, providing
preliminary evidence on the potential for counterfactual story generation. Our
code is available under
https://github.com/moXmiller/counterfactual-reasoning.git .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen3 Embedding: Advancing Text Embedding and Reranking Through
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce the Qwen3 Embedding series, a significant
advancement over its predecessor, the GTE-Qwen series, in text embedding and
reranking capabilities, built upon the Qwen3 foundation models. Leveraging the
Qwen3 LLMs' robust capabilities in multilingual text understanding and
generation, our innovative multi-stage training pipeline combines large-scale
unsupervised pre-training with supervised fine-tuning on high-quality datasets.
Effective model merging strategies further ensure the robustness and
adaptability of the Qwen3 Embedding series. During the training process, the
Qwen3 LLMs serve not only as backbone models but also play a crucial role in
synthesizing high-quality, rich, and diverse training data across multiple
domains and languages, thus enhancing the training pipeline. The Qwen3
Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both
embedding and reranking tasks, addressing diverse deployment scenarios where
users can optimize for either efficiency or effectiveness. Empirical
evaluations demonstrate that the Qwen3 Embedding series achieves
state-of-the-art results across diverse benchmarks. Notably, it excels on the
multilingual evaluation benchmark MTEB for text embedding, as well as in
various retrieval tasks, including code retrieval, cross-lingual retrieval and
multilingual retrieval. To facilitate reproducibility and promote
community-driven research and development, the Qwen3 Embedding models are
publicly available under the Apache 2.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoRAG: Evidentiality-guided Compression for Long Context RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonseok Jeong, Jinsu Kim, Dohyeon Lee, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable performance in Open-Domain
Question Answering (ODQA) by leveraging external documents through
Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer
context, context compression is necessary. However, prior compression methods
do not focus on filtering out non-evidential information, which limit the
performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or
\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing
retrieved documents based on evidentiality, ensuring whether answer generation
is supported by the correct evidence. As an additional step, ECoRAG reflects
whether the compressed content provides sufficient evidence, and if not,
retrieves more until sufficient. Experiments show that ECoRAG improves LLM
performance on ODQA tasks, outperforming existing compression methods.
Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency
but also minimizes token usage by retaining only the necessary information to
generate the correct answer. Code is available at
https://github.com/ldilab/ECoRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavik Chandna, Zubair Bashir, Procheta Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are known to exhibit social, demographic, and
gender biases, often as a consequence of the data on which they are trained. In
this work, we adopt a mechanistic interpretability approach to analyze how such
biases are structurally represented within models such as GPT-2 and Llama2.
Focusing on demographic and gender biases, we explore different metrics to
identify the internal edges responsible for biased behavior. We then assess the
stability, localization, and generalizability of these components across
dataset and linguistic variations. Through systematic ablations, we demonstrate
that bias-related computations are highly localized, often concentrated in a
small subset of layers. Moreover, the identified components change across
fine-tuning settings, including those unrelated to bias. Finally, we show that
removing these components not only reduces biased outputs but also affects
other NLP tasks, such as named entity recognition and linguistic acceptability
judgment because of the sharing of important components with these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledgeable-r1: Policy Optimization for Knowledge Exploration in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Lin, Yilin Wen, Du Su, Fei Sun, Muhan Chen, Chenfu Bao, Zhonghou Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a mainstream method for improving
performance on knowledge-intensive tasks. However,current RAG systems often
place too much emphasis on retrieved contexts. This can lead to reliance on
inaccurate sources and overlook the model's inherent knowledge, especially when
dealing with misleading or excessive information. To resolve this imbalance, we
propose Knowledgeable-r1 that using joint sampling and define multi policy
distributions in knowledge capability exploration to stimulate large language
models'self-integrated utilization of parametric and contextual knowledge.
Experiments show that Knowledgeable-r1 significantly enhances robustness and
reasoning accuracy in both parameters and contextual conflict tasks and general
RAG tasks, especially outperforming baselines by 17.07% in counterfactual
scenarios and demonstrating consistent gains across RAG tasks. Our code are
available at https://github.com/lcy80366872/ knowledgeable-r1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CIVET: Systematic Evaluation of Understanding in VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimo Rizzoli, Simone Alghisi, Olha Khomyn, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Vision-Language Models (VLMs) have achieved competitive performance in
various tasks, their comprehension of the underlying structure and semantics of
a scene remains understudied. To investigate the understanding of VLMs, we
study their capability regarding object properties and relations in a
controlled and interpretable manner. To this scope, we introduce CIVET, a novel
and extensible framework for systematiC evaluatIon Via controllEd sTimuli.
CIVET addresses the lack of standardized systematic evaluation for assessing
VLMs' understanding, enabling researchers to test hypotheses with statistical
rigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of
stimuli, free from annotation noise, dataset-specific biases, and uncontrolled
scene complexity. Our findings reveal that 1) current VLMs can accurately
recognize only a limited set of basic object properties; 2) their performance
heavily depends on the position of the object in the scene; 3) they struggle to
understand basic relations among objects. Furthermore, a comparative evaluation
with human annotators reveals that VLMs still fall short of achieving
human-level accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Judge Error Severity Like Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diege Sun, Guanyi Chen, Fan Zhao, Xiaorong Cheng, Tingting He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used as automated evaluators in
natural language generation, yet it remains unclear whether they can accurately
replicate human judgments of error severity. In this study, we systematically
compare human and LLM assessments of image descriptions containing controlled
semantic errors. We extend the experimental framework of van Miltenburg et al.
(2020) to both unimodal (text-only) and multimodal (text + image) settings,
evaluating four error types: age, gender, clothing type, and clothing colour.
Our findings reveal that humans assign varying levels of severity to different
error types, with visual context significantly amplifying perceived severity
for colour and type errors. Notably, most LLMs assign low scores to gender
errors but disproportionately high scores to colour errors, unlike humans, who
judge both as highly severe but for different reasons. This suggests that these
models may have internalised social norms influencing gender judgments but lack
the perceptual grounding to emulate human sensitivity to colour, which is
shaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,
replicates the human-like ranking of error severity, but it fails to
distinguish between error types as clearly as humans. Surprisingly,
DeepSeek-V3, a unimodal LLM, achieves the highest alignment with human
judgments across both unimodal and multimodal conditions, outperforming even
state-of-the-art multimodal models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AudioLens: A Closer Look at Auditory Attribute Perception of Large
  Audio-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Kai Yang, Neo Ho, Yi-Jyun Lee, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the internal mechanisms of large audio-language models (LALMs)
is crucial for interpreting their behavior and improving performance. This work
presents the first in-depth analysis of how LALMs internally perceive and
recognize auditory attributes. By applying vocabulary projection on three
state-of-the-art LALMs, we track how attribute information evolves across
layers and token positions. We find that attribute information generally
decreases with layer depth when recognition fails, and that resolving
attributes at earlier layers correlates with better accuracy. Moreover, LALMs
heavily rely on querying auditory inputs for predicting attributes instead of
aggregating necessary information in hidden states at attribute-mentioning
positions. Based on our findings, we demonstrate a method to enhance LALMs. Our
results offer insights into auditory attribute processing, paving the way for
future improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Locality as an Inductive Bias for Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiga Someya, Anej Svete, Brian DuSell, Timothy J. O'Donnell, Mario Giulianelli, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive biases are inherent in every machine learning system, shaping how
models generalize from finite data. In the case of neural language models
(LMs), debates persist as to whether these biases align with or diverge from
human processing constraints. To address this issue, we propose a quantitative
framework that allows for controlled investigations into the nature of these
biases. Within our framework, we introduce $m$-local entropy$\unicode{x2013}$an
information-theoretic measure derived from average lossy-context
surprisal$\unicode{x2013}$that captures the local uncertainty of a language by
quantifying how effectively the $m-1$ preceding symbols disambiguate the next
symbol. In experiments on both perturbed natural language corpora and languages
defined by probabilistic finite-state automata (PFSAs), we show that languages
with higher $m$-local entropy are more difficult for Transformer and LSTM LMs
to learn. These results suggest that neural LMs, much like humans, are highly
sensitive to the local statistical structure of a language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM
  Reasoning <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Parekh, Kartik Mehta, Ninareh Mehrabi, Kai-Wei Chang, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Event Detection (ED), the task of identifying event mentions in
natural language text without any training data, is critical for document
understanding in specialized domains. Understanding the complex event ontology,
extracting domain-specific triggers from the passage, and structuring them
appropriately overloads and limits the utility of Large Language Models (LLMs)
for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent
reasoning framework that decouples the task of ED using Dreamer and Grounder.
Dreamer encourages divergent reasoning through open-ended event discovery,
which helps to boost event coverage. Conversely, Grounder introduces convergent
reasoning to align the free-form predictions with the task-specific
instructions using finite-state machine guided constrained decoding.
Additionally, an LLM-Judge verifies the final outputs to ensure high precision.
Through extensive experiments on six datasets across five domains and nine
LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,
transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains
over the best baseline -- establishing DiCoRe as a strong zero-shot ED
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted at ACL ARR May 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NTNU System at the S&I Challenge 2025 SLA Open Track <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Yun Lin, Tien-Hong Lo, Yu-Hsuan Fang, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent line of research on spoken language assessment (SLA) employs neural
models such as BERT and wav2vec 2.0 (W2V) to evaluate speaking proficiency
across linguistic and acoustic modalities. Although both models effectively
capture features relevant to oral competence, each exhibits modality-specific
limitations. BERT-based methods rely on ASR transcripts, which often fail to
capture prosodic and phonetic cues for SLA. In contrast, W2V-based methods
excel at modeling acoustic features but lack semantic interpretability. To
overcome these limitations, we propose a system that integrates W2V with Phi-4
multimodal large language model (MLLM) through a score fusion strategy. The
proposed system achieves a root mean square error (RMSE) of 0.375 on the
official test set of the Speak & Improve Challenge 2025, securing second place
in the competition. For comparison, the RMSEs of the top-ranked, third-ranked,
and official baseline systems are 0.364, 0.384, and 0.444, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to the ISCA SLaTE-2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework
  for Misleading Text Detection on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Huang, Zikun Cui, Cuiqianhe Du, Chia-En Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misleading text detection on social media platforms is a critical research
area, as these texts can lead to public misunderstanding, social panic and even
economic losses. This paper proposes a novel framework - CL-ISR (Contrastive
Learning and Implicit Stance Reasoning), which combines contrastive learning
and implicit stance reasoning, to improve the detection accuracy of misleading
texts on social media. First, we use the contrastive learning algorithm to
improve the model's learning ability of semantic differences between truthful
and misleading texts. Contrastive learning could help the model to better
capture the distinguishing features between different categories by
constructing positive and negative sample pairs. This approach enables the
model to capture distinguishing features more effectively, particularly in
linguistically complicated situations. Second, we introduce the implicit stance
reasoning module, to explore the potential stance tendencies in the text and
their relationships with related topics. This method is effective for
identifying content that misleads through stance shifting or emotional
manipulation, because it can capture the implicit information behind the text.
Finally, we integrate these two algorithms together to form a new framework,
CL-ISR, which leverages the discriminative power of contrastive learning and
the interpretive depth of stance reasoning to significantly improve detection
effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Multimodal Framework for Human-Centered Street Assessment:
  Integrating Visual-Language Models for Perceptual Urban Diagnostics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HaoTian Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While objective street metrics derived from imagery or GIS have become
standard in urban analytics, they remain insufficient to capture subjective
perceptions essential to inclusive urban design. This study introduces a novel
Multimodal Street Evaluation Framework (MSEF) that fuses a vision transformer
(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable
dual-output assessment of streetscapes. Leveraging over 15,000 annotated
street-view images from Harbin, China, we fine-tune the framework using LoRA
and P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1
score of 0.84 on objective features and 89.3 percent agreement with aggregated
resident perceptions, validated across stratified socioeconomic geographies.
Beyond classification accuracy, MSEF captures context-dependent contradictions:
for instance, informal commerce boosts perceived vibrancy while simultaneously
reducing pedestrian comfort. It also identifies nonlinear and semantically
contingent patterns -- such as the divergent perceptual effects of
architectural transparency across residential and commercial zones -- revealing
the limits of universal spatial heuristics. By generating natural-language
rationales grounded in attention mechanisms, the framework bridges sensory data
with socio-affective inference, enabling transparent diagnostics aligned with
SDG 11. This work offers both methodological innovation in urban perception
modeling and practical utility for planning systems seeking to reconcile
infrastructural precision with lived experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parking, Perception, and Retail: Street-Level Determinants of Community
  Vitality in Harbin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HaoTian Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The commercial vitality of community-scale streets in Chinese cities is
shaped by complex interactions between vehicular accessibility, environmental
quality, and pedestrian perception. This study proposes an interpretable,
image-based framework to examine how street-level features -- including parked
vehicle density, greenery, cleanliness, and street width -- impact retail
performance and user satisfaction in Harbin, China. Leveraging street view
imagery and a multimodal large language model (VisualGLM-6B), we construct a
Community Commercial Vitality Index (CCVI) from Meituan and Dianping data and
analyze its relationship with spatial attributes extracted via GPT-4-based
perception modeling. Our findings reveal that while moderate vehicle presence
may enhance commercial access, excessive on-street parking -- especially in
narrow streets -- erodes walkability and reduces both satisfaction and
shop-level pricing. In contrast, streets with higher perceived greenery and
cleanliness show significantly greater satisfaction scores but only weak
associations with pricing. Street width moderates the effects of vehicle
presence, underscoring the importance of spatial configuration. These results
demonstrate the value of integrating AI-assisted perception with urban
morphological analysis to capture non-linear and context-sensitive drivers of
commercial success. This study advances both theoretical and methodological
frontiers by highlighting the conditional role of vehicle activity in
neighborhood commerce and demonstrating the feasibility of multimodal AI for
perceptual urban diagnostics. The implications extend to urban design, parking
management, and scalable planning tools for community revitalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection
  through Intent Differentiation and Emoji Interpretation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumitra Ghosh, Gopendra Vikram Singh,  Shambhavi, Sabarna Choudhury, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-harm detection on social media is critical for early intervention and
mental health support, yet remains challenging due to the subtle,
context-dependent nature of such expressions. Identifying self-harm intent aids
suicide prevention by enabling timely responses, but current large language
models (LLMs) struggle to interpret implicit cues in casual language and
emojis. This work enhances LLMs' comprehension of self-harm by distinguishing
intent through nuanced language-emoji interplay. We present the Centennial
Emoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with
contextual self-harm interpretations and the Self-Harm Identification aNd
intent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering
detailed annotations for self-harm labels, casual mentions (CMs), and serious
intents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)
fine-tunes LLMs for multi-task learning: self-harm detection (primary) and
CM/SI span detection (auxiliary); c) generates explainable rationales for
self-harm predictions. We evaluate the framework on three state-of-the-art
LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and
fine-tuned scenarios. By coupling intent differentiation with contextual cues,
our approach commendably enhances LLM performance in both detection and
explanation tasks, effectively addressing the inherent ambiguity in self-harm
signals. The SHINES dataset, CESM-100 and codebase are publicly available at:
https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 63rd Annual Meeting of the
  Association for Computational Linguistics (ACL 2025 Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RIVAL: Reinforcement Learning with Iterative and Adversarial
  Optimization for Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjiao Li, Mengran Yu, Chenyu Shi, Yanjun Zhao, Xiaojing Liu, Qiang Zhang, Qi Zhang, Xuanjing Huang, Jiayin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) possess strong multilingual capabilities, and
combining Reinforcement Learning from Human Feedback (RLHF) with translation
tasks has shown great potential. However, we observe that this paradigm
performs unexpectedly poorly when applied to colloquial subtitle translation
tasks. In this work, we investigate this issue and find that the offline reward
model (RM) gradually diverges from the online LLM due to distributional shift,
ultimately leading to undesirable training outcomes. To address this, we
propose RIVAL, an adversarial training framework that formulates the process as
a min-max game between the RM and the LLM. RIVAL iteratively updates the both
models, with the RM trained to distinguish strong from weak translations
(qualitative preference reward), and the LLM trained to enhance its translation
for closing this gap. To stabilize training and improve generalizability, we
also incorporate quantitative preference reward (e.g., BLEU) into the RM,
enabling reference-free quality modeling aligned with human evaluation. Through
extensive experiments, we demonstrate that the proposed adversarial training
framework significantly improves upon translation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does It Make Sense to Speak of Introspection in Large Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iulia Comşa, Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit compelling linguistic behaviour, and
sometimes offer self-reports, that is to say statements about their own nature,
inner workings, or behaviour. In humans, such reports are often attributed to a
faculty of introspection and are typically linked to consciousness. This raises
the question of how to interpret self-reports produced by LLMs, given their
increasing linguistic fluency and cognitive capabilities. To what extent (if
any) can the concept of introspection be meaningfully applied to LLMs? Here, we
present and critique two examples of apparent introspective self-report from
LLMs. In the first example, an LLM attempts to describe the process behind its
own ``creative'' writing, and we argue this is not a valid example of
introspection. In the second example, an LLM correctly infers the value of its
own temperature parameter, and we argue that this can be legitimately
considered a minimal example of introspection, albeit one that is (presumably)
not accompanied by conscious experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debatable Intelligence: Benchmarking LLM Judges via Debate Speech
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noy Sternlicht, Ariel Gera, Roy Bar-Haim, Tom Hope, Noam Slonim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Debate Speech Evaluation as a novel and challenging benchmark
for assessing LLM judges. Evaluating debate speeches requires a deep
understanding of the speech at multiple levels, including argument strength and
relevance, the coherence and organization of the speech, the appropriateness of
its style and tone, and so on. This task involves a unique set of cognitive
abilities that have previously received limited attention in systematic LLM
benchmarking. To explore such skills, we leverage a dataset of over 600
meticulously annotated debate speeches and present the first in-depth analysis
of how state-of-the-art LLMs compare to human judges on this task. Our findings
reveal a nuanced picture: while larger models can approximate individual human
judgments in some respects, they differ substantially in their overall judgment
behavior. We also investigate the ability of frontier LLMs to generate
persuasive, opinionated speeches, showing that models may perform at a human
level on this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/noy-sternlicht/Debatable-Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TALL -- A Trainable Architecture for Enhancing LLM Performance in
  Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Ofer, Orel Zamler, Amos Azaria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in high-resource languages but struggle
with low-resource languages due to limited training data. This paper presents
TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource
Languages), which integrates an LLM with two bilingual translation models. TALL
transforms low-resource inputs into high-resource representations, leveraging
the LLM's capabilities while preserving linguistic features through dimension
alignment layers and custom transformers. Our experiments on Hebrew demonstrate
significant improvements over several baselines, including direct use, naive
translation, and fine-tuning approaches. The architecture employs a
parameter-efficient strategy, freezing pre-trained components while training
only lightweight adapter modules, balancing computational efficiency with
performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Robustness Stress Testing of LLMs as Mathematical Problem
  Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Hou, Zeguan Xiao, Fei Yu, Yihan Jiang, Xuetao Wei, Hailiang Huang, Yun Chen, Guanhua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved distinguished performance on
various reasoning-intensive tasks. However, LLMs might still face the
challenges of robustness issues and fail unexpectedly in some simple reasoning
tasks. Previous works evaluate the LLM robustness with hand-crafted templates
or a limited set of perturbation rules, indicating potential data contamination
in pre-training or fine-tuning datasets. In this work, inspired by stress
testing in software engineering, we propose a novel framework, Automatic
Robustness Checker (AR-Checker), to generate mathematical problem variants that
maintain the semantic meanings of the original one but might fail the LLMs. The
AR-Checker framework generates mathematical problem variants through
multi-round parallel streams of LLM-based rewriting and verification. Our
framework can generate benchmark variants dynamically for each LLM, thus
minimizing the risk of data contamination. Experiments on GSM8K and MATH-500
demonstrate the strong performance of AR-Checker on mathematical tasks. We also
evaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,
and CommonsenseQA, where it also achieves strong performance, further proving
the effectiveness of AR-Checker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Summarization Length Through EOS Token Weighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeno Belligoli, Emmanouil Stergiadis, Eran Fainman, Ilya Gusev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling the length of generated text can be crucial in various
text-generation tasks, including summarization. Existing methods often require
complex model alterations, limiting compatibility with pre-trained models. We
address these limitations by developing a simple approach for controlling the
length of automatic text summaries by increasing the importance of correctly
predicting the EOS token in the cross-entropy loss computation. The proposed
methodology is agnostic to architecture and decoding algorithms and orthogonal
to other inference-time techniques to control the generation length. We tested
it with encoder-decoder and modern GPT-style LLMs, and show that this method
can control generation length, often without affecting the quality of the
summary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow
  Development <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenran Xu, Xue Yang, Yiyu Wang, Qingli Hu, Zijiao Wu, Longyue Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ComfyUI-Copilot, a large language model-powered plugin designed
to enhance the usability and efficiency of ComfyUI, an open-source platform for
AI-driven art creation. Despite its flexibility and user-friendly interface,
ComfyUI can present challenges to newcomers, including limited documentation,
model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot
addresses these challenges by offering intelligent node and model
recommendations, along with automated one-click workflow construction. At its
core, the system employs a hierarchical multi-agent framework comprising a
central assistant agent for task delegation and specialized worker agents for
different usages, supported by our curated ComfyUI knowledge bases to
streamline debugging and deployment. We validate the effectiveness of
ComfyUI-Copilot through both offline quantitative evaluations and online user
feedback, showing that it accurately recommends nodes and accelerates workflow
development. Additionally, use cases illustrate that ComfyUI-Copilot lowers
entry barriers for beginners and enhances workflow efficiency for experienced
users. The ComfyUI-Copilot installation package and a demo video are available
at https://github.com/AIDC-AI/ComfyUI-Copilot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Demo. Github: https://github.com/AIDC-AI/ComfyUI-Copilot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOP: Evaluating the Comprehension Process of Large Language Models from
  a Cognitive View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjie Xiao, Hongru Liang, Peixin Qin, Yao Zhang, Wenqiang Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the great potential of large language models(LLMs) in machine
comprehension, it is still disturbing to fully count on them in real-world
scenarios. This is probably because there is no rational explanation for
whether the comprehension process of LLMs is aligned with that of experts. In
this paper, we propose SCOP to carefully examine how LLMs perform during the
comprehension process from a cognitive view. Specifically, it is equipped with
a systematical definition of five requisite skills during the comprehension
process, a strict framework to construct testing data for these skills, and a
detailed analysis of advanced open-sourced and closed-sourced LLMs using the
testing data. With SCOP, we find that it is still challenging for LLMs to
perform an expert-level comprehension process. Even so, we notice that LLMs
share some similarities with experts, e.g., performing better at comprehending
local information than global information. Further analysis reveals that LLMs
can be somewhat unreliable -- they might reach correct answers through flawed
comprehension processes. Based on SCOP, we suggest that one direction for
improving LLMs is to focus more on the comprehension process, ensuring all
comprehension skills are thoroughly developed during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2004.14535 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Storage-Efficient Visual Document Retrieval: An Empirical Study
  on Reducing Patch-Level Embeddings <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Ma, Jinsong Li, Yuhang Zang, Xiaobao Wu, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Jiaqi Wang, Yixin Cao, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the strong performance of ColPali/ColQwen2 in Visualized Document
Retrieval (VDR), it encodes each page into multiple patch-level embeddings and
leads to excessive memory usage. This empirical study investigates methods to
reduce patch embeddings per page at minimum performance degradation. We
evaluate two token-reduction strategies: token pruning and token merging.
Regarding token pruning, we surprisingly observe that a simple random strategy
outperforms other sophisticated pruning methods, though still far from
satisfactory. Further analysis reveals that pruning is inherently unsuitable
for VDR as it requires removing certain page embeddings without query-specific
information. Turning to token merging (more suitable for VDR), we search for
the optimal combinations of merging strategy across three dimensions and
develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance
with only 11.8% of original memory usage, and preserves 94.6% effectiveness at
2.8% memory footprint. We expect our empirical findings and resulting
Light-ColPali/ColQwen2 offer valuable insights and establish a competitive
baseline for future research towards efficient VDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Semi-supervised Learning for Multi-domain ASR Through Incremental
  Retraining and Data Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andres Carofilis, Pradeep Rangappa, Srikanth Madikeri, Shashi Kumar, Sergio Burdisso, Jeena Prakash, Esau Villatoro-Tello, Petr Motlicek, Bidisha Sharma, Kadri Hacioglu, Shankar Venkatesan, Saurabh Vyas, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pretrained ASR models for specific domains is challenging when
labeled data is scarce. But unlabeled audio and labeled data from related
domains are often available. We propose an incremental semi-supervised learning
pipeline that first integrates a small in-domain labeled set and an auxiliary
dataset from a closely related domain, achieving a relative improvement of 4%
over no auxiliary data. Filtering based on multi-model consensus or named
entity recognition (NER) is then applied to select and iteratively refine
pseudo-labels, showing slower performance saturation compared to random
selection. Evaluated on the multi-domain Wow call center and Fisher English
corpora, it outperforms single-step fine-tuning. Consensus-based filtering
outperforms other methods, providing up to 22.3% relative improvement on Wow
and 24.8% on Fisher over single-step fine-tuning with random selection. NER is
the second-best filter, providing competitive performance at a lower
computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2025, Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced
  Algorithm Exams and Pave the Way for Editorial Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Marius Dumitran, Theodor-Pierre Moroianu, Vasile Paul Alexe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive evaluation of the performance of
state-of-the-art Large Language Models (LLMs) on challenging university-level
algorithms exams. By testing multiple models on both a Romanian exam and its
high-quality English translation, we analyze LLMs' problem-solving
capabilities, consistency, and multilingual performance. Our empirical study
reveals that the most recent models not only achieve scores comparable to
top-performing students but also demonstrate robust reasoning skills on
complex, multi-step algorithmic challenges, even though difficulties remain
with graph-based tasks. Building on these findings, we explore the potential of
LLMs to support educational environments through the generation of high-quality
editorial content, offering instructors a powerful tool to enhance student
feedback. The insights and best practices discussed herein pave the way for
further integration of generative AI in advanced algorithm education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages Pre-print Paper accepted to ITS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConECT <span class="highlight-title">Dataset</span>: Overcoming Data Scarcity in Context-Aware E-Commerce MT <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikołaj Pokrywka, Wojciech Kusa, Mieszko Rutkowski, Mikołaj Koszowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Machine Translation (NMT) has improved translation by using
Transformer-based models, but it still struggles with word ambiguity and
context. This problem is especially important in domain-specific applications,
which often have problems with unclear sentences or poor data quality. Our
research explores how adding information to models can improve translations in
the context of e-commerce data. To this end we create ConECT -- a new
Czech-to-Polish e-commerce product translation dataset coupled with images and
product metadata consisting of 11,400 sentence pairs. We then investigate and
compare different methods that are applicable to context-aware translation. We
test a vision-language model (VLM), finding that visual context aids
translation quality. Additionally, we explore the incorporation of contextual
information into text-to-text models, such as the product's category path or
image descriptions. The results of our study demonstrate that the incorporation
of contextual information leads to an improvement in the quality of machine
translation. We make the new dataset publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for
  Computational Linguistics)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junior Cedric Tonga, KV Aditya Srivatsa, Kaushal Kumar Maurya, Fajri Koto, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated the ability to generate
formative feedback and instructional hints in English, making them increasingly
relevant for AI-assisted education. However, their ability to provide effective
instructional support across different languages, especially for mathematically
grounded reasoning tasks, remains largely unexamined. In this work, we present
the first large-scale simulation of multilingual tutor-student interactions
using LLMs. A stronger model plays the role of the tutor, generating feedback
in the form of hints, while a weaker model simulates the student. We explore
352 experimental settings across 11 typologically diverse languages, four
state-of-the-art LLMs, and multiple prompting strategies to assess whether
language-specific feedback leads to measurable learning gains. Our study
examines how student input language, teacher feedback language, model choice,
and language resource level jointly influence performance. Results show that
multilingual hints can significantly improve learning outcomes, particularly in
low-resource languages when feedback is aligned with the student's native
language. These findings offer practical insights for developing multilingual,
LLM-based educational tools that are both effective and inclusive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, in submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Practitioner's Guide to Building ASR Models for Low-Resource
  Languages: A Case Study on Scottish Gaelic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondřej Klejch, William Lamb, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An effective approach to the development of ASR systems for low-resource
languages is to fine-tune an existing multilingual end-to-end model. When the
original model has been trained on large quantities of data from many
languages, fine-tuning can be effective with limited training data, even when
the language in question was not present in the original training data. The
fine-tuning approach has been encouraged by the availability of public-domain
E2E models and is widely believed to lead to state-of-the-art results. This
paper, however, challenges that belief. We show that an approach combining
hybrid HMMs with self-supervised models can yield substantially better
performance with limited training data. This combination allows better
utilisation of all available speech and text data through continued
self-supervised pre-training and semi-supervised training. We benchmark our
approach on Scottish Gaelic, achieving WER reductions of 32% relative over our
best fine-tuned Whisper model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Long Reasoning Models: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyu Mu, Jiali Zeng, Bei Li, Xinyan Guan, Fandong Meng, Jie Zhou, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in training long-context reasoning models via
reinforcement learning (RL), several open questions and counterintuitive
behaviors remain. This work focuses on three key aspects: (1) We systematically
analyze the roles of positive and negative samples in RL, revealing that
positive samples mainly facilitate data fitting, whereas negative samples
significantly enhance generalization and robustness. Interestingly, training
solely on negative samples can rival standard RL training performance. (2) We
identify substantial data inefficiency in group relative policy optimization,
where over half of the samples yield zero advantage. To address this, we
explore two straightforward strategies, including relative length rewards and
offline sample injection, to better leverage these data and enhance reasoning
efficiency and capability. (3) We investigate unstable performance across
various reasoning models and benchmarks, attributing instability to uncertain
problems with ambiguous outcomes, and demonstrate that multiple evaluation runs
mitigate this issue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Thinking LLMs Lie: Unveiling the Strategic Deception in
  Representations of Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Yihao Zhang, Meng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning
  Blind Spots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Pan, Mary-Anne Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), whilst great at extracting facts from text,
struggle with nested narrative reasoning. Existing long context and multi-hop
QA benchmarks inadequately test this, lacking realistic distractors or failing
to decouple context length from reasoning complexity, masking a fundamental LLM
limitation. We introduce Verbose ListOps, a novel benchmark that
programmatically transposes ListOps computations into lengthy, coherent
stories. This uniquely forces internal computation and state management of
nested reasoning problems by withholding intermediate results, and offers
fine-grained controls for both narrative size \emph{and} reasoning difficulty.
Whilst benchmarks like LongReason (2025) advance approaches for synthetically
expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints
a specific LLM vulnerability: difficulty in state management for nested
sub-reasoning amongst semantically-relevant, distracting narrative. Our
experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse
in performance on Verbose ListOps at modest (~10k token) narrative lengths,
despite effortlessly solving raw ListOps equations. Addressing this failure is
paramount for real-world text interpretation which requires identifying key
reasoning points, tracking conceptual intermediate results, and filtering
irrelevant information. Verbose ListOps, and its extensible generation
framework thus enables targeted reasoning enhancements beyond mere
context-window expansion; a critical step to automating the world's knowledge
work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive
  Programming Contests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the significant progress of large reasoning models in complex coding and
reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are
insufficient to evaluate the coding capabilities of large language models
(LLMs) in real competition environments. Moreover, current evaluation metrics
such as Pass@K fail to capture the reflective abilities of reasoning models. To
address these challenges, we propose \textbf{ICPC-Eval}, a top-level
competitive coding benchmark designed to probing the frontiers of LLM
reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent
ICPC contests held in various regions of the world, offering three key
contributions: 1) A challenging realistic ICPC competition scenario, featuring
a problem type and difficulty distribution consistent with actual contests. 2)
A robust test case generation method and a corresponding local evaluation
toolkit, enabling efficient and accurate local evaluation. 3) An effective
test-time scaling evaluation metric, Refine@K, which allows iterative repair of
solutions based on execution feedback. The results underscore the significant
challenge in evaluating complex reasoning abilities: top-tier reasoning models
like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their
in-context reasoning potential when compared to non-reasoning counterparts.
Furthermore, despite recent advancements in code generation, these models still
lag behind top-performing human teams. We release the benchmark at:
https://github.com/RUCAIBox/Slow_Thinking_with_LLMs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Effectiveness of Linguistic Knowledge in <span class="highlight-title">Pretrain</span>ed
  Language Models: A Case Study of Universal Dependencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal Dependencies (UD), while widely regarded as the most successful
linguistic framework for cross-lingual syntactic representation, remains
underexplored in terms of its effectiveness. This paper addresses this gap by
integrating UD into pretrained language models and assesses if UD can improve
their performance on a cross-lingual adversarial paraphrase identification
task. Experimental results show that incorporation of UD yields significant
improvements in accuracy and $F_1$ scores, with average gains of 3.85\% and
6.08\% respectively. These enhancements reduce the performance gap between
pretrained models and large language models in some language pairs, and even
outperform the latter in some others. Furthermore, the UD-based similarity
score between a given language and English is positively correlated to the
performance of models in that language. Both findings highlight the validity
and potential of UD in out-of-domain tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing LLMs: Length Control for Isometric Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dávid Javorský, Ondřej Bojar, François Yvon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we explore the effectiveness of isometric machine translation
across multiple language pairs (En$\to$De, En$\to$Fr, and En$\to$Es) under the
conditions of the IWSLT Isometric Shared Task 2022. Using eight open-source
large language models (LLMs) of varying sizes, we investigate how different
prompting strategies, varying numbers of few-shot examples, and demonstration
selection influence translation quality and length control. We discover that
the phrasing of instructions, when aligned with the properties of the provided
demonstrations, plays a crucial role in controlling the output length. Our
experiments show that LLMs tend to produce shorter translations only when
presented with extreme examples, while isometric demonstrations often lead to
the models disregarding length constraints. While few-shot prompting generally
enhances translation quality, further improvements are marginal across 5, 10,
and 20-shot settings. Finally, considering multiple outputs allows to notably
improve overall tradeoff between the length and quality, yielding
state-of-the-art performance for some language pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IWSLT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple-Choice Question Generation Using Large Language Models:
  Methodology and Educator Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Biancini, Alessio Ferrato, Carla Limongelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating Artificial Intelligence (AI) in educational settings has brought
new learning approaches, transforming the practices of both students and
educators. Among the various technologies driving this transformation, Large
Language Models (LLMs) have emerged as powerful tools for creating educational
materials and question answering, but there are still space for new
applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess
student knowledge, but manually generating these questions is
resource-intensive and requires significant time and cognitive effort. In our
opinion, LLMs offer a promising solution to these challenges. This paper
presents a novel comparative analysis of three widely known LLMs - Llama 2,
Mistral, and GPT-3.5 - to explore their potential for creating informative and
challenging MCQs. In our approach, we do not rely on the knowledge of the LLM,
but we inject the knowledge into the prompt to contrast the hallucinations,
giving the educators control over the test's source text, too. Our experiment
involving 21 educators shows that GPT-3.5 generates the most effective MCQs
across several known metrics. Additionally, it shows that there is still some
reluctance to adopt AI in the educational field. This study sheds light on the
potential of LLMs to generate MCQs and improve the educational experience,
providing valuable insights for the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright ACM 2024. This is the author's version of the work. It is
  posted here for your personal use. Not for redistribution. The definitive
  Version of Record was published in Adjunct Proceedings of the 32nd ACM
  Conference on User Modeling, Adaptation and Personalization (UMAP Adjunct
  '24), http://dx.doi.org/10.1145/3631700.3665233</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MockConf: A Student Interpretation <span class="highlight-title">Dataset</span>: Analysis, Word- and
  Span-level Alignment and Baselines <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dávid Javorský, Ondřej Bojar, François Yvon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In simultaneous interpreting, an interpreter renders a source speech into
another language with a very short lag, much sooner than sentences are
finished. In order to understand and later reproduce this dynamic and complex
task automatically, we need dedicated datasets and tools for analysis,
monitoring, and evaluation, such as parallel speech corpora, and tools for
their automatic annotation. Existing parallel corpora of translated texts and
associated alignment algorithms hardly fill this gap, as they fail to model
long-range interactions between speech segments or specific types of
divergences (e.g., shortening, simplification, functional generalization)
between the original and interpreted speeches. In this work, we introduce
MockConf, a student interpreting dataset that was collected from Mock
Conferences run as part of the students' curriculum. This dataset contains 7
hours of recordings in 5 European languages, transcribed and aligned at the
level of spans and words. We further implement and release InterAlign, a modern
web-based annotation tool for parallel word and span annotations on long
inputs, suitable for aligning simultaneous interpreting. We propose metrics for
the evaluation and a baseline for automatic alignment. Dataset and tools are
released to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Evaluation of Answer and Reasoning Consistency for Hallucination
  Detection in Large Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) extend large language models with explicit,
multi-step reasoning traces to enhance transparency and performance on complex
tasks. However, these reasoning traces can be redundant or logically
inconsistent, making them a new source of hallucination that is difficult to
detect. Existing hallucination detection methods focus primarily on
answer-level uncertainty and often fail to detect hallucinations or logical
inconsistencies arising from the model's reasoning trace. This oversight is
particularly problematic for LRMs, where the explicit thinking trace is not
only an important support to the model's decision-making process but also a key
source of potential hallucination. To this end, we propose RACE (Reasoning and
Answer Consistency Evaluation), a novel framework specifically tailored for
hallucination detection in LRMs. RACE operates by extracting essential
reasoning steps and computing four diagnostic signals: inter-sample consistency
of reasoning traces, entropy-based answer uncertainty, semantic alignment
between reasoning and answers, and internal coherence of reasoning. This joint
analysis enables fine-grained hallucination detection even when the final
answer appears correct. Experiments across datasets and different LLMs
demonstrate that RACE outperforms existing hallucination detection baselines,
offering a robust and generalizable solution for evaluating LRMs. Our code is
available at: https://github.com/bebr2/RACE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health
  Trajectories with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Ege Özsoy, David Bani-Harouni, Matthias Keicher, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare systems face significant challenges in managing and interpreting
vast, heterogeneous patient data for personalized care. Existing approaches
often focus on narrow use cases with a limited feature space, overlooking the
complex, longitudinal interactions needed for a holistic understanding of
patient health. In this work, we propose a novel approach to patient pathway
modeling by transforming diverse electronic health record (EHR) data into a
structured representation and designing a holistic pathway prediction model,
EHR2Path, optimized to predict future health trajectories. Further, we
introduce a novel summary mechanism that embeds long-term temporal context into
topic-specific summary tokens, improving performance over text-only models,
while being much more token-efficient. EHR2Path demonstrates strong performance
in both next time-step prediction and longitudinal simulation, outperforming
competitive baselines. It enables detailed simulations of patient trajectories,
inherently targeting diverse evaluation tasks, such as forecasting vital signs,
lab test results, or length-of-stay, opening a path towards predictive and
personalized healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reasoning-Based Approach to Cryptic Crossword Clue Solving <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Andrews, Sam Witteveen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptic crossword clues are challenging language tasks for which new test
sets are released daily by major newspapers on a global basis. Each cryptic
clue contains both the definition of the answer to be placed in the crossword
grid (in common with regular crosswords), and 'wordplay' that proves that the
answer is correct (i.e. a human solver can be confident that an answer is
correct without needing crossing words as confirmation). This work describes an
LLM-based reasoning system built from open-licensed components that solves
cryptic clues by (i) hypothesising answers; (ii) proposing wordplay
explanations; and (iii) using a verifier system that operates on codified
reasoning steps. Overall, this system establishes a new state-of-the-art
performance on the challenging Cryptonite dataset of clues from The Times and
The Telegraph newspapers in the UK. Because each proved solution is expressed
in Python, interpretable wordplay reasoning for proven answers is available for
inspection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 page paper plus Appendices. Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Vision-Language and Large Language Models for Automated
  Student Assessment in Indonesian Classrooms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurul Aisyah, Muhammad Dehan Al Kautsar, Arif Hidayat, Raqib Chowdhury, Fajri Koto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although vision-language and large language models (VLM and LLM) offer
promising opportunities for AI-driven educational assessment, their
effectiveness in real-world classroom settings, particularly in
underrepresented educational contexts, remains underexplored. In this study, we
evaluated the performance of a state-of-the-art VLM and several LLMs on 646
handwritten exam responses from grade 4 students in six Indonesian schools,
covering two subjects: Mathematics and English. These sheets contain more than
14K student answers that span multiple choice, short answer, and essay
questions. Assessment tasks include grading these responses and generating
personalized feedback. Our findings show that the VLM often struggles to
accurately recognize student handwriting, leading to error propagation in
downstream LLM grading. Nevertheless, LLM-generated feedback retains some
utility, even when derived from imperfect input, although limitations in
personalization and contextual relevance persist.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of intelligent proofreading system for English translation based
  on CNN and <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feijun Liu, Huifeng Wang, Kun Wang, Yizhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since automatic translations can contain errors that require substantial
human post-editing, machine translation proofreading is essential for improving
quality. This paper proposes a novel hybrid approach for robust proofreading
that combines convolutional neural networks (CNN) with Bidirectional Encoder
Representations from Transformers (BERT). In order to extract semantic
information from phrases and expressions, CNN uses a variety of convolution
kernel filters to capture local n-gram patterns. In the meanwhile, BERT creates
context-rich representations of whole sequences by utilizing stacked
bidirectional transformer encoders. Using BERT's attention processes, the
integrated error detection component relates tokens to spot translation
irregularities including word order problems and omissions. The correction
module then uses parallel English-German alignment and GRU decoder models in
conjunction with translation memory to propose logical modifications that
maintain original meaning. A unified end-to-end training process optimized for
post-editing performance is applied to the whole pipeline. The multi-domain
collection of WMT and the conversational dialogues of Open-Subtitles are two of
the English-German parallel corpora used to train the model. Multiple loss
functions supervise detection and correction capabilities. Experiments attain a
90% accuracy, 89.37% F1, and 16.24% MSE, exceeding recent proofreading
techniques by over 10% overall. Comparative benchmarking demonstrates
state-of-the-art performance in identifying and coherently rectifying
mistranslations and omissions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and
  Supervision Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Zhou, Jiayi Ye, Zipeng Ling, Yufei Han, Yue Huang, Haomin Zhuang, Zhenwen Liang, Kehan Guo, Taicheng Guo, Xiangqi Wang, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning is a core capability for many applications of large
language models (LLMs), yet existing benchmarks often rely solely on
final-answer accuracy, failing to capture the quality and structure of the
reasoning process. We propose FineLogic, a fine-grained evaluation framework
that assesses logical reasoning across three dimensions: overall benchmark
accuracy, stepwise soundness, and representation-level alignment. In addition,
to better understand how reasoning capabilities emerge, we conduct a
comprehensive study on the effects of supervision format during fine-tuning. We
construct four supervision styles (one natural language and three symbolic
variants) and train LLMs under each. Our findings reveal that natural language
supervision yields strong generalization even on out-of-distribution and
long-context tasks, while symbolic reasoning styles promote more structurally
sound and atomic inference chains. Further, our representation-level probing
shows that fine-tuning primarily improves reasoning behaviors through
step-by-step generation, rather than enhancing shortcut prediction or
internalized correctness. Together, our framework and analysis provide a more
rigorous and interpretable lens for evaluating and improving logical reasoning
in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards LLM-Centric Multimodal Fusion: A <span class="highlight-title">Survey</span> on Integration
  Strategies and Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jisu An, Junseok Lee, Jeoungeun Lee, Yongseok Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progress of Multimodal Large Language Models(MLLMs) has transformed
the AI landscape. These models combine pre-trained LLMs with various modality
encoders. This integration requires a systematic understanding of how different
modalities connect to the language backbone. Our survey presents an LLM-centric
analysis of current approaches. We examine methods for transforming and
aligning diverse modal inputs into the language embedding space. This addresses
a significant gap in existing literature. We propose a classification framework
for MLLMs based on three key dimensions. First, we examine architectural
strategies for modality integration. This includes both the specific
integration mechanisms and the fusion level. Second, we categorize
representation learning techniques as either joint or coordinate
representations. Third, we analyze training paradigms, including training
strategies and objective functions. By examining 125 MLLMs developed between
2021 and 2025, we identify emerging patterns in the field. Our taxonomy
provides researchers with a structured overview of current integration
techniques. These insights aim to guide the development of more robust
multimodal integration strategies for future models built on pre-trained
foundations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech inherently contains rich acoustic information that extends far beyond
the textual language. In real-world spoken language understanding, effective
interpretation often requires integrating semantic meaning (e.g., content),
paralinguistic features (e.g., emotions, speed, pitch) and phonological
characteristics (e.g., prosody, intonation, rhythm), which are embedded in
speech. While recent multimodal Speech Large Language Models (SpeechLLMs) have
demonstrated remarkable capabilities in processing audio information, their
ability to perform fine-grained perception and complex reasoning in natural
speech remains largely unexplored. To address this gap, we introduce MMSU, a
comprehensive benchmark designed specifically for understanding and reasoning
in spoken language. MMSU comprises 5,000 meticulously curated
audio-question-answer triplets across 47 distinct tasks. To ground our
benchmark in linguistic theory, we systematically incorporate a wide range of
linguistic phenomena, including phonetics, prosody, rhetoric, syntactics,
semantics, and paralinguistics. Through a rigorous evaluation of 14 advanced
SpeechLLMs, we identify substantial room for improvement in existing models,
highlighting meaningful directions for future optimization. MMSU establishes a
new standard for comprehensive assessment of spoken language understanding,
providing valuable insights for developing more sophisticated human-AI speech
interaction systems. MMSU benchmark is available at
https://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available
at https://github.com/dingdongwang/MMSU_Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MMSU benchmark is available at
  https://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available
  at https://github.com/dingdongwang/MMSU_Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Interpretation of Political Opinions in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Hu, Mengyue Yang, Mengnan Du, Weiru Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies of LLMs' political opinions mainly rely on evaluations of their
open-ended responses. Recent work indicates that there is a misalignment
between LLMs' responses and their internal intentions. This motivates us to
probe LLMs' internal mechanisms and help uncover their internal political
states. Additionally, we found that the analysis of LLMs' political opinions
often relies on single-axis concepts, which can lead to concept confounds. In
this work, we extend the single-axis to multi-dimensions and apply
interpretable representation engineering techniques for more transparent LLM
political concept learning. Specifically, we designed a four-dimensional
political learning framework and constructed a corresponding dataset for
fine-grained political concept vector learning. These vectors can be used to
detect and intervene in LLM internals. Experiments are conducted on eight
open-source LLMs with three representation engineering techniques. Results show
these vectors can disentangle political concept confounds. Detection tasks
validate the semantic meaning of the vectors and show good generalization and
robustness in OOD settings. Intervention Experiments show these vectors can
intervene in LLMs to generate responses with different political leanings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Reliable Evaluation Metrics for Scientific Text Revision <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léane Jourdan, Florian Boudin, Richard Dufour, Nicolas Hernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating text revision in scientific writing remains a challenge, as
traditional metrics such as ROUGE and BERTScore primarily focus on similarity
rather than capturing meaningful improvements. In this work, we analyse and
identify the limitations of these metrics and explore alternative evaluation
methods that better align with human judgments. We first conduct a manual
annotation study to assess the quality of different revisions. Then, we
investigate reference-free evaluation metrics from related NLP domains.
Additionally, we examine LLM-as-a-judge approaches, analysing their ability to
assess revisions with and without a gold reference. Our results show that LLMs
effectively assess instruction-following but struggle with correctness, while
domain-specific metrics provide complementary insights. We find that a hybrid
approach combining LLM-as-a-judge evaluation and task-specific metrics offers
the most reliable assessment of revision quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner
  for Query Expansion in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyuan Liu, Mengxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using
  Large Language Model-based Query Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyuan Liu, Mengxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong Evolution: Collaborative Learning between Large and Small
  Language Models for Continuous Emergent Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Zhou, Xiaoming Zhang, Litian Zhang, Yibo Zhang, Zhenyu Guan, Chaozhuo Li, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread dissemination of fake news on social media has significantly
impacted society, resulting in serious consequences. Conventional deep learning
methodologies employing small language models (SLMs) suffer from extensive
supervised training requirements and difficulties adapting to evolving news
environments due to data scarcity and distribution shifts. Large language
models (LLMs), despite robust zero-shot capabilities, fall short in accurately
detecting fake news owing to outdated knowledge and the absence of suitable
demonstrations. In this paper, we propose a novel Continuous Collaborative
Emergent Fake News Detection (C$^2$EFND) framework to address these challenges.
The C$^2$EFND framework strategically leverages both LLMs' generalization power
and SLMs' classification expertise via a multi-round collaborative learning
framework. We further introduce a lifelong knowledge editing module based on a
Mixture-of-Experts architecture to incrementally update LLMs and a replay-based
continue learning method to ensure SLMs retain prior knowledge without
retraining entirely. Extensive experiments on Pheme and Twitter16 datasets
demonstrate that C$^2$EFND significantly outperforms existed methods,
effectively improving detection accuracy and adaptability in continuous
emergent fake news scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning
  Capabilities Through Evaluation Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Sun, Weihong Lin, Jinzhu Wu, Yongfu Zhu, Xiaoqi Jian, Guangxiang Zhao, Change Jia, Linglin Zhang, Sai-er Hu, Yuhan Wu, Xiangzheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through
  Combat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs
through competition and combat. To complement a single model's lack of
diversity in generation and biases in evaluation, multiple LLMs form a "sparta
tribe" to compete against each other in fulfilling instructions while serving
as judges for the competition of others. For each iteration, one instruction
and two models are selected for a duel, the other models evaluate the two
responses, and their evaluation scores are aggregated through a adapted
elo-ranking based reputation system, where winners/losers of combat gain/lose
weight in evaluating others. The peer-evaluated combat results then become
preference pairs where the winning response is preferred over the losing one,
and all models learn from these preferences at the end of each iteration.
SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative
and collective competition process. Extensive experiments demonstrate that
SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines
across 10 out of 12 tasks and datasets with 7.0% average improvement. Further
analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen
tasks and leverages the expertise diversity of participating models to produce
more logical, direct and informative outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech
  translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavana Akkiraju, Aishwarya Pothula, Santosh Kesiraju, Anil Kumar Vuppala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the submission of IIITH-BUT to the IWSLT 2025 shared task
on speech translation for the low-resource Bhojpuri-Hindi language pair. We
explored the impact of hyperparameter optimisation and data augmentation
techniques on the performance of the SeamlessM4T model fine-tuned for this
specific task. We systematically investigated a range of hyperparameters
including learning rate schedules, number of update steps, warm-up steps, label
smoothing, and batch sizes; and report their effect on translation quality. To
address data scarcity, we applied speed perturbation and SpecAugment and
studied their effect on translation quality. We also examined the use of
cross-lingual signal through joint training with Marathi and Bhojpuri speech
data. Our experiments reveal that careful selection of hyperparameters and the
application of simple yet effective augmentation techniques significantly
improve performance in low-resource settings. We also analysed the translation
hypotheses to understand various kinds of errors that impacted the translation
quality in terms of BLEU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper is accepted to IWSLT2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based phoneme-to-grapheme for phoneme-based speech recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Te Ma, Min Bi, Saierdaer Yusuyin, Hao Huang, Zhijian Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In automatic speech recognition (ASR), phoneme-based multilingual
pre-training and crosslingual fine-tuning is attractive for its high data
efficiency and competitive results compared to subword-based models. However,
Weighted Finite State Transducer (WFST) based decoding is limited by its
complex pipeline and inability to leverage large language models (LLMs).
Therefore, we propose LLM-based phoneme-to-grapheme (LLM-P2G) decoding for
phoneme-based ASR, consisting of speech-to-phoneme (S2P) and
phoneme-to-grapheme (P2G). A challenge is that there seems to have information
loss in cascading S2P and P2G. To address this challenge, we propose two
training strategies: data augmentation with noisy phonemes (DANP), and
randomized top-$K$ marginalized (TKM) training and decoding. Our experimental
results show that LLM-P2G outperforms WFST-based systems in crosslingual ASR
for Polish and German, by relative WER reductions of 3.6% and 6.9%
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Test-Time Scaling with Model-Free Speculative Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woomin Song, Saket Dingliwal, Sai Muralidhar Jayanthi, Bhavana Ganesh, Jinwoo Shin, Aram Galstyan, Sravan Babu Bodapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated remarkable capabilities in reasoning tasks
through test-time scaling techniques like best-of-N sampling and tree search.
However, these approaches often demand substantial computational resources,
creating a critical trade-off between performance and efficiency. We introduce
STAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative
decoding approach that leverages the inherent redundancy in reasoning
trajectories to achieve significant acceleration without compromising accuracy.
Our analysis reveals that reasoning paths frequently reuse similar reasoning
patterns, enabling efficient model-free token prediction without requiring
separate draft models. By introducing stochastic drafting and preserving
probabilistic information through a memory-efficient logit-based N-gram module,
combined with optimized Gumbel-Top-K sampling and data-driven tree
construction, STAND significantly improves token acceptance rates. Extensive
evaluations across multiple models and reasoning tasks (AIME-2024,
GPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference
latency by 60-65% compared to standard autoregressive decoding while
maintaining accuracy. Furthermore, STAND outperforms state-of-the-art
speculative decoding methods by 14-28% in throughput and shows strong
performance even in single-trajectory scenarios, reducing inference latency by
48-58%. As a model-free approach, STAND can be applied to any existing language
model without additional training, being a powerful plug-and-play solution for
accelerating language model reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cracking the Code: Enhancing Implicit Hate Speech Detection through
  Coding Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Wei, Liangzhi Li, Tong Xiang, Xiao Liu, Noa Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The internet has become a hotspot for hate speech (HS), threatening societal
harmony and individual well-being. While automatic detection methods perform
well in identifying explicit hate speech (ex-HS), they struggle with more
subtle forms, such as implicit hate speech (im-HS). We tackle this problem by
introducing a new taxonomy for im-HS detection, defining six encoding
strategies named codetypes. We present two methods for integrating codetypes
into im-HS detection: 1) prompting large language models (LLMs) directly to
classify sentences based on generated responses, and 2) using LLMs as encoders
with codetypes embedded during the encoding process. Experiments show that the
use of codetypes improves im-HS detection in both Chinese and English datasets,
validating the effectiveness of our approach across different languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025),
  112-126</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recycling the Web: A Method to Enhance <span class="highlight-title">Pre-train</span>ing Data Quality and
  Quantity for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal
  Large Language Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gio Paik, Geewook Kim, Jinbae Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MMRefine, a MultiModal Refinement benchmark designed to
evaluate the error refinement capabilities of Multimodal Large Language Models
(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,
MMRefine provides a framework that evaluates MLLMs' abilities to detect and
correct errors across six distinct scenarios beyond just comparing final
accuracy before and after refinement. Furthermore, the benchmark analyzes the
refinement performance by categorizing errors into six error types. Experiments
with various open and closed MLLMs reveal bottlenecks and factors impeding
refinement performance, highlighting areas for improvement in effective
reasoning enhancement. Our code and dataset are publicly available at
https://github.com/naver-ai/MMRefine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Urania: Differentially Private Insights into AI Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daogao Liu, Edith Cohen, Badih Ghazi, Peter Kairouz, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Adam Sealfon, Da Yu, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normative Conflicts and Shallow AI Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphaël Millière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress of AI systems such as large language models (LLMs) raises
increasingly pressing concerns about their safe deployment. This paper examines
the value alignment problem for LLMs, arguing that current alignment strategies
are fundamentally inadequate to prevent misuse. Despite ongoing efforts to
instill norms such as helpfulness, honesty, and harmlessness in LLMs through
fine-tuning based on human preferences, they remain vulnerable to adversarial
attacks that exploit conflicts between these norms. I argue that this
vulnerability reflects a fundamental limitation of existing alignment methods:
they reinforce shallow behavioral dispositions rather than endowing LLMs with a
genuine capacity for normative deliberation. Drawing from on research in moral
psychology, I show how humans' ability to engage in deliberative reasoning
enhances their resilience against similar adversarial tactics. LLMs, by
contrast, lack a robust capacity to detect and rationally resolve normative
conflicts, leaving them susceptible to manipulation; even recent advances in
reasoning-focused LLMs have not addressed this vulnerability. This ``shallow
alignment'' problem carries significant implications for AI safety and
regulation, suggesting that current approaches are insufficient for mitigating
potential harms posed by increasingly capable AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Philosophical Studies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label
  Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Cheng Lin, Huang-Cheng Chou, Yu-Hsuan Li Liang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) systems often exhibit gender bias. However,
the effectiveness and robustness of existing debiasing methods in such
multi-label scenarios remain underexplored. To address this gap, we present
EMO-Debias, a large-scale comparison of 13 debiasing methods applied to
multi-label SER. Our study encompasses techniques from pre-processing,
regularization, adversarial learning, biased learners, and distributionally
robust optimization. Experiments conducted on acted and naturalistic emotion
datasets, using WavLM and XLSR representations, evaluate each method under
conditions of gender imbalance. Our analysis quantifies the trade-offs between
fairness and accuracy, identifying which approaches consistently reduce gender
performance gaps without compromising overall model performance. The findings
provide actionable insights for selecting effective debiasing strategies and
highlight the impact of dataset distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flex-TravelPlanner: A Benchmark for Flexible Planning with Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhyun Oh, Eunsu Kim, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world planning problems require constant adaptation to changing
requirements and balancing of competing constraints. However, current
benchmarks for evaluating LLMs' planning capabilities primarily focus on
static, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark
that evaluates language models' ability to reason flexibly in dynamic planning
scenarios. Building on the TravelPlanner dataset~\citep{xie2024travelplanner},
we introduce two novel evaluation settings: (1) sequential constraint
introduction across multiple turns, and (2) scenarios with explicitly
prioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B
reveals several key findings: models' performance on single-turn tasks poorly
predicts their ability to adapt plans across multiple turns; constraint
introduction order significantly affects performance; and models struggle with
constraint prioritization, often incorrectly favoring newly introduced lower
priority preferences over existing higher-priority constraints. These findings
highlight the importance of evaluating LLMs in more realistic, dynamic planning
scenarios and suggest specific directions for improving model performance on
complex planning tasks. The code and dataset for our framework are publicly
available at https://github.com/juhyunohh/FlexTravelBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TaDA: Training-free recipe for Decoding with Adaptive KV Cache
  Compression and Mean-centering <span class="chip">ACL-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinay Joshi, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key-value (KV) cache in transformer models is a critical component for
efficient decoding or inference, yet its memory demands scale poorly with
sequence length, posing a major challenge for scalable deployment of large
language models. Among several approaches to KV cache compression, quantization
of key and value activations has been widely explored. Most KV cache
quantization methods still need to manage sparse and noncontiguous outliers
separately. To address this, we introduce TaDA, a training-free recipe for KV
cache compression with quantization precision that adapts to error sensitivity
across layers and a mean centering to eliminate separate outlier handling. Our
approach yields substantial accuracy improvements for multiple models
supporting various context lengths. Moreover, our approach does not need to
separately manage outlier elements -- a persistent hurdle in most traditional
quantization methods. Experiments on standard benchmarks demonstrate that our
technique reduces KV cache memory footprint to 27% of the original 16-bit
baseline while achieving comparable accuracy. Our method paves the way for
scalable and high-performance reasoning in language models by potentially
enabling inference for longer context length models, reasoning models, and
longer chain of thoughts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL-2025 industry-track accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViCocktail: Automated Multi-Modal Data Collection for Vietnamese
  Audio-Visual Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai-Binh Nguyen, Thi Van Nguyen, Quoc Truong Do, Chi Mai Luong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Speech Recognition (AVSR) has gained significant attention
recently due to its robustness against noise, which often challenges
conventional speech recognition systems that rely solely on audio features.
Despite this advantage, AVSR models remain limited by the scarcity of extensive
datasets, especially for most languages beyond English. Automated data
collection offers a promising solution. This work presents a practical approach
to generate AVSR datasets from raw video, refining existing techniques for
improved efficiency and accessibility. We demonstrate its broad applicability
by developing a baseline AVSR model for Vietnamese. Experiments show the
automatically collected dataset enables a strong baseline, achieving
competitive performance with robust ASR in clean conditions and significantly
outperforming them in noisy environments like cocktail parties. This efficient
method provides a pathway to expand AVSR to more languages, particularly
under-resourced ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AceReason-Nemotron: Advancing Math and Code Reasoning through
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16400v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16400v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add pass@1024 evaluation results for LiveCodeBench v6. We release the
  models at:
  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The broader spectrum of in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03782v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03782v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of language models to learn a task from a few examples in context
has generated substantial interest. Here, we provide a perspective that
situates this type of supervised few-shot learning within a much broader
spectrum of meta-learned in-context learning. Indeed, we suggest that any
distribution of sequences in which context non-trivially decreases loss on
subsequent predictions can be interpreted as eliciting a kind of in-context
learning. We suggest that this perspective helps to unify the broad set of
in-context abilities that language models exhibit -- such as adapting to tasks
from instructions or role play, or extrapolating time series. This perspective
also sheds light on potential roots of in-context learning in lower-level
processing of linguistic dependencies (e.g. coreference or parallel
structures). Finally, taking this perspective highlights the importance of
generalization, which we suggest can be studied along several dimensions: not
only the ability to learn something novel, but also flexibility in learning
from different presentations, and in applying what is learned. We discuss
broader connections to past literature in meta-learning and goal-conditioned
agents, and other perspectives on learning and adaptation. We close by
suggesting that research on in-context learning should consider this broader
spectrum of in-context capabilities and types of generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is LLM the Silver Bullet to Low-Resource Languages Machine Translation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.24102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.24102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Resource Languages (LRLs) present significant challenges in natural
language processing due to their limited linguistic resources and
underrepresentation in standard datasets. While recent advances in Large
Language Models (LLMs) and Neural Machine Translation have substantially
improved translation capabilities for high-resource languages, performance
disparities persist for LRLs, particularly impacting privacy-sensitive and
resource-constrained scenarios. This paper systematically evaluates current
LLMs in 200 languages using the FLORES-200 benchmark and demonstrates their
limitations in LRL translation capability. We also explore alternative data
sources, including news articles and bilingual dictionaries, and demonstrate
how knowledge distillation from large pre-trained teacher models can
significantly improve the performance of small LLMs on LRL translation tasks.
For example, this approach increases EN->LB with the LLM-as-a-Judge score on
the validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine
different fine-tuning configurations, providing practical insights on optimal
data scale, training efficiency, and the preservation of generalization
capabilities of models under study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReasonGen-R1: CoT for Autoregressive Image generation models through SFT
  and RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although chain-of-thought reasoning and reinforcement learning (RL) have
driven breakthroughs in NLP, their integration into generative vision models
remains underexplored. We introduce ReasonGen-R1, a two-stage framework that
first imbues an autoregressive image generator with explicit text-based
"thinking" skills via supervised fine-tuning on a newly generated reasoning
dataset of written rationales, and then refines its outputs using Group
Relative Policy Optimization. To enable the model to reason through text before
generating images, We automatically generate and release a corpus of model
crafted rationales paired with visual prompts, enabling controlled planning of
object layouts, styles, and scene compositions. Our GRPO algorithm uses reward
signals from a pretrained vision language model to assess overall visual
quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and
the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong
baselines and prior state-of-the-art models. More: aka.ms/reasongen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice
  Question Answering <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian-George Crăciun, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Mihaela-Claudia Cercel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Language Models (PLMs) have shown remarkable performances in
recent years, setting a new paradigm for NLP research and industry. The legal
domain has received some attention from the NLP community partly due to its
textual nature. Some tasks from this domain are represented by
question-answering (QA) tasks. This work explores the legal domain
Multiple-Choice QA (MCQA) for a low-resource language. The contribution of this
work is multi-fold. We first introduce JuRO, the first openly available
Romanian legal MCQA dataset, comprising three different examinations and a
number of 10,836 total questions. Along with this dataset, we introduce CROL,
an organized corpus of laws that has a total of 93 distinct documents with
their modifications from 763 time spans, that we leveraged in this work for
Information Retrieval (IR) techniques. Moreover, we are the first to propose
Law-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is
derived from the aforementioned corpus. Lastly, we propose a novel approach for
MCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive
results with generally accepted SOTA methods and even exceeds them in most
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Benign import Toxic: Jailbreaking the Language Model via
  Adversarial Metaphors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current studies have exposed the risk of Large Language Models (LLMs)
generating harmful content by jailbreak attacks. However, they overlook that
the direct generation of harmful content from scratch is more difficult than
inducing LLM to calibrate benign content into harmful forms. In our study, we
introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)
to induce the LLM to calibrate malicious metaphors for jailbreaking.
Specifically, to answer harmful queries, AVATAR adaptively identifies a set of
benign but logically related metaphors as the initial seed. Then, driven by
these metaphors, the target LLM is induced to reason and calibrate about the
metaphorical content, thus jailbroken by either directly outputting harmful
responses or calibrating residuals between metaphorical and professional
harmful content. Experimental results demonstrate that AVATAR can effectively
and transferable jailbreak LLMs and achieve a state-of-the-art attack success
rate across multiple advanced LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2412.12145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10510v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10510v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Braun, Mark Rothermel, Marcus Rohrbach, Anna Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of disinformation demands reliable and scalable
fact-checking solutions. We present Dynamic Evidence-based FAct-checking with
Multimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for
open-domain, text-image claim verification. DEFAME operates in a six-stage
process, dynamically selecting the tools and search depth to extract and
evaluate textual and visual evidence. Unlike prior approaches that are
text-only, lack explainability, or rely solely on parametric knowledge, DEFAME
performs end-to-end verification, accounting for images in claims and evidence
while generating structured, multimodal reports. Evaluation on the popular
benchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all
previous methods, establishing itself as the new state-of-the-art fact-checking
system for uni- and multimodal fact-checking. Moreover, we introduce a new
multimodal benchmark, ClaimReview2024+, featuring claims after the knowledge
cutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms
the GPT-4o baselines, showing temporal generalizability and the potential for
real-time fact-checking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing unified models achieve strong performance in
vision-language understanding and text-to-image generation, they remain limited
in addressing image perception and manipulation -- capabilities increasingly
demanded in practical applications. Recently, OpenAI introduced the powerful
GPT-4o-Image model, which showcases advanced capabilities in comprehensive
image perception and manipulation, sparking widespread interest. Through
carefully designed experiments, we observe that GPT-4o-Image likely relies on
semantic encoders rather than VAEs for feature extraction, despite VAEs being
commonly regarded as crucial for image manipulation tasks. Inspired by this
insight, we propose UniWorld-V1, a unified generative framework built upon
semantic features extracted from powerful multimodal large language models and
contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1
achieves impressive performance across diverse tasks, including image
understanding, generation, manipulation, and perception. We fully open-source
the UniWorld-V1 framework, including model weights, training and evaluation
scripts, and datasets to promote reproducibility and further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Lessons of Developing Process Reward Models in Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Reward Models (PRMs) emerge as a promising approach for process
supervision in mathematical reasoning of Large Language Models (LLMs), which
aim to identify and mitigate intermediate errors in the reasoning processes.
However, the development of effective PRMs faces significant challenges,
particularly in data annotation and evaluation methodologies. In this paper,
through extensive experiments, we demonstrate that commonly used Monte Carlo
(MC) estimation-based data synthesis for PRMs typically yields inferior
performance and generalization compared to LLM-as-a-judge and human annotation
methods. MC estimation relies on completion models to evaluate current-step
correctness, leading to inaccurate step verification. Furthermore, we identify
potential biases in conventional Best-of-N (BoN) evaluation strategies for
PRMs: (1) The unreliable policy models generate responses with correct answers
but flawed processes, leading to a misalignment between the evaluation criteria
of BoN and the PRM objectives of process verification. (2) The tolerance of
PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a
significant proportion of minimum scores concentrated on the final answer
steps, revealing the shift from process to outcome-based assessment in BoN
Optimized PRMs. To address these challenges, we develop a consensus filtering
mechanism that effectively integrates MC estimation with LLM-as-a-judge and
advocates a more comprehensive evaluation framework that combines
response-level and step-level metrics. Based on the mechanisms, we
significantly improve both model performance and data efficiency in the BoN
evaluation and the step-wise error identification task. Finally, we release a
new state-of-the-art PRM that outperforms existing open-source alternatives and
provides practical guidelines for future research in building process
supervision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming a wide range of domains, yet
verifying their outputs remains a significant challenge, especially for complex
open-ended tasks such as consolidation, summarization, and knowledge
extraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,
and accurate verification method. CE reduces each LLM answer to a single
embedding vector using powerful modern embedding LLM models like
SFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied
on weaker encoders like BERT, forcing them to operate at token or sentence
granularity. In contrast, CE performs fast, semantically rich comparisons
directly at the whole-answer level, overcoming key limitations in both accuracy
and scalability. We conduct a comprehensive design and time complexity analysis
across 13 verification baselines, including classical text scorers (e.g.,
BLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators
(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,
versatility, and simplicity of CE. Empirical results show that CE reliably
detects hallucinations in both closed and open-ended tasks. We further present
evidence that CE generalizes beyond text to other modalities such as vision,
establishing it as a practical and versatile verification framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMBoundary: Advancing MLLM Knowledge Boundary Awareness through
  Reasoning Step Confidence Calibration <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have made
significant progress but continue to face inherent challenges in multimodal
reasoning, which requires multi-level (e.g., perception, reasoning) and
multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior
work on estimating model confidence tends to focus on the overall response for
training and calibration, but fails to assess confidence in each reasoning
step, leading to undesirable hallucination snowballing. In this work, we
present MMBoundary, a novel framework that advances the knowledge boundary
awareness of MLLMs through reasoning step confidence calibration. To achieve
this, we propose to incorporate complementary textual and cross-modal
self-rewarding signals to estimate confidence at each step of the MLLM
reasoning process. In addition to supervised fine-tuning MLLM on this set of
self-rewarded confidence estimation signal for initial confidence expression
warm-up, we introduce a reinforcement learning stage with multiple reward
functions for further aligning model knowledge and calibrating confidence at
each reasoning step, enhancing reasoning chain self-correction. Empirical
results show that MMBoundary significantly outperforms existing methods across
diverse domain datasets and metrics, achieving an average of 7.5% reduction in
multimodal confidence calibration errors and up to 8.3% improvement in task
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal
  Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.18053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.18053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) pose unique safety challenges due to
their integration of visual and textual data, thereby introducing new
dimensions of potential attacks and complex risk combinations. In this paper,
we begin with a detailed analysis aimed at disentangling risks through
step-by-step reasoning within multimodal inputs. We find that systematic
multimodal risk disentanglement substantially enhances the risk awareness of
MLLMs. Via leveraging the strong discriminative abilities of multimodal risk
disentanglement, we further introduce \textbf{DREAM}
(\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety
\textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety
alignment in MLLMs through supervised fine-tuning and iterative Reinforcement
Learning from AI Feedback (RLAIF). Experimental results show that DREAM
significantly boosts safety during both inference and training phases without
compromising performance on normal tasks (namely oversafety), achieving a
16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The
data and code are available at https://github.com/Kizna1ver/DREAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[NAACL 2025] The first four authors contribute equally, 23 pages,
  repo at https://github.com/Kizna1ver/DREAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Head RAG: Solving Multi-Aspect Problems with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Ales Kubicek, Robert Gerstenberger, Marcin Chrapek, Roman Niggli, Patrik Okanovic, Yi Zhu, Patrick Iff, Michal Podstawski, Lucas Weitzendorf, Mingyuan Chi, Joanna Gajda, Piotr Nyczyk, Jürgen Müller, Hubert Niewiadomski, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving observation is that different attention
heads learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, multi-aspect datasets, and real-world
use cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages
over 18 RAG baselines, empirical improvements of up to 20% in retrieval success
ratios, and benefits for downstream LLM generation. MRAG can be seamlessly
integrated with existing RAG frameworks and benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Intermediate Representations in
  Compilers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intermediate Representations (IRs) play a critical role in compiler design
and program analysis, yet their comprehension by Large Language Models (LLMs)
remains underexplored. In this paper, we present an explorative empirical study
evaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,
DeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,
we assess model performance across four core tasks: control flow graph
reconstruction, decompilation, code summarization, and execution reasoning.
While LLMs exhibit competence in parsing IR syntax and identifying high-level
structures, they consistently struggle with instruction-level reasoning,
especially in control flow reasoning, loop handling, and dynamic execution.
Common failure modes include misinterpreting branching instructions, omitting
critical operations, and relying on heuristic reasoning rather than precise
instruction-level logic. Our findings highlight the need for IR-specific
enhancements in LLM design. We recommend fine-tuning on structured IR datasets
and integrating control-flow-sensitive architectures to improve model
effectiveness. All experimental data and source code are publicly available at
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SNaRe: Domain-aware Data Generation for Low-Resource Event Detection <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Detection (ED) -- the task of identifying event mentions from natural
language text -- is critical for enabling reasoning in highly specialized
domains such as biomedicine, law, and epidemiology. Data generation has proven
to be effective in broadening its utility to wider applications without
requiring expensive expert annotations. However, when existing generation
approaches are applied to specialized domains, they struggle with label noise,
where annotations are incorrect, and domain drift, characterized by a
distributional mismatch between generated sentences and the target domain. To
address these issues, we introduce SNaRe, a domain-aware synthetic data
generation framework composed of three components: Scout, Narrator, and
Refiner. Scout extracts triggers from unlabeled target domain data and curates
a high-quality domain-specific trigger list using corpus-level statistics to
mitigate domain drift. Narrator, conditioned on these triggers, generates
high-quality domain-aligned sentences, and Refiner identifies additional event
mentions, ensuring high annotation quality. Experimentation on three diverse
domain ED datasets reveals how SNaRe outperforms the best baseline, achieving
average F1 gains of 3-7% in the zero-shot/few-shot settings and 4-20% F1
improvement for multilingual generation. Analyzing the generated trigger hit
rate and human evaluation substantiates SNaRe's stronger annotation quality and
reduced domain drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ACL ARR May 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ValueSim: Generating Backstories to Model Individual Value Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to exhibit increasingly human-like
capabilities, aligning them with human values has become critically important.
Contemporary advanced techniques, such as prompt learning and reinforcement
learning, are being deployed to better align LLMs with human values. However,
while these approaches address broad ethical considerations and helpfulness,
they rarely focus on simulating individualized human value systems. To address
this gap, we present ValueSim, a framework that simulates individual values
through the generation of personal backstories reflecting past experiences and
demographic information. ValueSim converts structured individual data into
narrative backstories and employs a multi-module architecture inspired by the
Cognitive-Affective Personality System to simulate individual values based on
these narratives. Testing ValueSim on a self-constructed benchmark derived from
the World Values Survey demonstrates an improvement in top-1 accuracy by over
10% compared to retrieval-augmented generation methods. Further analysis
reveals that performance enhances as additional user interaction history
becomes available, indicating the model's ability to refine its persona
simulation capabilities over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main paper + 13 pages appendix, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability in Practice: A <span class="highlight-title">Survey</span> of Explainable NLP Across Various
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Mohammadi, Ayoub Bagheri, Anastasia Giachanou, Daniel L. Oberski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) has become a cornerstone in many critical
sectors, including healthcare, finance, and customer relationship management.
This is especially true with the development and use of advanced models such as
GPT-based architectures and BERT, which are widely used in decision-making
processes. However, the black-box nature of these advanced NLP models has
created an urgent need for transparency and explainability. This review
explores explainable NLP (XNLP) with a focus on its practical deployment and
real-world applications, examining its implementation and the challenges faced
in domain-specific contexts. The paper underscores the importance of
explainability in NLP and provides a comprehensive perspective on how XNLP can
be designed to meet the unique demands of various sectors, from healthcare's
need for clear insights to finance's emphasis on fraud detection and risk
assessment. Additionally, this review aims to bridge the knowledge gap in XNLP
literature by offering a domain-specific exploration and discussing
underrepresented areas such as real-world applicability, metric evaluation, and
the role of human interaction in model assessment. The paper concludes by
suggesting future research directions that could enhance the understanding and
broader application of XNLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noy Sternlicht, Tom Hope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A hallmark of human innovation is the process of recombination -- creating
original ideas by integrating elements of existing mechanisms and concepts. In
this work, we automatically mine the scientific literature and build CHIMERA: a
large-scale knowledge base (KB) of recombination examples. CHIMERA can be used
to empirically explore at scale how scientists recombine concepts and take
inspiration from different areas, or to train supervised machine learning
models that learn to predict new creative cross-domain directions. To build
this KB, we present a novel information extraction task of extracting
recombination from scientific paper abstracts, collect a high-quality corpus of
hundreds of manually annotated abstracts, and use it to train an LLM-based
extraction model. The model is applied to a large corpus of papers in the AI
domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to
explore the properties of recombination in different subareas of AI. Finally,
we train a scientific hypothesis generation model using the KB, which predicts
new recombination directions that real-world researchers find inspiring. Our
data and code are available at https://github.com/noy-sternlicht/CHIMERA-KB
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://noy-sternlicht.github.io/CHIMERA-Web</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Social Simulations Are a Promising Research Method <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and verifiable large language model (LLM) simulations of human
research subjects promise an accessible data source for understanding human
behavior and training new AI systems. However, results to date have been
limited, and few social scientists have adopted this method. In this position
paper, we argue that the promise of LLM social simulations can be achieved by
addressing five tractable challenges. We ground our argument in a review of
empirical comparisons between LLMs and human research subjects, commentaries on
the topic, and related work. We identify promising directions, including
context-rich prompting and fine-tuning with social science datasets. We believe
that LLM social simulations can already be used for pilot and exploratory
studies, and more widespread use may soon be possible with rapidly advancing
LLM capabilities. Researchers should prioritize developing conceptual models
and iterative evaluations to make the best use of new AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniCharacter: Towards Immersive Role-Playing Agents with Seamless
  Speech-Language Personality Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Zhang, Run Luo, Xiong Liu, Yuchuan Wu, Ting-En Lin, Pengpeng Zeng, Qiang Qu, Feiteng Fang, Min Yang, Lianli Gao, Jingkuan Song, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-Playing Agents (RPAs), benefiting from large language models, is an
emerging interactive AI system that simulates roles or characters with diverse
personalities. However, existing methods primarily focus on mimicking dialogues
among roles in textual form, neglecting the role's voice traits (e.g., voice
style and emotions) as playing a crucial effect in interaction, which tends to
be more immersive experiences in realistic scenarios. Towards this goal, we
propose OmniCharacter, a first seamless speech-language personality interaction
model to achieve immersive RPAs with low latency. Specifically, OmniCharacter
enables agents to consistently exhibit role-specific personality traits and
vocal traits throughout the interaction, enabling a mixture of speech and
language responses. To align the model with speech-language scenarios, we
construct a dataset named OmniCharacter-10K, which involves more distinctive
characters (20), richly contextualized multi-round dialogue (10K), and dynamic
speech response (135K). Experimental results showcase that our method yields
better responses in terms of both content and style compared to existing RPAs
and mainstream speech-language models, with a response latency as low as 289ms.
Code and dataset are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impossibility of Fair LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of general-purpose artificial intelligence (AI) systems,
particularly large language models (LLMs), has raised pressing moral questions
about how to reduce bias and ensure fairness at scale. Researchers have
documented a sort of "bias" in the significant correlations between
demographics (e.g., race, gender) in LLM prompts and responses, but it remains
unclear how LLM fairness could be evaluated with more rigorous definitions,
such as group fairness or fair representations. We analyze a variety of
technical fairness frameworks and find inherent challenges in each that make
the development of a fair LLM intractable. We show that each framework either
does not logically extend to the general-purpose AI context or is infeasible in
practice, primarily due to the large amounts of unstructured training data and
the many potential combinations of human populations, use cases, and sensitive
attributes. These inherent challenges would persist for general-purpose AI,
including LLMs, even if empirical challenges, such as limited participatory
input and limited measurement methods, were overcome. Nonetheless, fairness
will remain an important type of model evaluation, and there are still
promising research directions, particularly the development of standards for
the responsibility of LLM developers, context-specific evaluations, and methods
of iterative, participatory, and AI-assisted evaluation that could scale
fairness across the diverse contexts of modern human-AI interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12649v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12649v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristian Lum, Jacy Reese Anthis, Kevin Robinson, Chirag Nagpal, Alexander D'Amour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard benchmarks of bias and fairness in large language models (LLMs)
measure the association between the user attributes stated or implied by a
prompt and the LLM's short text response, but human-AI interaction increasingly
requires long-form and context-specific system output to solve real-world
tasks. In the commonly studied domain of gender-occupation bias, we test
whether these benchmarks are robust to lengthening the LLM responses as a
measure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From
the current literature, we adapt three standard bias metrics (neutrality, skew,
and stereotype) and develop analogous RUTEd evaluations from three contexts of
real-world use: children's bedtime stories, user personas, and English language
learning exercises. We find that standard bias metrics have no significant
correlation with the more realistic bias metrics. For example, selecting the
least biased model based on the standard "trick tests" coincides with selecting
the least biased model as measured in more realistic use no more than random
chance. We suggest that there is not yet evidence to justify standard
benchmarks as reliable proxies of real-world AI biases, and we encourage
further development of evaluations grounded in particular contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging LLMs for Bangla Grammar Error Correction:Error
  Categorization, Synthetic Data, and Model Evaluation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pramit Bhattacharyya, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) perform exceedingly well in Natural Language
Understanding (NLU) tasks for many languages including English. However,
despite being the fifth most-spoken language globally, Grammatical Error
Correction (GEC) in Bangla remains underdeveloped. In this work, we investigate
how LLMs can be leveraged for improving Bangla GEC. For that, we first do an
extensive categorization of 12 error classes in Bangla, and take a survey of
native Bangla speakers to collect real-world errors. We next devise a
rule-based noise injection method to create grammatically incorrect sentences
corresponding to correct ones. The Vaiyakarana dataset, thus created, consists
of 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then
used to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show
that instruction-tuning with \name improves GEC performance of LLMs by 3-7
percentage points as compared to the zero-shot setting, and makes them achieve
human-like performance in grammatical error identification. Humans, though,
remain superior in error correction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL Findings, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GoRA: Gradient-driven Adaptive Low Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan He, Peng Ye, Yuchen Ren, Yuan Yuan, Luyang Zhou, Shucun Ju, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning
large language models (LLMs), with its effectiveness influenced by two key
factors: rank selection and weight initialization. While numerous LoRA variants
have been proposed to improve performance by addressing one of these aspects,
they often compromise usability or computational efficiency. In this paper, we
analyze and identify the core limitations of existing approaches and propose a
novel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that
simultaneously adapts both the rank and initialization strategy within a
unified framework. GoRA leverages gradient information during training to
dynamically assign optimal ranks and initialize low-rank adapter weights in an
adaptive manner. To our knowledge, GoRA is the first method that not only
addresses the limitations of prior approaches -- which often focus on either
rank selection or initialization in isolation -- but also unifies both aspects
within a single framework, enabling more effective and efficient adaptation.
Extensive experiments across various architectures and modalities show that
GoRA consistently outperforms existing LoRA-based methods while preserving the
efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for
mathematical reasoning, GoRA achieves a 5.13-point improvement over standard
LoRA and even outperforms full fine-tuning by 2.05 points under high-rank
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Argument Summarization and its Evaluation in the Era of Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Altemeyer, Steffen Eger, Johannes Daxenberger, Yanran Chen, Tim Altendorf, Philipp Cimiano, Benjamin Schiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized various Natural Language
Generation (NLG) tasks, including Argument Summarization (ArgSum), a key
subfield of Argument Mining (AM). This paper investigates the integration of
state-of-the-art LLMs into ArgSum, including for its evaluation. In particular,
we propose a novel prompt-based evaluation scheme, and validate it through a
novel human benchmark dataset. Our work makes three main contributions: (i) the
integration of LLMs into existing ArgSum frameworks, (ii) the development of a
new LLM-based ArgSum system, benchmarked against prior methods, and (iii) the
introduction of an advanced LLM-based evaluation scheme. We demonstrate that
the use of LLMs substantially improves both the generation and evaluation of
argument summaries, achieving state-of-the-art results and advancing the field
of ArgSum. We also show that among the four LLMs integrated in (i) and (ii),
Qwen-3-32B, despite having the fewest parameters, performs best, even
surpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Anytime Reasoning via Budget Relative Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09117v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09117v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) unlearning has demonstrated its essential role in
removing privacy and copyright-related responses, crucial for their legal and
safe applications. However, the pursuit of complete unlearning often comes with
substantial costs due to its compromises in their general functionality,
leading to a notorious trade-off between unlearning and retention. It motivates
this paper to explore enhanced unlearning schemes that can mitigate this
trade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an
improved framework that regulates the directions of gradient updates during the
unlearning procedure such that their side impacts on other, unrelated responses
can be minimized. GRU is easy and general to implement, demonstrating practical
effectiveness across a variety of well-established unlearning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Correction is More than Refinement: A Learning Framework for Visual
  and Language Reasoning Tasks <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Vision-Language Models (VLMs) have shown remarkable abilities in visual
and language reasoning tasks, they invariably generate flawed responses.
Self-correction that instructs models to refine their outputs presents a
promising solution to this issue. Previous studies have mainly concentrated on
Large Language Models (LLMs), while the self-correction abilities of VLMs,
particularly concerning both visual and linguistic information, remain largely
unexamined. This study investigates the self-correction capabilities of VLMs
during both inference and fine-tuning stages. We introduce a Self-Correction
Learning (SCL) approach that enables VLMs to learn from their self-generated
self-correction data through Direct Preference Optimization (DPO) without
relying on external feedback, facilitating self-improvement. Specifically, we
collect preferred and disfavored samples based on the correctness of initial
and refined responses, which are obtained by two-turn self-correction with VLMs
during the inference stage. Experimental results demonstrate that although VLMs
struggle to self-correct effectively during iterative inference without
additional fine-tuning and external feedback, they can enhance their
performance and avoid previous mistakes through preference fine-tuning when
their self-generated self-correction data are categorized into preferred and
disfavored samples. This study emphasizes that self-correction is not merely a
refinement process; rather, it should enhance the reasoning abilities of models
through additional training, enabling them to generate high-quality responses
directly without further refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the
  Limits of Embedding Space Capacity <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A range of recent works addresses the problem of compression of sequence of
tokens into a shorter sequence of real-valued vectors to be used as inputs
instead of token embeddings or key-value cache. These approaches are focused on
reduction of the amount of compute in existing language models rather than
minimization of number of bits needed to store text. Despite relying on
powerful models as encoders, the maximum attainable lossless compression ratio
is typically not higher than x10. This fact is highly intriguing because, in
theory, the maximum information capacity of large real-valued vectors is far
beyond the presented rates even for 16-bit precision and a modest vector size.
In this work, we explore the limits of compression by replacing the encoder
with a per-sample optimization procedure. We show that vectors with compression
ratios up to x1500 exist, which highlights two orders of magnitude gap between
existing and practically attainable solutions. Furthermore, we empirically show
that the compression limits are determined not by the length of the input but
by the amount of uncertainty to be reduced, namely, the cross-entropy loss on
this sequence without any conditioning. The obtained limits highlight the
substantial gap between the theoretical capacity of input embeddings and their
practical utilization, suggesting significant room for optimization in model
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position is Power: System <span class="highlight-title">Prompt</span>s as a Mechanism of Bias in Large
  Language Models (LLMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System prompts in Large Language Models (LLMs) are predefined directives that
guide model behaviour, taking precedence over user inputs in text processing
and generation. LLM deployers increasingly use them to ensure consistent
responses across contexts. While model providers set a foundation of system
prompts, deployers and third-party developers can append additional prompts
without visibility into others' additions, while this layered implementation
remains entirely hidden from end-users. As system prompts become more complex,
they can directly or indirectly introduce unaccounted for side effects. This
lack of transparency raises fundamental questions about how the position of
information in different directives shapes model outputs. As such, this work
examines how the placement of information affects model behaviour. To this end,
we compare how models process demographic information in system versus user
prompts across six commercially available LLMs and 50 demographic groups. Our
analysis reveals significant biases, manifesting in differences in user
representation and decision-making scenarios. Since these variations stem from
inaccessible and opaque system-level configurations, they risk
representational, allocative and potential other biases and downstream harms
beyond the user's ability to detect or correct. Our findings draw attention to
these critical issues, which have the potential to perpetuate harms if left
unexamined. Further, we argue that system prompt analysis must be incorporated
into AI auditing processes, particularly as customisable system prompts become
increasingly prevalent in commercial AI deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in Proceedings of ACM FAccT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knockout LLM Assessment: Using Large Language Models for Evaluations
  through Iterative Pairwise Comparisons <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isik Baran Sandan, Tu Anh Dinh, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to GEM @ ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via
  Sequential Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, Tao Gui, Chao Xin, Wei Chengzhi, Lin Yan, Qi Zhang, Yonghui Wu, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered
  GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has revolutionized Graphical User
Interface (GUI) automation through LLM-powered GUI agents, yet their ability to
process sensitive data with limited human oversight raises significant privacy
and security risks. This position paper identifies three key risks of GUI
agents and examines how they differ from traditional GUI automation and general
autonomous agents. Despite these risks, existing evaluations focus primarily on
performance, leaving privacy and security assessments largely unexplored. We
review current evaluation metrics for both GUI and general LLM agents and
outline five key challenges in integrating human evaluators for GUI agent
assessments. To address these gaps, we advocate for a human-centered evaluation
framework that incorporates risk assessments, enhances user awareness through
in-context consent, and embeds privacy and security considerations into GUI
agent design and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Jailbreaking Strategies Based on the Semantic Understanding
  Capabilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks on Large Language Models (LLMs) via jailbreaking
techniques-methods that circumvent their built-in safety and ethical
constraints-have emerged as a critical challenge in AI security. These attacks
compromise the reliability of LLMs by exploiting inherent weaknesses in their
comprehension capabilities. This paper investigates the efficacy of
jailbreaking strategies that are specifically adapted to the diverse levels of
understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking
Strategies Based on the Semantic Understanding Capabilities of Large Language
Models, a novel framework that classifies LLMs into Type I and Type II
categories according to their semantic comprehension abilities. For each
category, we design tailored jailbreaking strategies aimed at leveraging their
vulnerabilities to facilitate successful attacks. Extensive experiments
conducted on multiple LLMs demonstrate that our adaptive strategy markedly
improves the success rate of jailbreaking. Notably, our approach achieves an
exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action
  Analysis with Cross-Category Generalization <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Rui Mao, Deeksha Varshney, Erik Cambria, Gianmarco Mengaldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainability reports are key for evaluating companies' environmental,
social and governance, ESG performance, but their content is increasingly
obscured by greenwashing - sustainability claims that are misleading,
exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack
robustness against greenwashing risks, often extracting insights that reflect
misleading or exaggerated sustainability claims rather than objective ESG
performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis
with Cross-Category Generalization, as a novel dataset to improve the
robustness of ESG analysis amid the prevalence of greenwashing. By explicitly
linking sustainability aspects with their associated actions, A3CG facilitates
a more fine-grained and transparent evaluation of sustainability claims,
ensuring that insights are grounded in verifiable actions rather than vague or
misleading rhetoric. Additionally, A3CG emphasizes cross-category
generalization. This ensures robust model performance in aspect-action analysis
even when companies change their reports to selectively favor certain
sustainability areas. Through experiments on A3CG, we analyze state-of-the-art
supervised models and LLMs, uncovering their limitations and outlining key
directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Association for Computational Linguistics Main
  Conference (ACL 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deriving Strategic Market Insights with Large Language Models: A
  Benchmark for Forward Counterfactual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Rui Mao, Deeksha Varshney, Paul Pu Liang, Erik Cambria, Gianmarco Mengaldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual reasoning typically involves considering alternatives to
actual events. While often applied to understand past events, a distinct
form-forward counterfactual reasoning-focuses on anticipating plausible future
developments. This type of reasoning is invaluable in dynamic financial
markets, where anticipating market developments can powerfully unveil potential
risks and opportunities for stakeholders, guiding their decision-making.
However, performing this at scale is challenging due to the cognitive demands
involved, underscoring the need for automated solutions. Large Language Models
(LLMs) offer promise, but remain unexplored for this application. To address
this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward
Counterfactual Evaluation. By curating financial news headlines and providing
structured evaluation, Fin-Force supports LLM based forward counterfactual
generation. This paves the way for scalable and automated solutions for
exploring and anticipating future market developments, thereby providing
structured insights for decision-making. Through experiments on Fin-Force, we
evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing
their limitations and proposing insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Language Learning for Endangered Languages in Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaolin Li, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With approximately 7,000 languages spoken worldwide, current large language
models (LLMs) support only a small subset. Prior research indicates LLMs can
learn new languages for certain tasks without supervised data. We extend this
investigation to speech recognition, investigating whether LLMs can learn
unseen, low-resource languages through in-context learning (ICL). With
experiments on four diverse endangered languages that LLMs have not been
trained on, we find that providing more relevant text samples enhances
performance in both language modelling and Automatic Speech Recognition (ASR)
tasks. Furthermore, we show that the probability-based approach outperforms the
traditional instruction-based approach in language learning. Lastly, we show
ICL enables LLMs to achieve ASR performance that is comparable to or even
surpasses dedicated language models trained specifically for these languages,
while preserving the original capabilities of the LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiMo: Unlocking the Reasoning Potential of Language Model -- From
  <span class="highlight-title">Pretrain</span>ing to Posttraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        LLM-Core Xiaomi,  :, Bingquan Xia, Bowen Shen,  Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, Kai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full-Parameter Continual <span class="highlight-title">Pretrain</span>ing of Gemma2: Insights into Fluency
  and Domain Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vytenis Šliogeris, Povilas Daniušis, Artūras Nakvosas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we empirically investigate the relationship between
linguistic fluency and domain knowledge in the context of continual learning
with large language models (LLMs). Specifically, we enhance the linguistic
fluency of the Gemma2 LLM for the Lithuanian language by autoregressively
pretraining its full parameter set on the first 10\% of the Lithuanian language
component of the CulturaX dataset. To prevent catastrophic forgetting of the
model's existing domain knowledge, we apply Elastic Weight Consolidation (EWC),
leveraging Fisher information estimated using data from the Massive Multitask
Language Understanding (MMLU) benchmark. In the post-training evaluations, we
assess linguistic fluency through perplexity and evaluate domain knowledge
using accuracy on a suite of language understanding benchmarks, including
ARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both
English and Lithuanian. The empirical results demonstrate that EWC not only
mitigates catastrophic forgetting by preserving the model's performance in
terms of both linguistic fluency and domain knowledge but also improves or
maintains these capabilities for the newly added Lithuanian language. These
findings highlight the potential for more efficient adaptation of
general-purpose LLMs to under-represented languages without requiring access to
the original training data. The accompanying codebase is openly accessible at
https://github.com/Neurotechnology/LLM_EWC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NTPP: Generative Speech Language Modeling for Dual-Channel Spoken
  Dialogue via Next-Token-Pair Prediction <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the impressive capabilities of GPT-4o, there is growing interest
in enabling speech language models (SLMs) to engage in natural, fluid spoken
interactions with humans. Recent advancements have led to the development of
several SLMs that demonstrate promising results in this area. However, current
approaches have yet to fully exploit dual-channel speech data, which inherently
captures the structure and dynamics of human conversation. In this work, we
systematically explore the use of dual-channel speech data in the context of
modern large language models, and introduce a novel generative modeling
paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent
dual-channel spoken dialogue learning using decoder-only architectures for the
first time. We evaluate our approach on standard benchmarks, and empirical
results show that our proposed method, NTPP, significantly improves the
conversational abilities of SLMs in terms of turn-taking prediction, response
coherence, and naturalness. Moreover, compared to existing methods, NTPP
achieves substantially lower inference latency, highlighting its practical
efficiency for real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NorEval: A Norwegian Language Understanding and Generation Evaluation
  Benchmark <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Mikhailov, Tita Enstad, David Samuel, Hans Christian Farsethås, Andrey Kutuzov, Erik Velldal, Lilja Øvrelid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces NorEval, a new and comprehensive evaluation suite for
large-scale standardized benchmarking of Norwegian generative language models
(LMs). NorEval consists of 24 high-quality human-created datasets -- of which
five are created from scratch. In contrast to existing benchmarks for
Norwegian, NorEval covers a broad spectrum of task categories targeting
Norwegian language understanding and generation, establishes human baselines,
and focuses on both of the official written standards of the Norwegian
language: Bokm{\aa}l and Nynorsk. All our datasets and a collection of over 100
human-written prompts are integrated into LM Evaluation Harness, ensuring
flexible and reproducible evaluation. We describe the NorEval design and
present the results of benchmarking 19 open-source pre-trained and
instruction-tuned LMs for Norwegian in various scenarios. Our benchmark,
evaluation framework, and annotation materials are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Findings of the Association for Computational
  Linguistics: ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in
  Large Language Models for Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tseng, Titouan Parcollet, Rogier van Dalen, Shucong Zhang, Sourav Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work suggests that large language models (LLMs) can improve
performance of speech tasks compared to existing systems. To support their
claims, results on LibriSpeech and Common Voice are often quoted. However, this
work finds that a substantial amount of the LibriSpeech and Common Voice
evaluation sets appear in public LLM pretraining corpora. This calls into
question the reliability of findings drawn from these two datasets. To measure
contamination impact, LLMs trained with/without contamination are compared. A
contaminated LLM is more likely to generate test sentences it has seen during
training. Then, speech recognisers based on LLMs are compared. They show only
subtle error rate differences if the LLM is contaminated, but assign
significantly higher probabilities to transcriptions seen during LLM training.
Results show that LLM outputs can be biased by tiny amounts of data
contamination, highlighting the importance of evaluating LLM-based speech
systems with held-out data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages
  with Negligible Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Xing, Muyan Zhong, Zeqiang Lai, Liangchen Li, Jiawen Liu, Yaohui Wang, Jifeng Dai, Wenhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore a cost-effective framework for multilingual image
generation. We find that, unlike models tuned on high-quality images with
multilingual annotations, leveraging text encoders pre-trained on widely
available, noisy Internet image-text pairs significantly enhances data
efficiency in text-to-image (T2I) generation across multiple languages.Based on
this insight, we introduce MuLan, Multi-Language adapter, a lightweight
language adapter with fewer than 20M parameters, trained alongside a frozen
text encoder and image diffusion model. Compared to previous multilingual T2I
models, this framework offers: (1) Cost efficiency. Using readily accessible
English data and off-the-shelf multilingual text encoders minimizes the
training cost; (2) High performance. Achieving comparable generation
capabilities in over 110 languages with CLIP similarity scores nearly matching
those in English (39.57 for English vs. 39.61 for other languages); and (3)
Broad applicability. Seamlessly integrating with compatible community tools
like LoRA, LCM, ControlNet, and IP-Adapter, expanding its potential use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding
  Models Against Misinformation Edits <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, Scott Hale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online misinformation remains a critical challenge, and fact-checkers
increasingly rely on claim matching systems that use sentence embedding models
to retrieve relevant fact-checks. However, as users interact with claims
online, they often introduce edits, and it remains unclear whether current
embedding models used in retrieval are robust to such edits. To investigate
this, we introduce a perturbation framework that generates valid and natural
claim variations, enabling us to assess the robustness of a wide-range of
sentence embedding models in a multi-stage retrieval pipeline and evaluate the
effectiveness of various mitigation approaches. Our evaluation reveals that
standard embedding models exhibit notable performance drops on edited claims,
while LLM-distilled embedding models offer improved robustness at a higher
computational cost. Although a strong reranker helps to reduce the performance
drop, it cannot fully compensate for first-stage retrieval gaps. To address
these retrieval gaps, we evaluate train- and inference-time mitigation
approaches, demonstrating that they can improve in-domain robustness by up to
17 percentage points and boost out-of-domain generalization by 10 percentage
points. Overall, our findings provide practical improvements to claim-matching
systems, enabling more reliable fact-checking of evolving misinformation. Code
and data are available at
https://github.com/JabezNzomo99/claim-matching-robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Morphological Compositional Generalization in Large Language
  Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12656v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12656v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant progress in
various natural language generation and understanding tasks. However, their
linguistic generalization capabilities remain questionable, raising doubts
about whether these models learn language similarly to humans. While humans
exhibit compositional generalization and linguistic creativity in language use,
the extent to which LLMs replicate these abilities, particularly in morphology,
is under-explored. In this work, we systematically investigate the
morphological generalization abilities of LLMs through the lens of
compositionality. We define morphemes as compositional primitives and design a
novel suite of generative and discriminative tasks to assess morphological
productivity and systematicity. Focusing on agglutinative languages such as
Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned
multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs
struggle with morphological compositional generalization particularly when
applied to novel word roots, with performance declining sharply as
morphological complexity increases. While models can identify individual
morphological combinations better than chance, their performance lacks
systematicity, leading to significant accuracy gaps compared to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Text-based Protein Understanding: Retrieval or LLM? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, protein-text models have gained significant attention for
their potential in protein generation and understanding. Current approaches
focus on integrating protein-related knowledge into large language models
through continued pretraining and multi-modal alignment, enabling simultaneous
comprehension of textual descriptions and protein sequences. Through a thorough
analysis of existing model architectures and text-based protein understanding
benchmarks, we identify significant data leakage issues present in current
benchmarks. Moreover, conventional metrics derived from natural language
processing fail to accurately assess the model's performance in this domain. To
address these limitations, we reorganize existing datasets and introduce a
novel evaluation framework based on biological entities. Motivated by our
observation, we propose a retrieval-enhanced method, which significantly
outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy
and efficiency in training-free scenarios. Our code and data can be seen at
https://github.com/IDEA-XL/RAPM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowCut: Rethinking Redundancy via Information Flow for Efficient
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
pruning methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware pruning framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fewer Hallucinations, More Verification: A Three-Stage LLM-Based
  Framework for ASR Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangui Fang, Baixu Cheng, Jing Peng, Xu Li, Yu Xi, Chengwei Zhang, Guohui Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) error correction aims to correct
recognition errors while preserving accurate text. Although traditional
approaches demonstrate moderate effectiveness, LLMs offer a paradigm that
eliminates the need for training and labeled data. However, directly using LLMs
will encounter hallucinations problem, which may lead to the modification of
the correct text. To address this problem, we propose the Reliable LLM
Correction Framework (RLLM-CF), which consists of three stages: (1) error
pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)
reasoning process verification. The advantage of our method is that it does not
require additional information or fine-tuning of the model, and ensures the
correctness of the LLM correction under multi-pass programming. Experiments on
AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by
our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Yunchang Zhu, Jia Gu, Zihao Wei, Jingcheng Deng, Feiyang Pan, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Firm or Fickle? Evaluating Large Language Models Consistency in
  Sequential Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22353v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22353v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities across
various tasks, but their deployment in high-stake domains requires consistent
and coherent behavior across multiple rounds of user interaction. This paper
introduces a comprehensive framework for evaluating and improving LLM response
consistency, making three key contributions. Code and data are available at:
https://github.com/yubol-bobo/MT-Consistency. First, we introduce
Position-Weighted Consistency (PWC), a metric designed to capture both the
importance of early-stage stability and recovery patterns in multi-turn
interactions. Second, we present MT-Consistency, a carefully curated benchmark
dataset spanning diverse domains and difficulty levels, specifically designed
to evaluate LLM consistency under various challenging follow-up scenarios.
Third, we introduce Confidence-Aware Response Generation (CARG), a framework
that significantly improves response stability by explicitly integrating
internal model confidence scores during the generation process. Experimental
results demonstrate that CARG significantly improves response stability without
sacrificing accuracy, offering a practical path toward more dependable LLM
behavior in critical, real-world deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focus On This, Not That! Steering LLMs with Adaptive Feature
  Specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Instruction Tuning (IT) in training large language
models (LLMs), such models often leverage spurious or biased features learnt
from their training data and can become misaligned, leading to undesired
behaviours. While existing techniques can steer model behaviour at
inference-time, they are often post-hoc and do not embed steering as an
intrinsic model feature. In this work, we introduce Focus Instruction Tuning
(FIT), which trains LLMs to condition their responses by focusing on specific
features whilst ignoring others, leading to different behaviours based on what
features are specified. Across diverse benchmarks, we demonstrate that FIT: (i)
successfully steers behaviour at inference time; (ii) increases robustness by
amplifying core task signals and down-weighting spurious cues; (iii) mitigates
social bias by suppressing demographic attributes; and (iv) generalises under
distribution shifts and to previously unseen focus features. FIT therefore
offers a lightweight, intrinsic mechanism for building more robust, fair, and
easily controllable LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Diversity in In-Context Learning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyang Xiao, Haoyu Zhao, Lingxiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Trends in Language Model Robustness <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18213v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18213v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaus Howe, Ian McKenzie, Oskar Hollinsworth, Michał Zajac, Tom Tseng, Aaron Tucker, Pierre-Luc Bacon, Adam Gleave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing model size has unlocked a dazzling array of capabilities in modern
language models. At the same time, even frontier models remain vulnerable to
jailbreaks and prompt injections, despite concerted efforts to make them
robust. As both attack and defense gain access to more compute, and as models
become larger, what happens to robustness? We argue that to answer this
question requires a \emph{scaling} approach, which we employ in an extensive
study of language model robustness across several classification tasks, model
families, and adversarial attacks. We find that in the absence of explicit
safety training, larger models are not consistently more robust; however, scale
improves sample efficiency in adversarial training, though it worsens compute
efficiency. Further, we find that increasing attack compute smoothly improves
attack success rate against both undefended and adversarially trained models.
Finally, after exploring robustness transfer across attacks and threat models,
we combine attack and defense scaling rates to study the offense-defense
balance. We find that while attack scaling outpaces adversarial training across
all models studied, larger adversarially trained models might give defense the
advantage in the long run. These results underscore the utility of the scaling
lens, and provide a paradigm for evaluating future attacks and defenses on
frontier models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>59 pages; updated to ICML version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FastDraft: How to Train Your Draft <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, Guy Boudoukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative Decoding has gained popularity as an effective technique for
accelerating the auto-regressive inference process of Large Language Models.
However, Speculative Decoding entirely relies on the availability of efficient
draft models, which are often lacking for many existing language models due to
a stringent constraint of vocabulary compatibility. In this work we introduce
FastDraft, a novel and efficient approach for pre-training and aligning a draft
model to any large language model by incorporating efficient pre-training,
followed by fine-tuning over synthetic datasets generated by the target model.
We demonstrate FastDraft by training two highly parameter efficient drafts for
the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able
to produce a draft model with approximately 10 billion tokens on a single
server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2 accelerators in under 24
hours. Our results show that the draft model achieves impressive results in key
metrics of acceptance rate, block efficiency and up to 3x memory bound speed up
when evaluated on code completion and up to 2x in summarization, text
completion and instruction tasks. We validate our theoretical findings through
benchmarking on the latest Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra,
achieving a wall-clock time speedup of up to 2x, indicating a significant
reduction in runtime. Due to its high quality, FastDraft unlocks large language
models inference on AI-PC and other edge-devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Vector Grounding Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitri Coelho Mollo, Raphaël Millière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance of large language models (LLMs) on complex
linguistic tasks has sparked debate about their capabilities. Unlike humans,
these models learn language solely from textual data without directly
interacting with the world. Yet they generate seemingly meaningful text on
diverse topics. This achievement has renewed interest in the classical `Symbol
Grounding Problem' -- the question of whether the internal representations and
outputs of symbolic AI systems can possess intrinsic meaning that is not
parasitic on external interpretation. Although modern LLMs compute over vectors
rather than symbols, an analogous problem arises for these systems, which we
call the Vector Grounding Problem. This paper has two main goals. First, we
distinguish five main notions of grounding that are often conflated in the
literature, and argue that only one of them, which we call referential
grounding, is relevant to the Vector Grounding Problem. Second, drawing on
philosophical theories of representational content, we provide two arguments
for the claim that LLMs and related systems can achieve referential grounding:
(1) through preference fine-tuning methods that explicitly establish
world-involving functions, and (2) through pre-training alone, which in limited
domains may select for internal states with world-involving content, as
mechanistic interpretability research suggests. Through these pathways, LLMs
can establish connections to the world sufficient for intrinsic meaning. One
potentially surprising implication of our discussion is that that multimodality
and embodiment are neither necessary nor sufficient to overcome the Grounding
Problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language
  Models for Vision-Driven Embodied Agents <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09560v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09560v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied
agents offers a promising avenue for tackling real-world tasks. While
language-centric embodied agents have garnered substantial attention,
MLLM-based embodied agents remain underexplored due to the lack of
comprehensive evaluation frameworks. To bridge this gap, we introduce
EmbodiedBench, an extensive benchmark designed to evaluate vision-driven
embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing
tasks across four environments, ranging from high-level semantic tasks (e.g.,
household) to low-level tasks involving atomic actions (e.g., navigation and
manipulation); and (2) six meticulously curated subsets evaluating essential
agent capabilities like commonsense reasoning, complex instruction
understanding, spatial awareness, visual perception, and long-term planning.
Through extensive experiments, we evaluated 24 leading proprietary and
open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel
at high-level tasks but struggle with low-level manipulation, with the best
model, GPT-4o, scoring only 28.9\% on average. EmbodiedBench provides a
multifaceted standardized evaluation platform that not only highlights existing
challenges but also offers valuable insights to advance MLLM-based embodied
agents. Our code and dataset are available at https://embodiedbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Confidence Gap: Overconfidence, Calibration, and Distractor
  Effects in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Chhikara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show remarkable proficiency in natural language
tasks, yet their frequent overconfidence-misalignment between predicted
confidence and true correctness-poses significant risks in critical
decision-making applications. We present a comprehensive analysis on
calibration in LLMs across nine LLMs and three factual Question-Answering (QA)
datasets, systematically comparing standard free-generation settings against
structured distractor-augmented prompts. Our evaluation reveals that explicitly
incorporating distractors can substantially mitigate miscalibration, achieving
relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite
general trends, we uncover nuanced findings: large RLHF-tuned models display
inherent calibration strengths but can paradoxically suffer increased
miscalibration on easier queries, whereas smaller models benefit
disproportionately from distractor prompts but remain significantly
miscalibrated. Through detailed analyses across question types, we identify
persistent calibration failures, particularly in person-based queries. We
conclude with concrete recommendations-targeted fine-tuning, structured
prompting, and strategic model choice-to ensure reliable, trustworthy LLM
deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of
  Retrieval Noise Erosion in RAG Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Yan Wang, Yongrui Chen, Shenyu Zhang, Xinbang Dai, Sheng Bi, Guilin Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems enhance Large Language Models
(LLMs) by incorporating external retrieved information, mitigating issues such
as hallucination and outdated knowledge. However, RAG systems are highly
sensitive to retrieval noise prevalent in real-world scenarios. Existing
benchmarks fail to emulate the complex and heterogeneous noise distributions
encountered in real-world retrieval environments, undermining reliable
robustness assessment. In this paper, we define four categories of retrieval
noise based on linguistic properties and noise characteristics, aiming to
reflect the heterogeneity of noise in real-world scenarios. Building on this,
we introduce Magic Mushroom, a benchmark for replicating "magic mushroom"
noise: contexts that appear relevant on the surface but covertly mislead RAG
systems. Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop
question-answer pairs. More importantly, Magic Mushroom enables researchers to
flexibly configure combinations of retrieval noise according to specific
research objectives or application scenarios, allowing for highly controlled
evaluation setups. We evaluate LLM generators of varying parameter scales and
classic RAG denoising strategies under diverse noise distributions to
investigate their performance dynamics during progressive noise encroachment.
Our analysis reveals that both generators and denoising strategies have
significant room for improvement and exhibit extreme sensitivity to noise
distributions. Magic Mushroom emerges as a promising tool for evaluating and
advancing noise-robust RAG systems, accelerating their widespread deployment in
real-world applications. The Magic Mushroom benchmark is available at
https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Should I Trust You? Detecting Deception in Negotiations using
  Counterfactual RL <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12436v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12436v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wichayaporn Wongkamjan, Yanze Wang, Feng Gu, Denis Peskoff, Jonathan K. Kummerfeld, Jonathan May, Jordan Lee Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasingly common socio-technical problem is people being taken in by
offers that sound ``too good to be true'', where persuasion and trust shape
decision-making. This paper investigates how \abr{ai} can help detect these
deceptive scenarios. We analyze how humans strategically deceive each other in
\textit{Diplomacy}, a board game that requires both natural language
communication and strategic reasoning. This requires extracting logical forms
of proposed agreements in player communications and computing the relative
rewards of the proposal using agents' value functions. Combined with text-based
features, this can improve our deception detection. Our method detects human
deception with a high precision when compared to a Large Language Model
approach that flags many true messages as deceptive. Future human-\abr{ai}
interaction tools can build on our methods for deception detection by
triggering \textit{friction} to give users a chance of interrogating suspicious
proposals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Propaganda and Information Dissemination in the Russo-Ukrainian War:
  Natural Language Processing of Russian and Western Twitter Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaur Gouliev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conflict in Ukraine has been not only characterised by military
engagement but also by a significant information war, with social media
platforms like X, formerly known as Twitter playing an important role in
shaping public perception. This article provides an analysis of tweets from
propaganda accounts and trusted accounts collected from the onset of the war,
February 2022 until the middle of May 2022 with n=40,000 total tweets. We
utilise natural language processing and machine learning algorithms to assess
the sentiment and identify key themes, topics and narratives across the dataset
with human-in-the-loop (HITL) analysis throughout. Our findings indicate
distinct strategies in how information is created, spread, and targeted at
different audiences by both sides. Propaganda accounts frequently employ
emotionally charged language and disinformation to evoke fear and distrust,
whereas other accounts, primarily Western tend to focus on factual reporting
and humanitarian aspects of the conflict. Clustering analysis reveals groups of
accounts with similar behaviours, which we suspect indicates the presence of
coordinated efforts. This research attempts to contribute to our understanding
of the dynamics of information warfare and offers techniques for future studies
on social media influence in military conflicts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement
  Learning Injected by Elite Individuals <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Zhao, Ben Niu, Libo Qin, Shihan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue
systems to optimize dialogue policy, but it struggles to balance exploration
and exploitation due to the high dimensionality of state and action spaces.
This challenge often results in local optima or poor convergence. Evolutionary
Algorithms (EAs) have been proven to effectively explore the solution space of
neural networks by maintaining population diversity. Inspired by this, we
innovatively combine the global search capabilities of EA with the local
optimization of DRL to achieve a balance between exploration and exploitation.
Nevertheless, the inherent flexibility of natural language in dialogue tasks
complicates this direct integration, leading to prolonged evolutionary times.
Thus, we further propose an elite individual injection mechanism to enhance
EA's search efficiency by adaptively introducing best-performing individuals
into the population. Experiments across four datasets show that our approach
significantly improves the balance between exploration and exploitation,
boosting performance. Moreover, the effectiveness of the EII mechanism in
reducing exploration time has been demonstrated, achieving an efficient
integration of EA and DRL on task-oriented dialogue policy tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inducing lexicons of in-group language with socio-temporal context <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine de Kock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-group language is an important signifier of group dynamics. This paper
proposes a novel method for inducing lexicons of in-group language, which
incorporates its socio-temporal context. Existing methods for lexicon induction
do not capture the evolving nature of in-group language, nor the social
structure of the community. Using dynamic word and user embeddings trained on
conversations from online anti-women communities, our approach outperforms
prior methods for lexicon induction. We develop a test set for the task of
lexicon induction and a new lexicon of manosphere language, validated by human
experts, which quantifies the relevance of each term to a specific
sub-community at a given point in time. Finally, we present novel insights on
in-group language which illustrate the utility of this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Intention To Implementation: Automating Biomedical Research via
  LLMs <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09429v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09429v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Chen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional biomedical research is increasingly labor-intensive due to the
exponential growth of scientific literature and datasets. Artificial
intelligence (AI), particularly Large Language Models (LLMs), has the potential
to revolutionize this process by automating various steps. Still, significant
challenges remain, including the need for multidisciplinary expertise,
logicality of experimental design, and performance measurements. This paper
introduces BioResearcher, the first end-to-end automated system designed to
streamline the entire biomedical research process involving dry lab
experiments. BioResearcher employs a modular multi-agent architecture,
integrating specialized agents for search, literature processing, experimental
design, and programming. By decomposing complex tasks into logically related
sub-tasks and utilizing a hierarchical learning approach, BioResearcher
effectively addresses the challenges of multidisciplinary requirements and
logical complexity. Furthermore, BioResearcher incorporates an LLM-based
reviewer for in-process quality control and introduces novel evaluation metrics
to assess the quality and automation of experimental protocols. BioResearcher
successfully achieves an average execution success rate of 63.07% across eight
previously unmet research objectives. The generated protocols, on average,
outperform typical agent systems by 22.0% on five quality metrics. The system
demonstrates significant potential to reduce researchers' workloads and
accelerate biomedical discoveries, paving the way for future innovations in
automated research systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in SCIENCE CHINA Information Sciences. If you find our work
  useful, please cite us as: @article{ BioResearcher, author = "Yi Luo and
  Linghang Shi and Yihao Li and Aobo Zhuang and Yeyun Gong and Ling Liu and
  Chen Lin", title = "From Intention To Implementation: Automating Biomedical
  Research via LLMs", journal = "SCIENCE CHINA Information Sciences", year =
  "2025" }</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Options Are Created Equal: Textual Option Weighting for
  Token-Efficient LLM-Based Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JongWoo Kim, SeongYeub Chu, Bryan Wong, Mun Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently emerged as promising tools for
knowledge tracing (KT) due to their strong reasoning and generalization
abilities. While recent LLM-based KT methods have proposed new prompt formats,
they struggle to represent the full interaction histories of example learners
within a single prompt during in-context learning (ICL), resulting in limited
scalability and high computational cost under token constraints. In this work,
we present \textit{LLM-based Option-weighted Knowledge Tracing (LOKT)}, a
simple yet effective framework that encodes the interaction histories of
example learners in context as \textit{textual categorical option weights
(TCOW)}. TCOW are semantic labels (e.g., ``inadequate'') assigned to the
options selected by learners when answering questions, enhancing the
interpretability of LLMs. Experiments on multiple-choice datasets show that
LOKT outperforms existing non-LLM and LLM-based KT models in both cold-start
and warm-start settings. Moreover, LOKT enables scalable and cost-efficient
inference, achieving strong performance even under strict token constraints.
Our code is available at
\href{https://anonymous.4open.science/r/LOKT_model-3233}{https://anonymous.4open.science/r/LOKT\_model-3233}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rectified Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient long-sequence generation is a critical challenge for Large Language
Models. While recent sparse decoding methods improve efficiency, they suffer
from KV cache misalignment, where approximation errors accumulate and degrade
generation quality. In this work, we propose Rectified Sparse Attention (ReSA),
a simple yet effective method that combines block-sparse attention with
periodic dense rectification. By refreshing the KV cache at fixed intervals
using a dense forward pass, ReSA bounds error accumulation and preserves
alignment with the pretraining distribution. Experiments across math reasoning,
language modeling, and retrieval tasks demonstrate that ReSA achieves
near-lossless generation quality with significantly improved efficiency.
Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at
256K sequence length, making it a practical solution for scalable long-context
inference. Code is available at https://aka.ms/ReSA-LM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://modomodo-rl.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal
  Understanding in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, Fahad Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning in real-world video settings presents a fundamentally
different challenge than in static images or text. It requires interpreting
fine-grained visual information, accurately reading handwritten or digital
text, and integrating spoken cues, often dispersed non-linearly over time. In
such multimodal contexts, success hinges not just on perception, but on
selectively identifying and integrating the right contextual details from a
rich and noisy stream of content. To this end, we introduce VideoMathQA, a
benchmark designed to evaluate whether models can perform such temporally
extended cross-modal reasoning on videos. The benchmark spans 10 diverse
mathematical domains, covering videos ranging from 10 seconds to over 1 hour.
It requires models to interpret structured visual content, understand
instructional narratives, and jointly ground concepts across visual, audio, and
textual modalities. We employ graduate-level experts to ensure high quality,
totaling over $920$ man-hours of annotation. To reflect real-world scenarios,
questions are designed around three core reasoning challenges: direct problem
solving, where answers are grounded in the presented question; conceptual
transfer, which requires applying learned methods to new problems; and deep
instructional comprehension, involving multi-step reasoning over extended
explanations and partially worked-out solutions. Each question includes
multi-step reasoning annotations, enabling fine-grained diagnosis of model
capabilities. Through this benchmark, we highlight the limitations of existing
approaches and establish a systematic evaluation framework for models that must
reason, rather than merely perceive, across temporally extended and
modality-rich mathematical problem settings. Our benchmark and evaluation code
are available at: https://mbzuai-oryx.github.io/VideoMathQA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VideoMathQA Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Stoica, Vivek Ramanujan, Xiang Fan, Ali Farhadi, Ranjay Krishna, Judy Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unconditional flow-matching trains diffusion models to transport samples from
a source distribution to a target distribution by enforcing that the flows
between sample pairs are unique. However, in conditional settings (e.g.,
class-conditioned models), this uniqueness is no longer guaranteed--flows from
different conditions may overlap, leading to more ambiguous generations. We
introduce Contrastive Flow Matching, an extension to the flow matching
objective that explicitly enforces uniqueness across all conditional flows,
enhancing condition separation. Our approach adds a contrastive objective that
maximizes dissimilarities between predicted flows from arbitrary sample pairs.
We validate Contrastive Flow Matching by conducting extensive experiments
across varying model architectures on both class-conditioned (ImageNet-1k) and
text-to-image (CC3M) benchmarks. Notably, we find that training models with
Contrastive Flow Matching (1) improves training speed by a factor of up to 9x,
(2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9
compared to training the same models with flow matching. We release our code
at: https://github.com/gstoica27/DeltaFM.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene
  Reconstruction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of reconstructing dynamic 3D scenes with
complex motions. Some recent works define 3D Gaussian primitives in the
canonical space and use deformation fields to map canonical primitives to
observation spaces, achieving real-time dynamic view synthesis. However, these
methods often struggle to handle scenes with complex motions due to the
difficulty of optimizing deformation fields. To overcome this problem, we
propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives
to appear at arbitrary time and locations. In contrast to canonical Gaussian
primitives, our representation possesses the strong flexibility, thus improving
the ability to model dynamic 3D scenes. In addition, we endow each Gaussian
primitive with an motion function, allowing it to move to neighboring regions
over time, which reduces the temporal redundancy. Experiments results on
several datasets show that the rendering quality of our method outperforms
recent methods by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; Project page: https://zju3dv.github.io/freetimegs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) are commonly derived by extending
pre-trained Large Language Models (LLMs) with visual capabilities. In this
work, we investigate how MLLMs process visual inputs by analyzing their
attention mechanisms. We reveal a surprising sparsity phenomenon: only a small
subset (approximately less than 5%) of attention heads in LLMs actively
contribute to visual understanding, termed visual heads. To identify these
heads efficiently, we design a training-free framework that quantifies
head-level visual relevance through targeted response analysis. Building on
this discovery, we introduce SparseMM, a KV-Cache optimization strategy that
allocates asymmetric computation budgets to heads in LLMs based on their visual
scores, leveraging the sparity of visual heads for accelerating the inference
of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the
particularity of visual, SparseMM prioritizes stress and retaining visual
semantics during decoding. Extensive evaluations across mainstream multimodal
benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency
trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%
memory reduction during generation while maintaining performance parity on
efficiency test. Our project is open sourced at
https://github.com/CR400AF-A/SparseMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Inverse Rendering from Propagating Light 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anagh Malik, Benjamin Attal, Andrew Xie, Matthew O'Toole, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first system for physically based, neural inverse rendering
from multi-viewpoint videos of propagating light. Our approach relies on a
time-resolved extension of neural radiance caching -- a technique that
accelerates inverse rendering by storing infinite-bounce radiance arriving at
any point from any direction. The resulting model accurately accounts for
direct and indirect light transport effects and, when applied to captured
measurements from a flash lidar system, enables state-of-the-art 3D
reconstruction in the presence of strong indirect light. Further, we
demonstrate view synthesis of propagating light, automatic decomposition of
captured measurements into direct and indirect components, as well as novel
capabilities such as multi-view time-resolved relighting of captured scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://anaghmalik.com/InvProp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContentV: Efficient Training of Video Generation Models with Limited
  Compute 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenfeng Lin, Renjie Chen, Boyuan Liu, Shiyue Yan, Ruoyu Feng, Jiangchuan Wei, Yichen Zhang, Yimeng Zhou, Chao Feng, Jiao Ran, Qi Wu, Zuotao Liu, Mingyu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video generation demand increasingly efficient training
recipes to mitigate escalating computational costs. In this report, we present
ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art
performance (85.14 on VBench) after training on 256 x 64GB Neural Processing
Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality
videos across multiple resolutions and durations from text prompts, enabled by
three key innovations: (1) A minimalist architecture that maximizes reuse of
pre-trained image generation models for video generation; (2) A systematic
multi-stage training strategy leveraging flow matching for enhanced efficiency;
and (3) A cost-effective reinforcement learning with human feedback framework
that improves generation quality without requiring additional human
annotations. All the code and models are available at:
https://contentv.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://contentv.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refer to Anything with Vision-Language <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengcao Cao, Zijun Wei, Jason Kuen, Kangning Liu, Lingzhi Zhang, Jiuxiang Gu, HyunJoon Jung, Liang-Yan Gui, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image segmentation models have advanced to segment images into
high-quality masks for visual entities, and yet they cannot provide
comprehensive semantic understanding for complex queries based on both language
and vision. This limitation reduces their effectiveness in applications that
require user-friendly interactions driven by vision-language prompts. To bridge
this gap, we introduce a novel task of omnimodal referring expression
segmentation (ORES). In this task, a model produces a group of masks based on
arbitrary prompts specified by text only or text plus reference visual
entities. To address this new challenge, we propose a novel framework to "Refer
to Any Segmentation Mask Group" (RAS), which augments segmentation models with
complex multimodal interactions and comprehension via a mask-centric large
multimodal model. For training and benchmarking ORES models, we create datasets
MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by
text and reference entities. Through extensive evaluation, we demonstrate
superior performance of RAS on our new ORES task, as well as classic referring
expression segmentation (RES) and generalized referring expression segmentation
(GRES) tasks. Project page: https://Ref2Any.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via
  Spatial Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Ran, Yixuan Li, Linning Xu, Mulin Yu, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic 3D indoor scene synthesis is vital for embodied AI and digital
content creation. It can be naturally divided into two subtasks: object
generation and layout generation. While recent generative models have
significantly advanced object-level quality and controllability, layout
generation remains challenging due to limited datasets. Existing methods either
overfit to these datasets or rely on predefined constraints to optimize
numerical layout that sacrifice flexibility. As a result, they fail to generate
scenes that are both open-vocabulary and aligned with fine-grained user
instructions. We introduce DirectLayout, a framework that directly generates
numerical 3D layouts from text descriptions using generalizable spatial
reasoning of large language models (LLMs). DirectLayout decomposes the
generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting
it into 3D space, and refining object placements. To enable explicit spatial
reasoning and help the model grasp basic principles of object placement, we
employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.
Additionally, we design CoT-Grounded Generative Layout Reward to enhance
generalization and spatial planning. During inference, DirectLayout addresses
asset-layout mismatches via Iterative Asset-Layout Alignment through in-context
learning. Extensive experiments demonstrate that DirectLayout achieves
impressive semantic consistency, generalization and physical plausibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://directlayout.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defurnishing with X-Ray Vision: Joint Removal of Furniture from
  Panoramas and Mesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Dolhasz, Chen Ma, Dave Gausebeck, Kevin Chen, Gregor Miller, Lucas Hayne, Gunnar Hovden, Azwad Sabik, Olaf Brandt, Mira Slavcheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a pipeline for generating defurnished replicas of indoor spaces
represented as textured meshes and corresponding multi-view panoramic images.
To achieve this, we first segment and remove furniture from the mesh
representation, extend planes, and fill holes, obtaining a simplified
defurnished mesh (SDM). This SDM acts as an ``X-ray'' of the scene's underlying
structure, guiding the defurnishing process. We extract Canny edges from depth
and normal images rendered from the SDM. We then use these as a guide to remove
the furniture from panorama images via ControlNet inpainting. This control
signal ensures the availability of global geometric information that may be
hidden from a particular panoramic view by the furniture being removed. The
inpainted panoramas are used to texture the mesh. We show that our approach
produces higher quality assets than methods that rely on neural radiance
fields, which tend to produce blurry low-resolution images, or RGB-D
inpainting, which is highly susceptible to hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoMolmo: Spatio-Temporal Grounding Meets Pointing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghazi Shazan Ahmad, Ahmed Heakl, Hanan Gani, Abdelrahman Shaker, Zhiqiang Shen, Ranjay Krishna, Fahad Shahbaz Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal localization is vital for precise interactions across diverse
domains, from biological research to autonomous navigation and interactive
interfaces. Current video-based approaches, while proficient in tracking, lack
the sophisticated reasoning capabilities of large language models, limiting
their contextual understanding and generalization. We introduce VideoMolmo, a
large multimodal model tailored for fine-grained spatio-temporal pointing
conditioned on textual descriptions. Building upon the Molmo architecture,
VideoMolmo incorporates a temporal module utilizing an attention mechanism to
condition each frame on preceding frames, ensuring temporal consistency.
Additionally, our novel temporal mask fusion pipeline employs SAM2 for
bidirectional point propagation, significantly enhancing coherence across video
sequences. This two-step decomposition, i.e., first using the LLM to generate
precise pointing coordinates, then relying on a sequential mask-fusion module
to produce coherent segmentation, not only simplifies the task for the language
model but also enhances interpretability. Due to the lack of suitable datasets,
we curate a comprehensive dataset comprising 72k video-caption pairs annotated
with 100k object points. To evaluate the generalization of VideoMolmo, we
introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five
real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,
Video-GUI Interaction, and Robotics. We also evaluate our model on Referring
Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to
existing models, VideoMolmo substantially improves spatio-temporal pointing
accuracy and reasoning capability. Our code and models are publicly available
at https://github.com/mbzuai-oryx/VideoMolmo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Hour-Scale Video Training for Long Video-Language
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent long-form video-language understanding benchmarks have driven progress
in video large multimodal models (Video-LMMs). However, the scarcity of
well-annotated long videos has left the training of hour-long Video-LLMs
underexplored. To close this gap, we present VideoMarathon, a large-scale
hour-long video instruction-following dataset. This dataset includes around
9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60
minutes per video. Specifically, it contains 3.3M high-quality QA pairs,
spanning six fundamental topics: temporality, spatiality, object, action,
scene, and event. Compared to existing video instruction datasets,
VideoMarathon significantly extends training video durations up to 1 hour, and
supports 22 diverse tasks requiring both short- and long-term video
comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and
efficient Video-LMM for hour-scale video-language modeling. It enables
hour-long video training and inference at 1-FPS sampling by leveraging a memory
augmentation module, which adaptively integrates user question-relevant and
spatiotemporal-informative semantics from a cached full video context. In our
experiments, Hour-LLaVA achieves the best performance on multiple long
video-language benchmarks, demonstrating the high quality of the VideoMarathon
dataset and the superiority of the Hour-LLaVA model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://videomarathon.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical
  Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large
Language Models (LLMs), but it still remains challenging for extending it to
multimodal domains. Existing works either adopt a similar textual reasoning for
image input, or seek to interleave visual signals into mathematical CoT.
However, they face three key limitations for math problem-solving: reliance on
coarse-grained box-shaped image regions, limited perception of vision encoders
on math content, and dependence on external capabilities for visual
modification. In this paper, we propose MINT-CoT, introducing Mathematical
INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively
interleaves relevant visual tokens into textual reasoning steps via an
Interleave Token, which dynamically selects visual regions of any shapes within
math figures. To empower this capability, we construct the MINT-CoT dataset,
containing 54K mathematical problems aligning each reasoning step with visual
regions at the token level, accompanied by a rigorous data generation pipeline.
We further present a three-stage MINT-CoT training strategy, progressively
combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which
derives our MINT-CoT-7B model. Extensive experiments demonstrate the
effectiveness of our method for effective visual interleaved reasoning in
mathematical domains, where MINT-CoT-7B outperforms the baseline model by
+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our
code and data are available at https://github.com/xinyan-cxy/MINT-CoT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is released at https://github.com/xinyan-cxy/MINT-CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual
  Counting for MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lidong Lu, Guo Chen, Zhiqi Li, Yicheng Liu, Tong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in video understanding, current MLLMs struggle with counting
tasks. Existing benchmarks are limited by short videos, close-set queries, lack
of clue annotations, and weak multimodal coverage. In this paper, we introduce
CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with
1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It
supports both black-box and white-box evaluation, serving as a comprehensive
testbed for both end-to-end and reasoning-based counting. To explore ways to
improve model's counting capability, we propose AV-Reasoner, a model trained
with GRPO and curriculum learning to generalize counting ability from related
tasks. AV-Reasoner achieves state-of-the-art results across multiple
benchmarks, demonstrating the effectiveness of reinforcement learning. However,
experiments show that on out-of-domain benchmarks, reasoning in the language
space fails to bring performance gains. The code and benchmark have been
realeased on https://av-reasoner.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duochao Shi, Weijie Wang, Donny Y. Chen, Zeyu Zhang, Jia-Wang Bian, Bohan Zhuang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)
pipelines by unprojecting them into 3D point clouds for novel view synthesis.
This approach offers advantages such as efficient training, the use of known
camera poses, and accurate geometry estimation. However, depth discontinuities
at object boundaries often lead to fragmented or sparse point clouds, degrading
rendering quality -- a well-known limitation of depth-based representations. To
tackle this issue, we introduce PM-Loss, a novel regularization loss based on a
pointmap predicted by a pre-trained transformer. Although the pointmap itself
may be less accurate than the depth map, it effectively enforces geometric
smoothness, especially around object boundaries. With the improved depth map,
our method significantly improves the feed-forward 3DGS across various
architectures and scenes, delivering consistently better rendering results. Our
project page: https://aim-uofa.github.io/PMLoss
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://aim-uofa.github.io/PMLoss</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Your 3D Encoder Really Work? When <span class="highlight-title">Pretrain</span>-SFT from 2D VLMs Meets
  3D VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Yanpeng Zhou, Yufei Gao, Tao Tang, Jianhua Han, Yujie Yuan, Dave Zhenyu Chen, Jiawang Bian, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remarkable progress in 2D Vision-Language Models (VLMs) has spurred interest
in extending them to 3D settings for tasks like 3D Question Answering, Dense
Captioning, and Visual Grounding. Unlike 2D VLMs that typically process images
through an image encoder, 3D scenes, with their intricate spatial structures,
allow for diverse model architectures. Based on their encoder design, this
paper categorizes recent 3D VLMs into 3D object-centric, 2D image-based, and 3D
scene-centric approaches. Despite the architectural similarity of 3D
scene-centric VLMs to their 2D counterparts, they have exhibited comparatively
lower performance compared with the latest 3D object-centric and 2D image-based
approaches. To understand this gap, we conduct an in-depth analysis, revealing
that 3D scene-centric VLMs show limited reliance on the 3D scene encoder, and
the pre-train stage appears less effective than in 2D VLMs. Furthermore, we
observe that data scaling benefits are less pronounced on larger datasets. Our
investigation suggests that while these models possess cross-modal alignment
capabilities, they tend to over-rely on linguistic cues and overfit to frequent
answer distributions, thereby diminishing the effective utilization of the 3D
encoder. To address these limitations and encourage genuine 3D scene
understanding, we introduce a novel 3D Relevance Discrimination QA dataset
designed to disrupt shortcut learning and improve 3D understanding. Our
findings highlight the need for advanced evaluation and improved strategies for
better 3D understanding in 3D VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rho, Jun Myeong Choi, Biswadip Dey, Roni Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural rendering has made significant strides in 3D reconstruction and novel
view synthesis. With the integration with physics, it opens up new
applications. The inverse problem of estimating physics from visual data,
however, still remains challenging, limiting its effectiveness for applications
like physically accurate digital twin creation in robotics and XR. Existing
methods that incorporate physics into neural rendering frameworks typically
require dense multi-view videos as input, making them impractical for scalable,
real-world use. When presented with sparse multi-view videos, the sequential
optimization strategy used by existing approaches introduces significant error
accumulation, e.g., poor initial 3D reconstruction leads to bad material
parameter estimation in subsequent stages. Instead of sequential optimization,
directly optimizing all parameters at the same time also fails due to the
highly non-convex and often non-differentiable nature of the problem. We
propose ProJo4D, a progressive joint optimization framework that gradually
increases the set of jointly optimized parameters guided by their sensitivity,
leading to fully joint optimization over geometry, appearance, physical state,
and material property. Evaluations on PAC-NeRF and Spring-Gaus datasets show
that ProJo4D outperforms prior work in 4D future state prediction, novel view
rendering of future state, and material parameter estimation, demonstrating its
effectiveness in physically grounded 4D scene understanding. For demos, please
visit the project webpage: https://daniel03c1.github.io/ProJo4D/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARBLE: Material Recomposition and Blending in CLIP-Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ta-Ying Cheng, Prafull Sharma, Mark Boss, Varun Jampani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Editing materials of objects in images based on exemplar images is an active
area of research in computer vision and graphics. We propose MARBLE, a method
for performing material blending and recomposing fine-grained material
properties by finding material embeddings in CLIP-space and using that to
control pre-trained text-to-image models. We improve exemplar-based material
editing by finding a block in the denoising UNet responsible for material
attribution. Given two material exemplar-images, we find directions in the
CLIP-space for blending the materials. Further, we can achieve parametric
control over fine-grained material attributes such as roughness, metallic,
transparency, and glow using a shallow network to predict the direction for the
desired material attribute change. We perform qualitative and quantitative
analysis to demonstrate the efficacy of our proposed method. We also present
the ability of our method to perform multiple edits in a single forward pass
and applicability to painting.
  Project Page: https://marblecontrol.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaf Dünkel, Thomas Wimmer, Christian Theobalt, Christian Rupprecht, Adam Kortylewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding correspondences between semantically similar points across images and
object instances is one of the everlasting challenges in computer vision. While
large pre-trained vision models have recently been demonstrated as effective
priors for semantic matching, they still suffer from ambiguities for symmetric
objects or repeated object parts. We propose to improve semantic correspondence
estimation via 3D-aware pseudo-labeling. Specifically, we train an adapter to
refine off-the-shelf features using pseudo-labels obtained via 3D-aware
chaining, filtering wrong labels through relaxed cyclic consistency, and 3D
spherical prototype mapping constraints. While reducing the need for dataset
specific annotations compared to prior work, we set a new state-of-the-art on
SPair-71k by over 4% absolute gain and by over 7% against methods with similar
supervision requirements. The generality of our proposed approach simplifies
extension of training to other data sources, which we demonstrate in our
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://genintel.github.io/DIY-SC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceive Anything: Recognize, Explain, Caption, and Segment Anything in
  Images and Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Perceive Anything Model (PAM), a conceptually straightforward and
efficient framework for comprehensive region-level visual understanding in
images and videos. Our approach extends the powerful segmentation model SAM 2
by integrating Large Language Models (LLMs), enabling simultaneous object
segmentation with the generation of diverse, region-specific semantic outputs,
including categories, label definition, functional explanations, and detailed
captions. A key component, Semantic Perceiver, is introduced to efficiently
transform SAM 2's rich visual features, which inherently carry general vision,
localization, and semantic priors into multi-modal tokens for LLM
comprehension. To support robust multi-granularity understanding, we also
develop a dedicated data refinement and augmentation pipeline, yielding a
high-quality dataset of 1.5M image and 0.6M video region-semantic annotations,
including novel region-level streaming video caption data. PAM is designed for
lightweightness and efficiency, while also demonstrates strong performance
across a diverse range of region understanding tasks. It runs 1.2-2.4x faster
and consumes less GPU memory than prior approaches, offering a practical
solution for real-world applications. We believe that our effective approach
will serve as a strong baseline for future research in region-level visual
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures, Website: https://Perceive-Anything.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeedVR2: One-Step Video Restoration via Diffusion Adversarial
  Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion-based video restoration (VR) demonstrate
significant improvement in visual quality, yet yield a prohibitive
computational cost during inference. While several distillation-based
approaches have exhibited the potential of one-step image restoration,
extending existing approaches to VR remains challenging and underexplored,
particularly when dealing with high-resolution video in real-world settings. In
this work, we propose a one-step diffusion-based VR model, termed as SeedVR2,
which performs adversarial VR training against real data. To handle the
challenging high-resolution VR within a single step, we introduce several
enhancements to both model architecture and training procedures. Specifically,
an adaptive window attention mechanism is proposed, where the window size is
dynamically adjusted to fit the output resolutions, avoiding window
inconsistency observed under high-resolution VR using window attention with a
predefined window size. To stabilize and improve the adversarial post-training
towards VR, we further verify the effectiveness of a series of losses,
including a proposed feature matching loss without significantly sacrificing
training efficiency. Extensive experiments show that SeedVR2 can achieve
comparable or even better performance compared with existing VR approaches in a
single step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft Ver. Project page: https://iceclear.github.io/projects/seedvr2/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DM-SegNet: Dual-Mamba Architecture for 3D Medical Image Segmentation
  with Global Context Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D medical image segmentation demands architectures capable of
reconciling global context modeling with spatial topology preservation. While
State Space Models (SSMs) like Mamba show potential for sequence modeling,
existing medical SSMs suffer from encoder-decoder incompatibility: the
encoder's 1D sequence flattening compromises spatial structures, while
conventional decoders fail to leverage Mamba's state propagation. We present
DM-SegNet, a Dual-Mamba architecture integrating directional state transitions
with anatomy-aware hierarchical decoding. The core innovations include a
quadri-directional spatial Mamba module employing four-directional 3D scanning
to maintain anatomical spatial coherence, a gated spatial convolution layer
that enhances spatially sensitive feature representation prior to state
modeling, and a Mamba-driven decoding framework enabling bidirectional state
synchronization across scales. Extensive evaluation on two clinically
significant benchmarks demonstrates the efficacy of DM-SegNet: achieving
state-of-the-art Dice Similarity Coefficient (DSC) of 85.44% on the Synapse
dataset for abdominal organ segmentation and 90.22% on the BraTS2023 dataset
for brain tumor segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AliTok: Towards Sequence Modeling Alignment between Tokenizer and
  Autoregressive Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingyu Wu, Kai Zhu, Yu Liu, Longxiang Tang, Jian Yang, Yansong Peng, Wei Zhai, Yang Cao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive image generation aims to predict the next token based on
previous ones. However, existing image tokenizers encode tokens with
bidirectional dependencies during the compression process, which hinders the
effective modeling by autoregressive models. In this paper, we propose a novel
Aligned Tokenizer (AliTok), which utilizes a causal decoder to establish
unidirectional dependencies among encoded tokens, thereby aligning the token
modeling approach between the tokenizer and autoregressive model. Furthermore,
by incorporating prefix tokens and employing two-stage tokenizer training to
enhance reconstruction consistency, AliTok achieves great reconstruction
performance while being generation-friendly. On ImageNet-256 benchmark, using a
standard decoder-only autoregressive model as the generator with only 177M
parameters, AliTok achieves a gFID score of 1.50 and an IS of 305.9. When the
parameter count is increased to 662M, AliTok achieves a gFID score of 1.35,
surpassing the state-of-the-art diffusion method with 10x faster sampling
speed. The code and weights are available at
https://github.com/ali-vilab/alitok.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ali-vilab/alitok</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an
  Egocentric World? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Yuan, Ronghao Dang, Long Li, Wentong Li, Dian Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of multimodal large language models (MLLMs) has driven
breakthroughs in egocentric vision applications. These applications necessitate
persistent, context-aware understanding of objects, as users interact with
tools in dynamic and cluttered environments. However, existing embodied
benchmarks primarily focus on static scene exploration, emphasizing object's
appearance and spatial attributes while neglecting the assessment of dynamic
changes arising from users' interactions. To address this gap, we introduce
EOC-Bench, an innovative benchmark designed to systematically evaluate
object-centric embodied cognition in dynamic egocentric scenarios. Specially,
EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three
temporal categories: Past, Present, and Future, covering 11 fine-grained
evaluation dimensions and 3 visual object referencing types. To ensure thorough
assessment, we develop a mixed-format human-in-the-loop annotation framework
with four types of questions and design a novel multi-scale temporal accuracy
metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct
comprehensive evaluations of various proprietary, open-source, and object-level
MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object
cognitive capabilities of MLLMs, establishing a robust foundation for
developing reliable core models for embodied systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Vision Concept <span class="highlight-title">Transformer</span>s for Medical Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Hu, Songning Lai, Yuan Hua, Shu Yang, Jingfeng Zhang, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparency is a paramount concern in the medical field, prompting
researchers to delve into the realm of explainable AI (XAI). Among these XAI
methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent
space to human-understandable high-level concepts by generating a conceptual
layer for extracting conceptual features, which has drawn much attention
recently. However, existing methods rely solely on concept features to
determine the model's predictions, which overlook the intrinsic feature
embeddings within medical images. To address this utility gap between the
original models and concept-based models, we propose Vision Concept Transformer
(VCT). Furthermore, despite their benefits, CBMs have been found to negatively
impact model performance and fail to provide stable explanations when faced
with input perturbations, which limits their application in the medical field.
To address this faithfulness issue, this paper further proposes the Stable
Vision Concept Transformer (SVCT) based on VCT, which leverages the vision
transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT
employs conceptual features to enhance decision-making capabilities by fusing
them with image features and ensures model faithfulness through the integration
of Denoised Diffusion Smoothing. Comprehensive experiments on four medical
datasets demonstrate that our VCT and SVCT maintain accuracy while remaining
interpretable compared to baselines. Furthermore, even when subjected to
perturbations, our SVCT model consistently provides faithful explanations, thus
meeting the needs of the medical field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2304.06129 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bardienus P. Duisterhof, Jan Oberst, Bowen Wen, Stan Birchfield, Deva Ramanan, Jeffrey Ichnowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape completion has broad applications in robotics, digital twin
reconstruction, and extended reality (XR). Although recent advances in 3D
object and scene completion have achieved impressive results, existing methods
lack 3D consistency, are computationally expensive, and struggle to capture
sharp object boundaries. Our work (RaySt3R) addresses these limitations by
recasting 3D shape completion as a novel view synthesis problem. Specifically,
given a single RGB-D image and a novel viewpoint (encoded as a collection of
query rays), we train a feedforward transformer to predict depth maps, object
masks, and per-pixel confidence scores for those query rays. RaySt3R fuses
these predictions across multiple query views to reconstruct complete 3D
shapes. We evaluate RaySt3R on synthetic and real-world datasets, and observe
it achieves state-of-the-art performance, outperforming the baselines on all
datasets by up to 44% in 3D chamfer distance. Project page:
https://rayst3r.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video World Models with Long-term Spatial Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging world models autoregressively generate video frames in response to
actions, such as camera movements and text prompts, among other control
signals. Due to limited temporal context window sizes, these models often
struggle to maintain scene consistency during revisits, leading to severe
forgetting of previously generated environments. Inspired by the mechanisms of
human memory, we introduce a novel framework to enhancing long-term consistency
of video world models through a geometry-grounded long-term spatial memory. Our
framework includes mechanisms to store and retrieve information from the
long-term spatial memory and we curate custom datasets to train and evaluate
world models with explicitly stored 3D memory mechanisms. Our evaluations show
improved quality, consistency, and context length compared to relevant
baselines, paving the way towards long-term consistent world generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://spmem.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rectified Point Flow: Generic Point Cloud Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Sun, Liyuan Zhu, Shengyu Huang, Shuran Song, Iro Armeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Rectified Point Flow, a unified parameterization that formulates
pairwise point cloud registration and multi-part shape assembly as a single
conditional generative problem. Given unposed point clouds, our method learns a
continuous point-wise velocity field that transports noisy points toward their
target positions, from which part poses are recovered. In contrast to prior
work that regresses part-wise poses with ad-hoc symmetry handling, our method
intrinsically learns assembly symmetries without symmetry labels. Together with
a self-supervised encoder focused on overlapping points, our method achieves a
new state-of-the-art performance on six benchmarks spanning pairwise
registration and shape assembly. Notably, our unified formulation enables
effective joint training on diverse datasets, facilitating the learning of
shared geometric priors and consequently boosting accuracy. Project page:
https://rectified-pointflow.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://rectified-pointflow.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Wang, Yuantao Chen, Lixing Xiao, Weiqing Xiao, Bohan Li, Zhaoxi Chen, Chongjie Ye, Shaocong Xu, Saining Zhang, Ziyang Yan, Pierre Merriaux, Lei Lei, Tianfan Xue, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely
on photometric consistency to produce high-quality reconstructions. However, in
real-world scenarios, it is challenging to guarantee perfect photometric
consistency in acquired images. Appearance codes have been widely used to
address this issue, but their modeling capability is limited, as a single code
is applied to the entire image. Recently, the bilateral grid was introduced to
perform pixel-wise color mapping, but it is difficult to optimize and constrain
effectively. In this paper, we propose a novel multi-scale bilateral grid that
unifies appearance codes and bilateral grids. We demonstrate that this approach
significantly improves geometric accuracy in dynamic, decoupled autonomous
driving scene reconstruction, outperforming both appearance codes and bilateral
grids. This is crucial for autonomous driving, where accurate geometry is
important for obstacle avoidance and control. Our method shows strong results
across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further
demonstrate that the improvement in geometry is driven by the multi-scale
bilateral grid, which effectively reduces floaters caused by photometric
inconsistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bigcileng.github.io/bilateral-driving; Code:
  https://github.com/BigCiLeng/bilateral-driving</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Play to Replay: Composed Video Retrieval for Temporally
  Fine-Grained Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Animesh Gupta, Jay Parmar, Ishan Rajendrakumar Dave, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Video Retrieval (CoVR) retrieves a target video given a query video
and a modification text describing the intended change. Existing CoVR
benchmarks emphasize appearance shifts or coarse event changes and therefore do
not test the ability to capture subtle, fast-paced temporal differences. We
introduce TF-CoVR, the first large-scale benchmark dedicated to temporally
fine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K
triplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing
on temporal aspect, link each query to a single target segment taken from the
same video, limiting practical usefulness. In TF-CoVR, we instead construct
each <query, modification> pair by prompting an LLM with the label differences
between clips drawn from different videos; every pair is thus associated with
multiple valid target videos (3.9 on average), reflecting real-world tasks such
as sports-highlight generation. To model these temporal dynamics we propose
TF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video
encoder on fine-grained action classification to obtain temporally
discriminative embeddings; (ii) align the composed query with candidate videos
using contrastive learning. We conduct the first comprehensive study of image,
video, and general multimodal embedding (GME) models on temporally fine-grained
composed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,
TF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and
after fine-tuning raises the state-of-the-art from 19.83 to 25.82.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Foundation Models Generalise the Presentation Attack Detection
  Capabilities on ID Cards? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan E. Tapia, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, one of the main challenges in presentation attack detection (PAD)
on ID cards is obtaining generalisation capabilities for a diversity of
countries that are issuing ID cards. Most PAD systems are trained on one, two,
or three ID documents because of privacy protection concerns. As a result, they
do not obtain competitive results for commercial purposes when tested in an
unknown new ID card country. In this scenario, Foundation Models (FM) trained
on huge datasets can help to improve generalisation capabilities. This work
intends to improve and benchmark the capabilities of FM and how to use them to
adapt the generalisation on PAD of ID Documents. Different test protocols were
used, considering zero-shot and fine-tuning and two different ID card datasets.
One private dataset based on Chilean IDs and one open-set based on three ID
countries: Finland, Spain, and Slovakia. Our findings indicate that bona fide
images are the key to generalisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeanPO: Lean Preference Optimization for Likelihood Alignment in
  Video-LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Wang, Jinfa Huang, Li Yuan, Peixi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most Video Large Language Models (Video-LLMs) adopt preference alignment
techniques, e.g., DPO~\citep{rafailov2024dpo}, to optimize the reward margin
between a winning response ($y_w$) and a losing response ($y_l$). However, the
likelihood displacement observed in DPO indicates that both $\log \pi_\theta
(y_w\mid x)$ and $\log \pi_\theta (y_l\mid x) $ often decrease during training,
inadvertently boosting the probabilities of non-target responses. In this
paper, we systematically revisit this phenomenon from LLMs to Video-LLMs,
showing that it intensifies when dealing with the redundant complexity of video
content. To alleviate the impact of this phenomenon, we propose \emph{Lean
Preference Optimization} (LeanPO), a reference-free approach that reformulates
the implicit reward as the average likelihood of the response with respect to
the policy model. A key component of LeanPO is the reward-trustworthiness
correlated self-generated preference data pipeline, which carefully infuses
relevant prior knowledge into the model while continuously refining the
preference data via self-reflection. This allows the policy model to obtain
high-quality paired data and accurately estimate the newly defined reward, thus
mitigating the unintended drop. In addition, we introduce a dynamic label
smoothing strategy that mitigates the impact of noise in responses from diverse
video content, preventing the model from overfitting to spurious details.
Extensive experiments demonstrate that LeanPO significantly enhances the
performance of state-of-the-art Video-LLMs, consistently boosting baselines of
varying capacities with minimal additional training overhead. Moreover, LeanPO
offers a simple yet effective solution for aligning Video-LLM preferences with
human trustworthiness, paving the way toward the reliable and efficient
Video-LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Wang-Xiaodong1899/LeanPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Contrastive Learning for Cross-View Video Localization in
  Unstructured Off-road Terrains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyun Deng, Dongmyeong Lee, Amanda Adkins, Jesse Quattrociocchi, Christian Ellis, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust cross-view 3-DoF localization in GPS-denied, off-road environments
remains challenging due to (1) perceptual ambiguities from repetitive
vegetation and unstructured terrain, and (2) seasonal shifts that significantly
alter scene appearance, hindering alignment with outdated satellite imagery. To
address this, we introduce MoViX, a self-supervised cross-view video
localization framework that learns viewpoint- and season-invariant
representations while preserving directional awareness essential for accurate
localization. MoViX employs a pose-dependent positive sampling strategy to
enhance directional discrimination and temporally aligned hard negative mining
to discourage shortcut learning from seasonal cues. A motion-informed frame
sampler selects spatially diverse frames, and a lightweight temporal aggregator
emphasizes geometrically aligned observations while downweighting ambiguous
ones. At inference, MoViX runs within a Monte Carlo Localization framework,
using a learned cross-view matching module in place of handcrafted models.
Entropy-guided temperature scaling enables robust multi-hypothesis tracking and
confident convergence under visual ambiguity. We evaluate MoViX on the
TartanDrive 2.0 dataset, training on under 30 minutes of data and testing over
12.29 km. Despite outdated satellite imagery, MoViX localizes within 25 meters
of ground truth 93% of the time, and within 50 meters 100% of the time in
unseen regions, outperforming state-of-the-art baselines without
environment-specific tuning. We further demonstrate generalization on a
real-world off-road dataset from a geographically distinct site with a
different robot platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Latent Spaces with Flow Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel framework for aligning learnable latent spaces to
arbitrary target distributions by leveraging flow-based generative models as
priors. Our method first pretrains a flow model on the target features to
capture the underlying distribution. This fixed flow model subsequently
regularizes the latent space via an alignment loss, which reformulates the flow
matching objective to treat the latents as optimization targets. We formally
prove that minimizing this alignment loss establishes a computationally
tractable surrogate objective for maximizing a variational lower bound on the
log-likelihood of latents under the target distribution. Notably, the proposed
method eliminates computationally expensive likelihood evaluations and avoids
ODE solving during optimization. As a proof of concept, we demonstrate in a
controlled setting that the alignment loss landscape closely approximates the
negative log-likelihood of the target distribution. We further validate the
effectiveness of our approach through large-scale image generation experiments
on ImageNet with diverse target distributions, accompanied by detailed
discussions and ablation studies. With both theoretical and empirical
validation, our framework paves a new way for latent space alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAM-aware Test-time Adaptation for Universal Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Wu, Yicheng Wu, Yutong Xie, Wenjia Bai, You Zhang, Feilong Tang, Yulong Li, Yasmeen George, Imran Razzak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal medical image segmentation using the Segment Anything Model (SAM)
remains challenging due to its limited adaptability to medical domains.
Existing adaptations, such as MedSAM, enhance SAM's performance in medical
imaging but at the cost of reduced generalization to unseen data. Therefore, in
this paper, we propose SAM-aware Test-Time Adaptation (SAM-TTA), a
fundamentally different pipeline that preserves the generalization of SAM while
improving its segmentation performance in medical imaging via a test-time
framework. SAM-TTA tackles two key challenges: (1) input-level discrepancies
caused by differences in image acquisition between natural and medical images
and (2) semantic-level discrepancies due to fundamental differences in object
definition between natural and medical domains (e.g., clear boundaries vs.
ambiguous structures). Specifically, our SAM-TTA framework comprises (1)
Self-adaptive Bezier Curve-based Transformation (SBCT), which adaptively
converts single-channel medical images into three-channel SAM-compatible inputs
while maintaining structural integrity, to mitigate the input gap between
medical and natural images, and (2) Dual-scale Uncertainty-driven Mean Teacher
adaptation (DUMT), which employs consistency learning to align SAM's internal
representations to medical semantics, enabling efficient adaptation without
auxiliary supervision or expensive retraining. Extensive experiments on five
public datasets demonstrate that our SAM-TTA outperforms existing TTA
approaches and even surpasses fully fine-tuned models such as MedSAM in certain
scenarios, establishing a new paradigm for universal medical image
segmentation. Code can be found at https://github.com/JianghaoWu/SAM-TTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonkeyOCR: Document Parsing with a Structure-Recognition-Relation
  Triplet Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MonkeyOCR, a vision-language model for document parsing that
advances the state of the art by leveraging a Structure-Recognition-Relation
(SRR) triplet paradigm. This design simplifies what would otherwise be a
complex multi-tool pipeline (as in MinerU's modular approach) and avoids the
inefficiencies of processing full pages with giant end-to-end models (e.g.,
large multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted
into three fundamental questions - "Where is it?" (structure), "What is it?"
(recognition), and "How is it organized?" (relation) - corresponding to layout
analysis, content identification, and logical ordering. This focused
decomposition balances accuracy and speed: it enables efficient, scalable
processing without sacrificing precision. To train and evaluate this approach,
we introduce the MonkeyDoc (the most comprehensive document parsing dataset to
date), with 3.9 million instances spanning over ten document types in both
Chinese and English. Experiments show that MonkeyOCR outperforms MinerU by an
average of 5.1%, with particularly notable improvements on challenging content
such as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter
model surpasses much larger and top-performing models, including Qwen2.5-VL
(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on
English document parsing tasks. In addition, MonkeyOCR processes multi-page
documents significantly faster (0.84 pages per second compared to 0.65 for
MinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed
for inference on a single NVIDIA 3090 GPU. Code and models will be released at
https://github.com/Yuliang-Liu/MonkeyOCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSG-World: Learning a 3D Gaussian World Model from Dual State Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Hu, Xuexiang Wen, Xi Li, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building an efficient and physically consistent world model from limited
observations is a long standing challenge in vision and robotics. Many existing
world modeling pipelines are based on implicit generative models, which are
hard to train and often lack 3D or physical consistency. On the other hand,
explicit 3D methods built from a single state often require multi-stage
processing-such as segmentation, background completion, and inpainting-due to
occlusions. To address this, we leverage two perturbed observations of the same
scene under different object configurations. These dual states offer
complementary visibility, alleviating occlusion issues during state transitions
and enabling more stable and complete reconstruction. In this paper, we present
DSG-World, a novel end-to-end framework that explicitly constructs a 3D
Gaussian World model from Dual State observations. Our approach builds dual
segmentation-aware Gaussian fields and enforces bidirectional photometric and
semantic consistency. We further introduce a pseudo intermediate state for
symmetric alignment and design collaborative co-pruning trategies to refine
geometric completeness. DSG-World enables efficient real-to-simulation transfer
purely in the explicit Gaussian representation space, supporting high-fidelity
rendering and object-level scene manipulation without relying on dense
observations or multi-stage pipelines. Extensive experiments demonstrate strong
generalization to novel views and scene states, highlighting the effectiveness
of our approach for real-world 3D reconstruction and simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Vision-Language-Garment Models For Web Knowledge Garment
  Understanding and Generation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Ackermann, Kiyohiro Nakayama, Guandao Yang, Tong Wu, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models have demonstrated strong generalization, yet
their ability to transfer knowledge to specialized domains such as garment
generation remains underexplored. We introduce VLG, a vision-language-garment
model that synthesizes garments from textual descriptions and visual imagery.
Our experiments assess VLG's zero-shot generalization, investigating its
ability to transfer web-scale reasoning to unseen garment styles and prompts.
Preliminary results indicate promising transfer capabilities, highlighting the
potential for multimodal foundation models to adapt effectively to specialized
domains like fashion design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at MMFM CVPRW'25, code available at
  https://georgenakayama.github.io/AIpparel/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal
  Decoupled Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, breakthroughs in the video diffusion transformer have shown
remarkable capabilities in diverse motion generations. As for the
motion-transfer task, current methods mainly use two-stage Low-Rank Adaptations
(LoRAs) finetuning to obtain better performance. However, existing
adaptation-based motion transfer still suffers from motion inconsistency and
tuning inefficiency when applied to large video diffusion transformers. Naive
two-stage LoRA tuning struggles to maintain motion consistency between
generated and input videos due to the inherent spatial-temporal coupling in the
3D attention operator. Additionally, they require time-consuming fine-tuning
processes in both stages. To tackle these issues, we propose
Follow-Your-Motion, an efficient two-stage video motion transfer framework that
finetunes a powerful video diffusion transformer to synthesize complex
motion.Specifically, we propose a spatial-temporal decoupled LoRA to decouple
the attention architecture for spatial appearance and temporal motion
processing. During the second training stage, we design the sparse motion
sampling and adaptive RoPE to accelerate the tuning speed. To address the lack
of a benchmark for this field, we introduce MotionBench, a comprehensive
benchmark comprising diverse motion, including creative camera motion, single
object motion, multiple object motion, and complex human motion. We show
extensive evaluations on MotionBench to verify the superiority of
Follow-Your-Motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://follow-your-motion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with
  Expanded Field-of-View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Wang, Ziyi Wang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing semantic-aware 3D scenes from sparse views is a challenging
yet essential research direction, driven by the demands of emerging
applications such as virtual reality and embodied AI. Existing per-scene
optimization methods require dense input views and incur high computational
costs, while generalizable approaches often struggle to reconstruct regions
outside the input view cone. In this paper, we propose OGGSplat, an open
Gaussian growing method that expands the field-of-view in generalizable 3D
reconstruction. Our key insight is that the semantic attributes of open
Gaussians provide strong priors for image extrapolation, enabling both semantic
consistency and visual plausibility. Specifically, once open Gaussians are
initialized from sparse views, we introduce an RGB-semantic consistent
inpainting module applied to selected rendered views. This module enforces
bidirectional control between an image diffusion model and a semantic diffusion
model. The inpainted regions are then lifted back into 3D space for efficient
and progressive Gaussian parameter optimization. To evaluate our method, we
establish a Gaussian Outpainting (GO) benchmark that assesses both semantic and
generative quality of reconstructed open-vocabulary scenes. OGGSplat also
demonstrates promising semantic-aware scene reconstruction capabilities when
provided with two view images captured directly from a smartphone camera.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Beyond Detection: Enhancing Contextual Understanding in
  Embodied 3D Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yani Zhang, Dongming Wu, Hao Shi, Yingfei Liu, Tiancai Wang, Haoqiang Fan, Xingping Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied 3D grounding aims to localize target objects described in human
instructions from ego-centric viewpoint. Most methods typically follow a
two-stage paradigm where a trained 3D detector's optimized backbone parameters
are used to initialize a grounding model. In this study, we explore a
fundamental question: Does embodied 3D grounding benefit enough from detection?
To answer this question, we assess the grounding performance of detection
models using predicted boxes filtered by the target category. Surprisingly,
these detection models without any instruction-specific training outperform the
grounding models explicitly trained with language instructions. This indicates
that even category-level embodied 3D grounding may not be well resolved, let
alone more fine-grained context-aware grounding. Motivated by this finding, we
propose DEGround, which shares DETR queries as object representation for both
DEtection and Grounding and enables the grounding to benefit from basic
category classification and box detection. Based on this framework, we further
introduce a regional activation grounding module that highlights
instruction-related regions and a query-wise modulation module that
incorporates sentence-level semantic into the query representation,
strengthening the context-aware understanding of language instructions.
Remarkably, DEGround outperforms state-of-the-art model BIP3D by 7.52\% at
overall accuracy on the EmbodiedScan validation set. The source code will be
publicly available at https://github.com/zyn213/DEGround.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st place on embodiedscan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Cross-Modality Memorization in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Wen, Yangsibo Huang, Tom Goldstein, Ravi Kumar, Badih Ghazi, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding what and how neural networks memorize during training is
crucial, both from the perspective of unintentional memorization of potentially
sensitive information and from the standpoint of effective knowledge
acquisition for real-world, knowledge-intensive tasks. While previous studies
primarily investigate memorization within a single modality, such as text
memorization in large language models or image memorization in diffusion
models, unified multimodal models are becoming increasingly prevalent in
practical applications. In this work, we focus on the unique characteristics of
cross-modality memorization and conduct a systematic study centered on
vision-language models. To facilitate controlled experiments, we first
introduce a synthetic persona dataset comprising diverse synthetic person
images and textual descriptions. We quantify factual knowledge memorization and
cross-modal transferability by training models on a single modality and
evaluating their performance in the other. Our results reveal that facts
learned in one modality transfer to the other, but a significant gap exists
between recalling information in the source and target modalities. Furthermore,
we observe that this gap exists across various scenarios, including more
capable models, machine unlearning, and the multi-hop case. At the end, we
propose a baseline method to mitigate this challenge. We hope our study can
inspire future research on developing more robust multimodal learning
techniques to enhance cross-modal transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Based Autonomous MM-Wave Reflector Using ArUco-Driven
  Angle-of-Arrival Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josue Marroquin, Nan Inzali, Miles Dillon Lantz, Campbell Freeman, Amod Ashtekar, \\Ajinkya Umesh Mulik, Mohammed E Eltayeb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable millimeter-wave (mmWave) communication in non-line-of-sight (NLoS)
conditions remains a major challenge for both military and civilian operations,
especially in urban or infrastructure-limited environments. This paper presents
a vision-aided autonomous reflector system designed to enhance mmWave link
performance by dynamically steering signal reflections using a motorized
metallic plate. The proposed system leverages a monocular camera to detect
ArUco markers on allied transmitter and receiver nodes, estimate their angles
of arrival, and align the reflector in real time for optimal signal
redirection. This approach enables selective beam coverage by serving only
authenticated targets with visible markers and reduces the risk of unintended
signal exposure. The designed prototype, built on a Raspberry Pi 4 and
low-power hardware, operates autonomously without reliance on external
infrastructure or GPS. Experimental results at 60\,GHz demonstrate a 23\,dB
average gain in received signal strength and an 0.89 probability of maintaining
signal reception above a target threshold of -65 dB in an indoor environment,
far exceeding the static and no-reflector baselines. These results demonstrate
the system's potential for resilient and adaptive mmWave connectivity in
complex and dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MokA: Multimodal Low-Rank Adaptation for MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yake Wei, Yu Miao, Dongzhan Zhou, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we reveal that most current efficient multimodal fine-tuning
methods are hindered by a key limitation: they are directly borrowed from LLMs,
often neglecting the intrinsic differences of multimodal scenarios and even
affecting the full utilization of all modalities. Inspired by our empirical
observation, we argue that unimodal adaptation and cross-modal adaptation are
two essential parts for the effective fine-tuning of MLLMs. From this
perspective, we propose Multimodal low-rank Adaptation (MokA), a
multimodal-aware efficient fine-tuning strategy that takes multimodal
characteristics into consideration. It compresses unimodal information by
modality-specific parameters while explicitly enhancing cross-modal
interaction, ensuring both unimodal and cross-modal adaptation. Extensive
experiments cover three representative multimodal scenarios (audio-visual-text,
visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,
Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility
of the proposed method. Ablation studies and efficiency evaluation are also
conducted to fully asses our method. Overall, we think MokA provides a more
targeted solution for efficient adaptation of MLLMs, paving the way for further
exploration. The project page is at https://gewu-lab.github.io/MokA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single GPU Task Adaptation of Pathology Foundation Models for Whole
  Slide Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeraj Kumar, Swaraj Nanda, Siddharth Singi, Jamal Benhamida, David Kim, Jie-Fu Chen, Amir Momeni-Boroujeni, Gregory M. Goldgof, Gabriele Campanella, Chad Vanderbilt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology foundation models (PFMs) have emerged as powerful tools for
analyzing whole slide images (WSIs). However, adapting these pretrained PFMs
for specific clinical tasks presents considerable challenges, primarily due to
the availability of only weak (WSI-level) labels for gigapixel images,
necessitating multiple instance learning (MIL) paradigm for effective WSI
analysis. This paper proposes a novel approach for single-GPU \textbf{T}ask
\textbf{A}daptation of \textbf{PFM}s (TAPFM) that uses vision transformer
(\vit) attention for MIL aggregation while optimizing both for feature
representations and attention weights. The proposed approach maintains separate
computational graphs for MIL aggregator and the PFM to create stable training
dynamics that align with downstream task objectives during end-to-end
adaptation. Evaluated on mutation prediction tasks for bladder cancer and lung
adenocarcinoma across institutional and TCGA cohorts, TAPFM consistently
outperforms conventional approaches, with H-Optimus-0 (TAPFM) outperforming the
benchmarks. TAPFM effectively handles multi-label classification of actionable
mutations as well. Thus, TAPFM makes adaptation of powerful pre-trained PFMs
practical on standard hardware for various clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Any Anomalous Object: A Granular Video Anomaly Detection Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhi Huang, Chenxin Li, Haitao Zhang, Zixu Lin, Yunlong Lin, Hengyu Liu, Wuyang Li, Xinyu Liu, Jiechao Gao, Yue Huang, Xinghao Ding, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video anomaly detection (VAD) is crucial in scenarios such as surveillance
and autonomous driving, where timely detection of unexpected activities is
essential. Although existing methods have primarily focused on detecting
anomalous objects in videos -- either by identifying anomalous frames or
objects -- they often neglect finer-grained analysis, such as anomalous pixels,
which limits their ability to capture a broader range of anomalies. To address
this challenge, we propose a new framework called Track Any Anomalous Object
(TAO), which introduces a granular video anomaly detection pipeline that, for
the first time, integrates the detection of multiple fine-grained anomalous
objects into a unified framework. Unlike methods that assign anomaly scores to
every pixel, our approach transforms the problem into pixel-level tracking of
anomalous objects. By linking anomaly scores to downstream tasks such as
segmentation and tracking, our method removes the need for threshold tuning and
achieves more precise anomaly localization in long and complex video sequences.
Experiments demonstrate that TAO sets new benchmarks in accuracy and
robustness. Project page available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Through-the-Wall Radar Human Activity Recognition WITHOUT Using Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicheng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After a few years of research in the field of through-the-wall radar (TWR)
human activity recognition (HAR), I found that we seem to be stuck in the
mindset of training on radar image data through neural network models. The
earliest related works in this field based on template matching did not require
a training process, and I believe they have never died. Because these methods
possess a strong physical interpretability and are closer to the basis of
theoretical signal processing research. In this paper, I would like to try to
return to the original path by attempting to eschew neural networks to achieve
the TWR HAR task and challenge to achieve intelligent recognition as neural
network models. In detail, the range-time map and Doppler-time map of TWR are
first generated. Then, the initial regions of the human target foreground and
noise background on the maps are determined using corner detection method, and
the micro-Doppler signature is segmented using the multiphase active contour
model. The micro-Doppler segmentation feature is discretized into a
two-dimensional point cloud. Finally, the topological similarity between the
resulting point cloud and the point clouds of the template data is calculated
using Mapper algorithm to obtain the recognition results. The effectiveness of
the proposed method is demonstrated by numerical simulated and measured
experiments. The open-source code of this work is released at:
https://github.com/JoeyBGOfficial/Through-the-Wall-Radar-Human-Activity-Recognition-Without-Using-Neural-Networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRED: The Florence RGB-Event Drone <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Magrini, Niccolò Marini, Federico Becattini, Lorenzo Berlincioni, Niccolò Biondi, Pietro Pala, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small, fast, and lightweight drones present significant challenges for
traditional RGB cameras due to their limitations in capturing fast-moving
objects, especially under challenging lighting conditions. Event cameras offer
an ideal solution, providing high temporal definition and dynamic range, yet
existing benchmarks often lack fine temporal resolution or drone-specific
motion patterns, hindering progress in these areas. This paper introduces the
Florence RGB-Event Drone dataset (FRED), a novel multimodal dataset
specifically designed for drone detection, tracking, and trajectory
forecasting, combining RGB video and event streams. FRED features more than 7
hours of densely annotated drone trajectories, using 5 different drone models
and including challenging scenarios such as rain and adverse lighting
conditions. We provide detailed evaluation protocols and standard metrics for
each task, facilitating reproducible benchmarking. The authors hope FRED will
advance research in high-speed drone perception and multimodal spatiotemporal
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CIVET: Systematic Evaluation of Understanding in VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimo Rizzoli, Simone Alghisi, Olha Khomyn, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Vision-Language Models (VLMs) have achieved competitive performance in
various tasks, their comprehension of the underlying structure and semantics of
a scene remains understudied. To investigate the understanding of VLMs, we
study their capability regarding object properties and relations in a
controlled and interpretable manner. To this scope, we introduce CIVET, a novel
and extensible framework for systematiC evaluatIon Via controllEd sTimuli.
CIVET addresses the lack of standardized systematic evaluation for assessing
VLMs' understanding, enabling researchers to test hypotheses with statistical
rigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of
stimuli, free from annotation noise, dataset-specific biases, and uncontrolled
scene complexity. Our findings reveal that 1) current VLMs can accurately
recognize only a limited set of basic object properties; 2) their performance
heavily depends on the position of the object in the scene; 3) they struggle to
understand basic relations among objects. Furthermore, a comparative evaluation
with human annotators reveals that VLMs still fall short of achieving
human-level accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PixCell: A generative foundation model for digital histopathology images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srikar Yellapragada, Alexandros Graikos, Zilinghan Li, Kostas Triaridis, Varun Belagali, Saarthak Kapse, Tarak Nath Nandi, Ravi K Madduri, Prateek Prasanna, Tahsin Kurc, Rajarsi R. Gupta, Joel Saltz, Dimitris Samaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The digitization of histology slides has revolutionized pathology, providing
massive datasets for cancer diagnosis and research. Contrastive self-supervised
and vision-language models have been shown to effectively mine large pathology
datasets to learn discriminative representations. On the other hand, generative
models, capable of synthesizing realistic and diverse images, present a
compelling solution to address unique problems in pathology that involve
synthesizing images; overcoming annotated data scarcity, enabling
privacy-preserving data sharing, and performing inherently generative tasks,
such as virtual staining. We introduce PixCell, the first diffusion-based
generative foundation model for histopathology. We train PixCell on PanCan-30M,
a vast, diverse dataset derived from 69,184 H\&E-stained whole slide images
covering various cancer types. We employ a progressive training strategy and a
self-supervision-based conditioning that allows us to scale up training without
any annotated data. PixCell generates diverse and high-quality images across
multiple cancer types, which we find can be used in place of real data to train
a self-supervised discriminative model. Synthetic images shared between
institutions are subject to fewer regulatory barriers than would be the case
with real clinical images. Furthermore, we showcase the ability to precisely
control image generation using a small set of annotated images, which can be
used for both data augmentation and educational purposes. Testing on a cell
segmentation task, a mask-guided PixCell enables targeted data augmentation,
improving downstream performance. Finally, we demonstrate PixCell's ability to
use H\&E structural staining to infer results from molecular marker studies; we
use this capability to infer IHC staining from H\&E images. Our trained models
are publicly released to accelerate research in computational pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Manipulation Model for Robust Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Hopf, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deepfake detection models have achieved strong performance even on the
challenging cross-dataset task. However, detection performance under non-ideal
conditions remains very unstable, limiting success on some benchmark datasets
and making it easy to circumvent detection. Inspired by the move to a more
real-world degradation model in the area of image super-resolution, we have
developed a Practical Manipulation Model (PMM) that covers a larger set of
possible forgeries. We extend the space of pseudo-fakes by using Poisson
blending, more diverse masks, generator artifacts, and distractors.
Additionally, we improve the detectors' generality and robustness by adding
strong degradations to the training images. We demonstrate that these changes
not only significantly enhance the model's robustness to common image
degradations but also improve performance on standard benchmark datasets.
Specifically, we show clear increases of $3.51\%$ and $6.21\%$ AUC on the DFDC
and DFDCP datasets, respectively, over the s-o-t-a LAA backbone. Furthermore,
we highlight the lack of robustness in previous detectors and our improvements
in this regard. Code can be found at https://github.com/BenediktHopf/PMM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity
  and Generalization in Text-to-Image Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Revant Teotia, Candace Ross, Karen Ullrich, Sumit Chopra, Adriana Romero-Soriano, Melissa Hall, Matthew J. Muckley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image (T2I) models have achieved impressive
quality and consistency. However, this has come at the cost of representation
diversity. While automatic evaluation methods exist for benchmarking model
diversity, they either require reference image datasets or lack specificity
about the kind of diversity measured, limiting their adaptability and
interpretability. To address this gap, we introduce the Does-it/Can-it
framework, DIM-CIM, a reference-free measurement of default-mode diversity
("Does" the model generate images with expected attributes?) and generalization
capacity ("Can" the model generate diverse attributes for a particular
concept?). We construct the COCO-DIMCIM benchmark, which is seeded with COCO
concepts and captions and augmented by a large language model. With
COCO-DIMCIM, we find that widely-used models improve in generalization at the
cost of default-mode diversity when scaling from 1.5B to 8.1B parameters.
DIMCIM also identifies fine-grained failure cases, such as attributes that are
generated with generic prompts but are rarely generated when explicitly
requested. Finally, we use DIMCIM to evaluate the training data of a T2I model
and observe a correlation of 0.85 between diversity in training images and
default-mode diversity. Our work provides a flexible and interpretable
framework for assessing T2I model diversity and generalization, enabling a more
comprehensive understanding of model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video
  Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haosong Liu, Yuge Cheng, Zihan Liu, Aiyue Chen, Yiwu Yao, Chen Chen, Jingwen Leng, Yu Feng, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video diffusion transformers (vDiTs) have made impressive progress in
text-to-video generation, but their high computational demands present major
challenges for practical deployment. While existing acceleration methods reduce
workload at various granularities, they often rely on heuristics, limiting
their applicability.
  We introduce ASTRAEA, an automatic framework that searches for near-optimal
configurations for vDiT-based video generation. At its core, ASTRAEA proposes a
lightweight token selection mechanism and a memory-efficient, GPU-parallel
sparse attention strategy, enabling linear reductions in execution time with
minimal impact on generation quality. To determine optimal token reduction for
different timesteps, we further design a search framework that leverages a
classic evolutionary algorithm to automatically determine the distribution of
the token budget effectively. Together, ASTRAEA achieves up to 2.4x inference
speedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)
while retaining better video quality compared to the state-of-the-art methods
(<0.5% loss on the VBench score compared to the baseline vDiT models).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial
  Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety
  (TrustFAA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaee Cheong, Yang Liu, Harold Soh, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence and deployment of Emotion AI-powered facial
affect analysis (FAA) tools, concerns about the trustworthiness of these
systems have become more prominent. This first workshop on "Towards Trustworthy
Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and
Safety (TrustFAA)" aims to bring together researchers who are investigating
different challenges in relation to trustworthiness-such as interpretability,
uncertainty, biases, and privacy-across various facial affect analysis tasks,
including macro/ micro-expression recognition, facial action unit detection,
other corresponding applications such as pain and depression detection, as well
as human-robot interaction and collaboration. In alignment with FG2025's
emphasis on ethics, as demonstrated by the inclusion of an Ethical Impact
Statement requirement for this year's submissions, this workshop supports
FG2025's efforts by encouraging research, discussion and dialogue on
trustworthy FAA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic <span class="highlight-title">Dataset</span> Generation for Autonomous Mobile Robots Using 3D
  Gaussian Splatting for Vision Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneesh Deogan, Wout Beks, Peter Teurlings, Koen de Vos, Mark van den Brand, Rene van de Molengraft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotated datasets are critical for training neural networks for object
detection, yet their manual creation is time- and labour-intensive, subjective
to human error, and often limited in diversity. This challenge is particularly
pronounced in the domain of robotics, where diverse and dynamic scenarios
further complicate the creation of representative datasets. To address this, we
propose a novel method for automatically generating annotated synthetic data in
Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for
rapid synthetic data generation. We demonstrate that synthetic datasets can
achieve performance comparable to that of real-world datasets while
significantly reducing the time required to generate and annotate data.
Additionally, combining real-world and synthetic data significantly increases
object detection performance by leveraging the quality of real-world images
with the easier scalability of synthetic data. To our knowledge, this is the
first application of synthetic data for training object detection algorithms in
the highly dynamic and varied environment of robot soccer. Validation
experiments reveal that a detector trained on synthetic images performs on par
with one trained on manually annotated real-world images when tested on robot
soccer match scenarios. Our method offers a scalable and comprehensive
alternative to traditional dataset creation, eliminating the labour-intensive
error-prone manual annotation process. By generating datasets in a simulator
where all elements are intrinsically known, we ensure accurate annotations
while significantly reducing manual effort, which makes it particularly
valuable for robotics applications requiring diverse and scalable training
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Multimodal Framework for Human-Centered Street Assessment:
  Integrating Visual-Language Models for Perceptual Urban Diagnostics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HaoTian Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While objective street metrics derived from imagery or GIS have become
standard in urban analytics, they remain insufficient to capture subjective
perceptions essential to inclusive urban design. This study introduces a novel
Multimodal Street Evaluation Framework (MSEF) that fuses a vision transformer
(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable
dual-output assessment of streetscapes. Leveraging over 15,000 annotated
street-view images from Harbin, China, we fine-tune the framework using LoRA
and P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1
score of 0.84 on objective features and 89.3 percent agreement with aggregated
resident perceptions, validated across stratified socioeconomic geographies.
Beyond classification accuracy, MSEF captures context-dependent contradictions:
for instance, informal commerce boosts perceived vibrancy while simultaneously
reducing pedestrian comfort. It also identifies nonlinear and semantically
contingent patterns -- such as the divergent perceptual effects of
architectural transparency across residential and commercial zones -- revealing
the limits of universal spatial heuristics. By generating natural-language
rationales grounded in attention mechanisms, the framework bridges sensory data
with socio-affective inference, enabling transparent diagnostics aligned with
SDG 11. This work offers both methodological innovation in urban perception
modeling and practical utility for planning systems seeking to reconcile
infrastructural precision with lived experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeedEdit 3.0: Fast and High-Quality Generative Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0 [22],
which significantly improves over our previous version [27] in both aspects of
edit instruction following and image content (e.g., ID/IP) preservation on real
image inputs. Additional to model upgrading with T2I, in this report, we
present several key improvements. First, we develop an enhanced data curation
pipeline with a meta-info paradigm and meta-info embedding strategy that help
mix images from multiple data sources. This allows us to scale editing data
effectively, and meta information is helpfult to connect VLM with diffusion
model more closely. Second, we introduce a joint learning pipeline for
computing a diffusion loss and a reward loss. Finally, we evaluate SeedEdit 3.0
on our testing benchmarks, for real image editing, where it achieves a best
trade-off between multiple aspects, yielding a high usability rate of 56.1%,
compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our website: https://seed.bytedance.com/tech/seededit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parking, Perception, and Retail: Street-Level Determinants of Community
  Vitality in Harbin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HaoTian Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The commercial vitality of community-scale streets in Chinese cities is
shaped by complex interactions between vehicular accessibility, environmental
quality, and pedestrian perception. This study proposes an interpretable,
image-based framework to examine how street-level features -- including parked
vehicle density, greenery, cleanliness, and street width -- impact retail
performance and user satisfaction in Harbin, China. Leveraging street view
imagery and a multimodal large language model (VisualGLM-6B), we construct a
Community Commercial Vitality Index (CCVI) from Meituan and Dianping data and
analyze its relationship with spatial attributes extracted via GPT-4-based
perception modeling. Our findings reveal that while moderate vehicle presence
may enhance commercial access, excessive on-street parking -- especially in
narrow streets -- erodes walkability and reduces both satisfaction and
shop-level pricing. In contrast, streets with higher perceived greenery and
cleanliness show significantly greater satisfaction scores but only weak
associations with pricing. Street width moderates the effects of vehicle
presence, underscoring the importance of spatial configuration. These results
demonstrate the value of integrating AI-assisted perception with urban
morphological analysis to capture non-linear and context-sensitive drivers of
commercial success. This study advances both theoretical and methodological
frontiers by highlighting the conditional role of vehicle activity in
neighborhood commerce and demonstrating the feasibility of multimodal AI for
perceptual urban diagnostics. The implications extend to urban design, parking
management, and scalable planning tools for community revitalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Vietnamese Document Analysis and Recognition: Challenges and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Le, Thanh Lam, Dung Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vietnamese document analysis and recognition (DAR) is a crucial field with
applications in digitization, information retrieval, and automation. Despite
advancements in OCR and NLP, Vietnamese text recognition faces unique
challenges due to its complex diacritics, tonal variations, and lack of
large-scale annotated datasets. Traditional OCR methods often struggle with
real-world document variations, while deep learning approaches have shown
promise but remain limited by data scarcity and generalization issues.
Recently, large language models (LLMs) and vision-language models have
demonstrated remarkable improvements in text recognition and document
understanding, offering a new direction for Vietnamese DAR. However, challenges
such as domain adaptation, multimodal learning, and computational efficiency
persist. This survey provide a comprehensive review of existing techniques in
Vietnamese document recognition, highlights key limitations, and explores how
LLMs can revolutionize the field. We discuss future research directions,
including dataset development, model optimization, and the integration of
multimodal approaches for improved document intelligence. By addressing these
gaps, we aim to foster advancements in Vietnamese DAR and encourage
community-driven solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowDirector: Training-Free Flow Steering for Precise Text-to-Video
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhao Li, Yanming Yang, Chenxi Song, Chi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven video editing aims to modify video content according to natural
language instructions. While recent training-free approaches have made progress
by leveraging pre-trained diffusion models, they typically rely on
inversion-based techniques that map input videos into the latent space, which
often leads to temporal inconsistencies and degraded structural fidelity. To
address this, we propose FlowDirector, a novel inversion-free video editing
framework. Our framework models the editing process as a direct evolution in
data space, guiding the video via an Ordinary Differential Equation (ODE) to
smoothly transition along its inherent spatiotemporal manifold, thereby
preserving temporal coherence and structural details. To achieve localized and
controllable edits, we introduce an attention-guided masking mechanism that
modulates the ODE velocity field, preserving non-target regions both spatially
and temporally. Furthermore, to address incomplete edits and enhance semantic
alignment with editing instructions, we present a guidance-enhanced editing
strategy inspired by Classifier-Free Guidance, which leverages differential
signals between multiple candidate flows to steer the editing trajectory toward
stronger semantic alignment without compromising structural consistency.
Extensive experiments across benchmarks demonstrate that FlowDirector achieves
state-of-the-art performance in instruction adherence, temporal consistency,
and background preservation, establishing a new paradigm for efficient and
coherent video editing without inversion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page is https://flowdirector-edit.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DACN: Dual-Attention Convolutional Network for Hyperspectral Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman Muhammad, Jorma Laaksonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D convolutional neural networks (CNNs) have attracted significant attention
for hyperspectral image super-resolution tasks. However, a key limitation is
their reliance on local neighborhoods, which leads to a lack of global
contextual understanding. Moreover, band correlation and data scarcity continue
to limit their performance. To mitigate these issues, we introduce DACN, a
dual-attention convolutional network for hyperspectral image super-resolution.
Specifically, the model first employs augmented convolutions, integrating
multi-head attention to effectively capture both local and global feature
dependencies. Next, we infer separate attention maps for the channel and
spatial dimensions to determine where to focus across different channels and
spatial positions. Furthermore, a custom optimized loss function is proposed
that combines L2 regularization with spatial-spectral gradient loss to ensure
accurate spectral fidelity. Experimental results on two hyperspectral datasets
demonstrate that the combination of multi-head attention and channel attention
outperforms either attention mechanism used individually.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Understanding Cross-Class Features in Adversarial
  Training <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeming Wei, Yiwen Guo, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physical Annotation for Automated Optical Inspection: A Concept for
  In-Situ, Pointer-Based Trainingdata Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Krumpek, Oliver Heimann, Jörg Krüger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel physical annotation system designed to generate
training data for automated optical inspection. The system uses pointer-based
in-situ interaction to transfer the valuable expertise of trained inspection
personnel directly into a machine learning (ML) training pipeline. Unlike
conventional screen-based annotation methods, our system captures physical
trajectories and contours directly on the object, providing a more intuitive
and efficient way to label data. The core technology uses calibrated, tracked
pointers to accurately record user input and transform these spatial
interactions into standardised annotation formats that are compatible with
open-source annotation software. Additionally, a simple projector-based
interface projects visual guidance onto the object to assist users during the
annotation process, ensuring greater accuracy and consistency. The proposed
concept bridges the gap between human expertise and automated data generation,
enabling non-IT experts to contribute to the ML training pipeline and
preventing the loss of valuable training samples. Preliminary evaluation
results confirm the feasibility of capturing detailed annotation trajectories
and demonstrate that integration with CVAT streamlines the workflow for
subsequent ML tasks. This paper details the system architecture, calibration
procedures and interface design, and discusses its potential contribution to
future ML data generation for automated optical inspection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehoon Choi, Dongki Jung, Christopher Maxey, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in dynamic neural rendering, existing
methods fail to address the unique challenges posed by UAV-captured scenarios,
particularly those involving monocular camera setups, top-down perspective, and
multiple small, moving humans, which are not adequately represented in existing
datasets. In this work, we introduce UAV4D, a framework for enabling
photorealistic rendering for dynamic real-world scenes captured by UAVs.
Specifically, we address the challenge of reconstructing dynamic scenes with
multiple moving pedestrians from monocular video data without the need for
additional sensors. We use a combination of a 3D foundation model and a human
mesh reconstruction model to reconstruct both the scene background and humans.
We propose a novel approach to resolve the scene scale ambiguity and place both
humans and the scene in world coordinates by identifying human-scene contact
points. Additionally, we exploit the SMPL model and background mesh to
initialize Gaussian splats, enabling holistic scene rendering. We evaluated our
method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and
Okutama-Action, each with distinct characteristics and 10~50 humans. Our
results demonstrate the benefits of our approach over existing methods in novel
view synthesis, achieving a 1.5 dB PSNR improvement and superior visual
sharpness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow
  Development <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenran Xu, Xue Yang, Yiyu Wang, Qingli Hu, Zijiao Wu, Longyue Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ComfyUI-Copilot, a large language model-powered plugin designed
to enhance the usability and efficiency of ComfyUI, an open-source platform for
AI-driven art creation. Despite its flexibility and user-friendly interface,
ComfyUI can present challenges to newcomers, including limited documentation,
model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot
addresses these challenges by offering intelligent node and model
recommendations, along with automated one-click workflow construction. At its
core, the system employs a hierarchical multi-agent framework comprising a
central assistant agent for task delegation and specialized worker agents for
different usages, supported by our curated ComfyUI knowledge bases to
streamline debugging and deployment. We validate the effectiveness of
ComfyUI-Copilot through both offline quantitative evaluations and online user
feedback, showing that it accurately recommends nodes and accelerates workflow
development. Additionally, use cases illustrate that ComfyUI-Copilot lowers
entry barriers for beginners and enhances workflow efficiency for experienced
users. The ComfyUI-Copilot installation package and a demo video are available
at https://github.com/AIDC-AI/ComfyUI-Copilot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Demo. Github: https://github.com/AIDC-AI/ComfyUI-Copilot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfred T. Christiansen, Andreas H. Højrup, Morten K. Stephansen, Md Ibtihaj A. Sakib, Taman S. Poojary, Filip Slezak, Morten S. Laursen, Thomas B. Moeslund, Joakim B. Haurum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training neural networks for tasks such as 3D point cloud semantic
segmentation demands extensive datasets, yet obtaining and annotating
real-world point clouds is costly and labor-intensive. This work aims to
introduce a novel pipeline for generating realistic synthetic data, by
leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to
generate 3D assets of multiple different agricultural vehicles instead of using
generic models. These assets are placed in a simulated environment, where the
point clouds are generated using a simulated LiDAR. This is a flexible approach
that allows changing the LiDAR specifications without incurring additional
costs. We evaluated the impact of synthetic data on segmentation models such as
PointNet++, Point Transformer V3, and OACNN, by training and validating the
models only on synthetic data. Remarkably, the PTv3 model had an mIoU of
91.35\%, a noteworthy result given that the model had neither been trained nor
validated on any real data. Further studies even suggested that in certain
scenarios the models trained only on synthetically generated data performed
better than models trained on real-world data. Finally, experiments
demonstrated that the models can generalize across semantic classes, enabling
accurate predictions on mesh models they were never trained on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-Aware Radar-Camera Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyi Zhang, Zhu Yu, Chunhao Li, Runmin Zhang, Xiaokai Bai, Zili Zhou, Si-Yuan Cao, Wang Wang, Hui-Liang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation aims to determine the depth of each pixel from an
RGB image captured by a monocular camera. The development of deep learning has
significantly advanced this field by facilitating the learning of depth
features from some well-annotated datasets
\cite{Geiger_Lenz_Stiller_Urtasun_2013,silberman2012indoor}. Eigen \textit{et
al.} \cite{eigen2014depth} first introduce a multi-scale fusion network for
depth regression. Following this, subsequent improvements have come from
reinterpreting the regression task as a classification problem
\cite{bhat2021adabins,Li_Wang_Liu_Jiang_2022}, incorporating additional priors
\cite{shao2023nddepth,yang2023gedepth}, and developing more effective objective
function \cite{xian2020structure,Yin_Liu_Shen_Yan_2019}. Despite these
advances, generalizing to unseen domains remains a challenge. Recently, several
methods have employed affine-invariant loss to enable multi-dataset joint
training \cite{MiDaS,ZeroDepth,guizilini2023towards,Dany}. Among them, Depth
Anything \cite{Dany} has shown leading performance in zero-shot monocular depth
estimation. While it struggles to estimate accurate metric depth due to the
lack of explicit depth cues, it excels at extracting structural information
from unseen images, producing structure-detailed monocular depth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Cropped Regions: New Benchmark and Corresponding Baseline for
  Chinese Scene Text Retrieval in Diverse Layouts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengluo Li, Huawen Shen, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese scene text retrieval is a practical task that aims to search for
images containing visual instances of a Chinese query text. This task is
extremely challenging because Chinese text often features complex and diverse
layouts in real-world scenes. Current efforts tend to inherit the solution for
English scene text retrieval, failing to achieve satisfactory performance. In
this paper, we establish a Diversified Layout benchmark for Chinese Street View
Text Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval
performance across various text layouts, including vertical, cross-line, and
partial alignments. To address the limitations in existing methods, we propose
Chinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates
global visual information with multi-granularity alignment training. CSTR-CLIP
applies a two-stage training process to overcome previous limitations, such as
the exclusion of visual features outside the text region and reliance on
single-granularity alignment, thereby enabling the model to effectively handle
diverse text layouts. Experiments on existing benchmark show that CSTR-CLIP
outperforms the previous state-of-the-art model by 18.82% accuracy and also
provides faster inference speed. Further analysis on DL-CSVTR confirms the
superior performance of CSTR-CLIP in handling various text layouts. The dataset
and code will be publicly available to facilitate research in Chinese scene
text retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Bianchi, Antonio Liotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated sports skill assessment requires capturing fundamental movement
patterns that distinguish expert from novice performance, yet current video
sampling methods disrupt the temporal continuity essential for proficiency
evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling
(PATS), a novel sampling strategy that preserves complete fundamental movements
within continuous temporal segments for multi-view skill assessment. PATS
adaptively segments videos to ensure each analyzed portion contains full
execution of critical performance components, repeating this process across
multiple segments to maximize information coverage while maintaining temporal
coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses
the state-of-the-art accuracy across all viewing configurations (+0.65% to
+3.05%) and delivers substantial gains in challenging domains (+26.22%
bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that
PATS successfully adapts to diverse activity characteristics-from
high-frequency sampling for dynamic sports to fine-grained segmentation for
sequential skills-demonstrating its effectiveness as an adaptive approach to
temporal sampling that advances automated skill assessment for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-scale Image Super Resolution with a Single Auto-Regressive Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Sanchez, Isma Hadji, Adrian Bulat, Christos Tzelepis, Brais Martinez, Georgios Tzimiropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we tackle Image Super Resolution (ISR), using recent advances
in Visual Auto-Regressive (VAR) modeling. VAR iteratively estimates the
residual in latent space between gradually increasing image scales, a process
referred to as next-scale prediction. Thus, the strong priors learned during
pre-training align well with the downstream task (ISR). To our knowledge, only
VARSR has exploited this synergy so far, showing promising results. However,
due to the limitations of existing residual quantizers, VARSR works only at a
fixed resolution, i.e. it fails to map intermediate outputs to the
corresponding image scales. Additionally, it relies on a 1B transformer
architecture (VAR-d24), and leverages a large-scale private dataset to achieve
state-of-the-art results. We address these limitations through two novel
components: a) a Hierarchical Image Tokenization approach with a multi-scale
image tokenizer that progressively represents images at different scales while
simultaneously enforcing token overlap across scales, and b) a Direct
Preference Optimization (DPO) regularization term that, relying solely on the
LR and HR tokenizations, encourages the transformer to produce the latter over
the former. To the best of our knowledge, this is the first time a quantizer is
trained to force semantically consistent residuals at different scales, and the
first time that preference-based optimization is used to train a VAR. Using
these two components, our model can denoise the LR image and super-resolve at
half and full target upscale factors in a single forward pass. Additionally, we
achieve \textit{state-of-the-art results on ISR}, while using a small model
(300M params vs ~1B params of VARSR), and without using external training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Enrique Sanchez and Isma Hadji equally contributed to this work.
  Project site https://github.com/saic-fi/ms_sr_var</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextVidBench: A Benchmark for Long Video Scene Text Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Zhong, Ji Qi, Yuan Yao, Pengxin Luo, Yunfeng Yan, Donglian Qi, Zhiyuan Liu, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress on the short-video Text-Visual Question Answering
(ViteVQA) task - largely driven by benchmarks such as M4-ViteVQA - existing
datasets still suffer from limited video duration and narrow evaluation scopes,
making it difficult to adequately assess the growing capabilities of powerful
multimodal large language models (MLLMs). To address these limitations, we
introduce TextVidBench, the first benchmark specifically designed for
long-video text question answering (>3 minutes). TextVidBench makes three key
contributions: 1) Cross-domain long-video coverage: Spanning 9 categories
(e.g., news, sports, gaming), with an average video length of 2306 seconds,
enabling more realistic evaluation of long-video understanding. 2) A
three-stage evaluation framework: "Text Needle-in-Haystack -> Temporal
Grounding -> Text Dynamics Captioning". 3) High-quality fine-grained
annotations: Containing over 5,000 question-answer pairs with detailed semantic
labeling. Furthermore, we propose an efficient paradigm for improving large
models through: (i) introducing the IT-Rope mechanism and temporal prompt
engineering to enhance temporal perception, (ii) adopting non-uniform
positional encoding to better handle long video sequences, and (iii) applying
lightweight fine-tuning on video-text data. Extensive experiments on multiple
public datasets as well as TextVidBench demonstrate that our new benchmark
presents significant challenges to existing models, while our proposed method
offers valuable insights into improving long-video scene text understanding
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Bringing SAM to new heights: Leveraging elevation data for tree crown
  segmentation from drone imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélisande Teng, Arthur Ouaknine, Etienne Laliberté, <span class="highlight-author">Yoshua Bengio</span>, David Rolnick, Hugo Larochelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information on trees at the individual level is crucial for monitoring forest
ecosystems and planning forest management. Current monitoring methods involve
ground measurements, requiring extensive cost, time and labor. Advances in
drone remote sensing and computer vision offer great potential for mapping
individual trees from aerial imagery at broad-scale. Large pre-trained vision
models, such as the Segment Anything Model (SAM), represent a particularly
compelling choice given limited labeled data. In this work, we compare methods
leveraging SAM for the task of automatic tree crown instance segmentation in
high resolution drone imagery in three use cases: 1) boreal plantations, 2)
temperate forests and 3) tropical forests. We also study the integration of
elevation data into models, in the form of Digital Surface Model (DSM)
information, which can readily be obtained at no additional cost from RGB drone
imagery. We present BalSAM, a model leveraging SAM and DSM information, which
shows potential over other methods, particularly in the context of plantations.
We find that methods using SAM out-of-the-box do not outperform a custom Mask
R-CNN, even with well-designed prompts. However, efficiently tuning SAM
end-to-end and integrating DSM information are both promising avenues for tree
crown instance segmentation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FEAT: Full-Dimensional Efficient Attention <span class="highlight-title">Transformer</span> for Medical Video
  Generation <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huihan Wang, Zhiwen Yang, Hui Zhang, Dan Zhao, Bingzheng Wei, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing high-quality dynamic medical videos remains a significant
challenge due to the need for modeling both spatial consistency and temporal
dynamics. Existing Transformer-based approaches face critical limitations,
including insufficient channel interactions, high computational complexity from
self-attention, and coarse denoising guidance from timestep embeddings when
handling varying noise levels. In this work, we propose FEAT, a
full-dimensional efficient attention Transformer, which addresses these issues
through three key innovations: (1) a unified paradigm with sequential
spatial-temporal-channel attention mechanisms to capture global dependencies
across all dimensions, (2) a linear-complexity design for attention mechanisms
in each dimension, utilizing weighted key-value attention and global channel
attention, and (3) a residual value guidance module that provides fine-grained
pixel-level guidance to adapt to different noise levels. We evaluate FEAT on
standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only
23\% of the parameters of the state-of-the-art model Endora, achieves
comparable or even superior performance. Furthermore, FEAT-L surpasses all
comparison methods across multiple datasets, showcasing both superior
effectiveness and scalability. Code is available at
https://github.com/Yaziwel/FEAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been early accepted by MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual
  Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Gao, Yiming Bao, Xuezhan Tu, Bin Zhong, Minling Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current video-based multimodal large language models struggle with hour-level
video understanding due to computational constraints and inefficient
information extraction from extensive temporal sequences. We propose APVR
(Adaptive Pivot Visual information Retrieval), a training-free framework that
addresses the memory wall limitation through hierarchical visual information
retrieval. APVR operates via two complementary components: Pivot Frame
Retrieval employs semantic expansion and multi-modal confidence scoring to
identify semantically relevant video frames, while Pivot Token Retrieval
performs query-aware attention-driven token selection within the pivot frames.
This dual granularity approach enables processing of hour-long videos while
maintaining semantic fidelity. Experimental validation on LongVideoBench and
VideoMME demonstrates significant performance improvements, establishing
state-of-the-art results for not only training-free but also training-based
approaches while providing plug-and-play integration capability with existing
MLLM architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness as Architecture: Designing IQA Models to Withstand
  Adversarial Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Meleshin, Anna Chistyakova, Anastasia Antsiferova, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Quality Assessment (IQA) models are increasingly relied upon to
evaluate image quality in real-world systems -- from compression and
enhancement to generation and streaming. Yet their adoption brings a
fundamental risk: these models are inherently unstable. Adversarial
manipulations can easily fool them, inflating scores and undermining trust.
Traditionally, such vulnerabilities are addressed through data-driven defenses
-- adversarial retraining, regularization, or input purification. But what if
this is the wrong lens? What if robustness in perceptual models is not
something to learn but something to design? In this work, we propose a
provocative idea: robustness as an architectural prior. Rather than training
models to resist perturbations, we reshape their internal structure to suppress
sensitivity from the ground up. We achieve this by enforcing orthogonal
information flow, constraining the network to norm-preserving operations -- and
further stabilizing the system through pruning and fine-tuning. The result is a
robust IQA architecture that withstands adversarial attacks without requiring
adversarial training or significant changes to the original model. This
approach suggests a shift in perspective: from optimizing robustness through
data to engineering it through design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal
  Pattern Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Sun, Yipeng Wang, Junyu Shi, Zhiyuan Zhang, Yanmei Xiao, Lei Zhu, Manxi Jiang, Qiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has recently shown promise in automated embryo
selection for In-Vitro Fertilization (IVF). However, current approaches either
address partial embryo evaluation lacking holistic quality assessment or target
clinical outcomes inevitably confounded by extra-embryonic factors, both
limiting clinical utility. To bridge this gap, we propose a new task called
Video-Based Embryo Grading - the first paradigm that directly utilizes
full-length time-lapse monitoring (TLM) videos to predict embryologists'
overall quality assessments. To support this task, we curate a real-world
clinical dataset comprising over 2,500 TLM videos, each annotated with a
grading label indicating the overall quality of embryos. Grounded in clinical
decision-making principles, we propose a Complementary Spatial-Temporal Pattern
Mining (CoSTeM) framework that conceptually replicates embryologists'
evaluation process. The CoSTeM comprises two branches: (1) a morphological
branch using a Mixture of Cross-Attentive Experts layer and a Temporal
Selection Block to select discriminative local structural features, and (2) a
morphokinetic branch employing a Temporal Transformer to model global
developmental trajectories, synergistically integrating static and dynamic
determinants for grading embryos. Extensive experimental results demonstrate
the superiority of our design. This work provides a valuable methodological
framework for AI-assisted embryo selection. The dataset and source code will be
publicly available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CzechLynx: A <span class="highlight-title">Dataset</span> for Individual Identification and Pose Estimation
  of the Eurasian Lynx 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Picek, Elisa Belotti, Michal Bojda, Ludek Bufka, Vojtech Cermak, Martin Dula, Rostislav Dvorak, Luboslav Hrdy, Miroslav Jirik, Vaclav Kocourek, Josefa Krausova, Jirı Labuda, Jakub Straka, Ludek Toman, Vlado Trulık, Martin Vana, Miroslav Kutal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CzechLynx, the first large-scale, open-access dataset for
individual identification, 2D pose estimation, and instance segmentation of the
Eurasian lynx (Lynx lynx). CzechLynx includes more than 30k camera trap images
annotated with segmentation masks, identity labels, and 20-point skeletons and
covers 219 unique individuals across 15 years of systematic monitoring in two
geographically distinct regions: Southwest Bohemia and the Western Carpathians.
To increase the data variability, we create a complementary synthetic set with
more than 100k photorealistic images generated via a Unity-based pipeline and
diffusion-driven text-to-texture modeling, covering diverse environments,
poses, and coat-pattern variations. To allow testing generalization across
spatial and temporal domains, we define three tailored evaluation
protocols/splits: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware
closed-set. This dataset is targeted to be instrumental in benchmarking
state-of-the-art models and the development of novel methods for not just
individual animal re-identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Light and 3D: a methodological exploration of digitisation techniques
  adapted to a selection of objects from the Mus{é}e d'Arch{é}ologie
  Nationale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Laurent, Jean Mélou, Catherine Schwab, Rolande Simon-Millot, Sophie Féret, Thomas Sagory, Carole Fritz, Jean-Denis Durou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need to digitize heritage objects is now widely accepted. This article
presents the very fashionable context of the creation of ''digital twins''. It
illustrates the diversity of photographic 3D digitization methods, but this is
not its only objective. Using a selection of objects from the collections of
the mus{\'e}e d'Arch{\'e}ologie nationale, it shows that no single method is
suitable for all cases. Rather, the method to be recommended for a given object
should be the result of a concerted choice between those involved in heritage
and those involved in the digital domain, as each new object may require the
adaptation of existing tools. It would therefore be pointless to attempt an
absolute classification of 3D digitization methods. On the contrary, we need to
find the digital tool best suited to each object, taking into account not only
its characteristics, but also the future use of its digital twin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Synthetic Stereo <span class="highlight-title">Dataset</span>s using 3D Gaussian Splatting and
  Expert Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Slezak, Magnus K. Gjerde, Joakim B. Haurum, Ivan Nikolov, Morten S. Laursen, Thomas B. Moeslund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for
stereo dataset generation, offering an efficient alternative to Neural Radiance
Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore
utilizing the reconstructed geometry from the explicit 3D representations as
well as depth estimates from the FoundationStereo model in an expert knowledge
transfer setup. We find that when fine-tuning stereo models on 3DGS-generated
datasets, we demonstrate competitive performance in zero-shot generalization
benchmarks. When using the reconstructed geometry directly, we observe that it
is often noisy and contains artifacts, which propagate noise to the trained
model. In contrast, we find that the disparity estimates from FoundationStereo
are cleaner and consequently result in a better performance on the zero-shot
generalization benchmarks. Our method highlights the potential for low-cost,
high-fidelity dataset creation and fast fine-tuning for deep stereo models.
Moreover, we also reveal that while the latest Gaussian Splatting based methods
have achieved superior performance on established benchmarks, their robustness
falls short in challenging in-the-wild settings warranting further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual
  Grounding in 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxu Wang, Zhuofan Zhang, Ziyu Zhu, Yue Fan, Jing Xiong, Pengxiang Li, Xiaojian Ma, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding has made notable progress in localizing objects within
complex 3D scenes. However, grounding referring expressions beyond objects in
3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a
holistic 3D visual grounding benchmark consisting of 2,632 referring
expression-3D bounding box pairs spanning four different grounding levels:
human-activity areas, unoccupied space beyond objects, objects in the scene,
and fine-grained object parts. We assess a range of state-of-the-art 3D visual
grounding methods alongside large language models (LLMs) and multimodal LLMs
(MLLMs) on Anywhere3D-Bench. Experimental results reveal that space-level and
part-level visual grounding pose the greatest challenges: space-level tasks
require a more comprehensive spatial reasoning ability, for example, modeling
distances and spatial relations within 3D space, while part-level tasks demand
fine-grained perception of object composition. Even the best performance model,
OpenAI o4-mini, achieves only 23.57% accuracy on space-level tasks and 33.94%
on part-level tasks, significantly lower than its performance on area-level and
object-level tasks. These findings underscore a critical gap in current models'
capacity to understand and reason about 3D scene beyond object-level semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Plan via Supervised Contrastive Learning and Strategic
  Interpolation: A Chess Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Hamara, Greg Hamerly, Pablo Rivas, Andrew C. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern chess engines achieve superhuman performance through deep tree search
and regressive evaluation, while human players rely on intuition to select
candidate moves followed by a shallow search to validate them. To model this
intuition-driven planning process, we train a transformer encoder using
supervised contrastive learning to embed board states into a latent space
structured by positional evaluation. In this space, distance reflects
evaluative similarity, and visualized trajectories display interpretable
transitions between game states. We demonstrate that move selection can occur
entirely within this embedding space by advancing toward favorable regions,
without relying on deep search. Despite using only a 6-ply beam search, our
model achieves an estimated Elo rating of 2593. Performance improves with both
model size and embedding dimensionality, suggesting that latent planning may
offer a viable alternative to traditional search. Although we focus on chess,
the proposed embedding-based planning method can be generalized to other
perfect-information games where state evaluations are learnable. All source
code is available at https://github.com/andrewhamara/SOLIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable progress in both image generation
and editing. However, recent studies have revealed their vulnerability to
backdoor attacks, in which specific patterns embedded in the input can
manipulate the model's behavior. Most existing research in this area has
proposed attack frameworks focused on the image generation pipeline, leaving
backdoor attacks in image editing relatively unexplored. Among the few studies
targeting image editing, most utilize visible triggers, which are impractical
because they introduce noticeable alterations to the input image before
editing. In this paper, we propose a novel attack framework that embeds
invisible triggers into the image editing process via poisoned training data.
We leverage off-the-shelf deep watermarking models to encode imperceptible
watermarks as backdoor triggers. Our goal is to make the model produce the
predefined backdoor target when it receives watermarked inputs, while editing
clean images normally according to the given prompt. With extensive experiments
across different watermarking models, the proposed method achieves promising
attack success rates. In addition, the analysis results of the watermark
characteristics in term of backdoor attack further support the effectiveness of
our approach. The code is available
at:https://github.com/aiiu-lab/BackdoorImageEditing
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geological Field Restoration through the Lens of Image Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Trifonov, Ivan Oseledets, Ekaterina Muravleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new viewpoint on a reconstructing multidimensional geological
fields from sparse observations. Drawing inspiration from deterministic image
inpainting techniques, we model a partially observed spatial field as a
multidimensional tensor and recover missing values by enforcing a global
low-rank structure. Our approach combines ideas from tensor completion and
geostatistics, providing a robust optimization framework. Experiments on
synthetic geological fields demonstrate that used tensor completion method
significant improvements in reconstruction accuracy over ordinary kriging for
various percent of observed data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MineInsight: A Multi-sensor <span class="highlight-title">Dataset</span> for Humanitarian Demining Robotics
  in Off-Road Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Malizia, Charles Hamesse, Ken Hasselmann, Geert De Cubber, Nikolaos Tsiogkas, Eric Demeester, Rob Haelterman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of robotics in humanitarian demining increasingly involves computer
vision techniques to improve landmine detection capabilities. However, in the
absence of diverse and realistic datasets, the reliable validation of
algorithms remains a challenge for the research community. In this paper, we
introduce MineInsight, a publicly available multi-sensor, multi-spectral
dataset designed for off-road landmine detection. The dataset features 35
different targets (15 landmines and 20 commonly found objects) distributed
along three distinct tracks, providing a diverse and realistic testing
environment. MineInsight is, to the best of our knowledge, the first dataset to
integrate dual-view sensor scans from both an Unmanned Ground Vehicle and its
robotic arm, offering multiple viewpoints to mitigate occlusions and improve
spatial awareness. It features two LiDARs, as well as images captured at
diverse spectral ranges, including visible (RGB, monochrome), visible
short-wave infrared (VIS-SWIR), and long-wave infrared (LWIR). Additionally,
the dataset comes with an estimation of the location of the targets, offering a
benchmark for evaluating detection algorithms. We recorded approximately one
hour of data in both daylight and nighttime conditions, resulting in around
38,000 RGB frames, 53,000 VIS-SWIR frames, and 108,000 LWIR frames. MineInsight
serves as a benchmark for developing and evaluating landmine detection
algorithms. Our dataset is available at
https://github.com/mariomlz99/MineInsight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunshen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although perception systems have made remarkable advancements in recent
years, particularly in 2D reasoning segmentation, these systems still rely on
explicit human instruction or pre-defined categories to identify target objects
before executing visual recognition tasks. Such systems have matured
significantly, demonstrating the ability to reason and comprehend implicit user
intentions in two-dimensional contexts, producing accurate segmentation masks
based on complex and implicit query text. However, a comparable framework and
structure for 3D reasoning segmentation remain absent. This paper introduces
OpenMaskDINO3D, a LLM designed for comprehensive 3D understanding and
segmentation. OpenMaskDINO3D processes point cloud data and text prompts to
produce instance segmentation masks, excelling in many 3D tasks. By introducing
a SEG token and object identifier, we achieve high-precision 3D segmentation
mask generation, enabling the model to directly produce accurate point cloud
segmentation results from natural language instructions. Experimental results
on large-scale ScanNet datasets validate the effectiveness of our
OpenMaskDINO3D across various tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/Zhangkuns/OpenMaskDINO3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DualX-VSR: Dual Axial Spatial$\times$Temporal <span class="highlight-title">Transformer</span> for Real-World
  Video Super-Resolution without Motion Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Cao, Yihao Liu, Xiaohui Li. Yuanting Gao. Yu Zhou, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models like ViViT and TimeSformer have advanced video
understanding by effectively modeling spatiotemporal dependencies. Recent video
generation models, such as Sora and Vidu, further highlight the power of
transformers in long-range feature extraction and holistic spatiotemporal
modeling. However, directly applying these models to real-world video
super-resolution (VSR) is challenging, as VSR demands pixel-level precision,
which can be compromised by tokenization and sequential attention mechanisms.
While recent transformer-based VSR models attempt to address these issues using
smaller patches and local attention, they still face limitations such as
restricted receptive fields and dependence on optical flow-based alignment,
which can introduce inaccuracies in real-world settings. To overcome these
issues, we propose Dual Axial Spatial$\times$Temporal Transformer for
Real-World Video Super-Resolution (DualX-VSR), which introduces a novel dual
axial spatial$\times$temporal attention mechanism that integrates spatial and
temporal information along orthogonal directions. DualX-VSR eliminates the need
for motion compensation, offering a simplified structure that provides a
cohesive representation of spatiotemporal information. As a result, DualX-VSR
achieves high fidelity and superior performance in real-world VSR task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Pavlitska, Jamie Robb, Nikolai Polley, Melih Yazgan, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic adversarial attacks on various camera-based perception tasks of
autonomous vehicles have been successfully demonstrated so far. However, only a
few works considered attacks on traffic light detectors. This work shows how
CNNs for traffic light detection can be attacked with printed patches. We
propose a threat model, where each instance of a traffic light is attacked with
a patch placed under it, and describe a training strategy. We demonstrate
successful adversarial patch attacks in universal settings. Our experiments
show realistic targeted red-to-green label-flipping attacks and attacks on
pictogram classification. Finally, we perform a real-world evaluation with
printed patches and demonstrate attacks in the lab settings with a mobile
traffic light for construction sites and in a test area with stationary traffic
lights. Our code is available at
https://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike-TBR: a Noise Resilient Neuromorphic Event Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Magrini. Federico Becattini, Luca Cultrera, Lorenzo Berlincioni, Pietro Pala, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras offer significant advantages over traditional frame-based
sensors, including higher temporal resolution, lower latency and dynamic range.
However, efficiently converting event streams into formats compatible with
standard computer vision pipelines remains a challenging problem, particularly
in the presence of noise. In this paper, we propose Spike-TBR, a novel
event-based encoding strategy based on Temporal Binary Representation (TBR),
addressing its vulnerability to noise by integrating spiking neurons. Spike-TBR
combines the frame-based advantages of TBR with the noise-filtering
capabilities of spiking neural networks, creating a more robust representation
of event streams. We evaluate four variants of Spike-TBR, each using different
spiking neurons, across multiple datasets, demonstrating superior performance
in noise-affected scenarios while improving the results on clean data. Our
method bridges the gap between spike-based and frame-based processing, offering
a simple noise-resilient solution for event-driven vision applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MegaHan97K: A Large-Scale <span class="highlight-title">Dataset</span> for Mega-Category Chinese Character
  Recognition with over 97K Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyi Zhang, Yongxin Shi, Peirong Zhang, Yixin Zhao, Zhenhua Yang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational to the Chinese language and culture, Chinese characters
encompass extraordinarily extensive and ever-expanding categories, with the
latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate
recognition of this vast number of characters, termed mega-category
recognition, presents a formidable yet crucial challenge for cultural heritage
preservation and digital applications. Despite significant advances in Optical
Character Recognition (OCR), mega-category recognition remains unexplored due
to the absence of comprehensive datasets, with the largest existing dataset
containing merely 16,151 categories. To bridge this critical gap, we introduce
MegaHan97K, a mega-category, large-scale dataset covering an unprecedented
97,455 categories of Chinese characters. Our work offers three major
contributions: (1) MegaHan97K is the first dataset to fully support the latest
GB18030-2022 standard, providing at least six times more categories than
existing datasets; (2) It effectively addresses the long-tail distribution
problem by providing balanced samples across all categories through its three
distinct subsets: handwritten, historical and synthetic subsets; (3)
Comprehensive benchmarking experiments reveal new challenges in mega-category
scenarios, including increased storage demands, morphologically similar
character recognition, and zero-shot learning difficulties, while also
unlocking substantial opportunities for future research. To the best of our
knowledge, the MetaHan97K is likely the dataset with the largest classes not
only in the field of OCR but may also in the broader domain of pattern
recognition. The dataset is available at
https://github.com/SCUT-DLVCLab/MegaHan97K.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SupeRANSAC: One RANSAC to Rule Them All 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Barath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust estimation is a cornerstone in computer vision, particularly for tasks
like Structure-from-Motion and Simultaneous Localization and Mapping. RANSAC
and its variants are the gold standard for estimating geometric models (e.g.,
homographies, relative/absolute poses) from outlier-contaminated data. Despite
RANSAC's apparent simplicity, achieving consistently high performance across
different problems is challenging. While recent research often focuses on
improving specific RANSAC components (e.g., sampling, scoring), overall
performance is frequently more influenced by the "bells and whistles" (i.e.,
the implementation details and problem-specific optimizations) within a given
library. Popular frameworks like OpenCV and PoseLib demonstrate varying
performance, excelling in some tasks but lagging in others. We introduce
SupeRANSAC, a novel unified RANSAC pipeline, and provide a detailed analysis of
the techniques that make RANSAC effective for specific vision tasks, including
homography, fundamental/essential matrix, and absolute/rigid pose estimation.
SupeRANSAC is designed for consistent accuracy across these tasks, improving
upon the best existing methods by, for example, 6 AUC points on average for
fundamental matrix estimation. We demonstrate significant performance
improvements over the state-of-the-art on multiple problems and datasets. Code:
https://github.com/danini/superansac
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff
  Table <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Matsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor search (ANNS) is an essential building block for
applications like RAG but can sometimes yield results that are overly similar
to each other. In certain scenarios, search results should be similar to the
query and yet diverse. We propose LotusFilter, a post-processing module to
diversify ANNS results. We precompute a cutoff table summarizing vectors that
are close to each other. During the filtering, LotusFilter greedily looks up
the table to delete redundant vectors from the candidates. We demonstrated that
the LotusFilter operates fast (0.02 [ms/query]) in settings resembling
real-world RAG applications, utilizing features such as OpenAI embeddings. Our
code is publicly available at https://github.com/matsui528/lotf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. GitHub: https://github.com/matsui528/lotf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaia Di Lorenzo, Federico Tombari, Marc Pollefeys, Daniel Barath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning effective multi-modal 3D representations of objects is essential for
numerous applications, such as augmented reality and robotics. Existing methods
often rely on task-specific embeddings that are tailored either for semantic
understanding or geometric reconstruction. As a result, these embeddings
typically cannot be decoded into explicit geometry and simultaneously reused
across tasks. In this paper, we propose Object-X, a versatile multi-modal
object representation framework capable of encoding rich object embeddings
(e.g. images, point cloud, text) and decoding them back into detailed geometric
and visual reconstructions. Object-X operates by geometrically grounding the
captured modalities in a 3D voxel grid and learning an unstructured embedding
fusing the information from the voxels with the object attributes. The learned
embedding enables 3D Gaussian Splatting-based object reconstruction, while also
supporting a range of downstream tasks, including scene alignment, single-image
3D object reconstruction, and localization. Evaluations on two challenging
real-world datasets demonstrate that Object-X produces high-fidelity novel-view
synthesis comparable to standard 3D Gaussian Splatting, while significantly
improving geometric accuracy. Moreover, Object-X achieves competitive
performance with specialized methods in scene alignment and localization.
Critically, our object-centric descriptors require 3-4 orders of magnitude less
storage compared to traditional image- or point cloud-based approaches,
establishing Object-X as a scalable and highly practical solution for
multi-modal 3D scene representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning image burst stacking to reconstruct high-resolution
  ground-based solar observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Schirninger, Robert Jarolim, Astrid M. Veronig, Christoph Kuckein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large aperture ground based solar telescopes allow the solar atmosphere to be
resolved in unprecedented detail. However, observations are limited by Earths
turbulent atmosphere, requiring post image corrections. Current reconstruction
methods using short exposure bursts face challenges with strong turbulence and
high computational costs. We introduce a deep learning approach that
reconstructs 100 short exposure images into one high quality image in real
time. Using unpaired image to image translation, our model is trained on
degraded bursts with speckle reconstructions as references, improving
robustness and generalization. Our method shows an improved robustness in terms
of perceptual quality, especially when speckle reconstructions show artifacts.
An evaluation with a varying number of images per burst demonstrates that our
method makes efficient use of the combined image information and achieves the
best reconstructions when provided with the full image burst.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular
  Visual Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhan Woo, Seongwon Lee, Jinwoo Jang, Euntai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applying Visual Place Recognition (VPR) to real-world mobile robots and
similar applications, perspective-to-equirectangular (P2E) formulation
naturally emerges as a suitable approach to accommodate diverse query images
captured from various viewpoints. In this paper, we introduce HypeVPR, a novel
hierarchical embedding framework in hyperbolic space, designed to address the
unique challenges of P2E VPR. The key idea behind HypeVPR is that visual
environments captured by panoramic views exhibit inherent hierarchical
structures. To leverage this property, we employ hyperbolic space to represent
hierarchical feature relationships and preserve distance properties within the
feature space. To achieve this, we propose a hierarchical feature aggregation
mechanism that organizes local-to-global feature representations within
hyperbolic space. Additionally, HypeVPR adopts an efficient coarse-to-fine
search strategy, optimally balancing speed and accuracy to ensure robust
matching, even between descriptors from different image types. This approach
enables HypeVPR to outperform state-of-the-art methods while significantly
reducing retrieval time, achieving up to 5x faster retrieval across diverse
benchmark datasets. The code and models will be released at
https://github.com/suhan-woo/HypeVPR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Better SSIM Loss for Unsupervised Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Cao, Fuya Luo, Yongjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised monocular depth learning generally relies on the photometric
relation among temporally adjacent images. Most of previous works use both mean
absolute error (MAE) and structure similarity index measure (SSIM) with
conventional form as training loss. However, they ignore the effect of
different components in the SSIM function and the corresponding hyperparameters
on the training. To address these issues, this work proposes a new form of
SSIM. Compared with original SSIM function, the proposed new form uses addition
rather than multiplication to combine the luminance, contrast, and structural
similarity related components in SSIM. The loss function constructed with this
scheme helps result in smoother gradients and achieve higher performance on
unsupervised depth estimation. We conduct extensive experiments to determine
the relatively optimal combination of parameters for our new SSIM. Based on the
popular MonoDepth approach, the optimized SSIM loss function can remarkably
outperform the baseline on the KITTI-2015 outdoor dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ontology-based knowledge representation for bone disease diagnosis: a
  foundation for safe and sustainable medical artificial intelligence systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loan Dao, Ngoc Quoc Ly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical artificial intelligence (AI) systems frequently lack systematic
domain expertise integration, potentially compromising diagnostic reliability.
This study presents an ontology-based framework for bone disease diagnosis,
developed in collaboration with Ho Chi Minh City Hospital for Traumatology and
Orthopedics. The framework introduces three theoretical contributions: (1) a
hierarchical neural network architecture guided by bone disease ontology for
segmentation-classification tasks, incorporating Visual Language Models (VLMs)
through prompts, (2) an ontology-enhanced Visual Question Answering (VQA)
system for clinical reasoning, and (3) a multimodal deep learning model that
integrates imaging, clinical, and laboratory data through ontological
relationships. The methodology maintains clinical interpretability through
systematic knowledge digitization, standardized medical terminology mapping,
and modular architecture design. The framework demonstrates potential for
extension beyond bone diseases through its standardized structure and reusable
components. While theoretical foundations are established, experimental
validation remains pending due to current dataset and computational resource
limitations. Future work will focus on expanding the clinical dataset and
conducting comprehensive system validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truth in the Few: High-Value Data Selection for Efficient Multi-Modal
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenshen Li, Kaiyuan Deng, Lei Wang, Hao Yang, Chong Peng, Peng Yan, Fumin Shen, Heng Tao Shen, Xing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-modal large language models (MLLMs) have made significant
progress in complex reasoning tasks via reinforcement learning, it is commonly
believed that extensive training data is necessary for improving multi-modal
reasoning ability, inevitably leading to data redundancy and substantial
computational costs. However, can smaller high-value datasets match or
outperform full corpora for multi-modal reasoning in MLLMs? In this work, we
challenge this assumption through a key observation: meaningful multi-modal
reasoning is triggered by only a sparse subset of training samples, termed
cognitive samples, whereas the majority contribute marginally. Building on this
insight, we propose a novel data selection paradigm termed Reasoning Activation
Potential (RAP), which identifies cognitive samples by estimating each sample's
potential to stimulate genuine multi-modal reasoning by two complementary
estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential
outcome model principle, eliminates samples that overly rely on language priors
by comparing outputs between multi-modal and text-only inputs; 2) Attention
Confidence Estimator (ACE), which exploits token-level self-attention to
discard samples dominated by irrelevant but over-emphasized tokens in
intermediate reasoning stages. Moreover, we introduce a Difficulty-aware
Replacement Module (DRM) to substitute trivial instances with cognitively
challenging ones, thereby ensuring complexity for robust multi-modal reasoning.
Experiments on six datasets show that our RAP method consistently achieves
superior performance using only 9.3% of the training data, while reducing
computational costs by over 43%. Our code is available at
https://github.com/Leo-ssl/RAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater
  Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niki Martinel, Rita Pucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel dual-stream architecture that achieves state-of-the-art
underwater image enhancement by explicitly integrating the Jaffe-McGlamery
physical model with capsule clustering-based feature representation learning.
Our method simultaneously estimates transmission maps and spatially-varying
background light through a dedicated physics estimator while extracting
entity-level features via capsule clustering in a parallel stream. This
physics-guided approach enables parameter-free enhancement that respects
underwater formation constraints while preserving semantic structures and
fine-grained details. Our approach also features a novel optimization objective
ensuring both physical adherence and perceptual quality across multiple spatial
frequencies. To validate our approach, we conducted extensive experiments
across six challenging benchmarks. Results demonstrate consistent improvements
of $+0.5$dB PSNR over the best existing methods while requiring only one-third
of their computational complexity (FLOPs), or alternatively, more than $+1$dB
PSNR improvement when compared to methods with similar computational budgets.
Code and data \textit{will} be available at https://github.com/iN1k1/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in
  VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have achieved remarkable performance in image
captioning, but recent studies show they are vulnerable to backdoor attacks.
Attackers can inject imperceptible perturbations-such as local pixel triggers
or global semantic phrases-into the training data, causing the model to
generate malicious, attacker-controlled captions for specific inputs. These
attacks are hard to detect and defend due to their stealthiness and cross-modal
nature. By analyzing attack samples, we identify two key vulnerabilities: (1)
abnormal attention concentration on specific image regions, and (2) semantic
drift and incoherence in generated captions. To counter this, we propose
Semantic Reward Defense (SRD), a reinforcement learning framework that
mitigates backdoor behavior without prior knowledge of triggers. SRD uses a
Deep Q-Network to learn policies for applying discrete perturbations (e.g.,
occlusion, color masking) to sensitive image regions, aiming to disrupt the
activation of malicious pathways. We design a semantic fidelity score as the
reward signal, which jointly evaluates semantic consistency and linguistic
fluency of the output, guiding the agent toward generating robust yet faithful
captions. Experiments across mainstream VLMs and datasets show SRD reduces
attack success rates to 5.6%, while preserving caption quality on clean inputs
with less than 10% performance drop. SRD offers a trigger-agnostic,
interpretable defense paradigm against stealthy backdoor threats in multimodal
generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Annotation Gaps: Transferring Labels to Align Object Detection
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Kennerley, Angelica Alives-Reviro, Carola-Bibiane Schönlieb, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining multiple object detection datasets offers a path to improved
generalisation but is hindered by inconsistencies in class semantics and
bounding box annotations. Some methods to address this assume shared label
taxonomies and address only spatial inconsistencies; others require manual
relabelling, or produce a unified label space, which may be unsuitable when a
fixed target label space is required. We propose Label-Aligned Transfer (LAT),
a label transfer framework that systematically projects annotations from
diverse source datasets into the label space of a target dataset. LAT begins by
training dataset-specific detectors to generate pseudo-labels, which are then
combined with ground-truth annotations via a Privileged Proposal Generator
(PPG) that replaces the region proposal network in two-stage detectors. To
further refine region features, a Semantic Feature Fusion (SFF) module injects
class-aware context and features from overlapping proposals using a
confidence-weighted attention mechanism. This pipeline preserves
dataset-specific annotation granularity while enabling many-to-one label space
transfer across heterogeneous datasets, resulting in a semantically and
spatially aligned representation suitable for training a downstream detector.
LAT thus jointly addresses both class-level misalignments and bounding box
inconsistencies without relying on shared label spaces or manual annotations.
Across multiple benchmarks, LAT demonstrates consistent improvements in
target-domain detection performance, achieving gains of up to +4.8AP over
semi-supervised baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using In-Context Learning for Automatic Defect Labelling of Display
  Manufacturing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Babar Hussain, Qiang Liu, Gang Chen, Bihai She, Dahai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an AI-assisted auto-labeling system for display panel
defect detection that leverages in-context learning capabilities. We adopt and
enhance the SegGPT architecture with several domain-specific training
techniques and introduce a scribble-based annotation mechanism to streamline
the labeling process. Our two-stage training approach, validated on industrial
display panel datasets, demonstrates significant improvements over the baseline
model, achieving an average IoU increase of 0.22 and a 14% improvement in
recall across multiple product types, while maintaining approximately 60%
auto-labeling coverage. Experimental results show that models trained on our
auto-labeled data match the performance of those trained on human-labeled data,
offering a practical solution for reducing manual annotation efforts in
industrial inspection systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning dissection trajectories from expert surgical videos via
  imitation learning with equivariant diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Yonghao Long, Yueyao Chen, Hon-Chi Yip, Markus Scheppach, Philip Wai-Yan Chiu, Yeung Yam, Helen Mei-Ling Meng, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic Submucosal Dissection (ESD) is a well-established technique for
removing epithelial lesions. Predicting dissection trajectories in ESD videos
offers significant potential for enhancing surgical skill training and
simplifying the learning process, yet this area remains underexplored. While
imitation learning has shown promise in acquiring skills from expert
demonstrations, challenges persist in handling uncertain future movements,
learning geometric symmetries, and generalizing to diverse surgical scenarios.
To address these, we introduce a novel approach: Implicit Diffusion Policy with
Equivariant Representations for Imitation Learning (iDPOE). Our method models
expert behavior through a joint state action distribution, capturing the
stochastic nature of dissection trajectories and enabling robust visual
representation learning across various endoscopic views. By incorporating a
diffusion model into policy learning, iDPOE ensures efficient training and
sampling, leading to more accurate predictions and better generalization.
Additionally, we enhance the model's ability to generalize to geometric
symmetries by embedding equivariance into the learning process. To address
state mismatches, we develop a forward-process guided action inference strategy
for conditional sampling. Using an ESD video dataset of nearly 2000 clips,
experimental results show that our approach surpasses state-of-the-art methods,
both explicit and implicit, in trajectory prediction. To the best of our
knowledge, this is the first application of imitation learning to surgical
skill development for dissection trajectory prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Holistic Visual Quality Assessment of AI-Generated Videos: A
  LLM-Based Multi-Dimensional Evaluation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelu Qi, Ping Shi, Chaoyang Zhang, Shuqi Wang, Fei Zhao, Da Pan, Zefeng Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of AI-Generated Video (AIGV) technology has been remarkable
in recent years, significantly transforming the paradigm of video content
production. However, AIGVs still suffer from noticeable visual quality defects,
such as noise, blurriness, frame jitter and low dynamic degree, which severely
impact the user's viewing experience. Therefore, an effective automatic visual
quality assessment is of great importance for AIGV content regulation and
generative model improvement. In this work, we decompose the visual quality of
AIGVs into three dimensions: technical quality, motion quality, and video
semantics. For each dimension, we design corresponding encoder to achieve
effective feature representation. Moreover, considering the outstanding
performance of large language models (LLMs) in various vision and language
tasks, we introduce a LLM as the quality regression module. To better enable
the LLM to establish reasoning associations between multi-dimensional features
and visual quality, we propose a specially designed multi-modal prompt
engineering framework. Additionally, we incorporate LoRA fine-tuning technology
during the training phase, allowing the LLM to better adapt to specific tasks.
Our proposed method achieved \textbf{second place} in the NTIRE 2025 Quality
Assessment of AI-Generated Content Challenge: Track 2 AI Generated video,
demonstrating its effectiveness. Codes can be obtained at
https://github.com/QiZelu/AIGVEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Few-Shot Vision-Language Model Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxin Wang, Tian Liu, Shu Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained VLMs achieve strong performance on downstream tasks when adapted
with just a few labeled examples. As the adapted models inevitably encounter
out-of-distribution (OOD) test data that deviates from the in-distribution (ID)
task-specific training data, enhancing OOD generalization in few-shot
adaptation is critically important. We study robust few-shot VLM adaptation,
aiming to increase both ID and OOD accuracy. By comparing different adaptation
methods (e.g., prompt tuning, linear probing, contrastive finetuning, and full
finetuning), we uncover three key findings: (1) finetuning with proper
hyperparameters significantly outperforms the popular VLM adaptation methods
prompt tuning and linear probing; (2) visual encoder-only finetuning achieves
better efficiency and accuracy than contrastively finetuning both visual and
textual encoders; (3) finetuning the top layers of the visual encoder provides
the best balance between ID and OOD accuracy. Building on these findings, we
propose partial finetuning of the visual encoder empowered with two simple
augmentation techniques: (1) retrieval augmentation which retrieves
task-relevant data from the VLM's pretraining dataset to enhance adaptation,
and (2) adversarial perturbation which promotes robustness during finetuning.
Results show that the former/latter boosts OOD/ID accuracy while slightly
sacrificing the ID/OOD accuracy. Yet, perhaps understandably, naively combining
the two does not maintain their best OOD/ID accuracy. We address this dilemma
with the developed SRAPF, Stage-wise Retrieval Augmentation-based Adversarial
Partial Finetuning. SRAPF consists of two stages: (1) partial finetuning the
visual encoder using both ID and retrieved data, and (2) adversarial partial
finetuning with few-shot ID data. Extensive experiments demonstrate that SRAPF
achieves the state-of-the-art ID and OOD accuracy on the ImageNet OOD
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://hannawang09.github.io/projects/srapf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Line of Sight: On Linear Representations in VLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models can be equipped with multimodal capabilities by fine-tuning
on embeddings of visual inputs. But how do such multimodal models represent
images in their hidden activations? We explore representations of image
concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set
of ImageNet classes represented via linearly decodable features in the residual
stream. We show that the features are causal by performing targeted edits on
the model output. In order to increase the diversity of the studied linear
features, we train multimodal Sparse Autoencoders (SAEs), creating a highly
interpretable dictionary of text and image features. We find that although
model representations across modalities are quite disjoint, they become
increasingly shared in deeper layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta
  Token for Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngwan Lee, Kangsan Kim, Kwanyong Park, Ilcahe Jung, Soojin Jang, Seanie Lee, Yong-Ju Lee, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite emerging efforts to enhance the safety of Vision-Language Models
(VLMs), current approaches face two main shortcomings. 1) Existing
safety-tuning datasets and benchmarks only partially consider how image-text
interactions can yield harmful content, often overlooking contextually unsafe
outcomes from seemingly benign pairs. This narrow coverage leaves VLMs
vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely
primarily on data-centric tuning, with limited architectural innovations to
intrinsically strengthen safety. We address these gaps by introducing a
holistic safety dataset and benchmark, HoliSafe, that spans all five
safe/unsafe image-text combinations, providing a more robust basis for both
training and evaluation. We further propose SafeLLaVA, a novel VLM augmented
with a learnable safety meta token and a dedicated safety head. The meta token
encodes harmful visual cues during training, intrinsically guiding the language
model toward safer responses, while the safety head offers interpretable
harmfulness classification aligned with refusal rationales. Experiments show
that SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety
performance across multiple VLM benchmarks. Additionally, the HoliSafe
benchmark itself reveals critical vulnerabilities in existing models. We hope
that HoliSafe and SafeLLaVA will spur further research into robust and
interpretable VLM safety, expanding future avenues for multimodal alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://youngwanlee.github.io/holisafe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal
  Large Language Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gio Paik, Geewook Kim, Jinbae Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MMRefine, a MultiModal Refinement benchmark designed to
evaluate the error refinement capabilities of Multimodal Large Language Models
(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,
MMRefine provides a framework that evaluates MLLMs' abilities to detect and
correct errors across six distinct scenarios beyond just comparing final
accuracy before and after refinement. Furthermore, the benchmark analyzes the
refinement performance by categorizing errors into six error types. Experiments
with various open and closed MLLMs reveal bottlenecks and factors impeding
refinement performance, highlighting areas for improvement in effective
reasoning enhancement. Our code and dataset are publicly available at
https://github.com/naver-ai/MMRefine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARS: Radio Map Super-resolution and Reconstruction Method under Sparse
  Channel Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyun Deng, Na Liu, Wei Xie, Lianming Xu, Li Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio maps reflect the spatial distribution of signal strength and are
essential for applications like smart cities, IoT, and wireless network
planning. However, reconstructing accurate radio maps from sparse measurements
remains challenging. Traditional interpolation and inpainting methods lack
environmental awareness, while many deep learning approaches depend on detailed
scene data, limiting generalization. To address this, we propose MARS, a
Multi-scale Aware Radiomap Super-resolution method that combines CNNs and
Transformers with multi-scale feature fusion and residual connections. MARS
focuses on both global and local feature extraction, enhancing feature
representation across different receptive fields and improving reconstruction
accuracy. Experiments across different scenes and antenna locations show that
MARS outperforms baseline models in both MSE and SSIM, while maintaining low
computational cost, demonstrating strong practical potential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gen-n-Val: Agentic Image Data Generation and Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing-En Huang, I-Sheng Fang, Tzuhsuan Huang, Chih-Yu Wang, Jun-Cheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) and Vision Large Language Models
(VLLMs) have demonstrated impressive performance as agents across various tasks
while data scarcity and label noise remain significant challenges in computer
vision tasks, such as object detection and instance segmentation. A common
solution for resolving these issues is to generate synthetic data. However,
current synthetic data generation methods struggle with issues, such as
multiple objects per mask, inaccurate segmentation, and incorrect category
labels, limiting their effectiveness. To address these issues, we introduce
Gen-n-Val, a novel agentic data generation framework that leverages Layer
Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks
and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt
agent, an LLM, optimizes prompts for LD to generate high-quality foreground
instance images and segmentation masks. These optimized prompts ensure the
generation of single-object synthetic data with precise instance masks and
clean backgrounds. (2) The data validation agent, a VLLM, which filters out
low-quality synthetic instance images. The system prompts for both agents are
refined through TextGrad. Additionally, we use image harmonization to combine
multiple instances within scenes. Compared to state-of-the-art synthetic data
approaches like MosaicFusion, our approach reduces invalid synthetic data from
50% to 7% and improves performance by 1% mAP on rare classes in COCO instance
segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant
improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object
detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance
of YOLOv9 and YOLO11 families in instance segmentation and object detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Few-Shot Image Classification via Prototypical
  Concept-Guided Mixture of LoRA Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong Ji, Rongshuai Wei, Jingren Liu, Yanwei Pang, Jungong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to
enable their visual recognition processes more interpretable, but they often
struggle in data-scarce settings where insufficient training samples lead to
suboptimal performance.To address this limitation, we propose a Few-Shot
Prototypical Concept Classification (FSPCC) framework that systematically
mitigates two key challenges under low-data regimes: parametric imbalance and
representation misalignment. Specifically, our approach leverages a Mixture of
LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced
allocation of trainable parameters between the backbone and the PCL
module.Meanwhile, cross-module concept guidance enforces tight alignment
between the backbone's feature representations and the prototypical concept
activation patterns.In addition, we incorporate a multi-level feature
preservation strategy that fuses spatial and semantic cues across various
layers, thereby enriching the learned representations and mitigating the
challenges posed by limited data availability.Finally, to enhance
interpretability and minimize concept overlap, we introduce a geometry-aware
concept discrimination loss that enforces orthogonality among concepts,
encouraging more disentangled and transparent decision boundaries.Experimental
results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS,
Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach
consistently outperforms existing SEMs by a notable margin, with 4.2%-8.7%
relative gains in 5-way 5-shot classification.These findings highlight the
efficacy of coupling concept learning with few-shot adaptation to achieve both
higher accuracy and clearer model interpretability, paving the way for more
transparent visual recognition systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-Based Lie Group <span class="highlight-title">Transformer</span> for Real-World Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takayuki Komatsu, Yoshiyuki Ohmura, Kayato Nishitsunoi, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main goal of representation learning is to acquire meaningful
representations from real-world sensory inputs without supervision.
Representation learning explains some aspects of human development. Various
neural network (NN) models have been proposed that acquire empirically good
representations. However, the formulation of a good representation has not been
established. We recently proposed a method for categorizing changes between a
pair of sensory inputs. A unique feature of this approach is that
transformations between two sensory inputs are learned to satisfy algebraic
structural constraints. Conventional representation learning often assumes that
disentangled independent feature axes is a good representation; however, we
found that such a representation cannot account for conditional independence.
To overcome this problem, we proposed a new method using group decomposition in
Galois algebra theory. Although this method is promising for defining a more
general representation, it assumes pixel-to-pixel translation without feature
extraction, and can only process low-resolution images with no background,
which prevents real-world application. In this study, we provide a simple
method to apply our group decomposition theory to a more realistic scenario by
combining feature extraction and object segmentation. We replace pixel
translation with feature translation and formulate object segmentation as
grouping features under the same transformation. We validated the proposed
method on a practical dataset containing both real-world object and background.
We believe that our model will lead to a better understanding of human
development of object recognition in the real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fast Unsupervised Scheme for Polygonal Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bimal Kumar Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a fast and unsupervised scheme for a polygonal
approximation of a closed digital curve. It is demonstrated that the
approximation scheme is faster than state-of-the-art approximation and is
competitive with the same in Rosin's measure and in its aesthetic aspect. The
scheme comprises of three phases: initial segmentation, iterative vertex
insertion, and iterative merging, followed by vertex adjustment. The initial
segmentation is used to detect sharp turnings - the vertices that seemingly
have high curvature. It is likely that some of important vertices with low
curvature might have been missed out at the first phase and so iterative vertex
insertion is used to add vertices in a region where the curvature changes
slowly but steadily. The initial phase may pick up some undesirable vertices
and so merging is used to eliminate the redundant vertices. Finally, vertex
adjustment is used to facilitate enhancement in the aesthetic look of the
approximation. The quality of the approximations is measured using Rosin's
measure. The robustness of the proposed scheme with respect to geometric
transformation is observed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReasonGen-R1: CoT for Autoregressive Image generation models through SFT
  and RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although chain-of-thought reasoning and reinforcement learning (RL) have
driven breakthroughs in NLP, their integration into generative vision models
remains underexplored. We introduce ReasonGen-R1, a two-stage framework that
first imbues an autoregressive image generator with explicit text-based
"thinking" skills via supervised fine-tuning on a newly generated reasoning
dataset of written rationales, and then refines its outputs using Group
Relative Policy Optimization. To enable the model to reason through text before
generating images, We automatically generate and release a corpus of model
crafted rationales paired with visual prompts, enabling controlled planning of
object layouts, styles, and scene compositions. Our GRPO algorithm uses reward
signals from a pretrained vision language model to assess overall visual
quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and
the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong
baselines and prior state-of-the-art models. More: aka.ms/reasongen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10510v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10510v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Braun, Mark Rothermel, Marcus Rohrbach, Anna Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of disinformation demands reliable and scalable
fact-checking solutions. We present Dynamic Evidence-based FAct-checking with
Multimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for
open-domain, text-image claim verification. DEFAME operates in a six-stage
process, dynamically selecting the tools and search depth to extract and
evaluate textual and visual evidence. Unlike prior approaches that are
text-only, lack explainability, or rely solely on parametric knowledge, DEFAME
performs end-to-end verification, accounting for images in claims and evidence
while generating structured, multimodal reports. Evaluation on the popular
benchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all
previous methods, establishing itself as the new state-of-the-art fact-checking
system for uni- and multimodal fact-checking. Moreover, we introduce a new
multimodal benchmark, ClaimReview2024+, featuring claims after the knowledge
cutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms
the GPT-4o baselines, showing temporal generalizability and the potential for
real-time fact-checking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Poisson Surface Reconstruction with One Solve using Geometric
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidhanth Holalkere, David S. Bindel, Silvia Sellán, Alexander Terenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poisson Surface Reconstruction is a widely-used algorithm for reconstructing
a surface from an oriented point cloud. To facilitate applications where only
partial surface information is available, or scanning is performed
sequentially, a recent line of work proposes to incorporate uncertainty into
the reconstructed surface via Gaussian process models. The resulting algorithms
first perform Gaussian process interpolation, then solve a set of volumetric
partial differential equations globally in space, resulting in a
computationally expensive two-stage procedure. In this work, we apply
recently-developed techniques from geometric Gaussian processes to combine
interpolation and surface reconstruction into a single stage, requiring only
one linear solve per sample. The resulting reconstructed surface samples can be
queried locally in space, without the use of problem-dependent volumetric
meshes or grids. These capabilities enable one to (a) perform probabilistic
collision detection locally around the region of interest, (b) perform ray
casting without evaluating points not on the ray's trajectory, and (c) perform
next-view planning on a per-ray basis. They also do not requiring one to
approximate kernel matrix inverses with diagonal matrices as part of
intermediate computations, unlike prior methods. Results show that our approach
provides a cleaner, more-principled, and more-flexible stochastic surface
reconstruction pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxiong Lei, Mingyue Zhao, Yuheng Wang, Shijing He, Yusuke Sugano, Mohamed Khamis, Juan Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as
users naturally change their postures and device orientations. Traditional
calibration approaches, like one-off, fail to adapt to these dynamic
conditions, leading to degraded performance over time. We present MAC-Gaze, a
Motion-Aware continual Calibration approach that leverages smartphone Inertial
measurement unit (IMU) sensors and continual learning techniques to
automatically detect changes in user motion states and update the gaze tracking
model accordingly. Our system integrates a pre-trained visual gaze estimator
and an IMU-based activity recognition model with a clustering-based hybrid
decision-making mechanism that triggers recalibration when motion patterns
deviate significantly from previously encountered states. To enable
accumulative learning of new motion conditions while mitigating catastrophic
forgetting, we employ replay-based continual learning, allowing the model to
maintain performance across previously encountered motion conditions. We
evaluate our system through extensive experiments on the publicly available
RGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+
images, 800K+ IMU readings), encompassing a wide range of postures under
various motion conditions including sitting, standing, lying, and walking.
Results demonstrate that our method reduces gaze estimation error by 19.9% on
RGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to
1.92 cm) compared to traditional calibration approaches. Our framework provides
a robust solution for maintaining gaze estimation accuracy in mobile scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing unified models achieve strong performance in
vision-language understanding and text-to-image generation, they remain limited
in addressing image perception and manipulation -- capabilities increasingly
demanded in practical applications. Recently, OpenAI introduced the powerful
GPT-4o-Image model, which showcases advanced capabilities in comprehensive
image perception and manipulation, sparking widespread interest. Through
carefully designed experiments, we observe that GPT-4o-Image likely relies on
semantic encoders rather than VAEs for feature extraction, despite VAEs being
commonly regarded as crucial for image manipulation tasks. Inspired by this
insight, we propose UniWorld-V1, a unified generative framework built upon
semantic features extracted from powerful multimodal large language models and
contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1
achieves impressive performance across diverse tasks, including image
understanding, generation, manipulation, and perception. We fully open-source
the UniWorld-V1 framework, including model weights, training and evaluation
scripts, and datasets to promote reproducibility and further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal
  Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.18053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.18053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) pose unique safety challenges due to
their integration of visual and textual data, thereby introducing new
dimensions of potential attacks and complex risk combinations. In this paper,
we begin with a detailed analysis aimed at disentangling risks through
step-by-step reasoning within multimodal inputs. We find that systematic
multimodal risk disentanglement substantially enhances the risk awareness of
MLLMs. Via leveraging the strong discriminative abilities of multimodal risk
disentanglement, we further introduce \textbf{DREAM}
(\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety
\textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety
alignment in MLLMs through supervised fine-tuning and iterative Reinforcement
Learning from AI Feedback (RLAIF). Experimental results show that DREAM
significantly boosts safety during both inference and training phases without
compromising performance on normal tasks (namely oversafety), achieving a
16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The
data and code are available at https://github.com/Kizna1ver/DREAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[NAACL 2025] The first four authors contribute equally, 23 pages,
  repo at https://github.com/Kizna1ver/DREAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SR3D: Unleashing Single-view 3D Reconstruction for Transparent and
  Specular Object Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Zhang, Xiaoqi Li, Jiahui Xu, Kaichen Zhou, Hojin Bae, Yan Shen, Chuyan Xiong, Jiaming Liu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D robotic manipulation have improved grasping of
everyday objects, but transparent and specular materials remain challenging due
to depth sensing limitations. While several 3D reconstruction and depth
completion approaches address these challenges, they suffer from setup
complexity or limited observation information utilization. To address this,
leveraging the power of single view 3D object reconstruction approaches, we
propose a training free framework SR3D that enables robotic grasping of
transparent and specular objects from a single view observation. Specifically,
given single view RGB and depth images, SR3D first uses the external visual
models to generate 3D reconstructed object mesh based on RGB image. Then, the
key idea is to determine the 3D object's pose and scale to accurately localize
the reconstructed object back into its original depth corrupted 3D scene.
Therefore, we propose view matching and keypoint matching mechanisms,which
leverage both the 2D and 3D's inherent semantic and geometric information in
the observation to determine the object's 3D state within the scene, thereby
reconstructing an accurate 3D depth map for effective grasp detection.
Experiments in both simulation and real world show the reconstruction
effectiveness of SR3D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian
  Primitives for Weakly-Supervised Cross-View Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwei Wang, Shaoxun Wu, Yujiao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of weakly supervised cross-view
localization, where the goal is to estimate the pose of a ground camera
relative to a satellite image with noisy ground truth annotations. A common
approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye
View (BEV) synthesis. However, existing methods struggle with height ambiguity
due to the lack of depth information in ground images and satellite height
maps. Previous solutions either assume a flat ground plane or rely on complex
models, such as cross-view transformers. We propose BevSplat, a novel method
that resolves height ambiguity by using feature-based Gaussian primitives. Each
pixel in the ground image is represented by a 3D Gaussian with semantic and
spatial features, which are synthesized into a BEV feature map for relative
pose estimation. Additionally, to address challenges with panoramic query
images, we introduce an icosphere-based supervision strategy for the Gaussian
primitives. We validate our method on the widely used KITTI and VIGOR datasets,
which include both pinhole and panoramic query images. Experimental results
show that BevSplat significantly improves localization accuracy over prior
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection-Driven Object Count Optimization for Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oz Zafar, Yuval Cohen, Lior Wolf, Idan Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately controlling object count in text-to-image generation remains a key
challenge. Supervised methods often fail, as training data rarely covers all
count variations. Methods that manipulate the denoising process to add or
remove objects can help; however, they still require labeled data, limit
robustness and image quality, and rely on a slow, iterative process.
  Pre-trained differentiable counting models that rely on soft object density
summation exist and could steer generation, but employing them presents three
main challenges: (i) they are pre-trained on clean images, making them less
effective during denoising steps that operate on noisy inputs; (ii) they are
not robust to viewpoint changes; and (iii) optimization is computationally
expensive, requiring repeated model evaluations per image.
  We propose a new framework that uses pre-trained object counting techniques
and object detectors to guide generation. First, we optimize a counting token
using an outer-loop loss computed on fully generated images. Second, we
introduce a detection-driven scaling term that corrects errors caused by
viewpoint and proportion shifts, among other factors, without requiring
backpropagation through the detection model. Third, we show that the optimized
parameters can be reused for new prompts, removing the need for repeated
optimization. Our method provides efficiency through token reuse, flexibility
via compatibility with various detectors, and accuracy with improved counting
across diverse object categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyTop: Character Animation Diffusion with Any Topology <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit H. Bermano, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating motion for arbitrary skeletons is a longstanding challenge in
computer graphics, remaining largely unexplored due to the scarcity of diverse
datasets and the irregular nature of the data. In this work, we introduce
AnyTop, a diffusion model that generates motions for diverse characters with
distinct motion dynamics, using only their skeletal structure as input. Our
work features a transformer-based denoising network, tailored for arbitrary
skeleton learning, integrating topology information into the traditional
attention mechanism. Additionally, by incorporating textual joint descriptions
into the latent feature representation, AnyTop learns semantic correspondences
between joints across diverse skeletons. Our evaluation demonstrates that
AnyTop generalizes well, even with as few as three training examples per
topology, and can produce motions for unseen skeletons as well. Furthermore,
our model's latent space is highly informative, enabling downstream tasks such
as joint correspondence, temporal segmentation and motion editing. Our webpage,
https://anytop2025.github.io/Anytop-page, includes links to videos and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025. Video: https://www.youtube.com/watch?v=NWOdkM5hAbE,
  Project page: https://anytop2025.github.io/Anytop-page, Code:
  https://github.com/Anytop2025/Anytop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eddeep: Fast eddy-current distortion correction for diffusion MRI with
  deep learning <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10723v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10723v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Legouhy, Ross Callaghan, Whitney Stee, Philippe Peigneux, Hojjat Azadbakht, Hui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern diffusion MRI sequences commonly acquire a large number of volumes
with diffusion sensitization gradients of differing strengths or directions.
Such sequences rely on echo-planar imaging (EPI) to achieve reasonable scan
duration. However, EPI is vulnerable to off-resonance effects, leading to
tissue susceptibility and eddy-current induced distortions. The latter is
particularly problematic because it causes misalignment between volumes,
disrupting downstream modelling and analysis. The essential correction of eddy
distortions is typically done post-acquisition, with image registration.
However, this is non-trivial because correspondence between volumes can be
severely disrupted due to volume-specific signal attenuations induced by
varying directions and strengths of the applied gradients. This challenge has
been successfully addressed by the popular FSL~Eddy tool but at considerable
computational cost. We propose an alternative approach, leveraging recent
advances in image processing enabled by deep learning (DL). It consists of two
convolutional neural networks: 1) An image translator to restore correspondence
between images; 2) A registration model to align the translated images. Results
demonstrate comparable distortion estimates to FSL~Eddy, while requiring only
modest training sample sizes. This work, to the best of our knowledge, is the
first to tackle this problem with deep learning. Together with recently
developed DL-based susceptibility correction techniques, they pave the way for
real-time preprocessing of diffusion MRI, facilitating its wider uptake in the
clinic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MICCAI 2024 conference (without rebuttal). Github repo:
  https://github.com/CIG-UCL/eddeep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniCharacter: Towards Immersive Role-Playing Agents with Seamless
  Speech-Language Personality Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Zhang, Run Luo, Xiong Liu, Yuchuan Wu, Ting-En Lin, Pengpeng Zeng, Qiang Qu, Feiteng Fang, Min Yang, Lianli Gao, Jingkuan Song, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-Playing Agents (RPAs), benefiting from large language models, is an
emerging interactive AI system that simulates roles or characters with diverse
personalities. However, existing methods primarily focus on mimicking dialogues
among roles in textual form, neglecting the role's voice traits (e.g., voice
style and emotions) as playing a crucial effect in interaction, which tends to
be more immersive experiences in realistic scenarios. Towards this goal, we
propose OmniCharacter, a first seamless speech-language personality interaction
model to achieve immersive RPAs with low latency. Specifically, OmniCharacter
enables agents to consistently exhibit role-specific personality traits and
vocal traits throughout the interaction, enabling a mixture of speech and
language responses. To align the model with speech-language scenarios, we
construct a dataset named OmniCharacter-10K, which involves more distinctive
characters (20), richly contextualized multi-round dialogue (10K), and dynamic
speech response (135K). Experimental results showcase that our method yields
better responses in terms of both content and style compared to existing RPAs
and mainstream speech-language models, with a response latency as low as 289ms.
Code and dataset are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VCD: A <span class="highlight-title">Dataset</span> for Visual Commonsense Discovery in Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangqing Shen, Fanfan Wang, Siwei Wu, Rui Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual commonsense plays a vital role in understanding and reasoning about
the visual world. While commonsense knowledge bases like ConceptNet provide
structured collections of general facts, they lack visually grounded
representations. Scene graph datasets like Visual Genome, though rich in
object-level descriptions, primarily focus on directly observable information
and lack systematic categorization of commonsense knowledge. We present Visual
Commonsense Dataset (VCD), a large-scale dataset containing over 100,000 images
and 14 million object-commonsense pairs that bridges this gap. VCD introduces a
novel three-level taxonomy for visual commonsense, integrating both Seen
(directly observable) and Unseen (inferrable) commonsense across Property,
Action, and Space aspects. Each commonsense is represented as a triple where
the head entity is grounded to object bounding boxes in images, enabling
scene-dependent and object-specific visual commonsense representation. To
demonstrate VCD's utility, we develop VCM, a generative model that combines a
vision-language model with instruction tuning to discover diverse visual
commonsense from images. Extensive evaluations demonstrate both the high
quality of VCD and its value as a resource for advancing visually grounded
commonsense understanding and reasoning. Our dataset and code will be released
on https://github.com/NUSTM/VCD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing Everywhere: Parameter-Efficient Fine-Tuning for Medical
  Image Analysis via Target Parameter <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingliang Lei, Yiwen Ye, Zhisong Wang, Ziyang Chen, Minglei Shu, Weidong Cai, Yanning Zhang, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) techniques have emerged to address
overfitting and high computational costs associated with fully fine-tuning in
self-supervised learning. Mainstream PEFT methods add a few trainable
parameters while keeping the pre-trained backbone parameters fixed. These
methods achieve comparative, and often superior, performance to fully
fine-tuning, demonstrating the powerful representation ability of the
pre-trained backbone. Despite this success, these methods typically ignore the
initialization of the new parameters, often relying solely on random
initialization. We argue that if pre-training is significantly beneficial, it
should be applied to all parameters requiring representational capacity.
Motivated by this, we propose Target Parameter Pre-training (TPP), a simple yet
effective fine-tuning framework. TPP pre-trains target parameters, i.e., the
new parameters introduced during fine-tuning, in an additional stage before
PEFT. During this stage, the pre-trained backbone parameters are frozen, and
only the new parameters are trainable. A defined pretext task encourages the
new parameters to learn specific representations of downstream data.
Subsequently, when PEFT is employed, the pre-trained new parameters are loaded
to enhance fine-tuning efficiency. The proposed TPP framework is versatile,
allowing integration with various pre-trained backbones, pretext tasks, and
PEFT methods. We evaluated the fine-tuning performance of our method on seven
public datasets, covering four modalities and two task types. The results
demonstrate that TPP can be easily integrated into existing PEFT methods,
significantly improving performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convex Relaxation for Robust Vanishing Point Estimation in Manhattan
  World <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04788v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04788v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangyan Liao, Zhenjun Zhao, Haoang Li, Yi Zhou, Yingping Zeng, Hao Li, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the vanishing points (VPs) in a Manhattan world, as a fundamental
task in many 3D vision applications, consists of jointly inferring the line-VP
association and locating each VP. Existing methods are, however, either
sub-optimal solvers or pursuing global optimality at a significant cost of
computing time. In contrast to prior works, we introduce convex relaxation
techniques to solve this task for the first time. Specifically, we employ a
"soft" association scheme, realized via a truncated multi-selection error, that
allows for joint estimation of VPs' locations and line-VP associations. This
approach leads to a primal problem that can be reformulated into a
quadratically constrained quadratic programming (QCQP) problem, which is then
relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP
problem efficiently, we present a globally optimal outlier-robust iterative
solver (called GlobustVP), which independently searches for one VP and its
associated lines in each iteration, treating other lines as outliers. After
each independent update of all VPs, the mutual orthogonality between the three
VPs in a Manhattan world is reinforced via local refinement. Extensive
experiments on both synthetic and real-world data demonstrate that GlobustVP
achieves a favorable balance between efficiency, robustness, and global
optimality compared to previous works. The code is publicly available at
https://github.com/WU-CVGL/GlobustVP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025 as Award Candidate & Oral Presentation. The
  first two authors contributed equally to this work. Code:
  https://github.com/WU-CVGL/GlobustVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential
  via Self-Attention Redirection Guidance <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12974v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12974v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benlei Cui, Wenhao Sun, Xue-Mei Dong, Jingqun Tang, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion models have emerged as promising newcomers in the field
of generative models, shining brightly in image generation. However, when
employed for object removal tasks, they still encounter issues such as
generating random artifacts and the incapacity to repaint foreground object
areas with appropriate content after remova1l. To tackle these problems, we
propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion
models for stable and effective object removal. Firstly, in light of the
observation that the self-attention maps influence the structure and shape
details of the generated images, we propose Attention Activation and
Suppression (ASS), which re-engineers the self-attention mechanism within the
pre-trained diffusion models based on the given mask, thereby prioritizing the
background over the foreground object during the reverse generation process.
Moreover, we introduce Self-Attention Redirection Guidance (SARG), which
utilizes the self-attention redirected by ASS to guide the generation process,
effectively removing foreground objects within the mask while simultaneously
generating content that is both plausible and coherent. Experiments demonstrate
the stability and effectiveness of Attentive Eraser in object removal across a
variety of pre-trained diffusion models, outperforming even training-based
methods. Furthermore, Attentive Eraser can be implemented in various diffusion
model architectures and checkpoints, enabling excellent scalability. Code is
available at https://github.com/Anonym0u3/AttentiveEraser.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025(Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-Instruct++: Training One-step Text-to-image Generator Model to
  Align with Human Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-step text-to-image generator models offer advantages such as swift
inference efficiency, flexible architectures, and state-of-the-art generation
performance. In this paper, we study the problem of aligning one-step generator
models with human preferences for the first time. Inspired by the success of
reinforcement learning using human feedback (RLHF), we formulate the alignment
problem as maximizing expected human reward functions while adding an Integral
Kullback-Leibler divergence term to prevent the generator from diverging. By
overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the
first, fast-converging and image data-free human preference alignment method
for one-step text-to-image generators. We also introduce novel theoretical
insights, showing that using CFG for diffusion distillation is secretly doing
RLHF with DI++. Such an interesting finding brings understanding and potential
contributions to future research involving CFG. In the experiment sections, we
align both UNet-based and DiT-based one-step generators using DI++, which use
the Stable Diffusion 1.5 and the PixelArt-$\alpha$ as the reference diffusion
processes. The resulting DiT-based one-step text-to-image model achieves a
strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO
validation prompt dataset. It also achieves a leading Human preference Score
(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable
Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\alpha$. Both theoretical
contributions and empirical evidence indicate that DI++ is a strong
human-preference alignment approach for one-step text-to-image models. The
homepage of the paper is https://github.com/pkulwj1994/diff_instruct_pp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision: The paper was accepted by Transactions of Machine Learning
  Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAID: A <span class="highlight-title">Dataset</span> for Testing the Adversarial Robustness of AI-Generated
  Image Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustness. Our findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenLit: Reformulating Single-Image Relighting as Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrisha Bharadwaj, Haiwen Feng, Giorgio Becherini, Victoria Abrevaya, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating the illumination of a 3D scene within a single image represents
a fundamental challenge in computer vision and graphics. This problem has
traditionally been addressed using inverse rendering techniques, which involve
explicit 3D asset reconstruction and costly ray-tracing simulations. Meanwhile,
recent advancements in visual foundation models suggest that a new paradigm
could soon be possible -- one that replaces explicit physical models with
networks that are trained on large amounts of image and video data. In this
paper, we exploit the physical world understanding of a video diffusion model,
particularly Stable Video Diffusion, to relight a single image. We introduce
GenLit, a framework that distills the ability of a graphics engine to perform
light manipulation into a video-generation model, enabling users to directly
insert and manipulate a point light in the 3D world within a given image, and
generate results directly as a video sequence. We find that a model fine-tuned
on only a small synthetic dataset generalizes to real-world scenes, enabling
single-image relighting with plausible and convincing shadows. Our results
highlight the ability of video foundation models to capture rich information
about lighting, material, and, shape and our findings indicate that such
models, with minimal training, can be used to perform relighting without
explicit asset reconstruction or complex ray tracing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Place Recognition Meet Multiple Modalitie: A Comprehensive <span class="highlight-title">Review</span>,
  Current Challenges and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14068v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14068v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Li, Tianyi Shang, Pengjie Xu, Zhaojun Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is a cornerstone of vehicle navigation and mapping, which
is pivotal in enabling systems to determine whether a location has been
previously visited. This capability is critical for tasks such as loop closure
in Simultaneous Localization and Mapping (SLAM) and long-term navigation under
varying environmental conditions. In this survey, we comprehensively review
recent advancements in place recognition, emphasizing three representative
methodological paradigms: Convolutional Neural Network (CNN)-based approaches,
Transformer-based frameworks, and cross-modal strategies. We begin by
elucidating the significance of place recognition within the broader context of
autonomous systems. Subsequently, we trace the evolution of CNN-based methods,
highlighting their contributions to robust visual descriptor learning and
scalability in large-scale environments. We then examine the emerging class of
Transformer-based models, which leverage self-attention mechanisms to capture
global dependencies and offer improved generalization across diverse scenes.
Furthermore, we discuss cross-modal approaches that integrate heterogeneous
data sources such as Lidar, vision, and text description, thereby enhancing
resilience to viewpoint, illumination, and seasonal variations. We also
summarize standard datasets and evaluation metrics widely adopted in the
literature. Finally, we identify current research challenges and outline
prospective directions, including domain adaptation, real-time performance, and
lifelong learning, to inspire future advancements in this domain. The unified
framework of leading-edge place recognition methods, i.e., code library, and
the results of their experimental evaluations are available at
https://github.com/CV4RA/SOTA-Place-Recognitioner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Motion Agents in Dynamic and Cluttered Environments through
  LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper advances motion agents empowered by large language models (LLMs)
toward autonomous navigation in dynamic and cluttered environments,
significantly surpassing first and recent seminal but limited studies on LLM's
spatial reasoning, where movements are restricted in four directions in simple,
static environments in the presence of only single agents much less multiple
agents. Specifically, we investigate LLMs as spatial reasoners to overcome
these limitations by uniformly encoding environments (e.g., real indoor
floorplans), agents which can be dynamic obstacles and their paths as discrete
tokens akin to language tokens. Our training-free framework supports
multi-agent coordination, closed-loop replanning, and dynamic obstacle
avoidance without retraining or fine-tuning. We show that LLMs can generalize
across agents, tasks, and environments using only text-based interactions,
opening new possibilities for semantically grounded, interactive navigation in
both simulation and embodied systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ David and Goliath: Small One-step Model Beats Large Diffusion with Score
  Post-training <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo, Colin Zhang, Debing Zhang, Zhengyang Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diff-Instruct* (DI*), a data-efficient post-training approach for
one-step text-to-image generative models to improve its human preferences
without requiring image data. Our method frames alignment as online
reinforcement learning from human feedback (RLHF), which optimizes the one-step
model to maximize human reward functions while being regularized to be kept
close to a reference diffusion process. Unlike traditional RLHF approaches,
which rely on the Kullback-Leibler divergence as the regularization, we
introduce a novel general score-based divergence regularization that
substantially improves performance as well as post-training stability. Although
the general score-based RLHF objective is intractable to optimize, we derive a
strictly equivalent tractable loss function in theory that can efficiently
compute its \emph{gradient} for optimizations. We introduce
\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a
resolution of $1024\times 1024$, post-trained from DMD2 w.r.t SDXL. \textbf{Our
2.6B \emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in
ImageReward, PickScore, and CLIP score on the Parti prompts benchmark while
using only 1.88\% of the inference time. This result clearly shows that with
proper post-training, the small one-step model is capable of beating huge
multi-step diffusion models. Our model is open-sourced at this link:
https://github.com/pkulwj1994/diff_instruct_star. We hope our findings can
contribute to human-centric machine learning techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision: paper accepted by the ICML2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sonic: Shifting Focus to Global Audio Perception in Portrait Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, Qinglin Lu, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of talking face generation mainly explores the intricacies of
synchronizing facial movements and crafting visually appealing,
temporally-coherent animations. However, due to the limited exploration of
global audio perception, current approaches predominantly employ auxiliary
visual and spatial knowledge to stabilize the movements, which often results in
the deterioration of the naturalness and temporal inconsistencies.Considering
the essence of audio-driven animation, the audio signal serves as the ideal and
unique priors to adjust facial expressions and lip movements, without resorting
to interference of any visual signals. Based on this motivation, we propose a
novel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of
global audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,
we disentangle it into intra- and inter-clip audio perception and collaborate
with both aspects to enhance overall perception.For the intra-clip audio
perception, 1). \textbf{Context-enhanced audio learning}, in which long-range
intra-clip temporal audio knowledge is extracted to provide facial expression
and lip motion priors implicitly expressed as the tone and speed of speech. 2).
\textbf{Motion-decoupled controller}, in which the motion of the head and
expression movement are disentangled and independently controlled by
intra-audio clips. Most importantly, for inter-clip audio perception, as a
bridge to connect the intra-clips to achieve the global perception,
\textbf{Time-aware position shift fusion}, in which the global inter-clip audio
information is considered and fused for long-audio inference via through
consecutively time-aware shifted windows. Extensive experiments demonstrate
that the novel audio-driven paradigm outperform existing SOTA methodologies in
terms of video quality, temporally consistency, lip synchronization precision,
and motion diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>refer to our main-page \url{https://jixiaozhong.github.io/Sonic/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Visual Fusion Graph Neural Networks for Multi-Person
  Human-Object Interaction Recognition in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanqiu Qiao, Ruochen Li, Frederick W. B. Li, Yoshiki Kubotani, Shigeo Morishima, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-Object Interaction (HOI) recognition in videos requires understanding
both visual patterns and geometric relationships as they evolve over time.
Visual and geometric features offer complementary strengths. Visual features
capture appearance context, while geometric features provide structural
patterns. Effectively fusing these multimodal features without compromising
their unique characteristics remains challenging. We observe that establishing
robust, entity-specific representations before modeling interactions helps
preserve the strengths of each modality. Therefore, we hypothesize that a
bottom-up approach is crucial for effective multimodal fusion. Following this
insight, we propose the Geometric Visual Fusion Graph Neural Network
(GeoVis-GNN), which uses dual-attention feature fusion combined with
interdependent entity graph learning. It progressively builds from
entity-specific representations toward high-level interaction understanding. To
advance HOI recognition to real-world scenarios, we introduce the Concurrent
Partial Interaction Dataset (MPHOI-120). It captures dynamic multi-person
interactions involving concurrent actions and partial engagement. This dataset
helps address challenges like complex human-object dynamics and mutual
occlusions. Extensive experiments demonstrate the effectiveness of our method
across various HOI scenarios. These scenarios include two-person interactions,
single-person activities, bimanual manipulations, and complex concurrent
partial interactions. Our method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Expert Systems with Applications (ESWA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Images are Worth Variable Length of Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjun Mao, Rodolfo Corona, Xin Liang, Wenhao Yan, Zineng Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing vision encoders map images into a fixed-length sequence of
tokens, overlooking the fact that different images contain varying amounts of
information. For example, a visually complex image (e.g., a cluttered room)
inherently carries more information and thus deserves more tokens than a simple
image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a
dynamic vision encoder that produces a variable number of visual tokens (i.e.,
continuous representation vectors) to reconstruct each image. Our results show
that DOVE significantly reduces the average number of tokens while maintaining
high reconstruction quality. In several linear probing and downstream
multimodal tasks, it outperforms existing autoencoder-based tokenization
methods when using far fewer tokens, capturing more expressive semantic
features compared to fixed-length encoding. We further extend DOVE with
query-conditioned tokenization. By guiding the model to focus on query-relevant
regions, it achieves more efficient and targeted semantic extraction. Our code
and checkpoints are available at https://dove-encoder.github.io/dove-encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16462v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16462v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Zhixin Li, Zhi Liu, Pengyuan Zhou, Richang Hong, Qiyue Li, Han Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid deep convolution model for lung cancer detection with transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sugandha Saxena, S. N. Prasad, Ashwin M Polnaya, Shweta Agarwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in healthcare research have significantly enhanced our understanding
of disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung
cancer remains one of the leading causes of cancer-related mortality worldwide
due to challenges in early and accurate diagnosis. While current lung cancer
detection models show promise, there is considerable potential for further
improving the accuracy for timely intervention. To address this challenge, we
introduce a hybrid deep convolution model leveraging transfer learning, named
the Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the
precision of lung cancer detection by refining sensitivity and specificity.
This model has surpassed existing deep learning approaches through experimental
validation, achieving an accuracy of 98% and a sensitivity of 97%. By
overlaying sensitivity maps onto lung Computed Tomography (CT) scans, it
enables the visualization of regions most indicative of malignant or benign
classifications. This innovative method demonstrates exceptional performance in
distinguishing lung cancer with minimal false positives, thereby enhancing the
accuracy of medical diagnoses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors realized mistake in the model. Also some data was
  misinterpreted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging
  Benchmark <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Arbash, Ahmed Jamal Afifi, Ymane Belahsen, Margret Fuchs, Pedram Ghamisi, Paul Scheunders, Richard Gloaguen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global challenge of sustainable recycling demands automated, fast, and
accurate, state-of-the-art (SOTA) material detection systems that act as a
bedrock for a circular economy. Democratizing access to these cutting-edge
solutions that enable real-time waste analysis is essential for scaling up
recycling efforts and fostering the Green Deal. In response, we introduce
\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to
accelerate the recovery of critical raw materials through accurate electrolyzer
materials classification. The dataset comprises 55 co-registered
high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning
the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and
424,169 labeled ones. This enables non-invasive spectral analysis of shredded
electrolyzer samples, supporting quantitative and qualitative material
classification and spectral properties investigation. We evaluate a suite of
baseline machine learning (ML) methods alongside SOTA transformer-based deep
learning (DL) architectures, including Vision Transformer, SpectralFormer, and
the Multimodal Fusion Transformer, to investigate architectural bottlenecks for
further efficiency optimisation when deploying transformers in material
identification. We implement zero-shot detection techniques and majority voting
across pixel-level predictions to establish object-level classification
robustness. In adherence to the FAIR data principles, the electrolyzers-HSI
dataset and accompanying codebase are openly available at
https://github.com/hifexplo/Electrolyzers-HSI and
https://rodare.hzdr.de/record/3668, supporting reproducible research and
facilitating the broader adoption of smart and sustainable e-waste recycling
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reading Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charig Yang, Samiul Alam, Shakhrul Iman Siam, Michael J. Proulx, Lambert Mathias, Kiran Somasundaram, Luis Pesqueira, James Fort, Sheroze Sheriffdeen, Omkar Parkhi, Carl Ren, Mi Zhang, Yuning Chai, Richard Newcombe, Hyo Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:
  https://www.projectaria.com/datasets/reading-in-the-wild/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowCut: Rethinking Redundancy via Information Flow for Efficient
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
pruning methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware pruning framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohyun Lee, Yeho Gwon, Lukas Hoyer, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving robustness of the Segment Anything Model (SAM) to input
degradations is critical for its deployment in high-stakes applications such as
autonomous driving and robotics. Our approach to this challenge prioritizes
three key aspects: first, parameter efficiency to maintain the inherent
generalization capability of SAM; second, fine-grained and input-aware
robustification to precisely address the input corruption; and third, adherence
to standard training protocols for ease of training. To this end, we propose
gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into
intermediate layers of the frozen SAM, where each adapter dynamically adjusts
the effective rank of its weight matrix based on the input by selectively
activating (rank-1) components of the matrix using a learned gating module.
This adjustment enables fine-grained and input-aware robustification without
compromising the generalization capability of SAM. Our model, GaRA-SAM,
significantly outperforms prior work on all robust segmentation benchmarks. In
particular, it surpasses the previous best IoU score by up to 21.3\%p on ACDC,
a challenging real corrupted image dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Stability-Plasticity Trade-off in Continual Learning from
  an Architectural Perspective <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aojun Lu, Hangjie Yuan, Tao Feng, Yanan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters. Code:
https://github.com/byyx666/Dual-Arch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning for Text Recognition: A Critical <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Penarrubia, Jose J. Valero-Mas, Jorge Calvo-Zaragoza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text Recognition (TR) refers to the research area that focuses on retrieving
textual information from images, a topic that has seen significant advancements
in the last decade due to the use of Deep Neural Networks (DNN). However, these
solutions often necessitate vast amounts of manually labeled or synthetic data.
Addressing this challenge, Self-Supervised Learning (SSL) has gained attention
by utilizing large datasets of unlabeled data to train DNN, thereby generating
meaningful and robust representations. Although SSL was initially overlooked in
TR because of its unique characteristics, recent years have witnessed a surge
in the development of SSL methods specifically for this field. This rapid
development, however, has led to many methods being explored independently,
without taking previous efforts in methodology or comparison into account,
thereby hindering progress in the field of research. This paper, therefore,
seeks to consolidate the use of SSL in the field of TR, offering a critical and
comprehensive overview of the current state of the art. We will review and
analyze the existing methods, compare their results, and highlight
inconsistencies in the current literature. This thorough analysis aims to
provide general insights into the field, propose standardizations, identify new
research directions, and foster its proper development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Journal of Computer Vision (IJCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-CAD Generation Through Infusing Visual Feedback in Large
  Language Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating Computer-Aided Design (CAD) models requires significant expertise
and effort. Text-to-CAD, which converts textual descriptions into CAD
parametric sequences, is crucial in streamlining this process. Recent studies
have utilized ground-truth parametric sequences, known as sequential signals,
as supervision to achieve this goal. However, CAD models are inherently
multimodal, comprising parametric sequences and corresponding rendered visual
objects. Besides,the rendering process from parametric sequences to visual
objects is many-to-one. Therefore, both sequential and visual signals are
critical for effective training. In this work, we introduce CADFusion, a
framework that uses Large Language Models (LLMs) as the backbone and alternates
between two training stages: the sequential learning (SL) stage and the visual
feedback (VF) stage. In the SL stage, we train LLMs using ground-truth
parametric sequences, enabling the generation of logically coherent parametric
sequences. In the VF stage, we reward parametric sequences that render into
visually preferred objects and penalize those that do not, allowing LLMs to
learn how rendered visual objects are perceived and evaluated. These two stages
alternate throughout the training, ensuring balanced learning and preserving
benefits of both signals. Experiments demonstrate that CADFusion significantly
improves performance, both qualitatively and quantitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design
  and Generic Garment Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siran Li, Ruiyang Liu, Chen Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic digital garment modeling remains a labor-intensive task due to the
intricate process of translating 2D sewing patterns into high-fidelity,
simulation-ready 3D garments. We introduce GarmageNet, a unified generative
framework that automates the creation of 2D sewing patterns, the construction
of sewing relationships, and the synthesis of 3D garment initializations
compatible with physics-based simulation. Central to our approach is Garmage, a
novel garment representation that encodes each panel as a structured geometry
image, effectively bridging the semantic and geometric gap between 2D
structural patterns and 3D garment shapes. GarmageNet employs a latent
diffusion transformer to synthesize panel-wise geometry images and integrates
GarmageJigsaw, a neural module for predicting point-to-point sewing connections
along panel contours. To support training and evaluation, we build GarmageSet,
a large-scale dataset comprising over 10,000 professionally designed garments
with detailed structural and style annotations. Our method demonstrates
versatility and efficacy across multiple application scenarios, including
scalable garment generation from multi-modal design concepts (text prompts,
sketches, photographs), automatic modeling from raw flat sewing patterns,
pattern recovery from unstructured point clouds, and progressive garment
editing using conventional instructions-laying the foundation for fully
automated, production-ready pipelines in digital fashion. Project page:
https://style3d.github.io/garmagenet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapt before Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aojun Lu, Tao Feng, Hangjie Yuan, Chunhui Ding, Yanan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL. Code is available at
https://github.com/byyx666/ACL_code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language
  Models for Vision-Driven Embodied Agents <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09560v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09560v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied
agents offers a promising avenue for tackling real-world tasks. While
language-centric embodied agents have garnered substantial attention,
MLLM-based embodied agents remain underexplored due to the lack of
comprehensive evaluation frameworks. To bridge this gap, we introduce
EmbodiedBench, an extensive benchmark designed to evaluate vision-driven
embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing
tasks across four environments, ranging from high-level semantic tasks (e.g.,
household) to low-level tasks involving atomic actions (e.g., navigation and
manipulation); and (2) six meticulously curated subsets evaluating essential
agent capabilities like commonsense reasoning, complex instruction
understanding, spatial awareness, visual perception, and long-term planning.
Through extensive experiments, we evaluated 24 leading proprietary and
open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel
at high-level tasks but struggle with low-level manipulation, with the best
model, GPT-4o, scoring only 28.9\% on average. EmbodiedBench provides a
multifaceted standardized evaluation platform that not only highlights existing
challenges but also offers valuable insights to advance MLLM-based embodied
agents. Our code and dataset are available at https://embodiedbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmnimatteZero: Fast Training-free Omnimatte with <span class="highlight-title">Pre-train</span>ed Video
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dvir Samuel, Matan Levy, Nir Darshan, Gal Chechik, Rami Ben-Ari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Omnimatte, one aims to decompose a given video into semantically
meaningful layers, including the background and individual objects along with
their associated effects, such as shadows and reflections. Existing methods
often require extensive training or costly self-supervised optimization. In
this paper, we present OmnimatteZero, a training-free approach that leverages
off-the-shelf pre-trained video diffusion models for omnimatte. It can remove
objects from videos, extract individual object layers along with their effects,
and composite those objects onto new videos. These are accomplished by adapting
zero-shot image inpainting techniques for video object removal, a task they
fail to handle effectively out-of-the-box. To overcome this, we introduce
temporal and spatial attention guidance modules that steer the diffusion
process for accurate object removal and temporally consistent background
reconstruction. We further show that self-attention maps capture information
about the object and its footprints and use them to inpaint the object's
effects, leaving a clean background. Additionally, through simple latent
arithmetic, object layers can be isolated and recombined seamlessly with new
video layers to produce new videos. Evaluations show that OmnimatteZero not
only achieves superior performance in terms of background reconstruction but
also sets a new record for the fastest Omnimatte approach, achieving real-time
performance with minimal frame runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://dvirsamuel.github.io/omnimattezero.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time
  Reward Alignment in Score Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehoon Yoon, Yunhong Min, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based
initial particle sampling for effective inference-time reward alignment with a
score-based generative model. Inference-time reward alignment with score-based
generative models has recently gained significant traction, following a broader
paradigm shift from pre-training to post-training optimization. At the core of
this trend is the application of Sequential Monte Carlo (SMC) to the denoising
process. However, existing methods typically initialize particles from the
Gaussian prior, which inadequately captures reward-relevant regions and results
in reduced sampling efficiency. We demonstrate that initializing from the
reward-aware posterior significantly improves alignment performance. To enable
posterior sampling in high-dimensional latent spaces, we introduce the
preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines
dimension-robust proposals with gradient-informed dynamics. This approach
enables efficient and scalable posterior sampling and consistently improves
performance across various reward alignment tasks, including layout-to-image
generation, quantity-aware generation, and aesthetic-preference generation, as
demonstrated in our experiments. Project Webpage:
https://psi-sampler.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search Arena: Analyzing Search-Augmented LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code: https://github.com/lmarena/search-arena. Dataset:
  https://huggingface.co/datasets/lmarena-ai/search-arena-24k</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Comprehensibility of Multi-structured Financial Documents using
  LLMs and Pre-processing Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivani Upadhyay, Messiah Ataey, Shariyar Murtuza, Yifan Nie, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of complex structured data in hybrid sources, such as PDF
documents and web pages, presents unique challenges for current Large Language
Models (LLMs) and Multi-modal Large Language Models (MLLMs) in providing
accurate answers. Despite the recent advancements of MLLMs, they still often
falter when interpreting intricately structured information, such as nested
tables and multi-dimensional plots, leading to hallucinations and erroneous
outputs. This paper explores the capabilities of LLMs and MLLMs in
understanding and answering questions from complex data structures found in PDF
documents by leveraging industrial and open-source tools as part of a
pre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,
achieves an accuracy of 56% on multi-structured documents when fed documents
directly, and that integrating pre-processing tools raises the accuracy of LLMs
to 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is
publicly available at https://github.com/OGCDS/FinancialQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoRAG: Evidentiality-guided Compression for Long Context RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonseok Jeong, Jinsu Kim, Dohyeon Lee, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable performance in Open-Domain
Question Answering (ODQA) by leveraging external documents through
Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer
context, context compression is necessary. However, prior compression methods
do not focus on filtering out non-evidential information, which limit the
performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or
\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing
retrieved documents based on evidentiality, ensuring whether answer generation
is supported by the correct evidence. As an additional step, ECoRAG reflects
whether the compressed content provides sufficient evidence, and if not,
retrieves more until sufficient. Experiments show that ECoRAG improves LLM
performance on ODQA tasks, outperforming existing compression methods.
Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency
but also minimizes token usage by retaining only the necessary information to
generate the correct answer. Code is available at
https://github.com/ldilab/ECoRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledgeable-r1: Policy Optimization for Knowledge Exploration in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Lin, Yilin Wen, Du Su, Fei Sun, Muhan Chen, Chenfu Bao, Zhonghou Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a mainstream method for improving
performance on knowledge-intensive tasks. However,current RAG systems often
place too much emphasis on retrieved contexts. This can lead to reliance on
inaccurate sources and overlook the model's inherent knowledge, especially when
dealing with misleading or excessive information. To resolve this imbalance, we
propose Knowledgeable-r1 that using joint sampling and define multi policy
distributions in knowledge capability exploration to stimulate large language
models'self-integrated utilization of parametric and contextual knowledge.
Experiments show that Knowledgeable-r1 significantly enhances robustness and
reasoning accuracy in both parameters and contextual conflict tasks and general
RAG tasks, especially outperforming baselines by 17.07% in counterfactual
scenarios and demonstrating consistent gains across RAG tasks. Our code are
available at https://github.com/lcy80366872/ knowledgeable-r1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance
  LLM Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Zhao, Fengli Xu, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by advances in Large Language Models (LLMs), integrating them into
recommendation tasks has gained interest due to their strong semantic
understanding and prompt flexibility. Prior work encoded user-item interactions
or metadata into prompts for recommendations. In parallel, LLM reasoning,
boosted by test-time scaling and reinforcement learning, has excelled in fields
like mathematics and code, where reasoning traces and correctness signals are
clear, enabling high performance and interpretability. However, directly
applying these reasoning methods to recommendation is ineffective because user
feedback is implicit and lacks reasoning supervision. To address this, we
propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that
samples interaction chains from the user-item graph and converts them into
structured interaction-of-thoughts via a progressive masked prompting strategy,
with each thought representing stepwise reasoning grounded in interaction
context. This allows LLMs to simulate step-by-step decision-making based on
implicit patterns. We design a two-stage training pipeline: supervised
fine-tuning teaches basic reasoning from high-quality traces, and reinforcement
learning refines reasoning via reward signals, alleviating sparse explicit
supervision. Experiments on three real-world datasets show R2Rec outperforms
classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement
in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore,
the explicit reasoning chains enhance interpretability by revealing the
decision process. Our code is available at:
https://anonymous.4open.science/r/R2Rec-7C5D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Contrastive Learning in Session-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokun Zhang, Bo Xu, Fenglong Ma, Zhizheng Wang, Liang Yang, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation aims to predict intents of anonymous users based
on limited behaviors. With the ability in alleviating data sparsity,
contrastive learning is prevailing in the task. However, we spot that existing
contrastive learning based methods still suffer from three obstacles: (1) they
overlook item-level sparsity and primarily focus on session-level sparsity; (2)
they typically augment sessions using item IDs like crop, mask and reorder,
failing to ensure the semantic consistency of augmented views; (3) they treat
all positive-negative signals equally, without considering their varying
utility. To this end, we propose a novel multi-modal adaptive contrastive
learning framework called MACL for session-based recommendation. In MACL, a
multi-modal augmentation is devised to generate semantically consistent views
at both item and session levels by leveraging item multi-modal features.
Besides, we present an adaptive contrastive loss that distinguishes varying
contributions of positive-negative signals to improve self-supervised learning.
Extensive experiments on three real-world datasets demonstrate the superiority
of MACL over state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Storage-Efficient Visual Document Retrieval: An Empirical Study
  on Reducing Patch-Level Embeddings <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Ma, Jinsong Li, Yuhang Zang, Xiaobao Wu, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Jiaqi Wang, Yixin Cao, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the strong performance of ColPali/ColQwen2 in Visualized Document
Retrieval (VDR), it encodes each page into multiple patch-level embeddings and
leads to excessive memory usage. This empirical study investigates methods to
reduce patch embeddings per page at minimum performance degradation. We
evaluate two token-reduction strategies: token pruning and token merging.
Regarding token pruning, we surprisingly observe that a simple random strategy
outperforms other sophisticated pruning methods, though still far from
satisfactory. Further analysis reveals that pruning is inherently unsuitable
for VDR as it requires removing certain page embeddings without query-specific
information. Turning to token merging (more suitable for VDR), we search for
the optimal combinations of merging strategy across three dimensions and
develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance
with only 11.8% of original memory usage, and preserves 94.6% effectiveness at
2.8% memory footprint. We expect our empirical findings and resulting
Light-ColPali/ColQwen2 offer valuable insights and establish a competitive
baseline for future research towards efficient VDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning
  Blind Spots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Pan, Mary-Anne Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), whilst great at extracting facts from text,
struggle with nested narrative reasoning. Existing long context and multi-hop
QA benchmarks inadequately test this, lacking realistic distractors or failing
to decouple context length from reasoning complexity, masking a fundamental LLM
limitation. We introduce Verbose ListOps, a novel benchmark that
programmatically transposes ListOps computations into lengthy, coherent
stories. This uniquely forces internal computation and state management of
nested reasoning problems by withholding intermediate results, and offers
fine-grained controls for both narrative size \emph{and} reasoning difficulty.
Whilst benchmarks like LongReason (2025) advance approaches for synthetically
expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints
a specific LLM vulnerability: difficulty in state management for nested
sub-reasoning amongst semantically-relevant, distracting narrative. Our
experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse
in performance on Verbose ListOps at modest (~10k token) narrative lengths,
despite effortlessly solving raw ListOps equations. Addressing this failure is
paramount for real-world text interpretation which requires identifying key
reasoning points, tracking conceptual intermediate results, and filtering
irrelevant information. Verbose ListOps, and its extensible generation
framework thus enables targeted reasoning enhancements beyond mere
context-window expansion; a critical step to automating the world's knowledge
work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff
  Table <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Matsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor search (ANNS) is an essential building block for
applications like RAG but can sometimes yield results that are overly similar
to each other. In certain scenarios, search results should be similar to the
query and yet diverse. We propose LotusFilter, a post-processing module to
diversify ANNS results. We precompute a cutoff table summarizing vectors that
are close to each other. During the filtering, LotusFilter greedily looks up
the table to delete redundant vectors from the candidates. We demonstrated that
the LotusFilter operates fast (0.02 [ms/query]) in settings resembling
real-world RAG applications, utilizing features such as OpenAI embeddings. Our
code is publicly available at https://github.com/matsui528/lotf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. GitHub: https://github.com/matsui528/lotf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner
  for Query Expansion in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyuan Liu, Mengxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using
  Large Language Model-based Query Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyuan Liu, Mengxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for
  Recommender System Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Ma, Ziqi Xu, Yongli Ren, Danula Hettiachchi, Jeffrey Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional offline evaluation methods for recommender systems struggle to
capture the complexity of modern platforms due to sparse behavioural signals,
noisy data, and limited modelling of user personality traits. While simulation
frameworks can generate synthetic data to address these gaps, existing methods
fail to replicate behavioural diversity, limiting their effectiveness. To
overcome these challenges, we propose the Personality-driven User Behaviour
Simulator (PUB), an LLM-based simulation framework that integrates the Big Five
personality traits to model personalised user behaviour. PUB dynamically infers
user personality from behavioural logs (e.g., ratings, reviews) and item
metadata, then generates synthetic interactions that preserve statistical
fidelity to real-world data. Experiments on the Amazon review datasets show
that logs generated by PUB closely align with real user behaviour and reveal
meaningful associations between personality traits and recommendation outcomes.
These results highlight the potential of the personality-driven simulator to
advance recommender system evaluation, offering scalable, controllable,
high-fidelity alternatives to resource-intensive real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Head RAG: Solving Multi-Aspect Problems with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Ales Kubicek, Robert Gerstenberger, Marcin Chrapek, Roman Niggli, Patrik Okanovic, Yi Zhu, Patrick Iff, Michal Podstawski, Lucas Weitzendorf, Mingyuan Chi, Joanna Gajda, Piotr Nyczyk, Jürgen Müller, Hubert Niewiadomski, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving observation is that different attention
heads learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, multi-aspect datasets, and real-world
use cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages
over 18 RAG baselines, empirical improvements of up to 20% in retrieval success
ratios, and benefits for downstream LLM generation. MRAG can be seamlessly
integrated with existing RAG frameworks and benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World
  Spatial Reasoning Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18470v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18470v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhou Yu, Riyang Bao, Ruiyu Ning, Jinghong Peng, Gengchen Mai, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning remains a challenge for Large Language Models (LLMs), which
struggle with spatial data retrieval and reasoning. We propose Spatial
Retrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to
spatial tasks by integrating sparse spatial retrieval (spatial databases) and
dense semantic retrieval (LLM-based similarity). A multi-objective ranking
strategy balances spatial constraints and semantic relevance, while an
LLM-guided generator ensures coherent responses. Experiments on a real-world
tourism dataset show that Spatial-RAG significantly improves spatial question
answering, bridging the gap between LLMs and spatial intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time Hyper-Scaling with KV Cache Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity
  Analysis Between Alignment and Fine-tuning <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hsiung.cc/llm-similarity-risk/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Diffusion <span class="highlight-title">Transformer</span> Designs via Grafting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing model architectures requires decisions such as selecting operators
(e.g., attention, convolution) and configurations (e.g., depth, width).
However, evaluating the impact of these decisions on model quality requires
costly pretraining, limiting architectural investigation. Inspired by how new
software is built on existing code, we ask: can new architecture designs be
studied using pretrained models? To this end, we present grafting, a simple
approach for editing pretrained diffusion transformers (DiTs) to materialize
new architectures under small compute budgets. Informed by our analysis of
activation behavior and attention locality, we construct a testbed based on the
DiT-XL/2 design to study the impact of grafting on model quality. Using this
testbed, we develop a family of hybrid designs via grafting: replacing softmax
attention with gated convolution, local attention, and linear attention, and
replacing MLPs with variable expansion ratio and convolutional variants.
Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for
DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model
(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval
score. Finally, we present a case study that restructures DiT-XL/2 by
converting every pair of sequential transformer blocks into parallel blocks via
grafting. This reduces model depth by 2x and yields better quality (FID: 2.77)
than other models of comparable depth. Together, we show that new diffusion
model designs can be explored by grafting pretrained DiTs, with edits ranging
from operator replacement to architecture restructuring. Code and grafted
models: https://grafting.stanford.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages; Project website: https://grafting.stanford.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search Arena: Analyzing Search-Augmented LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code: https://github.com/lmarena/search-arena. Dataset:
  https://huggingface.co/datasets/lmarena-ai/search-arena-24k</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinetics: Rethinking Test-Time Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
for realizing the full potential of test-time scaling because, unlike training,
where parameter scaling saturates, test-time accuracy continues to improve
through increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Admissibility of Completely Randomized Trials: A Large-Deviation
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Imbens, Chao Qin, Stefan Wager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When an experimenter has the option of running an adaptive trial, is it
admissible to ignore this option and run a non-adaptive trial instead? We
provide a negative answer to this question in the best-arm identification
problem, where the experimenter aims to allocate measurement efforts
judiciously to confidently deploy the most effective treatment arm. We find
that, whenever there are at least three treatment arms, there exist simple
adaptive designs that universally and strictly dominate non-adaptive completely
randomized trials. This dominance is characterized by a notion called
efficiency exponent, which quantifies a design's statistical efficiency when
the experimental sample is large. Our analysis focuses on the class of batched
arm elimination designs, which progressively eliminate underperforming arms at
pre-specified batch intervals. We characterize simple sufficient conditions
under which these designs universally and strictly dominate completely
randomized trials. These results resolve the second open problem posed in Qin
[2022].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A one-page abstract of this work will appear at the 26th ACM
  Conference on Economics and Computation (EC'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via
  Latent Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingshuai Ji, Haomin Zhuang, Matthew Toole, James McKenzie, Xiaolong Liu, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quasiparticle interference (QPI) imaging is a powerful tool for probing
electronic structures in quantum materials, but extracting the single-scatterer
QPI pattern (i.e., the kernel) from a multi-scatterer image remains a
fundamentally ill-posed inverse problem. In this work, we propose the first
AI-based framework for QPI kernel extraction. We introduce a two-step learning
strategy that decouples kernel representation learning from
observation-to-kernel inference. In the first step, we train a variational
autoencoder to learn a compact latent space of scattering kernels. In the
second step, we align the latent representation of QPI observations with those
of the pre-learned kernels using a dedicated encoder. This design enables the
model to infer kernels robustly even under complex, entangled scattering
conditions. We construct a diverse and physically realistic QPI dataset
comprising 100 unique kernels and evaluate our method against a direct one-step
baseline. Experimental results demonstrate that our approach achieves
significantly higher extraction accuracy, and improved generalization to unseen
kernels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSM-2: Learning from Incomplete Wearable Sensor Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell A. Xu, Girish Narayanswamy, Kumar Ayush, Dimitris Spathis, Shun Liao, Shyam A. Tailor, Ahmed Metwally, A. Ali Heydari, Yuwei Zhang, Jake Garrison, Samy Abdel-Ghaffar, Xuhai Xu, Ken Gu, Jacob Sunshine, Ming-Zher Poh, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Mark Malhotra, Shwetak Patel, Yuzhe Yang, James M. Rehg, Xin Liu, Daniel McDuff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models, a cornerstone of recent advancements in machine learning,
have predominantly thrived on complete and well-structured data. Wearable
sensor data frequently suffers from significant missingness, posing a
substantial challenge for self-supervised learning (SSL) models that typically
assume complete data inputs. This paper introduces the second generation of
Large Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel
SSL approach that learns robust representations directly from incomplete data
without requiring explicit imputation. AIM's core novelty lies in its use of
learnable mask tokens to model both existing ("inherited") and artificially
introduced missingness, enabling it to robustly handle fragmented real-world
data during inference. Pre-trained on an extensive dataset of 40M hours of
day-long multimodal sensor data, our LSM-2 with AIM achieves the best
performance across a diverse range of tasks, including classification,
regression and generative modeling. Furthermore, LSM-2 with AIM exhibits
superior scaling performance, and critically, maintains high performance even
under targeted missingness scenarios, reflecting clinically coherent patterns,
such as the diagnostic value of nighttime biosignals for hypertension
prediction. This makes AIM a more reliable choice for real-world wearable data
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Xu and Narayanswamy are co-first authors. McDuff and Liu are co-last
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable, real-time neural decoding with hybrid state-space models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avery Hee-Woon Ryoo, Nanda H. Krishna, Ximeng Mao, Mehdi Azabou, Eva L. Dyer, Matthew G. Perich, Guillaume Lajoie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time decoding of neural activity is central to neuroscience and
neurotechnology applications, from closed-loop experiments to brain-computer
interfaces, where models are subject to strict latency constraints. Traditional
methods, including simple recurrent neural networks, are fast and lightweight
but often struggle to generalize to unseen data. In contrast, recent
Transformer-based approaches leverage large-scale pretraining for strong
generalization performance, but typically have much larger computational
requirements and are not always suitable for low-resource or real-time
settings. To address these shortcomings, we present POSSM, a novel hybrid
architecture that combines individual spike tokenization via a cross-attention
module with a recurrent state-space model (SSM) backbone to enable (1) fast and
causal online prediction on neural activity and (2) efficient generalization to
new sessions, individuals, and tasks through multi-dataset pretraining. We
evaluate POSSM's decoding performance and inference speed on intracortical
decoding of monkey motor tasks, and show that it extends to clinical
applications, namely handwriting and speech decoding in human subjects.
Notably, we demonstrate that pretraining on monkey motor-cortical recordings
improves decoding performance on the human handwriting task, highlighting the
exciting potential for cross-species transfer. In all of these tasks, we find
that POSSM achieves decoding accuracy comparable to state-of-the-art
Transformers, at a fraction of the inference cost (up to 9x faster on GPU).
These results suggest that hybrid SSMs are a promising approach to bridging the
gap between accuracy, inference speed, and generalization when training neural
decoders for real-time, closed-loop applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Data Efficiency for LLM Reinforcement Fine-tuning Through
  Difficulty-targeted Online Data Selection and Rollout Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has become an effective approach for fine-tuning
large language models (LLMs), particularly to enhance their reasoning
capabilities. However, RL fine-tuning remains highly resource-intensive, and
existing work has largely overlooked the problem of data efficiency. In this
paper, we propose two techniques to improve data efficiency in LLM RL
fine-tuning: difficulty-targeted online data selection and rollout replay. We
introduce the notion of adaptive difficulty to guide online data selection,
prioritizing questions of moderate difficulty that are more likely to yield
informative learning signals. To estimate adaptive difficulty efficiently, we
develop an attention-based framework that requires rollouts for only a small
reference set of questions. The adaptive difficulty of the remaining questions
is then estimated based on their similarity to this set. To further reduce
rollout cost, we introduce a rollout replay mechanism that reuses recent
rollouts, lowering per-step computation while maintaining stable updates.
Extensive experiments across 6 LLM-dataset combinations show that our method
reduces RL fine-tuning time by 25% to 65% to reach the same level of
performance as the original GRPO algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Entropic Unlearning: A Primal-Dual Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks
across diverse LLM architectures demonstrate that our approach consistently
matches or exceeds state-of-the-art baselines, effectively removing targeted
information while preserving downstream utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning normalized image densities via dual score matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florentin Guth, Zahra Kadkhodaie, Eero P Simoncelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning probability models from data is at the heart of many machine
learning endeavors, but is notoriously difficult due to the curse of
dimensionality. We introduce a new framework for learning \emph{normalized}
energy (log probability) models that is inspired from diffusion generative
models, which rely on networks optimized to estimate the score. We modify a
score network architecture to compute an energy while preserving its inductive
biases. The gradient of this energy network with respect to its input image is
the score of the learned density, which can be optimized using a denoising
objective. Importantly, the gradient with respect to the noise level provides
an additional score that can be optimized with a novel secondary objective,
ensuring consistent and normalized energies across noise levels. We train an
energy network with this \emph{dual} score matching objective on the ImageNet64
dataset, and obtain a cross-entropy (negative log likelihood) value comparable
to the state of the art. We further validate our approach by showing that our
energy model \emph{strongly generalizes}: estimated log probabilities are
nearly independent of the specific images in the training set. Finally, we
demonstrate that both image probability and dimensionality of local
neighborhoods vary significantly with image content, in contrast with
traditional assumptions such as concentration of measure or support on a
low-dimensional manifold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProRefine: Inference-time <span class="highlight-title">Prompt</span> Refinement with Textual Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Pandita, Tharindu Cyril Weerasooriya, Ankit Parag Shah, Christopher M. Homan, Wei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic workflows, where multiple AI agents collaborate to accomplish complex
tasks like reasoning or planning, are becoming increasingly prevalent. However,
these workflows often suffer from error propagation and sub-optimal
performance, largely due to poorly designed prompts that fail to effectively
guide individual agents. This is a critical problem because it limits the
reliability and scalability of these powerful systems. We introduce ProRefine,
an innovative inference-time prompt optimization method that leverages textual
feedback from large language models (LLMs) to address this challenge. ProRefine
dynamically refines prompts for multi-step reasoning tasks without additional
training or ground truth labels. Evaluated on five benchmark mathematical
reasoning datasets, ProRefine significantly surpasses zero-shot
Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only
boosts accuracy but also allows smaller models to match the performance of
larger ones, highlighting its potential for efficient and scalable AI
deployment, and democratizing access to high-performing AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Power Law Guided Dynamic Sifting for Efficient Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirav Koley, Prajwal Singhania, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient inference on GPUs using large language models remains challenging
due to memory bandwidth limitations, particularly during data transfers between
High Bandwidth Memory (HBM) and SRAM in attention computations. Approximate
attention methods address this issue by reducing computational and memory
overhead but often rely on expensive top-$k$ operations, which perform poorly
on GPUs. We propose SiftAttention, a novel approximate attention method that
replaces the top-$k$ step with a computationally efficient element-wise
filtering operation based on a threshold value. Our intuition for doing this is
based on our empirical observation that the $\tau$-th quantile of attention
scores follows a predictable power-law over sequential generation steps.
Exploiting this insight, our approach dynamically estimates a threshold value
per prompt at each generation step. Only attention scores above this threshold
and their corresponding value vectors are loaded/used to compute the attention
output, reducing data movement between HBM and SRAM. Our evaluation
demonstrates that SiftAttention preserves model quality better than existing
approximate attention methods while reducing memory bandwidth usage when
loading value vectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control Tax: The Price of Keeping AI in Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Terekhov, Zhen Ning David Liu, Caglar Gulcehre, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid integration of agentic AI into high-stakes real-world applications
requires robust oversight mechanisms. The emerging field of AI Control (AIC)
aims to provide such an oversight mechanism, but practical adoption depends
heavily on implementation overhead. To study this problem better, we introduce
the notion of Control tax -- the operational and financial cost of integrating
control measures into AI pipelines. Our work makes three key contributions to
the field of AIC: (1) we introduce a theoretical framework that quantifies the
Control Tax and maps classifier performance to safety assurances; (2) we
conduct comprehensive evaluations of state-of-the-art language models in
adversarial settings, where attacker models insert subtle backdoors into code
while monitoring models attempt to detect these vulnerabilities; and (3) we
provide empirical financial cost estimates for control protocols and develop
optimized monitoring strategies that balance safety and cost-effectiveness
while accounting for practical constraints like auditing budgets. Our framework
enables practitioners to make informed decisions by systematically connecting
safety guarantees with their costs, advancing AIC through principled economic
feasibility assessment across different deployment contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample Complexity and Representation Ability of Test-time Scaling
  Paradigms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation
  via Learning to Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Kumar Jain, Vibhakar Mohta, Subin Kim, Atiksh Bhardwaj, Juntao Ren, Yunhai Feng, Sanjiban Choudhury, Gokul Swamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Beyond Experience: Generalizing to Unseen State Space with
  Reservoir Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Declan A. Norton, Yuanzhao Zhang, Michelle Girvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning techniques offer an effective approach to modeling dynamical
systems solely from observed data. However, without explicit structural priors
-- built-in assumptions about the underlying dynamics -- these techniques
typically struggle to generalize to aspects of the dynamics that are poorly
represented in the training data. Here, we demonstrate that reservoir computing
-- a simple, efficient, and versatile machine learning framework often used for
data-driven modeling of dynamical systems -- can generalize to unexplored
regions of state space without explicit structural priors. First, we describe a
multiple-trajectory training scheme for reservoir computers that supports
training across a collection of disjoint time series, enabling effective use of
available training data. Then, applying this training scheme to multistable
dynamical systems, we show that RCs trained on trajectories from a single basin
of attraction can achieve out-of-domain generalization by capturing system
behavior in entirely unobserved basins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Vision Concept <span class="highlight-title">Transformer</span>s for Medical Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Hu, Songning Lai, Yuan Hua, Shu Yang, Jingfeng Zhang, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparency is a paramount concern in the medical field, prompting
researchers to delve into the realm of explainable AI (XAI). Among these XAI
methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent
space to human-understandable high-level concepts by generating a conceptual
layer for extracting conceptual features, which has drawn much attention
recently. However, existing methods rely solely on concept features to
determine the model's predictions, which overlook the intrinsic feature
embeddings within medical images. To address this utility gap between the
original models and concept-based models, we propose Vision Concept Transformer
(VCT). Furthermore, despite their benefits, CBMs have been found to negatively
impact model performance and fail to provide stable explanations when faced
with input perturbations, which limits their application in the medical field.
To address this faithfulness issue, this paper further proposes the Stable
Vision Concept Transformer (SVCT) based on VCT, which leverages the vision
transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT
employs conceptual features to enhance decision-making capabilities by fusing
them with image features and ensures model faithfulness through the integration
of Denoised Diffusion Smoothing. Comprehensive experiments on four medical
datasets demonstrate that our VCT and SVCT maintain accuracy while remaining
interpretable compared to baselines. Furthermore, even when subjected to
perturbations, our SVCT model consistently provides faithful explanations, thus
meeting the needs of the medical field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2304.06129 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-DataShapley: Neural Modeling for Training Data Valuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Sun, Yu Xiong, Runze Wu, Xinyu Cai, Changjie Fan, Lan Zhang, Xiang-Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The value and copyright of training data are crucial in the artificial
intelligence industry. Service platforms should protect data providers'
legitimate rights and fairly reward them for their contributions. Shapley
value, a potent tool for evaluating contributions, outperforms other methods in
theory, but its computational overhead escalates exponentially with the number
of data providers. Recent works based on Shapley values attempt to mitigate
computation complexity by approximation algorithms. However, they need to
retrain for each test sample, leading to intolerable costs. We propose
Fast-DataShapley, a one-pass training method that leverages the weighted least
squares characterization of the Shapley value to train a reusable explainer
model with real-time reasoning speed. Given new test samples, no retraining is
required to calculate the Shapley values of the training data. Additionally, we
propose three methods with theoretical guarantees to reduce training overhead
from two aspects: the approximate calculation of the utility function and the
group calculation of the training data. We analyze time complexity to show the
efficiency of our methods. The experimental evaluations on various image
datasets demonstrate superior performance and efficiency compared to baselines.
Specifically, the performance is improved to more than 2.5 times, and the
explainer's training speed can be increased by two orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Unlock Time Series Editing? Diffusion-Driven Approach with
  Multi-Grained Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Chu Xin Cheng, Runlong Yu, Yuyang Ye, Shiwei Tong, Zhaofeng Liu, Defu Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in time series generation have shown promise, yet controlling
properties in generated sequences remains challenging. Time Series Editing
(TSE) - making precise modifications while preserving temporal coherence -
consider both point-level constraints and segment-level controls that current
methods struggle to provide. We introduce the CocktailEdit framework to enable
simultaneous, flexible control across different types of constraints. This
framework combines two key mechanisms: a confidence-weighted anchor control for
point-wise constraints and a classifier-based control for managing statistical
properties such as sums and averages over segments. Our methods achieve precise
local control during the denoising inference stage while maintaining temporal
coherence and integrating seamlessly, with any conditionally trained
diffusion-based time series models. Extensive experiments across diverse
datasets and models demonstrate its effectiveness. Our work bridges the gap
between pure generative modeling and real-world time series editing needs,
offering a flexible solution for human-in-the-loop time series generation and
editing. The code and demo are provided for validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight analyses of first-order methods with error feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Berg Thomsen, Adrien Taylor, Aymeric Dieuleveut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication between agents often constitutes a major computational
bottleneck in distributed learning. One of the most common mitigation
strategies is to compress the information exchanged, thereby reducing
communication overhead. To counteract the degradation in convergence associated
with compressed communication, error feedback schemes -- most notably
$\mathrm{EF}$ and $\mathrm{EF}^{21}$ -- were introduced. In this work, we
provide a tight analysis of both of these methods. Specifically, we find the
Lyapunov function that yields the best possible convergence rate for each
method -- with matching lower bounds. This principled approach yields sharp
performance guarantees and enables a rigorous, apples-to-apples comparison
between $\mathrm{EF}$, $\mathrm{EF}^{21}$, and compressed gradient descent. Our
analysis is carried out in a simplified yet representative setting, which
allows for clean theoretical insights and fair comparison of the underlying
mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning long range dependencies through time reversal symmetry breaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Pourcel, Maxence Ernoult
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,
as RNNs could natively be embodied into dynamical systems. This calls for
dedicated learning algorithms obeying to core physical principles, with
efficient techniques to simulate these systems and guide their design. We
propose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably
computes loss gradients as finite differences of physical trajectories of
non-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three
"forward passes" irrespective of model size, without explicit Jacobian
computation, nor incurring any variance in the gradient estimation. Motivated
by the physical realization of our algorithm, we first introduce RHEL in
continuous time and demonstrate its formal equivalence with the continuous
adjoint state method. To facilitate the simulation of Hamiltonian systems
trained by RHEL, we propose a discrete-time version of RHEL which is equivalent
to Backpropagation Through Time (BPTT) when applied to a class of recurrent
modules which we call Hamiltonian Recurrent Units (HRUs). This setting allows
us to demonstrate the scalability of RHEL by generalizing these results to
hierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to
train HSSMs with linear and nonlinear dynamics on a variety of time-series
tasks ranging from mid-range to long-range classification and regression with
sequence length reaching $\sim 50k$. We show that RHEL consistently matches the
performance of BPTT across all models and tasks. This work opens new doors for
the design of scalable, energy-efficient physical systems endowed with
self-learning capabilities for sequence modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, Nick Haber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reasoning models (LRMs) achieve higher performance on challenging
reasoning tasks by generating more tokens at inference time, but this verbosity
often wastes computation on easy problems. Existing solutions, including
supervised finetuning on shorter traces, user-controlled budgets, or RL with
uniform penalties, either require data curation, manual configuration, or treat
all problems alike regardless of difficulty. We introduce Adaptive Length
Penalty (ALP), a reinforcement learning objective tailoring generation length
to per-prompt solve rate. During training, ALP monitors each prompt's online
solve rate through multiple rollouts and adds a differentiable penalty whose
magnitude scales inversely with that rate, so confident (easy) prompts incur a
high cost for extra tokens while hard prompts remain unhindered. Posttraining
DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly
dropping performance. Relative to fixed-budget and uniform penalty baselines,
ALP redistributes its reduced budget more intelligently by cutting compute on
easy prompts and reallocating saved tokens to difficult ones, delivering higher
accuracy on the hardest problems with higher cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conservative classifiers do consistently well with improving agents:
  characterizing statistical and online learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dravyansh Sharma, Alec Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is now ubiquitous in societal decision-making, for example
in evaluating job candidates or loan applications, and it is increasingly
important to take into account how classified agents will react to the learning
algorithms. The majority of recent literature on strategic classification has
focused on reducing and countering deceptive behaviors by the classified
agents, but recent work of Attias et al. identifies surprising properties of
learnability when the agents genuinely improve in order to attain the desirable
classification, such as smaller generalization error than standard
PAC-learning. In this paper we characterize so-called learnability with
improvements across multiple new axes. We introduce an asymmetric variant of
minimally consistent concept classes and use it to provide an exact
characterization of proper learning with improvements in the realizable
setting. While prior work studies learnability only under general, arbitrary
agent improvement regions, we give positive results for more natural Euclidean
ball improvement sets. In particular, we characterize improper learning under a
mild generative assumption on the data distribution. We further show how to
learn in more challenging settings, achieving lower generalization error under
well-studied bounded noise models and obtaining mistake bounds in realizable
and agnostic online learning. We resolve open questions posed by Attias et al.
for both proper and improper learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Convergence of Gradient Descent on Learning <span class="highlight-title">Transformer</span>s with
  Residual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Qin, Jinxin Zhou, Zhihui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have emerged as fundamental tools across various
scientific and engineering disciplines, owing to their outstanding performance
in diverse applications. Despite this empirical success, the theoretical
foundations of Transformers remain relatively underdeveloped, particularly in
understanding their training dynamics. Existing research predominantly examines
isolated components--such as self-attention mechanisms and feedforward
networks--without thoroughly investigating the interdependencies between these
components, especially when residual connections are present. In this paper, we
aim to bridge this gap by analyzing the convergence behavior of a structurally
complete yet single-layer Transformer, comprising self-attention, a feedforward
network, and residual connections. We demonstrate that, under appropriate
initialization, gradient descent exhibits a linear convergence rate, where the
convergence speed is determined by the minimum and maximum singular values of
the output matrix from the attention layer. Moreover, our analysis reveals that
residual connections serve to ameliorate the ill-conditioning of this output
matrix, an issue stemming from the low-rank structure imposed by the softmax
operation, thereby promoting enhanced optimization stability. We also extend
our theoretical findings to a multi-layer Transformer architecture, confirming
the linear convergence rate of gradient descent under suitable initialization.
Empirical results corroborate our theoretical insights, illustrating the
beneficial role of residual connections in promoting convergence stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Moment Identification for Nonlinear PDEs via a Neural ODE
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxuan Chen, Su Yang, Panayotis G. Kevrekidis, Wei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven framework for learning reduced-order moment dynamics
from PDE-governed systems using Neural ODEs. In contrast to derivative-based
methods like SINDy, which necessitate densely sampled data and are sensitive to
noise, our approach based on Neural ODEs directly models moment trajectories,
enabling robust learning from sparse and potentially irregular time series.
Using as an application platform the nonlinear Schr\"{o}dinger equation, the
framework accurately recovers governing moment dynamics when closure is
available, even with limited and irregular observations. For systems without
analytical closure, we introduce a data-driven coordinate transformation
strategy based on Stiefel manifold optimization, enabling the discovery of
low-dimensional representations in which the moment dynamics become closed,
facilitating interpretable and reliable modeling. We also explore cases where a
closure model is not known, such as a Fisher-KPP reaction-diffusion system.
Here we demonstrate that Neural ODEs can still effectively approximate the
unclosed moment dynamics and achieve superior extrapolation accuracy compared
to physical-expert-derived ODE models. This advantage remains robust even under
sparse and irregular sampling, highlighting the method's robustness in
data-limited settings. Our results highlight the Neural ODE framework as a
powerful and flexible tool for learning interpretable, low-dimensional moment
dynamics in complex PDE-governed systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Latent Spaces with Flow Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel framework for aligning learnable latent spaces to
arbitrary target distributions by leveraging flow-based generative models as
priors. Our method first pretrains a flow model on the target features to
capture the underlying distribution. This fixed flow model subsequently
regularizes the latent space via an alignment loss, which reformulates the flow
matching objective to treat the latents as optimization targets. We formally
prove that minimizing this alignment loss establishes a computationally
tractable surrogate objective for maximizing a variational lower bound on the
log-likelihood of latents under the target distribution. Notably, the proposed
method eliminates computationally expensive likelihood evaluations and avoids
ODE solving during optimization. As a proof of concept, we demonstrate in a
controlled setting that the alignment loss landscape closely approximates the
negative log-likelihood of the target distribution. We further validate the
effectiveness of our approach through large-scale image generation experiments
on ImageNet with diverse target distributions, accompanied by detailed
discussions and ablation studies. With both theoretical and empirical
validation, our framework paves a new way for latent space alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valérie Costa, Thomas Fel, Ekdeep Singh Lubana, Bahareh Tolooshams, Demba Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) have recently become central tools for
interpretability, leveraging dictionary learning principles to extract sparse,
interpretable features from neural representations whose underlying structure
is typically unknown. This paper evaluates SAEs in a controlled setting using
MNIST, which reveals that current shallow architectures implicitly rely on a
quasi-orthogonality assumption that limits the ability to extract correlated
features. To move beyond this, we introduce a multi-iteration SAE by unrolling
Matching Pursuit (MP-SAE), enabling the residual-guided extraction of
correlated features that arise in hierarchical settings such as handwritten
digit generation while guaranteeing monotonic improvement of the reconstruction
as more atoms are selected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Complementary work to arXiv:2506.03093</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MesaNet: Sequence Modeling by Locally Optimal Test-Time Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, Rif A. Saurous, Guillaume Lajoie, Charlotte Frenkel, Razvan Pascanu, Blaise Agüera y Arcas, João Sacramento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence modeling is currently dominated by causal transformer architectures
that use softmax self-attention. Although widely adopted, transformers require
scaling memory and compute linearly during inference. A recent stream of work
linearized the softmax operation, resulting in powerful recurrent neural
network (RNN) models with constant memory and compute costs such as DeltaNet,
Mamba or xLSTM. These models can be unified by noting that their recurrent
layer dynamics can all be derived from an in-context regression objective,
approximately optimized through an online learning rule. Here, we join this
line of work and introduce a numerically stable, chunkwise parallelizable
version of the recently proposed Mesa layer (von Oswald et al., 2024), and
study it in language modeling at the billion-parameter scale. This layer again
stems from an in-context loss, but which is now minimized to optimality at
every time point using a fast conjugate gradient solver. Through an extensive
suite of experiments, we show that optimal test-time training enables reaching
lower language modeling perplexity and higher downstream benchmark performance
than previous RNNs, especially on tasks requiring long context understanding.
This performance gain comes at the cost of additional flops spent during
inference time. Our results are therefore intriguingly related to recent trends
of increasing test-time compute to improve performance -- here by spending
compute to solve sequential optimization problems within the neural network
itself.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Tempering Sampler with Diffusion <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Severi Rissanen, RuiKang OuYang, Jiajun He, Wenlin Chen, Markus Heinonen, Arno Solin, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on designing neural samplers that amortize the
process of sampling from unnormalized densities. However, despite significant
advancements, they still fall short of the state-of-the-art MCMC approach,
Parallel Tempering (PT), when it comes to the efficiency of target evaluations.
On the other hand, unlike a well-trained neural sampler, PT yields only
dependent samples and needs to be rerun -- at considerable computational cost
-- whenever new samples are required. To address these weaknesses, we propose
the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion
models sequentially across temperatures, leveraging the advantages of PT to
improve the training of neural samplers. We also introduce a novel method to
combine high-temperature diffusion models to generate approximate
lower-temperature samples, which are minimally refined using MCMC and used to
train the next diffusion model. PTSD enables efficient reuse of sample
information across temperature levels while generating well-mixed, uncorrelated
samples. Our method significantly improves target evaluation efficiency,
outperforming diffusion-based neural samplers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagonal Batching Unlocks Parallelism in Recurrent Memory <span class="highlight-title">Transformer</span>s
  for Long Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danil Sivtsov, Ivan Rodkin, Gleb Kuzmin, Yuri Kuratov, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models struggle with long-context inference due to their
quadratic time and linear memory complexity. Recurrent Memory Transformers
(RMTs) offer a solution by reducing the asymptotic cost to linear time and
constant memory usage. However, their memory update mechanism leads to
sequential execution, causing a performance bottleneck.
  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism
across segments in RMTs while preserving exact recurrence. This approach
eliminates the sequential constraint, enabling efficient GPU inference even for
single long-context inputs without complex batching and pipelining techniques.
Because the technique is purely a run-time computation reordering, existing RMT
models adopt it with no retraining.
  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup
over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential
RMT implementation on 131,072-token sequences. By removing sequential
bottleneck, Diagonal Batching reduces inference cost and latency, thereby
strengthening RMTs as a practical solution for real-world, long-context
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Provably Efficient Algorithms to Estimate
  Shapley Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Chen, Akshay Seshadri, Mattia J. Villani, Pradeep Niroula, Shouvanik Chakrabarti, Archan Ray, Pranav Deshpande, Romina Yalovetzky, Marco Pistoia, Niraj Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shapley values have emerged as a critical tool for explaining which features
impact the decisions made by machine learning models. However, computing exact
Shapley values is difficult, generally requiring an exponential (in the feature
dimension) number of model evaluations. To address this, many model-agnostic
randomized estimators have been developed, the most influential and widely used
being the KernelSHAP method (Lundberg & Lee, 2017). While related estimators
such as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco &
Witter, 2025) are known to satisfy theoretical guarantees, bounds for
KernelSHAP have remained elusive. We describe a broad and unified framework
that encompasses KernelSHAP and related estimators constructed using both with
and without replacement sampling strategies. We then prove strong
non-asymptotic theoretical guarantees that apply to all estimators from our
framework. This provides, to the best of our knowledge, the first theoretical
guarantees for KernelSHAP and sheds further light on tradeoffs between existing
estimators. Through comprehensive benchmarking on small and medium dimensional
datasets for Decision-Tree models, we validate our approach against exact
Shapley values, consistently achieving low mean squared error with modest
sample sizes. Furthermore, we make specific implementation improvements to
enable scalability of our methods to high-dimensional datasets. Our methods,
tested on datasets such MNIST and CIFAR10, provide consistently better results
compared to the KernelSHAP library.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new decentralized robust kernel-based learning algorithm within
the framework of reproducing kernel Hilbert space (RKHS) by utilizing a
networked system that can be represented as a connected graph. The robust loss
function $\mathcal{L}_\sigma$ induced by a windowing function $W$ and a
robustness scaling parameter $\sigma>0$, can encompass a broad spectrum of
robust losses. Consequently, the proposed algorithm effectively provides a
unified decentralized learning framework for robust regression, which
fundamentally differs from the existing distributed robust kernel learning
schemes, all of which are divide-and-conquer based. We rigorously establish the
learning theory and offer a comprehensive convergence analysis for the
algorithm. We show each local robust estimator generated from the decentralized
algorithm can be utilized to approximate the regression function. Based on
kernel-based integral operator techniques, we derive general high confidence
convergence bounds for each local approximating sequence in terms of the mean
square distance, RKHS norm, and generalization error, respectively. Moreover,
we provide rigorous selection rules for local sample size and show that, under
properly selected step size and scaling parameter $\sigma$, the decentralized
robust algorithm can achieve optimal learning rates (up to logarithmic factors)
in both norms. The parameter $\sigma$ is shown to be essential for enhancing
robustness while also ensuring favorable convergence behavior. The intrinsic
connection among decentralization, sample selection, robustness of the
algorithm, and its convergence is clearly reflected.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Hu, Hongbo Bo, Jun Hong, Xiaowei Liu, Weiru Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) often suffer from degree bias in node
classification tasks, where prediction performance varies across nodes with
different degrees. Several approaches, which adopt Graph Contrastive Learning
(GCL), have been proposed to mitigate this bias. However, the limited number of
positive pairs and the equal weighting of all positives and negatives in GCL
still lead to low-degree nodes acquiring insufficient and noisy information.
This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to
mitigate degree bias. It adds more positive pairs by leveraging node labels and
adaptively weights positive and negative pairs based on their learning
hardness. In addition, we develop an experimental framework named SHARP to
extend HAR to a broader range of scenarios. Both our theoretical analysis and
experiments validate the effectiveness of SHARP. The experimental results
across four datasets show that SHARP achieves better performance against
baselines at both global and degree levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Common Pile v0.1: An 8TB <span class="highlight-title">Dataset</span> of Public Domain and Openly
  Licensed Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are typically trained on enormous quantities of
unlicensed text, a practice that has led to scrutiny due to possible
intellectual property infringement and ethical concerns. Training LLMs on
openly licensed text presents a first step towards addressing these issues, but
prior data collection efforts have yielded datasets too small or low-quality to
produce performant LLMs. To address this gap, we collect, curate, and release
the Common Pile v0.1, an eight terabyte collection of openly licensed text
designed for LLM pretraining. The Common Pile comprises content from 30 sources
that span diverse domains including research papers, code, books,
encyclopedias, educational materials, audio transcripts, and more. Crucially,
we validate our efforts by training two 7 billion parameter LLMs on text from
the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion
tokens respectively. Both models attain competitive performance to LLMs trained
on unlicensed text with similar computational budgets, such as Llama 1 and 2
7B. In addition to releasing the Common Pile v0.1 itself, we also release the
code used in its creation as well as the training mixture and checkpoints for
the Comma v0.1 models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthiness Preservation by Copies of Machine Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Ceragioli, Giuseppe Primiero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common practice of ML systems development concerns the training of the same
model under different data sets, and the use of the same (training and test)
sets for different learning models. The first case is a desirable practice for
identifying high quality and unbiased training conditions. The latter case
coincides with the search for optimal models under a common dataset for
training. These differently obtained systems have been considered akin to
copies. In the quest for responsible AI, a legitimate but hardly investigated
question is how to verify that trustworthiness is preserved by copies. In this
paper we introduce a calculus to model and verify probabilistic complex queries
over data and define four distinct notions: Justifiably, Equally, Weakly and
Almost Trustworthy which can be checked analysing the (partial) behaviour of
the copy with respect to its original. We provide a study of the relations
between these notions of trustworthiness, and how they compose with each other
and under logical operations. The aim is to offer a computational tool to check
the trustworthiness of possibly complex systems copied from an original whose
behavour is known.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar Mathias Drton, Negar Kiyavash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates causal effect identification in latent variable
Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,
addressing two prominent setups that are challenging in the presence of latent
confounding: (1) a single proxy variable that may causally influence the
treatment and (2) underspecified instrumental variable cases where fewer
instruments exist than treatments. We prove that causal effects are
identifiable with a single proxy or instrument and provide corresponding
estimation methods. Experimental results demonstrate the accuracy and
robustness of our approaches compared to existing methods, advancing the
theoretical and practical understanding of causal inference in linear systems
with latent confounders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s Meet In-Context Learning: A Universal Approximation Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gen Li, Yuchen Jiao, Yu Huang, Yuting Wei, Yuxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models are capable of in-context learning, the ability
to perform new tasks at inference time using only a handful of input-output
examples in the prompt, without any fine-tuning or parameter updates. We
develop a universal approximation theory to better understand how transformers
enable in-context learning. For any class of functions (each representing a
distinct task), we demonstrate how to construct a transformer that, without any
further weight updates, can perform reliable prediction given only a few
in-context examples. In contrast to much of the recent literature that frames
transformers as algorithm approximators -- i.e., constructing transformers to
emulate the iterations of optimization algorithms as a means to approximate
solutions of learning problems -- our work adopts a fundamentally different
approach rooted in universal function approximation. This alternative approach
offers approximation guarantees that are not constrained by the effectiveness
of the optimization algorithms being approximated, thereby extending far beyond
convex problems and linear function classes. Our construction sheds light on
how transformers can simultaneously learn general-purpose representations and
adapt dynamically to in-context examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Cross-Modality Memorization in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Wen, Yangsibo Huang, Tom Goldstein, Ravi Kumar, Badih Ghazi, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding what and how neural networks memorize during training is
crucial, both from the perspective of unintentional memorization of potentially
sensitive information and from the standpoint of effective knowledge
acquisition for real-world, knowledge-intensive tasks. While previous studies
primarily investigate memorization within a single modality, such as text
memorization in large language models or image memorization in diffusion
models, unified multimodal models are becoming increasingly prevalent in
practical applications. In this work, we focus on the unique characteristics of
cross-modality memorization and conduct a systematic study centered on
vision-language models. To facilitate controlled experiments, we first
introduce a synthetic persona dataset comprising diverse synthetic person
images and textual descriptions. We quantify factual knowledge memorization and
cross-modal transferability by training models on a single modality and
evaluating their performance in the other. Our results reveal that facts
learned in one modality transfer to the other, but a significant gap exists
between recalling information in the source and target modalities. Furthermore,
we observe that this gap exists across various scenarios, including more
capable models, machine unlearning, and the multi-hop case. At the end, we
propose a baseline method to mitigate this challenge. We hope our study can
inspire future research on developing more robust multimodal learning
techniques to enhance cross-modal transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locality Preserving Markovian Transition for Instance Retrieval <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifei Luo, Wenzheng Wu, Hantao Yao, Lu Yu, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based re-ranking methods are effective in modeling the data
manifolds through similarity propagation in affinity graphs. However, positive
signals tend to diminish over several steps away from the source, reducing
discriminative power beyond local regions. To address this issue, we introduce
the Locality Preserving Markovian Transition (LPMT) framework, which employs a
long-term thermodynamic transition process with multiple states for accurate
manifold distance measurement. The proposed LPMT first integrates diffusion
processes across separate graphs using Bidirectional Collaborative Diffusion
(BCD) to establish strong similarity relationships. Afterwards, Locality State
Embedding (LSE) encodes each instance into a distribution for enhanced local
consistency. These distributions are interconnected via the Thermodynamic
Markovian Transition (TMT) process, enabling efficient global retrieval while
maintaining local effectiveness. Experimental results across diverse tasks
confirm the effectiveness of LPMT for instance retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual reasoning: an analysis of in-context emergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Miller, Bernhard Schölkopf, Siyuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale neural language models (LMs) exhibit remarkable performance in
in-context learning: the ability to learn and reason the input context on the
fly without parameter update. This work studies in-context counterfactual
reasoning in language models, that is, to predict the consequences of changes
under hypothetical scenarios. We focus on studying a well-defined synthetic
setup: a linear regression task that requires noise abduction, where accurate
prediction is based on inferring and copying the contextual noise from factual
observations. We show that language models are capable of counterfactual
reasoning in this controlled setup and provide insights that counterfactual
reasoning for a broad class of functions can be reduced to a transformation on
in-context observations; we find self-attention, model depth, and data
diversity in pre-training drive performance in Transformers. More
interestingly, our findings extend beyond regression tasks and show that
Transformers can perform noise abduction on sequential data, providing
preliminary evidence on the potential for counterfactual story generation. Our
code is available under
https://github.com/moXmiller/counterfactual-reasoning.git .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeRPO: Tree Relative Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Associative Memory and Generative Diffusion in the Zero-noise Limit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Hess, Quaid Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connections between generative diffusion and continuous-state associative
memory models are studied. Morse-Smale dynamical systems are emphasized as
universal approximators of gradient-based associative memory models and
diffusion models as white-noise perturbed systems thereof. Universal properties
of associative memory that follow from this description are described and used
to characterize a generic transition from generation to memory as noise levels
diminish. Structural stability inherited by Morse-Smale flows is shown to imply
a notion of stability for diffusions at vanishing noise levels. Applied to one-
and two-parameter families of gradients, this indicates stability at all but
isolated points of associative memory learning landscapes and the learning and
generation landscapes of diffusion models with gradient drift in the zero-noise
limit, at which small sets of generic bifurcations characterize qualitative
transitions between stable systems. Examples illustrating the characterization
of these landscapes by sequences of these bifurcations are given, along with
structural stability criterion for classic and modern Hopfield networks
(equivalently, the attention mechanism).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavle Vasiljevic, Milica Matic, Miroslav Popovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, federated learning frameworks such as Python TestBed for Federated
Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms
have emerged to tackle user privacy concerns and efficiency in embedded
systems. Even more recently, an efficient federated anomaly detection
algorithm, FLiForest, based on Isolation Forests has been developed, offering a
low-resource, unsupervised method well-suited for edge deployment and
continuous learning. In this paper, we present an application of Isolation
Forest-based temperature anomaly detection, developed using the previously
mentioned federated learning frameworks, aimed at small edge devices and IoT
systems running MicroPython. The system has been experimentally evaluated,
achieving over 96% accuracy in distinguishing normal from abnormal readings and
above 78% precision in detecting anomalies across all tested configurations,
while maintaining a memory usage below 160 KB during model training. These
results highlight its suitability for resource-constrained environments and
edge systems, while upholding federated learning principles of data privacy and
collaborative learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 algorithms, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM
  Reasoning <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Parekh, Kartik Mehta, Ninareh Mehrabi, Kai-Wei Chang, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Event Detection (ED), the task of identifying event mentions in
natural language text without any training data, is critical for document
understanding in specialized domains. Understanding the complex event ontology,
extracting domain-specific triggers from the passage, and structuring them
appropriately overloads and limits the utility of Large Language Models (LLMs)
for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent
reasoning framework that decouples the task of ED using Dreamer and Grounder.
Dreamer encourages divergent reasoning through open-ended event discovery,
which helps to boost event coverage. Conversely, Grounder introduces convergent
reasoning to align the free-form predictions with the task-specific
instructions using finite-state machine guided constrained decoding.
Additionally, an LLM-Judge verifies the final outputs to ensure high precision.
Through extensive experiments on six datasets across five domains and nine
LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,
transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains
over the best baseline -- establishing DiCoRe as a strong zero-shot ED
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted at ACL ARR May 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Attacks on Sequence Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Rossi, Michael Aerni, Jie Zhang, Florian Tramèr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 8th Deep Learning Security and Privacy Workshop
  (DLSP) workshop (best paper award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear Causal Discovery for Grouped Data <span class="chip">UAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Göbler, Tobias Windisch, Mathias Drton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring cause-effect relationships from observational data has gained
significant attention in recent years, but most methods are limited to scalar
random variables. In many important domains, including neuroscience,
psychology, social science, and industrial manufacturing, the causal units of
interest are groups of variables rather than individual scalar measurements.
Motivated by these applications, we extend nonlinear additive noise models to
handle random vectors, establishing a two-step approach for causal graph
learning: First, infer the causal order among random vectors. Second, perform
model selection to identify the best graph consistent with this order. We
introduce effective and novel solutions for both steps in the vector case,
demonstrating strong performance in simulations. Finally, we apply our method
to real-world assembly line data with partial knowledge of causal ordering
among variable groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, to be published at UAI'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span> on the Evaluation of Generative Models in Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Lerch, Claire Arthur, Nick Bryan-Kinns, Corey Ford, Qianyi Sun, Ashvala Vinay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on generative systems in music has seen considerable attention and
growth in recent years. A variety of attempts have been made to systematically
evaluate such systems. We provide an interdisciplinary review of the common
evaluation targets, methodologies, and metrics for the evaluation of both
system output and model usability, covering subjective and objective
approaches, qualitative and quantitative approaches, as well as empirical and
computational methods. We discuss the advantages and challenges of such
approaches from a musicological, an engineering, and an HCI perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM CSUR, 26-Jun-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Amplification Through Synthetic Data: Insights from Linear
  Regression <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Pierquin, Aurélien Bellet, Marc Tommasi, Matthieu Boussard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Implicit Variational Inference via Kernelized Path Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Pielok, Bernd Bischl, David Rügamer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-implicit variational inference (SIVI) is a powerful framework for
approximating complex posterior distributions, but training with the
Kullback-Leibler (KL) divergence can be challenging due to high variance and
bias in high-dimensional settings. While current state-of-the-art semi-implicit
variational inference methods, particularly Kernel Semi-Implicit Variational
Inference (KSIVI), have been shown to work in high dimensions, training remains
moderately expensive. In this work, we propose a kernelized KL divergence
estimator that stabilizes training through nonparametric smoothing. To further
reduce the bias, we introduce an importance sampling correction. We provide a
theoretical connection to the amortized version of the Stein variational
gradient descent, which estimates the score gradient via Stein's identity,
showing that both methods minimize the same objective, but our semi-implicit
approach achieves lower gradient variance. In addition, our method's bias in
function space is benign, leading to more stable and efficient optimization.
Empirical results demonstrate that our method outperforms or matches
state-of-the-art SIVI methods in both performance and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMBER2024 -- A Benchmark <span class="highlight-title">Dataset</span> for Holistic Evaluation of Malware
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert J. Joyce, Gideon Miller, Phil Roth, Richard Zak, Elliott Zaresky-Williams, Hyrum Anderson, Edward Raff, James Holt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnHiPPO: Uncertainty-aware Initialization for State Space Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marten Lienen, Abdullah Saydemir, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State space models are emerging as a dominant model class for sequence
problems with many relying on the HiPPO framework to initialize their dynamics.
However, HiPPO fundamentally assumes data to be noise-free; an assumption often
violated in practice. We extend the HiPPO theory with measurement noise and
derive an uncertainty-aware initialization for state space model dynamics. In
our analysis, we interpret HiPPO as a linear stochastic control problem where
the data enters as a noise-free control signal. We then reformulate the problem
so that the data become noisy outputs of a latent system and arrive at an
alternative dynamics initialization that infers the posterior of this latent
system from the data without increasing runtime. Our experiments show that our
initialization improves the resistance of state-space models to noise both at
training and inference time. Find our implementation at
https://cs.cit.tum.de/daml/unhippo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NIMO: a Nonlinear Interpretable MOdel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijian Xu, Marcello Massimo Negri, Volker Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks (NNs) have achieved tremendous success over the past decade,
yet they are still extremely difficult to interpret. In contrast, linear models
are less expressive but offer inherent interpretability. Linear coefficients
are interpretable as the marginal effect of a feature on the prediction,
assuming all other features are kept fixed. To combine the benefits of both
approaches, we introduce NIMO (Nonlinear Interpretable MOdel). The key idea is
to define a model where the NN is designed to learn nonlinear corrections to
the linear model predictions, while also maintaining the original
interpretability of the linear coefficients. Relevantly, we develop an
optimization algorithm based on profile likelihood that elegantly allows for
optimizing over the NN parameters while updating the linear coefficients
analytically. By relying on adaptive ridge regression we can easily incorporate
sparsity constraints as well. We show empirically that we can recover the
underlying linear coefficients while significantly improving the predictive
accuracy. Compared to other hybrid interpretable approaches, our model is the
only one that actually maintains the same interpretability of linear
coefficients as in linear models. We also achieve higher performance on various
regression and classification settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliably detecting model failures in deployment without labels <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viet Nguyen Changjian Shui, Vijay Giri, Siddarth Arya, Amol Verma, Fahad Razak, Rahul G. Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distribution of data changes over time; models operating operating in
dynamic environments need retraining. But knowing when to retrain, without
access to labels, is an open challenge since some, but not all shifts degrade
model performance. This paper formalizes and addresses the problem of
post-deployment deterioration (PDD) monitoring. We propose D3M, a practical and
efficient monitoring algorithm based on the disagreement of predictive models,
achieving low false positive rates under non-deteriorating shifts and provides
sample complexity bounds for high true positive rates under deteriorating
shifts. Empirical results on both standard benchmark and a real-world
large-scale internal medicine dataset demonstrate the effectiveness of the
framework and highlight its viability as an alert mechanism for high-stakes
machine learning pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 6 figures, 7 tables, submitted to NeurIPS 2025, includes
  theoretical analysis and extensive empirical evaluation across benchmark and
  clinical datasets. Code available at https://github.com/teivng/d3m. Viet
  Nguyen and Changjian Shui contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iN2V: Bringing Transductive Node Embeddings to Inductive Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Lell, Ansgar Scherp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shallow node embeddings like node2vec (N2V) can be used for nodes without
features or to supplement existing features with structure-based information.
Embedding methods like N2V are limited in their application on new nodes, which
restricts them to the transductive setting where the entire graph, including
the test nodes, is available during training. We propose inductive node2vec
(iN2V), which combines a post-hoc procedure to compute embeddings for nodes
unseen during training and modifications to the original N2V training procedure
to prepare the embeddings for this post-hoc procedure. We conduct experiments
on several benchmark datasets and demonstrate that iN2V is an effective
approach to bringing transductive embeddings to an inductive setting. Using
iN2V embeddings improves node classification by 1 point on average, with up to
6 points of improvement depending on the dataset and the number of unseen
nodes. Our iN2V is a plug-in approach to create new or enrich existing
embeddings. It can also be combined with other embedding methods, making it a
versatile approach for inductive node representation learning. Code to
reproduce the results is available at https://github.com/Foisunt/iN2V .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIMING: Temporality-Aware Integrated Gradients for Time Series
  Explanation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeongwon Jang, Changhun Kim, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent explainable artificial intelligence (XAI) methods for time series
primarily estimate point-wise attribution magnitudes, while overlooking the
directional impact on predictions, leading to suboptimal identification of
significant points. Our analysis shows that conventional Integrated Gradients
(IG) effectively capture critical points with both positive and negative
impacts on predictions. However, current evaluation metrics fail to assess this
capability, as they inadvertently cancel out opposing feature contributions. To
address this limitation, we propose novel evaluation metrics-Cumulative
Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to
systematically assess whether attribution methods accurately identify
significant positive and negative points in time series XAI. Under these
metrics, conventional IG outperforms recent counterparts. However, directly
applying IG to time series data may lead to suboptimal outcomes, as generated
paths ignore temporal relationships and introduce out-of-distribution samples.
To overcome these challenges, we introduce TIMING, which enhances IG by
incorporating temporal awareness while maintaining its theoretical properties.
Extensive experiments on synthetic and real-world time series benchmarks
demonstrate that TIMING outperforms existing time series XAI baselines. Our
code is available at https://github.com/drumpt/TIMING.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Spotlight Presentation; Code is available at
  https://github.com/drumpt/TIMING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Understanding Cross-Class Features in Adversarial
  Training <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeming Wei, Yiwen Guo, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence Should Genuinely Support Clinical Reasoning and
  Decision Making To Bridge the Translational Gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kacper Sokol, James Fackler, Julia E Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence promises to revolutionise medicine, yet its impact
remains limited because of the pervasive translational gap. We posit that the
prevailing technology-centric approaches underpin this challenge, rendering
such systems fundamentally incompatible with clinical practice, specifically
diagnostic reasoning and decision making. Instead, we propose a novel
sociotechnical conceptualisation of data-driven support tools designed to
complement doctors' cognitive and epistemic activities. Crucially, it
prioritises real-world impact over superhuman performance on inconsequential
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tuning the Right Foundation Models is What you Need for Partial Label
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang He, Wei Tang, Tong Wei, Min-Ling Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial label learning (PLL) seeks to train generalizable classifiers from
datasets with inexact supervision, a common challenge in real-world
applications. Existing studies have developed numerous approaches to
progressively refine and recover ground-truth labels by training convolutional
neural networks. However, limited attention has been given to foundation models
that offer transferrable representations. In this work, we empirically conduct
comprehensive evaluations of 11 foundation models across 13 PLL approaches on 8
benchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an
efficient fine-tuning framework for foundation models in PLL. Our findings
reveal that current PLL approaches tend to 1) achieve significant performance
gains when using foundation models, 2) exhibit remarkably similar performance
to each other, 3) maintain stable performance across varying ambiguity levels,
while 4) are susceptible to foundation model selection and adaptation
strategies. Additionally, we demonstrate the efficacy of text-embedding
classifier initialization and effective candidate label filtering using
zero-shot CLIP. Our experimental results and analysis underscore the
limitations of current PLL approaches and provide valuable insights for
developing more generalizable PLL models. The source code can be found at
https://github.com/SEU-hk/PartialCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code can be found at \url{https://github.com/SEU-hk/PartialCLIP}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Summarization Length Through EOS Token Weighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeno Belligoli, Emmanouil Stergiadis, Eran Fainman, Ilya Gusev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling the length of generated text can be crucial in various
text-generation tasks, including summarization. Existing methods often require
complex model alterations, limiting compatibility with pre-trained models. We
address these limitations by developing a simple approach for controlling the
length of automatic text summaries by increasing the importance of correctly
predicting the EOS token in the cross-entropy loss computation. The proposed
methodology is agnostic to architecture and decoding algorithms and orthogonal
to other inference-time techniques to control the generation length. We tested
it with encoder-decoder and modern GPT-style LLMs, and show that this method
can control generation length, often without affecting the quality of the
summary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Point Proximity Encoding For Vector-Mode Geospatial Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Collins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-mode geospatial data -- points, lines, and polygons -- must be encoded
into an appropriate form in order to be used with traditional machine learning
and artificial intelligence models. Encoding methods attempt to represent a
given shape as a vector that captures its essential geometric properties. This
paper presents an encoding method based on scaled distances from a shape to a
set of reference points within a region of interest. The method, MultiPoint
Proximity (MPP) encoding, can be applied to any type of shape, enabling the
parameterization of machine learning models with encoded representations of
vector-mode geospatial features. We show that MPP encoding possesses the
desirable properties of shape-centricity and continuity, can be used to
differentiate spatial objects based on their geometric features, and can
capture pairwise spatial relationships with high precision. In all cases, MPP
encoding is shown to perform better than an alternative method based on
rasterization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reasonable Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nektarios Kalampalikis, Kavya Gupta, Georgi Vitanov, Isabel Valera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose $\textbf{C}$oncept $\textbf{REA}$soning
$\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)
that: (i) explicitly encodes concept-concept (${\texttt{C-C}}$) and
concept-task (${\texttt{C$\rightarrow$Y}}$) relationships to enforce a desired
model reasoning; and (ii) use a regularized side-channel to achieve competitive
task performance, while keeping high concept importance. Specifically, CREAM
architecturally embeds (bi)directed concept-concept, and concept to task
relationships specified by a human expert, while severing undesired information
flows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates
a black-box side-channel that is regularized to encourage task predictions to
be grounded in the relevant concepts, thereby utilizing the side-channel only
when necessary to enhance performance. Our experiments show that: (i) CREAM
mainly relies on concepts while achieving task performance on par with
black-box models; and (ii) the embedded ${\texttt{C-C}}$ and
${\texttt{C$\rightarrow$Y}}$ relationships ease model interventions and
mitigate concept leakage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QiMeng: Fully Automated Hardware and Software Design for Processor Chip 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhang, Yuanbo Wen, Shuyao Cheng, Di Huang, Shaohui Peng, Jiaming Guo, Pengwei Jin, Jiacheng Zhao, Tianrui Ma, Yaoyu Zhu, Yifan Hao, Yongwei Zhao, Shengwen Liang, Ying Wang, Xing Hu, Zidong Du, Huimin Cui, Ling Li, Qi Guo, Yunji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processor chip design technology serves as a key frontier driving
breakthroughs in computer science and related fields. With the rapid
advancement of information technology, conventional design paradigms face three
major challenges: the physical constraints of fabrication technologies, the
escalating demands for design resources, and the increasing diversity of
ecosystems. Automated processor chip design has emerged as a transformative
solution to address these challenges. While recent breakthroughs in Artificial
Intelligence (AI), particularly Large Language Models (LLMs) techniques, have
opened new possibilities for fully automated processor chip design, substantial
challenges remain in establishing domain-specific LLMs for processor chip
design.
  In this paper, we propose QiMeng, a novel system for fully automated hardware
and software design of processor chips. QiMeng comprises three hierarchical
layers. In the bottom-layer, we construct a domain-specific Large Processor
Chip Model (LPCM) that introduces novel designs in architecture, training, and
inference, to address key challenges such as knowledge representation gap, data
scarcity, correctness assurance, and enormous solution space. In the
middle-layer, leveraging the LPCM's knowledge representation and inference
capabilities, we develop the Hardware Design Agent and the Software Design
Agent to automate the design of hardware and software for processor chips.
Currently, several components of QiMeng have been completed and successfully
applied in various top-layer applications, demonstrating significant advantages
and providing a feasible solution for efficient, fully automated
hardware/software design of processor chips. Future research will focus on
integrating all components and performing iterative top-down and bottom-up
design processes to establish a comprehensive QiMeng system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashkan Soleymani, Georgios Piliouras, Gabriele Farina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work [Soleymani et al., 2025] introduced a variant of Optimistic
Multiplicative Weights Updates (OMWU) that adaptively controls the learning
pace in a dynamic, non-monotone manner, achieving new state-of-the-art regret
minimization guarantees in general games. In this work, we demonstrate that
no-regret learning acceleration through adaptive pacing of the learners is not
an isolated phenomenon. We introduce \emph{Cautious Optimism}, a framework for
substantially faster regularized learning in general games. Cautious Optimism
takes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs
an accelerated no-regret learning algorithm by pacing the underlying FTRL with
minimal computational overhead. Importantly, we retain uncoupledness (learners
do not need to know other players' utilities). Cautious Optimistic FTRL
achieves near-optimal $O_T(\log T)$ regret in diverse self-play
(mixing-and-matching regularizers) while preserving the optimal $O(\sqrt{T})$
regret in adversarial scenarios. In contrast to prior works (e.g. Syrgkanis et
al. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic
step-sizes, showcasing a novel route for fast learning in general games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract appeared at Twenty-Sixth ACM Conference on
  Economics and Computation (EC), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPTQuant: Function-Preserving Transforms for LLM Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris van Breugel, Yelysei Bondarenko, Paul Whatmough, Markus Nagel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require substantial compute, and thus energy, at
inference time. While quantizing weights and activations is effective at
improving efficiency, naive quantization of LLMs can significantly degrade
performance due to large magnitude outliers. This paper describes FPTQuant,
which introduces four novel, lightweight, and expressive function-preserving
transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable
pre-RoPE transform for queries and keys, (2) a mergeable transform for values,
(3) a mergeable scaling transform within the MLP block, and (4) a cheap,
dynamic scaling transform. By leveraging the equivariances and independencies
inherent to canonical transformer operation, we designed these FPTs to maintain
the model's function while shaping the intermediate activation distributions to
be more quantization friendly. FPTQuant requires no custom kernels and adds
virtually no overhead during inference. The FPTs are trained both locally to
reduce outliers, and end-to-end such that the outputs of the quantized and
full-precision models match. FPTQuant enables static INT4 quantization with
minimal overhead and shows SOTA speed-up of up to 3.9 times over FP.
Empirically, FPTQuant has an excellent accuracy-speed trade-off -- it is
performing on par or exceeding most prior work and only shows slightly lower
accuracy compared to a method that is up to 29% slower.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic AI for Intent-Based Industrial Automation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Lima Romero, Ricardo Suyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of Agentic AI systems, empowered by autonomous large
language models (LLMs) agents with planning and tool-usage capabilities,
enables new possibilities for the evolution of industrial automation and
reduces the complexity introduced by Industry 4.0. This work proposes a
conceptual framework that integrates Agentic AI with the intent-based paradigm,
originally developed in network research, to simplify human-machine interaction
(HMI) and better align automation systems with the human-centric, sustainable,
and resilient principles of Industry 5.0. Based on the intent-based processing,
the framework allows human operators to express high-level business or
operational goals in natural language, which are decomposed into actionable
components. These intents are broken into expectations, conditions, targets,
context, and information that guide sub-agents equipped with specialized tools
to execute domain-specific tasks. A proof of concept was implemented using the
CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the
feasibility of intent decomposition, agent orchestration, and autonomous
decision-making in predictive maintenance scenarios. The results confirm the
potential of this approach to reduce technical barriers and enable scalable,
intent-driven automation, despite data quality and explainability concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint - Submitted to 16th IEEE/IAS International Conference on
  Industry Applications - INDUSCON 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Joint Interventional Effects from Single-Variable Interventions
  in Additive Models <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Kekić, Sergio Hernan Garrido Mejia, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating causal effects of joint interventions on multiple variables is
crucial in many domains, but obtaining data from such simultaneous
interventions can be challenging. Our study explores how to learn joint
interventional effects using only observational data and single-variable
interventions. We present an identifiability result for this problem, showing
that for a class of nonlinear additive outcome mechanisms, joint effects can be
inferred without access to joint interventional data. We propose a practical
estimator that decomposes the causal effect into confounded and unconfounded
contributions for each intervention variable. Experiments on synthetic data
demonstrate that our method achieves performance comparable to models trained
directly on joint interventional data, outperforming a purely observational
estimator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at the International Conference on Machine Learning
  (ICML) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting ICU In-Hospital Mortality Using Adaptive <span class="highlight-title">Transformer</span> Layer
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Ruoyun He, Guoguang Lao, Ting Liu, Hejiao Luo, Changqi Qin, Hongying Luo, Junmin Huang, Zihan Wei, Lu Chen, Yongzhi Xu, Ziqian Bi, Junhao Song, Tianyang Wang, Chia Xin Liang, Xinyuan Song, Huafeng Liu, Junfeng Hao, Chunjie Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early identification of high-risk ICU patients is crucial for directing
limited medical resources. We introduce ALFIA (Adaptive Layer Fusion with
Intelligent Attention), a modular, attention-based architecture that jointly
trains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting
mechanism to fuse multi-layer semantic features from a BERT backbone. Trained
on our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses
state-of-the-art tabular classifiers in AUPRC while preserving a balanced
precision-recall profile. The embeddings produced by ALFIA's fusion module,
capturing both fine-grained clinical cues and high-level concepts, enable
seamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro
networks as ALFIA-nn, yielding additional performance gains. Our experiments
confirm ALFIA's superior early-warning performance, by operating directly on
routine clinical text, it furnishes clinicians with a convenient yet robust
tool for risk stratification and timely intervention in critical-care settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energentic Intelligence: From Self-Sustaining Systems to Enduring
  Artificial Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atahan Karagoz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Energentic Intelligence, a class of autonomous systems
defined not by task performance, but by their capacity to sustain themselves
through internal energy regulation. Departing from conventional reward-driven
paradigms, these agents treat survival-maintaining functional operation under
fluctuating energetic and thermal conditions-as the central objective. We
formalize this principle through an energy-based utility function and a
viability-constrained survival horizon, and propose a modular architecture that
integrates energy harvesting, thermal regulation, and adaptive computation into
a closed-loop control system. A simulated environment demonstrates the
emergence of stable, resource-aware behavior without external supervision.
Together, these contributions provide a theoretical and architectural
foundation for deploying autonomous agents in resource-volatile settings where
persistence must be self-regulated and infrastructure cannot be assumed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Long Reasoning Models: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyu Mu, Jiali Zeng, Bei Li, Xinyan Guan, Fandong Meng, Jie Zhou, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in training long-context reasoning models via
reinforcement learning (RL), several open questions and counterintuitive
behaviors remain. This work focuses on three key aspects: (1) We systematically
analyze the roles of positive and negative samples in RL, revealing that
positive samples mainly facilitate data fitting, whereas negative samples
significantly enhance generalization and robustness. Interestingly, training
solely on negative samples can rival standard RL training performance. (2) We
identify substantial data inefficiency in group relative policy optimization,
where over half of the samples yield zero advantage. To address this, we
explore two straightforward strategies, including relative length rewards and
offline sample injection, to better leverage these data and enhance reasoning
efficiency and capability. (3) We investigate unstable performance across
various reasoning models and benchmarks, attributing instability to uncertain
problems with ambiguous outcomes, and demonstrate that multiple evaluation runs
mitigate this issue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Thinking LLMs Lie: Unveiling the Strategic Deception in
  Representations of Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Yihao Zhang, Meng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning
  Blind Spots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Pan, Mary-Anne Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), whilst great at extracting facts from text,
struggle with nested narrative reasoning. Existing long context and multi-hop
QA benchmarks inadequately test this, lacking realistic distractors or failing
to decouple context length from reasoning complexity, masking a fundamental LLM
limitation. We introduce Verbose ListOps, a novel benchmark that
programmatically transposes ListOps computations into lengthy, coherent
stories. This uniquely forces internal computation and state management of
nested reasoning problems by withholding intermediate results, and offers
fine-grained controls for both narrative size \emph{and} reasoning difficulty.
Whilst benchmarks like LongReason (2025) advance approaches for synthetically
expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints
a specific LLM vulnerability: difficulty in state management for nested
sub-reasoning amongst semantically-relevant, distracting narrative. Our
experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse
in performance on Verbose ListOps at modest (~10k token) narrative lengths,
despite effortlessly solving raw ListOps equations. Addressing this failure is
paramount for real-world text interpretation which requires identifying key
reasoning points, tracking conceptual intermediate results, and filtering
irrelevant information. Verbose ListOps, and its extensible generation
framework thus enables targeted reasoning enhancements beyond mere
context-window expansion; a critical step to automating the world's knowledge
work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TQml Simulator: Optimized Simulation of Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viacheslav Kuzmin, Basil Kyriacou, Mateusz Papierz, Mo Kordzanganeh, Alexey Melnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hardware-efficient circuits employed in Quantum Machine Learning are
typically composed of alternating layers of uniformly applied gates. High-speed
numerical simulators for such circuits are crucial for advancing research in
this field. In this work, we numerically benchmark universal and gate-specific
techniques for simulating the action of layers of gates on quantum state
vectors, aiming to accelerate the overall simulation of Quantum Machine
Learning algorithms. Our analysis shows that the optimal simulation method for
a given layer of gates depends on the number of qubits involved, and that a
tailored combination of techniques can yield substantial performance gains in
the forward and backward passes for a given circuit. Building on these
insights, we developed a numerical simulator, named TQml Simulator, that
employs the most efficient simulation method for each layer in a given circuit.
We evaluated TQml Simulator on circuits constructed from standard gate sets,
such as rotations and CNOTs, as well as on native gates from IonQ and IBM
quantum processing units. In most cases, our simulator outperforms equivalent
Pennylane's default.qubit simulator by approximately 2- to 100-fold, depending
on the circuit, the number of qubits, the batch size of the input data, and the
hardware used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms
  Angle-Based Methods for Assessment of Hip Dysplasia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Paul, George Grammatopoulos, Adwaye Rambojun, Neill D. F. Campbell, Harinderjit S. Gill, Tony Shardlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,
early diagnosis of dysplasia is important to provide opportunities for surgical
interventions aimed at reducing the risk of hip OA. We have developed a
pipeline for semi-automated classification of dysplasia using volumetric CT
scans of patients' hips and a minimal set of clinically annotated landmarks,
combining the framework of the Gaussian Process Latent Variable Model with
diffeomorphism to create a statistical shape model, which we termed the
Gaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT
scans, 100 for model training and 92 for testing. The GPDSSM effectively
distinguishes dysplastic samples from controls while also highlighting regions
of the underlying surface that show dysplastic variations. As well as improving
classification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),
the GPDSSM can save time for clinicians by removing the need to manually
measure angles and interpreting 2D scans for possible markers of dysplasia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ kTULA: A Langevin sampling algorithm with improved KL bounds under
  super-linear log-gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iosif Lytras, Sotirios Sabanis, Ying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in deep learning, where the global Lipschitz
continuity condition is often not satisfied, we examine the problem of sampling
from distributions with super-linearly growing log-gradients. We propose a
novel tamed Langevin dynamics-based algorithm, called kTULA, to solve the
aforementioned sampling problem, and provide a theoretical guarantee for its
performance. More precisely, we establish a non-asymptotic convergence bound in
Kullback-Leibler (KL) divergence with the best-known rate of convergence equal
to $2-\overline{\epsilon}$, $\overline{\epsilon}>0$, which significantly
improves relevant results in existing literature. This enables us to obtain an
improved non-asymptotic error bound in Wasserstein-2 distance, which can be
used to further derive a non-asymptotic guarantee for kTULA to solve the
associated optimization problems. To illustrate the applicability of kTULA, we
apply the proposed algorithm to the problem of sampling from a high-dimensional
double-well potential distribution and to an optimization problem involving a
neural network. We show that our main results can be used to provide
theoretical guarantees for the performance of kTULA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ There Was Never a Bottleneck in Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Almudévar, José Miguel Hernández-Lobato, Alfonso Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning representations are often difficult to interpret, which can
hinder their deployment in sensitive applications. Concept Bottleneck Models
(CBMs) have emerged as a promising approach to mitigate this issue by learning
representations that support target task performance while ensuring that each
component predicts a concrete concept from a predefined set. In this work, we
argue that CBMs do not impose a true bottleneck: the fact that a component can
predict a concept does not guarantee that it encodes only information about
that concept. This shortcoming raises concerns regarding interpretability and
the validity of intervention procedures. To overcome this limitation, we
propose Minimal Concept Bottleneck Models (MCBMs), which incorporate an
Information Bottleneck (IB) objective to constrain each representation
component to retain only the information relevant to its corresponding concept.
This IB is implemented via a variational regularization term added to the
training loss. As a result, MCBMs support concept-level interventions with
theoretical guarantees, remain consistent with Bayesian principles, and offer
greater flexibility in key design choices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Multimodal Representations through an Information Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Almudévar, José Miguel Hernández-Lobato, Sameer Khurana, Ricard Marxer, Alfonso Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive losses have been extensively used as a tool for multimodal
representation learning. However, it has been empirically observed that their
use is not effective to learn an aligned representation space. In this paper,
we argue that this phenomenon is caused by the presence of modality-specific
information in the representation space. Although some of the most widely used
contrastive losses maximize the mutual information between representations of
both modalities, they are not designed to remove the modality-specific
information. We give a theoretical description of this problem through the lens
of the Information Bottleneck Principle. We also empirically analyze how
different hyperparameters affect the emergence of this phenomenon in a
controlled experimental setup. Finally, we propose a regularization term in the
loss function that is derived by means of a variational approximation and aims
to increase the representational alignment. We analyze in a set of controlled
experiments and real-world applications the advantages of including this
regularization term.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs for sensory-motor control: Combining in-context and iterative
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jônata Tyska Carvalho, Stefano Nolfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method that enables large language models (LLMs) to control
embodied agents by directly mapping continuous observation vectors to
continuous action vectors. Initially, the LLMs generate a control strategy
based on a textual description of the agent, its environment, and the intended
goal. This strategy is then iteratively refined through a learning process in
which the LLMs are repeatedly prompted to improve the current strategy, using
performance feedback and sensory-motor data collected during its evaluation.
The method is validated on classic control tasks from the Gymnasium library and
the inverted pendulum task from the MuJoCo library. In most cases, it
successfully identifies optimal or high-performing solutions by integrating
symbolic knowledge derived through reasoning with sub-symbolic sensory-motor
data gathered as the agent interacts with its environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages (13 pages are from appendix), 6 figures, code for
  experiments replication and supplementary material provided at
  https://github.com/jtyska/llm-robotics-article/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Autoencoders, Again? <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Lu, Tong He, Xuening Zhu, David Wipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Is there really much more to say about sparse autoencoders (SAEs)?
Autoencoders in general, and SAEs in particular, represent deep architectures
that are capable of modeling low-dimensional latent structure in data. Such
structure could reflect, among other things, correlation patterns in large
language model activations, or complex natural image manifolds. And yet despite
the wide-ranging applicability, there have been relatively few changes to SAEs
beyond the original recipe from decades ago, namely, standard deep
encoder/decoder layers trained with a classical/deterministic sparse
regularizer applied within the latent space. One possible exception is the
variational autoencoder (VAE), which adopts a stochastic encoder module capable
of producing sparse representations when applied to manifold data. In this work
we formalize underappreciated weaknesses with both canonical SAEs, as well as
analogous VAEs applied to similar tasks, and propose a hybrid alternative model
that circumvents these prior limitations. In terms of theoretical support, we
prove that global minima of our proposed model recover certain forms of
structured data spread across a union of manifolds. Meanwhile, empirical
evaluations on synthetic and real-world datasets substantiate the efficacy of
our approach in accurately estimating underlying manifold dimensions and
producing sparser latent representations without compromising reconstruction
error. In general, we are able to exceed the performance of equivalent-capacity
SAEs and VAEs, as well as recent diffusion models where applicable, within
domains such as images and language model activation patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference on Machine Learning (ICML)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving AI-generated music with user-guided training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishwa Mohan Singh, Sai Anirudh Aryasomayajula, Ahan Chatterjee, Beste Aydemir, Rifat Mehreen Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI music generation has advanced rapidly, with models like diffusion and
autoregressive algorithms enabling high-fidelity outputs. These tools can alter
styles, mix instruments, or isolate them. Since sound can be visualized as
spectrograms, image-generation algorithms can be applied to generate novel
music. However, these algorithms are typically trained on fixed datasets, which
makes it challenging for them to interpret and respond to user input
accurately. This is especially problematic because music is highly subjective
and requires a level of personalization that image generation does not provide.
In this work, we propose a human-computation approach to gradually improve the
performance of these algorithms based on user interactions. The
human-computation element involves aggregating and selecting user ratings to
use as the loss function for fine-tuning the model. We employ a genetic
algorithm that incorporates user feedback to enhance the baseline performance
of a model initially trained on a fixed dataset. The effectiveness of this
approach is measured by the average increase in user ratings with each
iteration. In the pilot test, the first iteration showed an average rating
increase of 0.2 compared to the baseline. The second iteration further improved
upon this, achieving an additional increase of 0.39 over the first iteration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Select for presentation in HHAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health
  Trajectories with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Ege Özsoy, David Bani-Harouni, Matthias Keicher, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare systems face significant challenges in managing and interpreting
vast, heterogeneous patient data for personalized care. Existing approaches
often focus on narrow use cases with a limited feature space, overlooking the
complex, longitudinal interactions needed for a holistic understanding of
patient health. In this work, we propose a novel approach to patient pathway
modeling by transforming diverse electronic health record (EHR) data into a
structured representation and designing a holistic pathway prediction model,
EHR2Path, optimized to predict future health trajectories. Further, we
introduce a novel summary mechanism that embeds long-term temporal context into
topic-specific summary tokens, improving performance over text-only models,
while being much more token-efficient. EHR2Path demonstrates strong performance
in both next time-step prediction and longitudinal simulation, outperforming
competitive baselines. It enables detailed simulations of patient trajectories,
inherently targeting diverse evaluation tasks, such as forecasting vital signs,
lab test results, or length-of-stay, opening a path towards predictive and
personalized healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Pavlitska, Jamie Robb, Nikolai Polley, Melih Yazgan, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic adversarial attacks on various camera-based perception tasks of
autonomous vehicles have been successfully demonstrated so far. However, only a
few works considered attacks on traffic light detectors. This work shows how
CNNs for traffic light detection can be attacked with printed patches. We
propose a threat model, where each instance of a traffic light is attacked with
a patch placed under it, and describe a training strategy. We demonstrate
successful adversarial patch attacks in universal settings. Our experiments
show realistic targeted red-to-green label-flipping attacks and attacks on
pictogram classification. Finally, we perform a real-world evaluation with
printed patches and demonstrate attacks in the lab settings with a mobile
traffic light for construction sites and in a test area with stationary traffic
lights. Our code is available at
https://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AceReason-Nemotron: Advancing Math and Code Reasoning through
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16400v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16400v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add pass@1024 evaluation results for LiveCodeBench v6. We release the
  models at:
  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The broader spectrum of in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03782v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03782v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of language models to learn a task from a few examples in context
has generated substantial interest. Here, we provide a perspective that
situates this type of supervised few-shot learning within a much broader
spectrum of meta-learned in-context learning. Indeed, we suggest that any
distribution of sequences in which context non-trivially decreases loss on
subsequent predictions can be interpreted as eliciting a kind of in-context
learning. We suggest that this perspective helps to unify the broad set of
in-context abilities that language models exhibit -- such as adapting to tasks
from instructions or role play, or extrapolating time series. This perspective
also sheds light on potential roots of in-context learning in lower-level
processing of linguistic dependencies (e.g. coreference or parallel
structures). Finally, taking this perspective highlights the importance of
generalization, which we suggest can be studied along several dimensions: not
only the ability to learn something novel, but also flexibility in learning
from different presentations, and in applying what is learned. We discuss
broader connections to past literature in meta-learning and goal-conditioned
agents, and other perspectives on learning and adaptation. We close by
suggesting that research on in-context learning should consider this broader
spectrum of in-context capabilities and types of generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context is Key: A Benchmark for Forecasting with Essential Textual
  Information <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18959v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18959v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Robert Williams, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste, Irina Rish, Nicolas Chapados, Alexandre Drouin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting is a critical task in decision-making across numerous domains.
While historical numerical data provide a start, they fail to convey the
complete context for reliable and accurate predictions. Human forecasters
frequently rely on additional information, such as background knowledge and
constraints, which can efficiently be communicated through natural language.
However, in spite of recent progress with LLM-based forecasters, their ability
to effectively integrate this textual information remains an open question. To
address this, we introduce "Context is Key" (CiK), a time-series forecasting
benchmark that pairs numerical data with diverse types of carefully crafted
textual context, requiring models to integrate both modalities; crucially,
every task in CiK requires understanding textual context to be solved
successfully. We evaluate a range of approaches, including statistical models,
time series foundation models, and LLM-based forecasters, and propose a simple
yet effective LLM prompting method that outperforms all other tested methods on
our benchmark. Our experiments highlight the importance of incorporating
contextual information, demonstrate surprising performance when using LLM-based
forecasting models, and also reveal some of their critical shortcomings. This
benchmark aims to advance multimodal forecasting by promoting models that are
both accurate and accessible to decision-makers with varied technical
expertise. The benchmark can be visualized at
https://servicenow.github.io/context-is-key-forecasting/v0/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning from Simulated Interactions via Multitask Prospective
  Rehearsal for Bionic Limb Behavior Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01114v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01114v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharmita Dey, Benjamin Paassen, Sarath Ravindran Nair, Sabri Boughorbel, Arndt F. Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lower limb amputations and neuromuscular impairments severely restrict
mobility, necessitating advancements beyond conventional prosthetics. While
motorized bionic limbs show promise, their effectiveness depends on replicating
the dynamic coordination of human movement across diverse environments. In this
paper, we introduce a model for human behavior in the context of bionic
prosthesis control. Our approach leverages human locomotion demonstrations to
learn the synergistic coupling of the lower limbs, enabling the prediction of
the kinematic behavior of a missing limb during tasks such as walking, climbing
inclines, and stairs. We propose a multitasking, continually adaptive model
that anticipates and refines movements over time. At the core of our method is
a technique called multitask prospective rehearsal, that anticipates and
synthesizes future movements based on the previous prediction and employs a
corrective mechanism for subsequent predictions. Our evolving architecture
merges lightweight, task-specific modules on a shared backbone, ensuring both
specificity and scalability. We validate our model through experiments on
real-world human gait datasets, including transtibial amputees, across a wide
range of locomotion tasks. Results demonstrate that our approach consistently
outperforms baseline models, particularly in scenarios with distributional
shifts, adversarial perturbations, and noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transactions on Machine Learning Research (TMLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking LLM Advancement: Compute-Dependent and Independent Paths to
  Progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Sanderson, Teddy Foley, Spencer Guo, Anqi Qu, Henry Josephson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory efforts to govern large language model (LLM) development have
predominantly focused on restricting access to high-performance computational
resources. This study evaluates the efficacy of such measures by examining
whether LLM capabilities can advance through algorithmic innovation in
compute-constrained environments. We propose a novel framework distinguishing
compute-dependent innovations--which yield disproportionate benefits at high
compute--from compute-independent innovations, which improve efficiency across
compute scales. The impact is quantified using Compute-Equivalent Gain (CEG).
Experimental validation with nanoGPT models confirms that compute-independent
advancements yield significant performance gains (e.g., with combined CEG up to
$3.5\times$) across the tested scales. In contrast, compute-dependent
advancements were detrimental to performance at smaller experimental scales,
but showed improved CEG (on par with the baseline) as model size increased, a
trend consistent with their definition of yielding primary benefits at higher
compute. Crucially, these findings indicate that restrictions on computational
hardware, while potentially slowing LLM progress, are insufficient to prevent
all capability gains driven by algorithmic advancements. We argue that
effective AI oversight must therefore incorporate mechanisms for understanding,
anticipating, and potentially guiding algorithmic research, moving beyond a
singular focus on hardware. The proposed framework also serves as an analytical
tool for forecasting AI progress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A theoretical framework for overfitting in energy-based modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19158v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19158v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Catania, Aurélien Decelle, Cyril Furtlehner, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the impact of limited data on training pairwise energy-based
models for inverse problems aimed at identifying interaction networks.
Utilizing the Gaussian model as testbed, we dissect training trajectories
across the eigenbasis of the coupling matrix, exploiting the independent
evolution of eigenmodes and revealing that the learning timescales are tied to
the spectral decomposition of the empirical covariance matrix. We see that
optimal points for early stopping arise from the interplay between these
timescales and the initial conditions of training. Moreover, we show that
finite data corrections can be accurately modeled through asymptotic random
matrix theory calculations and provide the counterpart of generalized
cross-validation in the energy based model context. Our analytical framework
extends to binary-variable maximum-entropy pairwise models with minimal
variations. These findings offer strategies to control overfitting in
discrete-variable models through empirical shrinkage corrections, improving the
management of overfitting in energy-based generative models. Finally, we
propose a generalization to arbitrary energy-based models by deriving the
neural tangent kernel dynamics of the score function under the score-matching
algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 20 figures (including appendix). Accepted at Proceedings of
  the 42nd International Conference on Machine Learning, Vancouver, Canada.
  PMLR 267, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Timing is Important: Risk-aware Fund Allocation based on Time-Series
  Forecasting <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyuan Lyu, Linfeng Du, Yunpeng Weng, Qiufang Ying, Zhiyan Xu, Wen Zou, Haolun Wu, Xiuqiang He, Xing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025 ADS Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blink of an eye: a simple theory for feature localization in generative
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Li, Aayush Karan, Sitan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models can exhibit unexpected behavior in the blink of an eye.
In a recent computer use demo, a language model switched from coding to
Googling pictures of Yellowstone, and these sudden shifts in behavior have also
been observed in reasoning patterns and jailbreaks. This phenomenon is not
unique to autoregressive models: in diffusion models, key features of the final
output are decided in narrow ``critical windows'' of the generation process. In
this work we develop a simple, unifying theory to explain this phenomenon using
the formalism of stochastic localization samplers. We show that it emerges
generically as the generation process localizes to a sub-population of the
distribution it models.
  While critical windows have been studied at length in diffusion models,
existing theory heavily relies on strong distributional assumptions and the
particulars of Gaussian diffusion. In contrast to existing work our theory (1)
applies to autoregressive and diffusion models; (2) makes no distributional
assumptions; (3) quantitatively improves previous bounds even when specialized
to diffusions; and (4) requires basic tools and no stochastic calculus or
statistical-physics-based machinery. We also identify an intriguing connection
to the all-or-nothing phenomenon from statistical inference. Finally, we
validate our predictions empirically for LLMs and find that critical windows
often coincide with failures in problem solving for various math and reasoning
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Poisson Surface Reconstruction with One Solve using Geometric
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidhanth Holalkere, David S. Bindel, Silvia Sellán, Alexander Terenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poisson Surface Reconstruction is a widely-used algorithm for reconstructing
a surface from an oriented point cloud. To facilitate applications where only
partial surface information is available, or scanning is performed
sequentially, a recent line of work proposes to incorporate uncertainty into
the reconstructed surface via Gaussian process models. The resulting algorithms
first perform Gaussian process interpolation, then solve a set of volumetric
partial differential equations globally in space, resulting in a
computationally expensive two-stage procedure. In this work, we apply
recently-developed techniques from geometric Gaussian processes to combine
interpolation and surface reconstruction into a single stage, requiring only
one linear solve per sample. The resulting reconstructed surface samples can be
queried locally in space, without the use of problem-dependent volumetric
meshes or grids. These capabilities enable one to (a) perform probabilistic
collision detection locally around the region of interest, (b) perform ray
casting without evaluating points not on the ray's trajectory, and (c) perform
next-view planning on a per-ray basis. They also do not requiring one to
approximate kernel matrix inverses with diagonal matrices as part of
intermediate computations, unlike prior methods. Results show that our approach
provides a cleaner, more-principled, and more-flexible stochastic surface
reconstruction pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Discovery from Conditionally Stationary Time Series <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06257v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06257v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carles Balsells-Rodas, Xavier Sumba, Tanmayee Narendra, Ruibo Tu, Gabriele Schweikert, Hedvig Kjellstrom, Yingzhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery, i.e., inferring underlying causal relationships from
observational data, is highly challenging for AI systems. In a time series
modeling context, traditional causal discovery methods mainly consider
constrained scenarios with fully observed variables and/or data from stationary
time-series. We develop a causal discovery approach to handle a wide class of
nonstationary time series that are conditionally stationary, where the
nonstationary behaviour is modeled as stationarity conditioned on a set of
latent state variables. Named State-Dependent Causal Inference (SDCI), our
approach is able to recover the underlying causal dependencies, with provable
identifiablity for the state-dependent causal structures. Empirical experiments
on nonlinear particle interaction data and gene regulatory networks demonstrate
SDCI's superior performance over baseline causal discovery methods. Improved
results over non-causal RNNs on modeling NBA player movements demonstrate the
potential of our method and motivate the use of causality-driven methods for
forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Lessons of Developing Process Reward Models in Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Reward Models (PRMs) emerge as a promising approach for process
supervision in mathematical reasoning of Large Language Models (LLMs), which
aim to identify and mitigate intermediate errors in the reasoning processes.
However, the development of effective PRMs faces significant challenges,
particularly in data annotation and evaluation methodologies. In this paper,
through extensive experiments, we demonstrate that commonly used Monte Carlo
(MC) estimation-based data synthesis for PRMs typically yields inferior
performance and generalization compared to LLM-as-a-judge and human annotation
methods. MC estimation relies on completion models to evaluate current-step
correctness, leading to inaccurate step verification. Furthermore, we identify
potential biases in conventional Best-of-N (BoN) evaluation strategies for
PRMs: (1) The unreliable policy models generate responses with correct answers
but flawed processes, leading to a misalignment between the evaluation criteria
of BoN and the PRM objectives of process verification. (2) The tolerance of
PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a
significant proportion of minimum scores concentrated on the final answer
steps, revealing the shift from process to outcome-based assessment in BoN
Optimized PRMs. To address these challenges, we develop a consensus filtering
mechanism that effectively integrates MC estimation with LLM-as-a-judge and
advocates a more comprehensive evaluation framework that combines
response-level and step-level metrics. Based on the mechanisms, we
significantly improve both model performance and data efficiency in the BoN
evaluation and the step-wise error identification task. Finally, we release a
new state-of-the-art PRM that outperforms existing open-source alternatives and
provides practical guidelines for future research in building process
supervision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning for Individual Optimal Policy from Heterogeneous
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Miao, Babak Shahbaba, Annie Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing The Power of <span class="highlight-title">Pre-Train</span>ed Language Models for Irregularly
  Sampled Time Series <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Zhang, Chenlong Yin, Hao Liu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Language Models (PLMs), such as ChatGPT, have significantly
advanced the field of natural language processing. This progress has inspired a
series of innovative studies that explore the adaptation of PLMs to time series
analysis, intending to create a unified foundation model that addresses various
time series analytical tasks. However, these efforts predominantly focus on
Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by
Irregularly Sampled Time Series (ISTS), which are characterized by uneven
sampling intervals and prevalent missing data. To bridge this gap, this work
takes the first step in exploring the potential of PLMs for ISTS analysis. We
begin by investigating the effect of various methods for representing ISTS,
aiming to maximize the efficacy of PLMs in the analysis. Furthermore, we
propose a unified PLM-based framework, named ISTS-PLM, to address diverse ISTS
analytical tasks. It integrates novel time-aware and variable-aware PLMs
tailored to tackle the intractable intra- and inter-time series modeling in
ISTS. Finally, extensive experiments on a comprehensive benchmark demonstrate
that the ISTS-PLM, utilizing a structured and effective series-based
representation for ISTS, consistently achieves state-of-the-art performance
across various analytical tasks, such as classification, interpolation,
extrapolation, few-shot and zero-shot learning scenarios, spanning scientific
domains like healthcare, biomechanics, and climate science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Wave To Explain Them All: A Unifying Perspective On Feature
  Attribution <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Kasmi, Amandine Brunetto, Thomas Fel, Jayneel Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature attribution methods aim to improve the transparency of deep neural
networks by identifying the input features that influence a model's decision.
Pixel-based heatmaps have become the standard for attributing features to
high-dimensional inputs, such as images, audio representations, and volumes.
While intuitive and convenient, these pixel-based attributions fail to capture
the underlying structure of the data. Moreover, the choice of domain for
computing attributions has often been overlooked. This work demonstrates that
the wavelet domain allows for informative and meaningful attributions. It
handles any input dimension and offers a unified approach to feature
attribution. Our method, the Wavelet Attribution Method (WAM), leverages the
spatial and scale-localized properties of wavelet coefficients to provide
explanations that capture both the where and what of a model's decision-making
process. We show that WAM quantitatively matches or outperforms existing
gradient-based methods across multiple modalities, including audio, images, and
volumes. Additionally, we discuss how WAM bridges attribution with broader
aspects of model robustness and transparency. Project page:
https://gabrielkasmi.github.io/wam/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abnormal component analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Valla, Pavlo Mozharovskyi, Florence d'Alché-Buc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At the crossway of machine learning and data analysis, anomaly detection aims
at identifying observations that exhibit abnormal behaviour. Be it measurement
errors, disease development, severe weather, production quality default(s)
(items) or failed equipment, financial frauds or crisis events, their on-time
identification and isolation constitute an important task in almost any area of
industry and science. While a substantial body of literature is devoted to
detection of anomalies, little attention is payed to their explanation. This is
the case mostly due to intrinsically non-supervised nature of the task and
non-robustness of the exploratory methods like principal component analysis
(PCA).
  We introduce a new statistical tool dedicated for exploratory analysis of
abnormal observations using data depth as a score. Abnormal component analysis
(shortly ACA) is a method that searches a low-dimensional data representation
that best visualises and explains anomalies. This low-dimensional
representation not only allows to distinguish groups of anomalies better than
the methods of the state of the art, but as well provides a -- linear in
variables and thus easily interpretable -- explanation for anomalies. In a
comparative simulation and real-data study, ACA also proves advantageous for
anomaly analysis with respect to methods present in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chaotic Hedging with Iterated Integrals and Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10166v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10166v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Neufeld, Philipp Schmocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we derive an $L^p$-chaos expansion based on iterated
Stratonovich integrals with respect to a given exponentially integrable
continuous semimartingale. By omitting the orthogonality of the expansion, we
show that every $p$-integrable functional, $p \in [1,\infty)$, can be
approximated by a finite sum of iterated Stratonovich integrals. Using
(possibly random) neural networks as integrands, we therefere obtain universal
approximation results for $p$-integrable financial derivatives in the
$L^p$-sense. Moreover, we can approximately solve the $L^p$-hedging problem
(coinciding for $p = 2$ with the quadratic hedging problem), where the
approximating hedging strategy can be computed in closed form within short
runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Intermediate Representations in
  Compilers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intermediate Representations (IRs) play a critical role in compiler design
and program analysis, yet their comprehension by Large Language Models (LLMs)
remains underexplored. In this paper, we present an explorative empirical study
evaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,
DeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,
we assess model performance across four core tasks: control flow graph
reconstruction, decompilation, code summarization, and execution reasoning.
While LLMs exhibit competence in parsing IR syntax and identifying high-level
structures, they consistently struggle with instruction-level reasoning,
especially in control flow reasoning, loop handling, and dynamic execution.
Common failure modes include misinterpreting branching instructions, omitting
critical operations, and relying on heuristic reasoning rather than precise
instruction-level logic. Our findings highlight the need for IR-specific
enhancements in LLM design. We recommend fine-tuning on structured IR datasets
and integrating control-flow-sensitive architectures to improve model
effectiveness. All experimental data and source code are publicly available at
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes
  and Sample Compression Hypernetworks <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13577v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13577v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Leblanc, Mathieu Bazinet, Nathaniel D'Amours, Alexandre Drouin, Pascal Germain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both PAC-Bayesian and Sample Compress learning frameworks are instrumental
for deriving tight (non-vacuous) generalization bounds for neural networks. We
leverage these results in a meta-learning scheme, relying on a hypernetwork
that outputs the parameters of a downstream predictor from a dataset input. The
originality of our approach lies in the investigated hypernetwork architectures
that encode the dataset before decoding the parameters: (1) a PAC-Bayesian
encoder that expresses a posterior distribution over a latent space, (2) a
Sample Compress encoder that selects a small sample of the dataset input along
with a message from a discrete set, and (3) a hybrid between both approaches
motivated by a new Sample Compress theorem handling continuous messages. The
latter theorem exploits the pivotal information transiting at the
encoder-decoder junction in order to compute generalization guarantees for each
downstream predictor obtained by our meta-learning scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025 (also at the NeurIPS 2024 Workshop on
  Compression in Machine Learning)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-Oriented Time-Series Forecasting: Foundation Framework Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca-Andrei Fechete, Mohamed Sana, Fadhel Ayed, Nicola Piovesan, Wenjie Li, Antonio De Domenico, Tareq Si Salem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional time-series forecasting often focuses only on minimizing
prediction errors, ignoring the specific requirements of real-world
applications that employ them. This paper presents a new training methodology,
which allows a forecasting model to dynamically adjust its focus based on the
importance of forecast ranges specified by the end application. Unlike previous
methods that fix these ranges beforehand, our training approach breaks down
predictions over the entire signal range into smaller segments, which are then
dynamically weighted and combined to produce accurate forecasts within a region
of interest. We tested our method on standard datasets, including a new
wireless communication dataset, and found that not only it improves prediction
accuracy but also enhances the performance of end application employing the
forecasting model. This research provides a basis for creating forecasting
systems that better connect prediction and decision-making in various practical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Robustness of Deep Reinforcement Learning for Autonomous
  Surface Vehicle Control in Field Tests <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis F. W. Batista, Stéphanie Aravecchia, Seth Hutchinson, Cédric Pradalier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 2025 IEEE ICRA Workshop on Field Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection-Driven Object Count Optimization for Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oz Zafar, Yuval Cohen, Lior Wolf, Idan Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately controlling object count in text-to-image generation remains a key
challenge. Supervised methods often fail, as training data rarely covers all
count variations. Methods that manipulate the denoising process to add or
remove objects can help; however, they still require labeled data, limit
robustness and image quality, and rely on a slow, iterative process.
  Pre-trained differentiable counting models that rely on soft object density
summation exist and could steer generation, but employing them presents three
main challenges: (i) they are pre-trained on clean images, making them less
effective during denoising steps that operate on noisy inputs; (ii) they are
not robust to viewpoint changes; and (iii) optimization is computationally
expensive, requiring repeated model evaluations per image.
  We propose a new framework that uses pre-trained object counting techniques
and object detectors to guide generation. First, we optimize a counting token
using an outer-loop loss computed on fully generated images. Second, we
introduce a detection-driven scaling term that corrects errors caused by
viewpoint and proportion shifts, among other factors, without requiring
backpropagation through the detection model. Third, we show that the optimized
parameters can be reused for new prompts, removing the need for repeated
optimization. Our method provides efficiency through token reuse, flexibility
via compatibility with various detectors, and accuracy with improved counting
across diverse object categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janik Kreit, Dominic Schuh, Kim A. Nicoli, Lena Funcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have recently garnered significant attention across
various fields, from physics to chemistry, where sampling from unnormalized
Boltzmann-like distributions represents a fundamental challenge. In particular,
autoregressive models and normalizing flows have become prominent due to their
appealing ability to yield closed-form probability densities. Moreover, it is
well-established that incorporating prior knowledge - such as symmetries - into
deep neural networks can substantially improve training performances. In this
context, recent advances have focused on developing symmetry-equivariant
generative models, achieving remarkable results. Building upon these
foundations, this paper introduces Symmetry-Enforcing Stochastic Modulation
(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the
incorporation of inductive biases (e.g., symmetries) into normalizing flows
through a novel technique called stochastic modulation. This approach enhances
the flexibility of the generative model, allowing to effectively learn a
variety of exact and broken symmetries. Our numerical experiments benchmark
SESaMo in different scenarios, including an 8-Gaussian mixture model and
physically relevant field theories, such as the $\phi^4$ theory and the Hubbard
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreak Attack Initializations as Extractors of Compliance Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Levi, Rom Himelstein, Yaniv Nemcovsky, Avi Mendelson, Chaim Baskin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-aligned LLMs respond to prompts with either compliance or refusal,
each corresponding to distinct directions in the model's activation space.
Recent works show that initializing attacks via self-transfer from other
prompts significantly enhances their performance. However, the underlying
mechanisms of these initializations remain unclear, and attacks utilize
arbitrary or hand-picked initializations. This work presents that each
gradient-based jailbreak attack and subsequent initialization gradually
converge to a single compliance direction that suppresses refusal, thereby
enabling an efficient transition from refusal to compliance. Based on this
insight, we propose CRI, an initialization framework that aims to project
unseen prompts further along compliance directions. We demonstrate our approach
on multiple attacks, models, and datasets, achieving an increased attack
success rate (ASR) and reduced computational overhead, highlighting the
fragility of safety-aligned LLMs. A reference implementation is available at:
https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Rock Pushability on Rough Planetary Terrain <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuba Girgin, Emre Girgin, Cagri Kilic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper presented at the Workshop on Field Robotics, ICRA 2025,
  Atlanta, GA, United States</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural Alignment Improves Graph Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18334v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18334v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans Hao-Hsun Hsu, Shikun Liu, Han Zhao, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based learning excels at capturing interaction patterns in diverse
domains like recommendation, fraud detection, and particle physics. However,
its performance often degrades under distribution shifts, especially those
altering network connectivity. Current methods to address these shifts
typically require retraining with the source dataset, which is often infeasible
due to computational or privacy limitations. We introduce Test-Time Structural
Alignment (TSA), a novel algorithm for Graph Test-Time Adaptation (GTTA) that
aligns graph structures during inference without accessing the source data.
Grounded in a theoretical understanding of graph data distribution shifts, TSA
employs three synergistic strategies: uncertainty-aware neighborhood weighting
to accommodate neighbor label distribution shifts, adaptive balancing of
self-node and aggregated neighborhood representations based on their
signal-to-noise ratio, and decision boundary refinement to correct residual
label and feature shifts. Extensive experiments on synthetic and real-world
datasets demonstrate TSA's consistent outperformance of both non-graph TTA
methods and state-of-the-art GTTA baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impossibility of Fair LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of general-purpose artificial intelligence (AI) systems,
particularly large language models (LLMs), has raised pressing moral questions
about how to reduce bias and ensure fairness at scale. Researchers have
documented a sort of "bias" in the significant correlations between
demographics (e.g., race, gender) in LLM prompts and responses, but it remains
unclear how LLM fairness could be evaluated with more rigorous definitions,
such as group fairness or fair representations. We analyze a variety of
technical fairness frameworks and find inherent challenges in each that make
the development of a fair LLM intractable. We show that each framework either
does not logically extend to the general-purpose AI context or is infeasible in
practice, primarily due to the large amounts of unstructured training data and
the many potential combinations of human populations, use cases, and sensitive
attributes. These inherent challenges would persist for general-purpose AI,
including LLMs, even if empirical challenges, such as limited participatory
input and limited measurement methods, were overcome. Nonetheless, fairness
will remain an important type of model evaluation, and there are still
promising research directions, particularly the development of standards for
the responsibility of LLM developers, context-specific evaluations, and methods
of iterative, participatory, and AI-assisted evaluation that could scale
fairness across the diverse contexts of modern human-AI interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoformulation of Mathematical Optimization Models Using LLMs <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolás Astorga, Tennison Liu, Yuanzhang Xiao, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical optimization is fundamental to decision-making across diverse
domains, from operations research to healthcare. Yet, translating real-world
problems into optimization models remains a difficult task, often demanding
specialized expertise. This paper approaches the problem of
$\textit{autoformulation}$: the automated creation of solver-ready optimization
models from natural language problem descriptions. We identify three core
challenges of autoformulation: $\textit{(1)}$ the vast, problem-dependent
hypothesis space, $\textit{(2)}$ efficient and diverse exploration of this
space under uncertainty, and $\textit{(3)}$ evaluation of formulation
correctness against problem description. To address these challenges, we
present a novel method leveraging $\textit{Large Language Models}$ (LLMs) with
$\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of
optimization modeling to generate and systematically explore possible
formulations. To enhance search efficiency, we introduce symbolic pruning to
eliminate trivially equivalent search paths (branches), and employ LLM-based
evaluation of partial formulations to guide search. Empirical analysis on
linear and mixed-integer programming benchmarks demonstrates our method's
effectiveness, with significant performance gains from both LLM-based value
estimation and symbolic pruning techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>*Astorga and Liu contributed equally. Published as a conference paper
  at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GoRA: Gradient-driven Adaptive Low Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan He, Peng Ye, Yuchen Ren, Yuan Yuan, Luyang Zhou, Shucun Ju, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning
large language models (LLMs), with its effectiveness influenced by two key
factors: rank selection and weight initialization. While numerous LoRA variants
have been proposed to improve performance by addressing one of these aspects,
they often compromise usability or computational efficiency. In this paper, we
analyze and identify the core limitations of existing approaches and propose a
novel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that
simultaneously adapts both the rank and initialization strategy within a
unified framework. GoRA leverages gradient information during training to
dynamically assign optimal ranks and initialize low-rank adapter weights in an
adaptive manner. To our knowledge, GoRA is the first method that not only
addresses the limitations of prior approaches -- which often focus on either
rank selection or initialization in isolation -- but also unifies both aspects
within a single framework, enabling more effective and efficient adaptation.
Extensive experiments across various architectures and modalities show that
GoRA consistently outperforms existing LoRA-based methods while preserving the
efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for
mathematical reasoning, GoRA achieves a 5.13-point improvement over standard
LoRA and even outperforms full fine-tuning by 2.05 points under high-rank
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini Diffuser: Fast Multi-task Diffusion Policy Training Using Two-level
  Mini-batches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Hu, Pinhao Song, Kehan Wen, Renaud Detry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method that reduces, by an order of magnitude, the time and
memory needed to train multi-task vision-language robotic diffusion policies.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: In image
generation, the target is high-dimensional. By contrast, in action generation,
the dimensionality of the target is comparatively small, and only the image
condition is high-dimensional. Our approach, \emph{Mini Diffuser}, exploits
this asymmetry by introducing \emph{two-level minibatching}, which pairs
multiple noised action samples with each vision-language condition, instead of
the conventional one-to-one sampling strategy. To support this batching scheme,
we introduce architectural adaptations to the diffusion transformer that
prevent information leakage across samples while maintaining full conditioning
access. In RLBench simulations, Mini-Diffuser achieves 95\% of the performance
of state-of-the-art multi-task diffusion policies, while using only 5\% of the
training time and 7\% of the memory. Real-world experiments further validate
that Mini-Diffuser preserves the key strengths of diffusion-based policies,
including the ability to model multimodal action distributions and produce
behavior conditioned on diverse perceptual inputs. Code available at
mini-diffuse-actor.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neurosymbolic Association Rule Mining from Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erkan Karabulut, Paul Groth, Victoria Degeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association Rule Mining (ARM) is the task of mining patterns among data
features in the form of logical rules, with applications across a myriad of
domains. However, high-dimensional datasets often result in an excessive number
of rules, increasing execution time and negatively impacting downstream task
performance. Managing this rule explosion remains a central challenge in ARM
research. To address this, we introduce Aerial+, a novel neurosymbolic ARM
method. Aerial+ leverages an under-complete autoencoder to create a neural
representation of the data, capturing associations between features. It
extracts rules from this neural representation by exploiting the model's
reconstruction mechanism. Extensive evaluations on five datasets against seven
baselines demonstrate that Aerial+ achieves state-of-the-art results by
learning more concise, high-quality rule sets with full data coverage. When
integrated into rule-based interpretable machine learning models, Aerial+
significantly reduces execution time while maintaining or improving accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Compressive-Expressive Communication Framework for Compositional
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19182v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19182v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Elberg, Felipe del Rio, Mircea Petrache, Denis Parra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional generalization--the ability to interpret novel combinations of
familiar elements--is a hallmark of human cognition and language. Despite
recent advances, deep neural networks still struggle to acquire this property
reliably. In this work, we introduce CELEBI (Compressive-Expressive Language
Emergence through a discrete Bottleneck and Iterated learning), a novel
self-supervised framework for inducing compositionality in learned
representations from pre-trained models, through a reconstruction-based
communication game between a sender and a receiver. Building on theories of
language emergence, we integrate three mechanisms that jointly promote
compressibility, expressivity, and efficiency in the emergent language. First,
interactive decoding incentivizes intermediate reasoning by requiring the
receiver to produce partial reconstructions after each symbol. Second, a
reconstruction-based imitation phase, inspired by iterated learning, trains
successive generations of agents to imitate reconstructions rather than
messages, enforcing a tighter communication bottleneck. Third, pairwise
distance maximization regularizes message diversity by encouraging high
distances between messages, with formal links to entropy maximization. Our
method significantly improves both the efficiency and compositionality of the
learned messages on the Shapes3D and MPI3D datasets, surpassing prior discrete
communication frameworks in both reconstruction accuracy and topographic
similarity. This work provides new theoretical and empirical evidence for the
emergence of structured, generalizable communication protocols from
simplicity-based inductive biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Anytime Reasoning via Budget Relative Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Selective Homomorphic Encryption Approach for Faster
  Privacy-Preserving Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulkadir Korkmaz, Praveen Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has come forward as a critical approach for
privacy-preserving machine learning in healthcare, allowing collaborative model
training across decentralized medical datasets without exchanging clients'
data. However, current security implementations for these systems face a
fundamental trade-off: rigorous cryptographic protections like fully
homomorphic encryption (FHE) impose prohibitive computational overhead, while
lightweight alternatives risk vulnerable data leakage through model updates. To
address this issue, we present FAS (Fast and Secure Federated Learning), a
novel approach that strategically combines selective homomorphic encryption,
differential privacy, and bitwise scrambling to achieve robust security without
compromising practical usability. Our approach eliminates the need for model
pretraining phases while dynamically protecting high-risk model parameters
through layered encryption and obfuscation. We implemented FAS using the Flower
framework and evaluated it on a cluster of eleven physical machines. Our
approach was up to 90\% faster than applying FHE on the model weights. In
addition, we eliminated the computational overhead that is required by
competitors such as FedML-HE and MaskCrypt. Our approach was up to 1.5$\times$
faster than the competitors while achieving comparable security results.
  Experimental evaluations on medical imaging datasets confirm that FAS
maintains similar security results to conventional FHE against gradient
inversion attacks while preserving diagnostic model accuracy. These results
position FAS as a practical solution for latency-sensitive healthcare
applications where both privacy preservation and computational efficiency are
requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Multi-Agent Offline Reinforcement Learning and the Role of
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Zamboni, Enrico Brunetti, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) focuses on learning policies solely from
a batch of previously collected data. offering the potential to leverage such
datasets effectively without the need for costly or risky active exploration.
While recent advances in Offline Multi-Agent RL (MARL) have shown promise, most
existing methods either rely on large datasets jointly collected by all agents
or agent-specific datasets collected independently. The former approach ensures
strong performance but raises scalability concerns, while the latter emphasizes
scalability at the expense of performance guarantees. In this work, we propose
a novel scalable routine for both dataset collection and offline learning.
Agents first collect diverse datasets coherently with a pre-specified
information-sharing network and subsequently learn coherent localized policies
without requiring either full observability or falling back to complete
decentralization. We theoretically demonstrate that this structured approach
allows a multi-agent extension of the seminal Fitted Q-Iteration (FQI)
algorithm to globally converge, in high probability, to near-optimal policies.
The convergence is subject to error terms that depend on the informativeness of
the shared information. Furthermore, we show how this approach allows to bound
the inherent error of the supervised-learning phase of FQI with the mutual
information between shared and unshared information. Our algorithm, SCAlable
Multi-agent FQI (SCAM-FQI), is then evaluated on a distributed decision-making
problem. The empirical results align with our theoretical findings, supporting
the effectiveness of SCAM-FQI in achieving a balance between scalability and
policy performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bottlenecked <span class="highlight-title">Transformer</span>s: Periodic KV Cache Abstraction for Generalised
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Oomerjee, Zafeirios Fountas, Zhongwei Yu, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (KV cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the KV cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven pruning mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing KV-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A unified weighting framework for evaluating nearest neighbour
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Urs Lenz, Henri Bollaert, Chris Cornelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first comprehensive and large-scale evaluation of classical
(NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification. We
standardise existing proposals for nearest neighbour weighting with kernel
functions, applied to the distance values and/or ranks of the nearest
neighbours of a test instance. In particular, we show that the theoretically
optimal Samworth weights converge to a kernel. Kernel functions are closely
related to fuzzy negation operators, and we propose a new kernel based on Yager
negation. We also consider various distance and scaling measures, which we show
can be related to each other. Through a systematic series of experiments on 85
real-life classification datasets, we find that NN, FNN and FRNN all perform
best with Boscovich distance, and that NN and FRNN perform best with a
combination of Samworth rank- and distance-weights and scaling by the mean
absolute deviation around the median ($r_1$), the standard deviation ($r_2$) or
the semi-interquartile range ($r_{\infty}^*$), while FNN performs best with
only Samworth distance-weights and $r_1$- or $r_2$-scaling. However, NN
achieves comparable performance with Yager-$\frac{1}{2}$ distance-weights,
which are simpler to implement than a combination of Samworth distance- and
rank-weights. Finally, FRNN generally outperforms NN, which in turn performs
systematically better than FNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09117v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09117v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) unlearning has demonstrated its essential role in
removing privacy and copyright-related responses, crucial for their legal and
safe applications. However, the pursuit of complete unlearning often comes with
substantial costs due to its compromises in their general functionality,
leading to a notorious trade-off between unlearning and retention. It motivates
this paper to explore enhanced unlearning schemes that can mitigate this
trade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an
improved framework that regulates the directions of gradient updates during the
unlearning procedure such that their side impacts on other, unrelated responses
can be minimized. GRU is easy and general to implement, demonstrating practical
effectiveness across a variety of well-established unlearning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Expressive Power of Looped <span class="highlight-title">Transformer</span>s: Theoretical Analysis and
  Enhancement via Timestep Encoding <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01405v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01405v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xu, Issei Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Looped Transformers provide advantages in parameter efficiency, computational
capabilities, and generalization for reasoning tasks. However, their expressive
power regarding function approximation remains underexplored. In this paper, we
establish the approximation rate of Looped Transformers by defining the modulus
of continuity for sequence-to-sequence functions. This reveals a limitation
specific to the looped architecture. That is, the analysis prompts the
incorporation of scaling parameters for each loop, conditioned on timestep
encoding. Experiments validate the theoretical results, showing that increasing
the number of loops enhances performance, with further gains achieved through
the timestep encoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stein Variational Evolution Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius V. Braun, Robert T. Lange, Marc Toussaint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stein Variational Gradient Descent (SVGD) is a highly efficient method to
sample from an unnormalized probability distribution. However, the SVGD update
relies on gradients of the log-density, which may not always be available.
Existing gradient-free versions of SVGD make use of simple Monte Carlo
approximations or gradients from surrogate distributions, both with
limitations. To improve gradient-free Stein variational inference, we combine
SVGD steps with evolution strategy (ES) updates. Our results demonstrate that
the resulting algorithm generates high-quality samples from unnormalized target
densities without requiring gradient information. Compared to prior
gradient-free SVGD methods, we find that the integration of the ES update in
SVGD significantly improves the performance on multiple challenging benchmark
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A new architecture of high-order deep neural networks that learn
  martingales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syoiti Ninomiya, Yuming Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the
  Limits of Embedding Space Capacity <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A range of recent works addresses the problem of compression of sequence of
tokens into a shorter sequence of real-valued vectors to be used as inputs
instead of token embeddings or key-value cache. These approaches are focused on
reduction of the amount of compute in existing language models rather than
minimization of number of bits needed to store text. Despite relying on
powerful models as encoders, the maximum attainable lossless compression ratio
is typically not higher than x10. This fact is highly intriguing because, in
theory, the maximum information capacity of large real-valued vectors is far
beyond the presented rates even for 16-bit precision and a modest vector size.
In this work, we explore the limits of compression by replacing the encoder
with a per-sample optimization procedure. We show that vectors with compression
ratios up to x1500 exist, which highlights two orders of magnitude gap between
existing and practically attainable solutions. Furthermore, we empirically show
that the compression limits are determined not by the length of the input but
by the amount of uncertainty to be reduced, namely, the cross-entropy loss on
this sequence without any conditioning. The obtained limits highlight the
substantial gap between the theoretical capacity of input embeddings and their
practical utilization, suggesting significant room for optimization in model
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Masked Autoencoders Also Listen to Birds? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Rauch, René Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders (MAEs) have shown competitive results in audio
classification by learning rich semantic representations through an efficient
self-supervised reconstruction task. However, general-purpose models fail to
generalize well when applied directly to fine-grained audio domains.
Specifically, bird-sound classification requires distinguishing subtle
inter-species differences and managing high intra-species acoustic variability,
thereby revealing the performance limitations of general-domain Audio-MAE
models. This work demonstrates that bridging this domain gap requires more than
domain-specific pretraining data; adapting the entire training pipeline is
crucial. We systematically revisit and adapt the pretraining recipe,
fine-tuning methods, and frozen feature utilization to bird sounds using
BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our
resulting Bird-MAE achieves new state-of-the-art results in BirdSet's
multi-label classification benchmark. Additionally, we introduce the
parameter-efficient prototypical probing, enhancing the utility of frozen MAE
representations and closely approaching fine-tuning performance in low-resource
settings. Bird-MAE's prototypical probes outperform linear probing by up to
37%$_\text{p}$ in MAP and narrow the gap to fine-tuning to approximately
3.3%$_\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also
demonstrates robust few-shot capabilities with prototypical probing in our
newly established few-shot benchmark on BirdSet, highlighting the potential of
tailored self-supervised learning pipelines for fine-grained audio domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review @TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monotonic anomaly detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Urs Lenz, Matthijs van Leeuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised anomaly detection is based on the principle that potential
anomalies are those records that look different from normal training data.
However, in some cases we are specifically interested in anomalies that
correspond to high attribute values (or low, but not both). We present two
asymmetrical distance measures that take this monotonicity into account: ramp
distance and signed distance. Through experiments on synthetic and real-life
datasets, we show that ramp distance increases anomaly detection performance
over the traditional absolute distance. While signed distance also performs
well on synthetic data, it performs substantially poorer on real-life datasets.
We argue that this is a consequence of the fact that when using signed
distance, low values of certain attributes automatically compensate for high
values of other attributes, such that anomaly detection is reduced to counting
the total attribute value sum, which is too simplistic in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-based Training Methods for Scalable Neural Implicit Sampler <span class="chip">NeurIPS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo, Boya Zhang, Zhihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently sampling from un-normalized target distributions is a fundamental
problem in scientific computing and machine learning. Traditional approaches
such as Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased
samples from such distributions but suffer from computational inefficiency,
particularly when dealing with high-dimensional targets, as they require
numerous iterations to generate a batch of samples. In this paper, we introduce
an efficient and scalable neural implicit sampler that overcomes these
limitations. The implicit sampler can generate large batches of samples with
low computational costs by leveraging a neural transformation that directly
maps easily sampled latent vectors to target samples without the need for
iterative procedures. To train the neural implicit samplers, we introduce two
novel methods: the KL training method and the Fisher training method. The
former method minimizes the Kullback-Leibler divergence, while the latter
minimizes the Fisher divergence between the sampler and the target
distributions. By employing the two training methods, we effectively optimize
the neural implicit samplers to learn and generate from the desired target
distribution. To demonstrate the effectiveness, efficiency, and scalability of
our proposed samplers, we evaluate them on three sampling benchmarks with
different scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision: The paper was accepted by NeurIPS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-Instruct++: Training One-step Text-to-image Generator Model to
  Align with Human Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-step text-to-image generator models offer advantages such as swift
inference efficiency, flexible architectures, and state-of-the-art generation
performance. In this paper, we study the problem of aligning one-step generator
models with human preferences for the first time. Inspired by the success of
reinforcement learning using human feedback (RLHF), we formulate the alignment
problem as maximizing expected human reward functions while adding an Integral
Kullback-Leibler divergence term to prevent the generator from diverging. By
overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the
first, fast-converging and image data-free human preference alignment method
for one-step text-to-image generators. We also introduce novel theoretical
insights, showing that using CFG for diffusion distillation is secretly doing
RLHF with DI++. Such an interesting finding brings understanding and potential
contributions to future research involving CFG. In the experiment sections, we
align both UNet-based and DiT-based one-step generators using DI++, which use
the Stable Diffusion 1.5 and the PixelArt-$\alpha$ as the reference diffusion
processes. The resulting DiT-based one-step text-to-image model achieves a
strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO
validation prompt dataset. It also achieves a leading Human preference Score
(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable
Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\alpha$. Both theoretical
contributions and empirical evidence indicate that DI++ is a strong
human-preference alignment approach for one-step text-to-image models. The
homepage of the paper is https://github.com/pkulwj1994/diff_instruct_pp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision: The paper was accepted by Transactions of Machine Learning
  Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAID: A <span class="highlight-title">Dataset</span> for Testing the Adversarial Robustness of AI-Generated
  Image Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustness. Our findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piloting Structure-Based Drug Design via Modality-Specific Optimal
  Schedule <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyue Qiu, Yuxuan Song, Zhehuan Fan, Peidong Liu, Zhe Zhang, Mingyue Zheng, Hao Zhou, Wei-Ying Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure-Based Drug Design (SBDD) is crucial for identifying bioactive
molecules. Recent deep generative models are faced with challenges in geometric
structure modeling. A major bottleneck lies in the twisted probability path of
multi-modalities -- continuous 3D positions and discrete 2D topologies -- which
jointly determine molecular geometries. By establishing the fact that noise
schedules decide the Variational Lower Bound (VLB) for the twisted probability
path, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored
area, which optimizes VLB as a path integral for SBDD. Our model effectively
enhances molecular geometries and interaction modeling, achieving
state-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10%
improvement upon strong baselines, while maintaining high affinities and robust
intramolecular validity evaluated on held-out test set. Code is available at
https://github.com/AlgoMole/MolCRAFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Time Series Processing for <span class="highlight-title">Transformer</span>s and State-Space Models
  through Token Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17951v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17951v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Götz, Marcel Kollovieh, Stephan Günnemann, Leo Schwinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in subquadratic attention mechanisms or state-space
models, processing long token sequences still imposes significant computational
requirements. Token merging has emerged as a solution to increase computational
efficiency in computer vision architectures. In this work, we perform the first
investigations of token merging in time series analysis on both transformers
and state-space models. We further introduce local merging, a domain-specific
token merging algorithm that selectively combines tokens within a local
neighborhood, achieving two major benefits: a) Local merging can adjust its
computational complexity from quadratic to linear based on the neighborhood
size to effectively scale to long sequences; b) Local merging is the first
causal merging scheme enabling token merging in transformer decoders. Further,
we identify spectral properties of the input data that reliably predict the
potential benefits of local merging without requiring evaluation on downstream
tasks. Our comprehensive empirical evaluation demonstrates that local merging
offers substantial efficiency gains with minimal impact on accuracy, achieving
up to 5400% acceleration on the recently proposed Chronos foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages in total, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weak Generative Sampler to Efficiently Sample Invariant Distribution of
  Stochastic Differential Equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Cai, Yu Cao, Yuanfei Huang, Xiang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling invariant distributions from an It\^o diffusion process presents a
significant challenge in stochastic simulation. Traditional numerical solvers
for stochastic differential equations require both a fine step size and a
lengthy simulation period, resulting in biased and correlated samples. The
current deep learning-based method solves the stationary Fokker--Planck
equation to determine the invariant probability density function in the form of
deep neural networks, but they generally do not directly address the problem of
sampling from the computed density function. In this work, we introduce a
framework that employs a weak generative sampler (WGS) to directly generate
independent and identically distributed (iid) samples induced by a
transformation map derived from the stationary Fokker--Planck equation. Our
proposed loss function is based on the weak form of the Fokker--Planck
equation, integrating normalizing flows to characterize the invariant
distribution and facilitate sample generation from a base distribution. Our
randomized test function circumvents the need for min-max optimization in the
traditional weak formulation. Our method necessitates neither the
computationally intensive calculation of the Jacobian determinant nor the
invertibility of the transformation map. A crucial component of our framework
is the adaptively chosen family of test functions in the form of Gaussian
kernel functions with centers related to the generated data samples.
Experimental results on several benchmark examples demonstrate the
effectiveness and scalability of our method, which offers both low
computational costs and excellent capability in exploring multiple metastable
states.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ David and Goliath: Small One-step Model Beats Large Diffusion with Score
  Post-training <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo, Colin Zhang, Debing Zhang, Zhengyang Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diff-Instruct* (DI*), a data-efficient post-training approach for
one-step text-to-image generative models to improve its human preferences
without requiring image data. Our method frames alignment as online
reinforcement learning from human feedback (RLHF), which optimizes the one-step
model to maximize human reward functions while being regularized to be kept
close to a reference diffusion process. Unlike traditional RLHF approaches,
which rely on the Kullback-Leibler divergence as the regularization, we
introduce a novel general score-based divergence regularization that
substantially improves performance as well as post-training stability. Although
the general score-based RLHF objective is intractable to optimize, we derive a
strictly equivalent tractable loss function in theory that can efficiently
compute its \emph{gradient} for optimizations. We introduce
\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a
resolution of $1024\times 1024$, post-trained from DMD2 w.r.t SDXL. \textbf{Our
2.6B \emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in
ImageReward, PickScore, and CLIP score on the Parti prompts benchmark while
using only 1.88\% of the inference time. This result clearly shows that with
proper post-training, the small one-step model is capable of beating huge
multi-step diffusion models. Our model is open-sourced at this link:
https://github.com/pkulwj1994/diff_instruct_star. We hope our findings can
contribute to human-centric machine learning techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision: paper accepted by the ICML2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Transformer</span>s Do Enumerative Geometry? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baran Hashemi, Roderic G. Corominas, Alessandro Giacchetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can Transformers model and learn enumerative geometry? What is a robust
procedure for using Transformers in abductive knowledge discovery within a
mathematician-machine collaboration? In this work, we introduce a
Transformer-based approach to computational enumerative geometry, specifically
targeting the computation of $\psi$-class intersection numbers on the moduli
space of curves. By reformulating the problem as a continuous optimization
task, we compute intersection numbers across a wide value range from $10^{-45}$
to $10^{45}$. To capture the recursive nature inherent in these intersection
numbers, we propose the Dynamic Range Activator (DRA), a new activation
function that enhances the Transformer's ability to model recursive patterns
and handle severe heteroscedasticity. Given precision requirements for
computing the intersections, we quantify the uncertainty of the predictions
using Conformal Prediction with a dynamic sliding window adaptive to the
partitions of equivalent number of marked points. To the best of our knowledge,
there has been no prior work on modeling recursive functions with such a
high-variance and factorial growth. Beyond simply computing intersection
numbers, we explore the enumerative "world-model" of Transformers. Our
interpretability analysis reveals that the network is implicitly modeling the
Virasoro constraints in a purely data-driven manner. Moreover, through
abductive hypothesis testing, probing, and causal inference, we uncover
evidence of an emergent internal representation of the the large-genus
asymptotic of $\psi$-class intersection numbers. These findings suggest that
the network internalizes the parameters of the asymptotic closed-form and the
polynomiality phenomenon of intersection numbers in a non-linear manner. This
opens up new possibilities in inferring asymptotic closed-form expressions
directly from limited amount of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiMo: Unlocking the Reasoning Potential of Language Model -- From
  <span class="highlight-title">Pretrain</span>ing to Posttraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        LLM-Core Xiaomi,  :, Bingquan Xia, Bowen Shen,  Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, Kai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Train One Sparse Autoencoder Across Multiple Sparsity Budgets to
  Preserve Interpretability and Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Balagansky, Yaroslav Aksenov, Daniil Laptev, Vadim Kurochkin, Gleb Gerasimov, Nikita Koryagin, Daniil Gavrilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are all models wrong? Fundamental limits in distribution-free empirical
  model falsification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel M. Müller, Yuetian Luo, Rina Foygel Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In statistics and machine learning, when we train a fitted model on available
data, we typically want to ensure that we are searching within a model class
that contains at least one accurate model -- that is, we would like to ensure
an upper bound on the model class risk (the lowest possible risk that can be
attained by any model in the class). However, it is also of interest to
establish lower bounds on the model class risk, for instance so that we can
determine whether our fitted model is at least approximately optimal within the
class, or, so that we can decide whether the model class is unsuitable for the
particular task at hand. Particularly in the setting of interpolation learning
where machine learning models are trained to reach zero error on the training
data, we might ask if, at the very least, a positive lower bound on the model
class risk is possible -- or are we unable to detect that "all models are
wrong"? In this work, we answer these questions in a distribution-free setting
by establishing a model-agnostic, fundamental hardness result for the problem
of constructing a lower bound on the best test error achievable over a model
class, and examine its implications on specific model classes such as
tree-based methods and linear regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSFM-Bench: A Comprehensive and Unified Benchmarking of Foundation
  Models for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11802v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11802v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Li, Xiangfei Qiu, Peng Chen, Yihang Wang, Hanyin Cheng, Yang Shu, Jilin Hu, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Forecasting (TSF) is key functionality in numerous fields, such
as financial investment, weather services, and energy management. Although
increasingly capable TSF methods occur, many of them require domain-specific
data collection and model training and do not generalize well when applied in
other domains. Time Series Foundation Models (TSFMs) that are pre-trained on
massive heterogeneous time series data aim to overcome these limitations. The
prospects for generalizability have spurred the development of a new generation
of TSFMs. This study proposes a benchmark, TSFM-Bench, to facilitate
comprehensive and unified evaluation of TSFMs. TSFM-Bench covers a wide range
of TSFMs, including those based on large language models and those pre-trained
on time series data. TSFM-Bench supports multiple forecasting scenarios,
including zero-shot, few-shot, and full-shot, enabling assessment across the
full range of adaptation strategies. TSFM-Bench also provides a standardized
experimental protocols for critical evaluation processes such as dataset
splitting, loading, normalization, and few-shot sampling, facilitating
consistency and fairness. We report on an extensive evaluation of TSFMs across
a diverse range of datasets spanning multiple domains and exhibiting varied
statistical characteristics. Specifically, we identify pros and cons and
inherent limitations of existing TSFMs, and we propose potential directions for
new model designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Diffusion Models for Parameterized Quantum Circuit Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Barta, Darya Martyniuk, Johannes Jung, Adrian Paschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing holds immense potential, yet its practical success depends
on multiple factors, including advances in quantum circuit design. In this
paper, we introduce a generative approach based on denoising diffusion models
(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent
diffusion model pipeline of F\"urrutter et al. [1], our model effectively
conditions the synthesis process, enabling the simultaneous generation of
circuit architectures and their continuous gate parameters. We demonstrate our
approach in synthesizing PQCs optimized for generating high-fidelity
Greenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum
machine learning (QML) classification tasks. Our results indicate a strong
generalization across varying gate sets and scaling qubit counts, highlighting
the versatility and computational efficiency of diffusion-based methods. This
work illustrates the potential of generative models as a powerful tool for
accelerating and optimizing the design of PQCs, supporting the development of
more practical and scalable quantum applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Multi-Output Gaussian Processes with Stochastic Variational
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Jiang, Sokratia Georgaka, Magnus Rattray, Mauricio A. Álvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-Output Gaussian Process is is a popular tool for modelling data
from multiple sources. A typical choice to build a covariance function for a
MOGP is the Linear Model of Coregionalization (LMC) which parametrically models
the covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises
this idea by modelling the covariance between outputs using a kernel applied to
latent variables, one per output, leading to a flexible MOGP model that allows
efficient generalization to new outputs with few data points. Computational
complexity in LV-MOGP grows linearly with the number of outputs, which makes it
unsuitable for problems with a large number of outputs. In this paper, we
propose a stochastic variational inference approach for the LV-MOGP that allows
mini-batches for both inputs and outputs, making computational complexity per
training iteration independent of the number of outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full-Parameter Continual <span class="highlight-title">Pretrain</span>ing of Gemma2: Insights into Fluency
  and Domain Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vytenis Šliogeris, Povilas Daniušis, Artūras Nakvosas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we empirically investigate the relationship between
linguistic fluency and domain knowledge in the context of continual learning
with large language models (LLMs). Specifically, we enhance the linguistic
fluency of the Gemma2 LLM for the Lithuanian language by autoregressively
pretraining its full parameter set on the first 10\% of the Lithuanian language
component of the CulturaX dataset. To prevent catastrophic forgetting of the
model's existing domain knowledge, we apply Elastic Weight Consolidation (EWC),
leveraging Fisher information estimated using data from the Massive Multitask
Language Understanding (MMLU) benchmark. In the post-training evaluations, we
assess linguistic fluency through perplexity and evaluate domain knowledge
using accuracy on a suite of language understanding benchmarks, including
ARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both
English and Lithuanian. The empirical results demonstrate that EWC not only
mitigates catastrophic forgetting by preserving the model's performance in
terms of both linguistic fluency and domain knowledge but also improves or
maintains these capabilities for the newly added Lithuanian language. These
findings highlight the potential for more efficient adaptation of
general-purpose LLMs to under-represented languages without requiring access to
the original training data. The accompanying codebase is openly accessible at
https://github.com/Neurotechnology/LLM_EWC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinySQL: A Progressive Text-to-SQL <span class="highlight-title">Dataset</span> for Mechanistic
  Interpretability Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abir Harrasse, Philip Quirke, Clement Neo, Dhruv Nathawani, Amir Abdullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanistic interpretability research faces a gap between analyzing simple
circuits in toy tasks and discovering features in large models. To bridge this
gap, we propose text-to-SQL generation as an ideal task to study, as it
combines the formal structure of toy tasks with real-world complexity. We
introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL
operations, and train models ranging from 33M to 1B parameters to establish a
comprehensive testbed for interpretability. We apply multiple complementary
interpretability techniques, including Edge Attribution Patching and Sparse
Autoencoders, to identify minimal circuits and components supporting SQL
generation. We compare circuits for different SQL subskills, evaluating their
minimality, reliability, and identifiability. Finally, we conduct a layerwise
logit lens analysis to reveal how models compose SQL queries across layers:
from intent recognition to schema resolution to structured generation. Our work
provides a robust framework for probing and comparing interpretability methods
in a structured, progressively complex setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 19 figures, 7 tables, 18 trained models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Tuning Spectral Clustering for Speaker Diarization <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Raghav, Avisek Gupta, Md Sahidullah, Swagatam Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectral clustering has proven effective in grouping speech representations
for speaker diarization tasks, although post-processing the affinity matrix
remains difficult due to the need for careful tuning before constructing the
Laplacian. In this study, we present a novel pruning algorithm to create a
sparse affinity matrix called spectral clustering on p-neighborhood retained
affinity matrix (SC-pNA). Our method improves on node-specific fixed neighbor
selection by allowing a variable number of neighbors, eliminating the need for
external tuning data as the pruning parameters are derived directly from the
affinity matrix. SC-pNA does so by identifying two clusters in every row of the
initial affinity matrix, and retains only the top p % similarity scores from
the cluster containing larger similarities. Spectral clustering is performed
subsequently, with the number of clusters determined as the maximum eigengap.
Experimental results on the challenging DIHARD-III dataset highlight the
superiority of SC-pNA, which is also computationally more efficient than
existing auto-tuning approaches. Our implementations are available at
https://github.com/nikhilraghav29/SC-pNA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the camera-ready version accepted for publication in the
  ICASSP 2025 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiplicative Dynamic Mode Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Boullé, Matthew J. Colbrook
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Koopman operators are infinite-dimensional operators that linearize nonlinear
dynamical systems, facilitating the study of their spectral properties and
enabling the prediction of the time evolution of observable quantities. Recent
methods have aimed to approximate Koopman operators while preserving key
structures. However, approximating Koopman operators typically requires a
dictionary of observables to capture the system's behavior in a
finite-dimensional subspace. The selection of these functions is often
heuristic, may result in the loss of spectral information, and can severely
complicate structure preservation. This paper introduces Multiplicative Dynamic
Mode Decomposition (MultDMD), which enforces the multiplicative structure
inherent in the Koopman operator within its finite-dimensional approximation.
Leveraging this multiplicative property, we guide the selection of observables
and define a constrained optimization problem for the matrix approximation,
which can be efficiently solved. MultDMD presents a structured approach to
finite-dimensional approximations and can more accurately reflect the spectral
properties of the Koopman operator. We elaborate on the theoretical framework
of MultDMD, detailing its formulation, optimization strategy, and convergence
properties. The efficacy of MultDMD is demonstrated through several examples,
including the nonlinear pendulum, the Lorenz system, and fluid dynamics data,
where we demonstrate its remarkable robustness to noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures. To appear in SIAM Journal on Applied Dynamical
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-granularity Knowledge Transfer for Continual Reinforcement
  Learning <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Pan, Lingfei Ren, Yihui Feng, Linbo Xiong, Wei Wei, Yonghao Li, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual reinforcement learning (CRL) empowers RL agents with the ability to
learn a sequence of tasks, accumulating knowledge learned in the past and using
the knowledge for problemsolving or future task learning. However, existing
methods often focus on transferring fine-grained knowledge across similar
tasks, which neglects the multi-granularity structure of human cognitive
control, resulting in insufficient knowledge transfer across diverse tasks. To
enhance coarse-grained knowledge transfer, we propose a novel framework called
MT-Core (as shorthand for Multi-granularity knowledge Transfer for Continual
reinforcement learning). MT-Core has a key characteristic of multi-granularity
policy learning: 1) a coarsegrained policy formulation for utilizing the
powerful reasoning ability of the large language model (LLM) to set goals, and
2) a fine-grained policy learning through RL which is oriented by the goals. We
also construct a new policy library (knowledge base) to store policies that can
be retrieved for multi-granularity knowledge transfer. Experimental results
demonstrate the superiority of the proposed MT-Core in handling diverse CRL
tasks versus popular baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the 34th International Joint Conference on Artificial Intelligence
  (IJCAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fokker-Planck to Callan-Symanzik: evolution of weight matrices under
  training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Bu, Uri Kol, Ziming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamical evolution of a neural network during training has been an
incredibly fascinating subject of study. First principal derivation of generic
evolution of variables in statistical physics systems has proved useful when
used to describe training dynamics conceptually, which in practice means
numerically solving equations such as Fokker-Planck equation. Simulating entire
networks inevitably runs into the curse of dimensionality. In this paper, we
utilize Fokker-Planck to simulate the probability density evolution of
individual weight matrices in the bottleneck layers of a simple
2-bottleneck-layered auto-encoder and compare the theoretical evolutions
against the empirical ones by examining the output data distributions. We also
derive physically relevant partial differential equations such as
Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation
we have.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning pure quantum states (almost) without regret 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josep Lumbreras, Mikhail Terekhov, Marco Tomamichel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of sample-optimal quantum state tomography with minimal
disturbance to the samples. Can we efficiently learn a precise description of a
quantum state through sequential measurements of samples while at the same time
making sure that the post-measurement state of the samples is only minimally
perturbed? Defining regret as the cumulative disturbance of all samples, the
challenge is to find a balance between the most informative sequence of
measurements on the one hand and measurements incurring minimal regret on the
other. Here we answer this question for qubit states by exhibiting a protocol
that for pure states achieves maximal precision while incurring a regret that
grows only polylogarithmically with the number of samples, a scaling that we
show to be optimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid deep convolution model for lung cancer detection with transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sugandha Saxena, S. N. Prasad, Ashwin M Polnaya, Shweta Agarwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in healthcare research have significantly enhanced our understanding
of disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung
cancer remains one of the leading causes of cancer-related mortality worldwide
due to challenges in early and accurate diagnosis. While current lung cancer
detection models show promise, there is considerable potential for further
improving the accuracy for timely intervention. To address this challenge, we
introduce a hybrid deep convolution model leveraging transfer learning, named
the Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the
precision of lung cancer detection by refining sensitivity and specificity.
This model has surpassed existing deep learning approaches through experimental
validation, achieving an accuracy of 98% and a sensitivity of 97%. By
overlaying sensitivity maps onto lung Computed Tomography (CT) scans, it
enables the visualization of regions most indicative of malignant or benign
classifications. This innovative method demonstrates exceptional performance in
distinguishing lung cancer with minimal false positives, thereby enhancing the
accuracy of medical diagnoses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors realized mistake in the model. Also some data was
  misinterpreted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reading Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charig Yang, Samiul Alam, Shakhrul Iman Siam, Michael J. Proulx, Lambert Mathias, Kiran Somasundaram, Luis Pesqueira, James Fort, Sheroze Sheriffdeen, Omkar Parkhi, Carl Ren, Mi Zhang, Yuning Chai, Richard Newcombe, Hyo Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:
  https://www.projectaria.com/datasets/reading-in-the-wild/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Graph Contrastive Learning for Gene Regulatory Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sho Oshima, Yuji Okamoto, Taisei Tosaki, Ryosuke Kojima, Yasushi Okuno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is effective for obtaining a meaningful latent
space utilizing the structure of graph data and is widely applied, including
biological networks. In particular, Graph Contrastive Learning (GCL) has
emerged as a powerful self-supervised method that relies on applying
perturbations to graphs for data augmentation. However, when applying existing
GCL methods to biological networks such as Gene Regulatory Networks (GRNs),
they overlooked meaningful biologically relevant perturbations, e.g., gene
knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive
Learning), a novel GCL method for GRNs that directly incorporates biological
perturbations derived from gene knockdown experiments as the supervision.
SupGCL mathematically extends existing GCL methods that utilize non-biological
perturbations to probabilistic models that introduce actual biological gene
perturbation utilizing gene knockdown data. Using the GRN representation
obtained by our proposed method, our aim is to improve the performance of
biological downstream tasks such as patient hazard prediction and disease
subtype classification (graph-level task), and gene function classification
(node-level task). We applied SupGCL on real GRN datasets derived from patients
with multiple types of cancer, and in all experiments SupGCL achieves better
performance than state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Desktop: XR-Driven Segmentation with Meta Quest 3 and MX Ink 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisle Faray de Paiva, Gijs Luijten, Ana Sofia Ferreira Santos, Moon Kim, Behrus Puladi, Jens Kleesiek, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging segmentation is essential in clinical settings for diagnosing
diseases, planning surgeries, and other procedures. However, manual annotation
is a cumbersome and effortful task. To mitigate these aspects, this study
implements and evaluates the usability and clinical applicability of an
extended reality (XR)-based segmentation tool for anatomical CT scans, using
the Meta Quest 3 headset and Logitech MX Ink stylus. We develop an immersive
interface enabling real-time interaction with 2D and 3D medical imaging data in
a customizable workspace designed to mitigate workflow fragmentation and
cognitive demands inherent to conventional manual segmentation tools. The
platform combines stylus-driven annotation, mirroring traditional pen-on-paper
workflows, with instant 3D volumetric rendering. A user study with a public
craniofacial CT dataset demonstrated the tool's foundational viability,
achieving a System Usability Scale (SUS) score of 66, within the expected range
for medical applications. Participants highlighted the system's intuitive
controls (scoring 4.1/5 for self-descriptiveness on ISONORM metrics) and
spatial interaction design, with qualitative feedback highlighting strengths in
hybrid 2D/3D navigation and realistic stylus ergonomics. While users identified
opportunities to enhance task-specific precision and error management, the
platform's core workflow enabled dynamic slice adjustment, reducing cognitive
load compared to desktop tools. Results position the XR-stylus paradigm as a
promising foundation for immersive segmentation tools, with iterative
refinements targeting haptic feedback calibration and workflow personalization
to advance adoption in preoperative planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truth in the Few: High-Value Data Selection for Efficient Multi-Modal
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenshen Li, Kaiyuan Deng, Lei Wang, Hao Yang, Chong Peng, Peng Yan, Fumin Shen, Heng Tao Shen, Xing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-modal large language models (MLLMs) have made significant
progress in complex reasoning tasks via reinforcement learning, it is commonly
believed that extensive training data is necessary for improving multi-modal
reasoning ability, inevitably leading to data redundancy and substantial
computational costs. However, can smaller high-value datasets match or
outperform full corpora for multi-modal reasoning in MLLMs? In this work, we
challenge this assumption through a key observation: meaningful multi-modal
reasoning is triggered by only a sparse subset of training samples, termed
cognitive samples, whereas the majority contribute marginally. Building on this
insight, we propose a novel data selection paradigm termed Reasoning Activation
Potential (RAP), which identifies cognitive samples by estimating each sample's
potential to stimulate genuine multi-modal reasoning by two complementary
estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential
outcome model principle, eliminates samples that overly rely on language priors
by comparing outputs between multi-modal and text-only inputs; 2) Attention
Confidence Estimator (ACE), which exploits token-level self-attention to
discard samples dominated by irrelevant but over-emphasized tokens in
intermediate reasoning stages. Moreover, we introduce a Difficulty-aware
Replacement Module (DRM) to substitute trivial instances with cognitively
challenging ones, thereby ensuring complexity for robust multi-modal reasoning.
Experiments on six datasets show that our RAP method consistently achieves
superior performance using only 9.3% of the training data, while reducing
computational costs by over 43%. Our code is available at
https://github.com/Leo-ssl/RAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Frequency for Single Image Super-Resolution with Learnable
  Separable Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches often enhance the performance of single-image
super-resolution (SISR) methods by incorporating auxiliary structures, such as
specialized loss functions, to indirectly boost the quality of low-resolution
images. In this paper, we propose a plug-and-play module called Learnable
Separable Kernels (LSKs), which are formally rank-one matrices designed to
directly enhance image frequency components. We begin by explaining why LSKs
are particularly suitable for SISR tasks from a frequency perspective. Baseline
methods incorporating LSKs demonstrate a significant reduction of over 60\% in
both the number of parameters and computational requirements. This reduction is
achieved through the decomposition of LSKs into orthogonal and mergeable
one-dimensional kernels. Additionally, we perform an interpretable analysis of
the feature maps generated by LSKs. Visualization results reveal the capability
of LSKs to enhance image frequency components effectively. Extensive
experiments show that incorporating LSKs not only reduces the number of
parameters and computational load but also improves overall model performance.
Moreover, these experiments demonstrate that models utilizing LSKs exhibit
superior performance, particularly as the upscaling factor increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FGAS: Fixed Decoder Network-Based Audio Steganography with Adversarial
  Perturbation Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yan, Yu Cheng, Zhaoxia Yin, Xinpeng Zhang, Shilin Wang, Tanfeng Sun, Xinghao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Artificial Intelligence Generated Content (AIGC) has
made high-fidelity generated audio widely available across the Internet,
providing diverse cover signals for covert communication. Driven by advances in
deep learning, current audio steganography schemes are mainly based on
encoding-decoding network architectures. While these methods greatly improve
the security of audio steganography, they typically require complex training
and large pre-trained models. To address the aforementioned issues, this paper
pioneers a Fixed Decoder Network-Based Audio Steganography with Adversarial
Perturbation Generation (FGAS). Adversarial perturbations carrying secret
message are embedded into the cover audio to generate stego audio. The receiver
only needs to share the structure and weights of the fixed decoder network to
accurately extract the secret message from the stego audio, this eliminates the
reliance on large pre-trained models. In FGAS, we propose an audio Adversarial
Perturbation Generation (APG) strategy and design a lightweight fixed decoder.
The fixed decoder guarantees reliable extraction of the hidden message, while
the adversarial perturbations are optimized to keep the stego audio
perceptually and statistically close to the cover audio, thereby improving
resistance to steganalysis. The experimental results show that FGAS
significantly improves the quality of stego audio, achieving an average PSNR
gain of over 10 dB compared to SOTA methods. Moreover, FGAS exhibits superior
anti-steganalysis performance under different relative payloads; under
high-capacity embedding, it achieves a classification error rate about 2%
higher, indicating stronger anti-steganalysis performance compared to current
SOTA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ David and Goliath: Small One-step Model Beats Large Diffusion with Score
  Post-training <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo, Colin Zhang, Debing Zhang, Zhengyang Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diff-Instruct* (DI*), a data-efficient post-training approach for
one-step text-to-image generative models to improve its human preferences
without requiring image data. Our method frames alignment as online
reinforcement learning from human feedback (RLHF), which optimizes the one-step
model to maximize human reward functions while being regularized to be kept
close to a reference diffusion process. Unlike traditional RLHF approaches,
which rely on the Kullback-Leibler divergence as the regularization, we
introduce a novel general score-based divergence regularization that
substantially improves performance as well as post-training stability. Although
the general score-based RLHF objective is intractable to optimize, we derive a
strictly equivalent tractable loss function in theory that can efficiently
compute its \emph{gradient} for optimizations. We introduce
\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a
resolution of $1024\times 1024$, post-trained from DMD2 w.r.t SDXL. \textbf{Our
2.6B \emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in
ImageReward, PickScore, and CLIP score on the Parti prompts benchmark while
using only 1.88\% of the inference time. This result clearly shows that with
proper post-training, the small one-step model is capable of beating huge
multi-step diffusion models. Our model is open-sourced at this link:
https://github.com/pkulwj1994/diff_instruct_star. We hope our findings can
contribute to human-centric machine learning techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision: paper accepted by the ICML2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sonic: Shifting Focus to Global Audio Perception in Portrait Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, Qinglin Lu, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of talking face generation mainly explores the intricacies of
synchronizing facial movements and crafting visually appealing,
temporally-coherent animations. However, due to the limited exploration of
global audio perception, current approaches predominantly employ auxiliary
visual and spatial knowledge to stabilize the movements, which often results in
the deterioration of the naturalness and temporal inconsistencies.Considering
the essence of audio-driven animation, the audio signal serves as the ideal and
unique priors to adjust facial expressions and lip movements, without resorting
to interference of any visual signals. Based on this motivation, we propose a
novel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of
global audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,
we disentangle it into intra- and inter-clip audio perception and collaborate
with both aspects to enhance overall perception.For the intra-clip audio
perception, 1). \textbf{Context-enhanced audio learning}, in which long-range
intra-clip temporal audio knowledge is extracted to provide facial expression
and lip motion priors implicitly expressed as the tone and speed of speech. 2).
\textbf{Motion-decoupled controller}, in which the motion of the head and
expression movement are disentangled and independently controlled by
intra-audio clips. Most importantly, for inter-clip audio perception, as a
bridge to connect the intra-clips to achieve the global perception,
\textbf{Time-aware position shift fusion}, in which the global inter-clip audio
information is considered and fused for long-audio inference via through
consecutively time-aware shifted windows. Extensive experiments demonstrate
that the novel audio-driven paradigm outperform existing SOTA methodologies in
terms of video quality, temporally consistency, lip synchronization precision,
and motion diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>refer to our main-page \url{https://jixiaozhong.github.io/Sonic/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16462v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16462v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Zhixin Li, Zhi Liu, Pengyuan Zhou, Richang Hong, Qiyue Li, Han Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with
  Minimal Multimodal Speech Tokens <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in
noisy environments by combining auditory and visual information. However,
recent Large Language Model (LLM) based AVSR systems incur high computational
costs due to the high temporal resolution of audio-visual speech processed by
LLMs. In this work, we introduce an efficient multimodal speech LLM framework
that minimizes token length while preserving essential linguistic content. Our
approach employs an early AV-fusion module for streamlined feature integration,
an audio-visual speech Q-Former that dynamically allocates tokens based on
input duration, and a refined query allocation strategy with a speech rate
predictor to adjust token allocation according to speaking speed of each audio
sample. Extensive experiments on the LRS3 dataset show that our method achieves
state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens
per second. Moreover, our approach not only reduces token usage by 86% compared
to the previous multimodal speech LLM framework, but also improves
computational efficiency by reducing FLOPs by 35.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of ACL 2025. The code and models are available
  https://github.com/JeongHun0716/MMS-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAVL: A Multilingual Audio-Video Lyrics <span class="highlight-title">Dataset</span> for Animated Song
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woohyun Cho, Youngmin Kim, Sunghyun Lee, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 8 figures, our codes and datasets are available at
  https://github.com/k1064190/MAVL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Visual Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhou, Bingxuan Li, Mohan Tang, Xiaomeng Jin, Te-Lin Wu, Kuan-Hao Huang, Heng Ji, Kai-Wei Chang, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) often struggle to recognize novel concepts, as
they rely on pre-trained knowledge and have limited ability to capture subtle
visual details. Domain-specific knowledge gaps in training also make them prone
to confusing visually similar, commonly misrepresented, or low-resource
concepts. To help LMMs better align nuanced visual features with language,
improving their ability to recognize and reason about novel or rare concepts,
we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA
extracts key contrastive textual and visual features of target concepts against
the known concepts they are misrecognized as, and then uses multimodal
generative models to produce targeted synthetic data. Automatic filtering of
extracted features and augmented images is implemented to guarantee their
quality, as verified by human annotators. We show the effectiveness and
efficiency of CoDA on low-resource concept and diverse scene recognition
datasets including INaturalist and SUN. We additionally collect NovelSpecies, a
benchmark dataset consisting of newly discovered animal species that are
guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these
three datasets show CoDA significantly improves SOTA visual data augmentation
strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains
in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-04T00:00:00Z">2025-06-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I'm Sorry Dave, I'm Afraid I Can't Return That: On YouTube Search API
  Use in Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Efstratiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  YouTube is among the most widely-used platforms worldwide, and has seen a lot
of recent academic attention. Despite its popularity and the number of studies
conducted on it, much less is understood about the way in which YouTube's Data
API, and especially the Search endpoint, operates. In this paper, we analyze
the API's behavior by running identical queries across a period of 12 weeks.
Our findings suggest that the search endpoint returns highly inconsistent
results between queries in ways that are not officially documented.
Specifically, the API seems to randomize returned videos based on the relative
popularity of the respective topic during the query period, making it nearly
impossible to obtain representative historical video samples, especially during
non-peak topical periods. Our results also suggest that the API may prioritize
shorter, more popular videos, although the role of channel popularity is not as
clear. We conclude with suggested strategies for researchers using the API for
data collection, as well as future research directions on expanding the API's
use-cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Query Fairness Under Unawareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Jaenich, Alejandro Moreo, Alessandro Fabris, Graham McDonald, Andrea Esuli, Iadh Ounis, Fabrizio Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional ranking algorithms are designed to retrieve the most relevant
items for a user's query, but they often inherit biases from data that can
unfairly disadvantage vulnerable groups. Fairness in information access systems
(IAS) is typically assessed by comparing the distribution of groups in a
ranking to a target distribution, such as the overall group distribution in the
dataset. These fairness metrics depend on knowing the true group labels for
each item. However, when groups are defined by demographic or sensitive
attributes, these labels are often unknown, leading to a setting known as
"fairness under unawareness". To address this, group membership can be inferred
using machine-learned classifiers, and group prevalence is estimated by
counting the predicted labels. Unfortunately, such an estimation is known to be
unreliable under dataset shift, compromising the accuracy of fairness
evaluations. In this paper, we introduce a robust fairness estimator based on
quantification that effectively handles multiple sensitive attributes beyond
binary classifications. Our method outperforms existing baselines across
various sensitive attributes and, to the best of our knowledge, is the first to
establish a reliable protocol for measuring fairness under unawareness across
multiple queries and groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generative Adaptive Replay Continual Learning Model for Temporal
  Knowledge Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhang, Wei Chen, Youfang Lin, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning
(TKGR) methods focus on significantly reducing computational cost and
mitigating catastrophic forgetting caused by fine-tuning models with new data.
However, existing CL-based TKGR methods still face two key limitations: (1)
They usually one-sidedly reorganize individual historical facts, while
overlooking the historical context essential for accurately understanding the
historical semantics of these facts; (2) They preserve historical knowledge by
simply replaying historical facts, while ignoring the potential conflicts
between historical and emerging facts. In this paper, we propose a Deep
Generative Adaptive Replay (DGAR) method, which can generate and adaptively
replay historical entity distribution representations from the whole historical
context. To address the first challenge, historical context prompts as sampling
units are built to preserve the whole historical context information. To
overcome the second challenge, a pre-trained diffusion model is adopted to
generate the historical distribution. During the generation process, the common
features between the historical and current distributions are enhanced under
the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay
mechanism is designed to effectively integrate historical and current
distributions. Experimental results demonstrate that DGAR significantly
outperforms baselines in reasoning and mitigating forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GORACS: Group-level Optimal Transport-guided Coreset Selection for
  LLM-based Recommender Systems <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiehua Mei, Hengrui Chen, Peng Yu, Jiaqing Liang, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have shown great potential in
recommender systems, the prohibitive computational costs for fine-tuning LLMs
on entire datasets hinder their successful deployment in real-world scenarios.
To develop affordable and effective LLM-based recommender systems, we focus on
the task of coreset selection which identifies a small subset of fine-tuning
data to optimize the test loss, thereby facilitating efficient LLMs'
fine-tuning. Although there exist some intuitive solutions of subset selection,
including distribution-based and importance-based approaches, they often lead
to suboptimal performance due to the misalignment with downstream fine-tuning
objectives or weak generalization ability caused by individual-level sample
selection. To overcome these challenges, we propose GORACS, which is a novel
Group-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based
recommender systems. GORACS is designed based on two key principles for coreset
selection: 1) selecting the subsets that minimize the test loss to align with
fine-tuning objectives, and 2) enhancing model generalization through
group-level data selection. Corresponding to these two principles, GORACS has
two key components: 1) a Proxy Optimization Objective (POO) leveraging optimal
transport and gradient information to bound the intractable test loss, thus
reducing computational costs by avoiding repeated LLM retraining, and 2) a
two-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient
group-level selection. Our extensive experiments across diverse recommendation
datasets and tasks validate that GORACS significantly reduces fine-tuning costs
of LLMs while achieving superior performance over the state-of-the-art
baselines and full data training. The source code of GORACS are available at
https://github.com/Mithas-114/GORACS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Embedding Empowered Entity Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma J. Gerritse, Faegheh Hasibi, Arjen P. de Vries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we investigate methods for entity retrieval using graph
embeddings. While various methods have been proposed over the years, most
utilize a single graph embedding and entity linking approach. This hinders our
understanding of how different graph embedding and entity linking methods
impact entity retrieval. To address this gap, we investigate the effects of
three different categories of graph embedding techniques and five different
entity linking methods. We perform a reranking of entities using the distance
between the embeddings of annotated entities and the entities we wish to
rerank. We conclude that the selection of both graph embeddings and entity
linkers significantly impacts the effectiveness of entity retrieval. For graph
embeddings, methods that incorporate both graph structure and textual
descriptions of entities are the most effective. For entity linking, both
precision and recall concerning concepts are important for optimal retrieval
performance. Additionally, it is essential for the graph to encompass as many
entities as possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2005.02843</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-objective Aligned Bidword Generation Model for E-commerce Search
  Advertising <span class="chip">SIGIR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhui Liu, Chunyuan Yuan, Ming Pang, Zheng Fang, Li Yuan, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo, Jingping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRAWLDoc: A <span class="highlight-title">Dataset</span> for Robust Ranking of Bibliographic Documents <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Karl, Ansgar Scherp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publication databases rely on accurate metadata extraction from diverse web
sources, yet variations in web layouts and data formats present challenges for
metadata providers. This paper introduces CRAWLDoc, a new method for contextual
ranking of linked web documents. Starting with a publication's URL, such as a
digital object identifier, CRAWLDoc retrieves the landing page and all linked
web resources, including PDFs, ORCID profiles, and supplementary materials. It
embeds these resources, along with anchor texts and the URLs, into a unified
representation. For evaluating CRAWLDoc, we have created a new, manually
labeled dataset of 600 publications from six top publishers in computer
science. Our method CRAWLDoc demonstrates a robust and layout-independent
ranking of relevant documents across publishers and data formats. It lays the
foundation for improved metadata extraction from web documents with various
layouts and formats. Our source code and dataset can be accessed at
https://github.com/FKarl/CRAWLDoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SCOLIA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Mental Models of Generative Conversational Search and The
  Effect of Interface Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chadha Degachi, Samuel Kernan Freire, Evangelos Niforatos, Gerd Kortuem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The experience and adoption of conversational search is tied to the accuracy
and completeness of users' mental models -- their internal frameworks for
understanding and predicting system behaviour. Thus, understanding these models
can reveal areas for design interventions. Transparency is one such
intervention which can improve system interpretability and enable mental model
alignment. While past research has explored mental models of search engines,
those of generative conversational search remain underexplored, even while the
popularity of these systems soars. To address this, we conducted a study with
16 participants, who performed 4 search tasks using 4 conversational interfaces
of varying transparency levels. Our analysis revealed that most user mental
models were too abstract to support users in explaining individual search
instances. These results suggest that 1) mental models may pose a barrier to
appropriate trust in conversational search, and 2) hybrid web-conversational
search is a promising novel direction for future search interface design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">Transformer</span>s for Discriminative Recommendation via Generative
  <span class="highlight-title">Pretrain</span>ing <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen, Bing Wang, Xiaoyi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminative recommendation tasks, such as CTR (click-through rate) and CVR
(conversion rate) prediction, play critical roles in the ranking stage of
large-scale industrial recommender systems. However, training a discriminative
model encounters a significant overfitting issue induced by data sparsity.
Moreover, this overfitting issue worsens with larger models, causing them to
underperform smaller ones. To address the overfitting issue and enhance model
scalability, we propose a framework named GPSD (\textbf{G}enerative
\textbf{P}retraining for \textbf{S}calable \textbf{D}iscriminative
Recommendation), drawing inspiration from generative training, which exhibits
no evident signs of overfitting. GPSD leverages the parameters learned from a
pretrained generative model to initialize a discriminative model, and
subsequently applies a sparse parameter freezing strategy. Extensive
experiments conducted on both industrial-scale and publicly available datasets
demonstrate the superior performance of GPSD. Moreover, it delivers remarkable
improvements in online A/B tests. GPSD offers two primary advantages: 1) it
substantially narrows the generalization gap in model training, resulting in
better test performance; and 2) it leverages the scalability of Transformers,
delivering consistent performance gains as models are scaled up. Specifically,
we observe consistent performance improvements as the model dense parameters
scale from 13K to 0.3B, closely adhering to power laws. These findings pave the
way for unifying the architectures of recommendation models and language
models, enabling the direct application of techniques well-established in large
language models to recommendation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProRank: <span class="highlight-title">Prompt</span> Warmup via Reinforcement Learning for Small Language
  Models Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianming Li, Aamir Shakir, Rui Huang, Julius Lipp, Jing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking is fundamental to information retrieval and retrieval-augmented
generation, with recent Large Language Models (LLMs) significantly advancing
reranking quality. While recent advances with LLMs have significantly improved
document reranking quality, current approaches primarily rely on large-scale
LLMs (>7B parameters) through zero-shot prompting, presenting high
computational costs. Small Language Models (SLMs) offer a promising alternative
because of their efficiency, but our preliminary quantitative analysis reveals
they struggle with understanding task prompts without fine-tuning. This limits
their effectiveness for document reranking tasks. To address this issue, we
introduce a novel two-stage training approach, ProRank, for SLM-based document
reranking. First, we propose a prompt warmup stage using reinforcement learning
GRPO to steer SLMs to understand task prompts and generate more accurate
coarse-grained binary relevance scores for document reranking. Then, we
continuously fine-tune the SLMs with a fine-grained score learning stage
without introducing additional layers to further improve the reranking quality.
Comprehensive experimental results demonstrate that the proposed ProRank
consistently outperforms both the most advanced open-source and proprietary
reranking models. Notably, our lightweight ProRank-0.5B model even surpasses
the powerful 32B LLM reranking model on the BEIR benchmark, establishing that
properly trained SLMs can achieve superior document reranking performance while
maintaining computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified
  Theory and Risk Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized
  Domain Question-Answering <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) struggles with domain-specific
enterprise datasets, often isolated behind firewalls and rich in complex,
specialized terminology unseen by LLMs during pre-training. Semantic
variability across domains like medicine, networking, or law hampers RAG's
context precision, while fine-tuning solutions are costly, slow, and lack
generalization as new data emerges. Achieving zero-shot precision with
retrievers without fine-tuning still remains a key challenge. We introduce
'MetaGen Blended RAG', a novel enterprise search approach that enhances
semantic retrievers through a metadata generation pipeline and hybrid query
indexes using dense and sparse vectors. By leveraging key concepts, topics, and
acronyms, our method creates metadata-enriched semantic indexes and boosted
hybrid queries, delivering robust, scalable performance without fine-tuning. On
the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks
and even rivaling fine-tuned models on that dataset, while also excelling on
datasets like SQuAD and NQ. This approach redefines enterprise search using a
new approach to building semantic retrievers with unmatched generalization
across specialized domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Paper Submitted for NeurIPS 2025- The Thirty-Ninth Annual
  Conference on Neural Information Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MammAlps: A multi-view video behavior monitoring <span class="highlight-title">dataset</span> of wild mammals
  in the Swiss Alps <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer Sumbül, Alexander Mathis, Devis Tuia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring wildlife is essential for ecology and ethology, especially in
light of the increasing human impact on ecosystems. Camera traps have emerged
as habitat-centric sensors enabling the study of wildlife populations at scale
with minimal disturbance. However, the lack of annotated video datasets limits
the development of powerful video understanding models needed to process the
vast amount of fieldwork data collected. To advance research in wild animal
behavior monitoring we present MammAlps, a multimodal and multi-view dataset of
wildlife behavior monitoring from 9 camera-traps in the Swiss National Park.
MammAlps contains over 14 hours of video with audio, 2D segmentation maps and
8.5 hours of individual tracks densely labeled for species and behavior. Based
on 6135 single animal clips, we propose the first hierarchical and multimodal
animal behavior recognition benchmark using audio, video and reference scene
segmentation maps as inputs. Furthermore, we also propose a second
ecology-oriented benchmark aiming at identifying activities, species, number of
individuals and meteorological conditions from 397 multi-view and long-term
ecological events, including false positive triggers. We advocate that both
tasks are complementary and contribute to bridging the gap between machine
learning and ecology. Code and data are available at:
https://github.com/eceo-epfl/MammAlps
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; Benchmark and code at:
  https://github.com/eceo-epfl/MammAlps. After submission of v1, we noticed
  that a few audio files were not correctly aligned with the corresponding
  video. We fixed the issue, which had little to no impact on performance. We
  also now report results for three runs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Cascade Ranking as One Network <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09492v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09492v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunli Wang, Zhen Zhang, Zhiqiang Wang, Zixuan Yang, Yu Li, Jian Yang, Shiyang Wen, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cascade Ranking is a prevalent architecture in large-scale top-k selection
systems like recommendation and advertising platforms. Traditional training
methods focus on single-stage optimization, neglecting interactions between
stages. Recent advances have introduced interaction-aware training paradigms,
but still struggle to 1) align training objectives with the goal of the entire
cascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn
effective collaboration patterns for different stages. To address these
challenges, we propose LCRON, which introduces a novel surrogate loss function
derived from the lower bound probability that ground truth items are selected
by cascade ranking, ensuring alignment with the overall objective of the
system. According to the properties of the derived bound, we further design an
auxiliary loss for each stage to drive the reduction of this bound, leading to
a more robust and effective top-k selection. LCRON enables end-to-end training
of the entire cascade ranking system as a unified network. Experimental results
demonstrate that LCRON achieves significant improvement over existing methods
on public benchmarks and industrial applications, addressing key limitations in
cascade ranking training and significantly enhancing system performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model
  Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Cai Chen, Yi Pin Xu, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In electronic design, engineers often manually search through extensive
documents to retrieve component parameters required for constructing SPICE
models, a process that is both labor-intensive and time-consuming. To address
this challenge, we present an automated framework called D2S-FLOW that
leverages large language models (LLMs) to extract electrical parameters from
datasheets and generate SPICE models with high precision and efficiency,
significantly reducing the need for manual intervention. Unlike traditional RAG
systems, D2S-FLOW employs a workflow to enhance precision in handling
unstructured documents and inconsistent naming conventions through three
innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical
Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity
Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER
utilizes document structure for precise parameter localization, and HNEN
standardizes terminology via semantic inference. Experimental results
demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1
score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the
strongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it
reduces API token consumption by 38% and minimizes the irrelevant information
ratio to 4%, showcasing substantial improvements in resource efficiency. This
research provides an effective automated solution for circuit design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LARES: Latent Reasoning for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Liu, Bowen Zheng, Xiaolei Wang, Wayne Xin Zhao, Jinpeng Wang, Sheng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems have become increasingly important in
real-world applications that model user behavior sequences to predict their
preferences. However, existing sequential recommendation methods predominantly
rely on non-reasoning paradigms, which may limit the model's computational
capacity and result in suboptimal recommendation performance. To address these
limitations, we present LARES, a novel and scalable LAtent REasoning framework
for Sequential recommendation that enhances model's representation capabilities
through increasing the computation density of parameters by depth-recurrent
latent reasoning. Our proposed approach employs a recurrent architecture that
allows flexible expansion of reasoning depth without increasing parameter
complexity, thereby effectively capturing dynamic and intricate user interest
patterns. A key difference of LARES lies in refining all input tokens at each
implicit reasoning step to improve the computation utilization. To fully unlock
the model's reasoning potential, we design a two-phase training strategy: (1)
Self-supervised pre-training (SPT) with dual alignment objectives; (2)
Reinforcement post-training (RPT). During the first phase, we introduce
trajectory-level alignment and step-level alignment objectives, which enable
the model to learn recommendation-oriented latent reasoning patterns without
requiring supplementary annotated data. The subsequent phase utilizes
reinforcement learning (RL) to harness the model's exploratory ability, further
refining its reasoning capabilities. Comprehensive experiments on real-world
benchmarks demonstrate our framework's superior performance. Notably, LARES
exhibits seamless compatibility with existing advanced models, further
improving their recommendation performance. Our code is available at
https://anonymous.4open.science/r/LARES-E458/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query
  Expansion <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyze whether the generated documents contain information
entailed by ground-truth evidence and assess their impact on performance. Our
findings indicate that, on average, performance improvements consistently
occurred for claims whose generated documents included sentences entailed by
gold evidence. This suggests that knowledge leakage may be present in
fact-verification benchmarks, potentially inflating the perceived performance
of LLM-based query expansion methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Recommender with End-to-End Learnable Item Tokenization <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05546v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05546v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Liu, Bowen Zheng, Cheng Ling, Lantao Hu, Han Li, Wayne Xin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation systems have gained increasing attention as an
innovative approach that directly generates item identifiers for recommendation
tasks. Despite their potential, a major challenge is the effective construction
of item identifiers that align well with recommender systems. Current
approaches often treat item tokenization and generative recommendation training
as separate processes, which can lead to suboptimal performance. To overcome
this issue, we introduce ETEGRec, a novel End-To-End Generative Recommender
that unifies item tokenization and generative recommendation into a cohesive
framework. Built on a dual encoder-decoder architecture, ETEGRec consists of an
item tokenizer and a generative recommender. To enable synergistic interaction
between these components, we propose a recommendation-oriented alignment
strategy, which includes two key optimization objectives: sequence-item
alignment and preference-semantic alignment. These objectives tightly couple
the learning processes of the item tokenizer and the generative recommender,
fostering mutual enhancement. Additionally, we develop an alternating
optimization technique to ensure stable and efficient end-to-end training of
the entire framework. Extensive experiments demonstrate the superior
performance of our approach compared to traditional sequential recommendation
models and existing generative recommendation baselines. Our code is available
at https://github.com/RUCAIBox/ETEGRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROGRAG: A Robustly Optimized GraphRAG Framework <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) commonly struggle with specialized or emerging
topics which are rarely seen in the training corpus. Graph-based
retrieval-augmented generation (GraphRAG) addresses this by structuring domain
knowledge as a graph for dynamic retrieval. However, existing pipelines involve
complex engineering workflows, making it difficult to isolate the impact of
individual components. It is also challenging to evaluate the retrieval
effectiveness due to the overlap between the pretraining and evaluation
datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG
framework. Specifically, we propose a multi-stage retrieval mechanism that
integrates dual-level with logic form retrieval methods to improve retrieval
robustness without increasing computational cost. To further refine the system,
we incorporate various result verification methods and adopt an incremental
database construction approach. Through extensive ablation experiments, we
rigorously assess the effectiveness of each component. Our implementation
includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct
initially underperformed. ROGRAG significantly improves the score from 60.0% to
75.0% and outperforms mainstream methods. Experiments on domain-specific
datasets reveal that dual-level retrieval enhances fuzzy matching, while logic
form retrieval improves structured reasoning, highlighting the importance of
multi-stage retrieval.ROGRAG is released as an open-source resource and
supports installation with pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2025 demo track, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Offline Reinforcement Learning Algorithm Customized for Multi-Task
  Fusion in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17589v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17589v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Cong Xu, Ming Zhao, Jiawei Zhu, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for
combining multiple scores outputted by Multi-Task Learning (MTL) into a final
score to maximize user satisfaction, which determines the ultimate
recommendation results. Recently, to optimize long-term user satisfaction
within a recommendation session, Reinforcement Learning (RL) is used for MTF in
the industry. However, the offline RL algorithms used for MTF so far have the
following severe problems: 1) to avoid out-of-distribution (OOD) problem, their
constraints are overly strict, which seriously damage their performance; 2)
they are unaware of the exploration policy used for producing training data and
never interact with real environment, so only suboptimal policy can be learned;
3) the traditional exploration policies are inefficient and hurt user
experience. To solve the above problems, we propose a novel method named
IntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF
integrates offline RL model with our online exploration policy to relax
overstrict and complicated constraints, which significantly improves its
performance. We also design an extremely efficient exploration policy, which
eliminates low-value exploration space and focuses on exploring potential
high-value state-action pairs. Moreover, we adopt progressive training mode to
further enhance our model's performance with the help of our exploration
policy. We conduct extensive offline and online experiments in the short video
channel of Tencent News. The results demonstrate that our model outperforms
other models remarkably. IntegratedRL-MTF has been fully deployed in our RS and
other large-scale RSs in Tencent, which have achieved significant improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiang Yao, Weizhao Jin, Srivatsan Ravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The entity resolution problem requires finding pairs across datasets that
belong to different owners but refer to the same entity in the real world. To
train and evaluate solutions (either rule-based or machine-learning-based) to
the entity resolution problem, generating a ground truth dataset with entity
pairs or clusters is needed. However, such a data annotation process involves
humans as domain oracles to review the plaintext data for all candidate record
pairs from different parties, which inevitably infringes the privacy of data
owners, especially in privacy-sensitive cases like medical records. To the best
of our knowledge, there is no prior work on privacy-preserving ground truth
dataset generation, especially in the domain of entity resolution. We propose a
novel blind annotation protocol based on homomorphic encryption that allows
domain oracles to collaboratively label ground truths without sharing data in
plaintext with other parties. In addition, we design a domain-specific
easy-to-use language that hides the sophisticated underlying homomorphic
encryption layer. Rigorous proof of the privacy guarantee is provided and our
empirical experiments via an annotation simulator indicate the feasibility of
our privacy-preserving protocol (f-measure on average achieves more than 90\%
compared with the real ground truths).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose Cross-source knowledge \textbf{Re}conciliation for
Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively
reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a
four-stage pipeline: it first generates an internal response from parametric
knowledge, then selects the most relevant multimodal evidence via joint
similarity assessment, generates an external response, and finally integrates
both to produce a reliable answer. Additionally, a specialized training
paradigm enhances knowledge source discrimination, multimodal integration, and
unified answer generation. Experiments on KB-VQA benchmarks show that
CoRe-MMRAG achieves substantial improvements over baseline methods, achieving
5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMM4Rec: A Transfer-Efficient Framework for Multi-modal Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fan, Yanrong Hu, Kai Fang, Qingyang Liu, Hongjiu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) systems model user preferences by analyzing
interaction histories. Although transferable multi-modal SR architectures
demonstrate superior performance compared to traditional ID-based approaches,
current methods incur substantial fine-tuning costs when adapting to new
domains due to complex optimization requirements and negative transfer effects
- a significant deployment bottleneck that hinders engineers from efficiently
repurposing pre-trained models for novel application scenarios with minimal
tuning overhead. We propose MMM4Rec (Multi-Modal Mamba for Sequential
Recommendation), a novel multi-modal SR framework that incorporates a dedicated
algebraic constraint mechanism for efficient transfer learning. By combining
State Space Duality (SSD)'s temporal decay properties with a time-aware
modeling design, our model dynamically prioritizes key modality information,
overcoming limitations of Transformer-based approaches. The framework
implements a constrained two-stage process: (1) sequence-level cross-modal
alignment via shared projection matrices, followed by (2) temporal fusion using
our newly designed Cross-SSD module and dual-channel Fourier adaptive
filtering. This architecture maintains semantic consistency while suppressing
noise propagation.MMM4Rec achieves rapid fine-tuning convergence with simple
cross-entropy loss, significantly improving multi-modal recommendation accuracy
while maintaining strong transferability. Extensive experiments demonstrate
MMM4Rec's state-of-the-art performance, achieving the maximum 31.78% NDCG@10
improvement over existing models and exhibiting 10 times faster average
convergence speed when transferring to large-scale downstream datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What LLMs Miss in Recommendations: Bridging the Gap with
  Retrieval-Augmented Collaborative Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahrooz Pouryousef, Ali Montazeralghaem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-item interactions contain rich collaborative signals that form the
backbone of many successful recommender systems. While recent work has explored
the use of large language models (LLMs) for recommendation, it remains unclear
whether LLMs can effectively reason over this type of collaborative
information. In this paper, we conduct a systematic comparison between LLMs and
classical matrix factorization (MF) models to assess LLMs' ability to leverage
user-item interaction data. We further introduce a simple retrieval-augmented
generation (RAG) method that enhances LLMs by grounding their predictions in
structured interaction data. Our experiments reveal that current LLMs often
fall short in capturing collaborative patterns inherent to MF models, but that
our RAG-based approach substantially improves recommendation
quality-highlighting a promising direction for future LLM-based recommenders.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Photoreal Scene Reconstruction from an Egocentric Device <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Lv, Maurizio Monge, Ka Chen, Yufeng Zhu, Michael Goesele, Jakob Engel, Zhao Dong, Richard Newcombe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the challenges associated with using egocentric
devices to photorealistic reconstruct the scene in high dynamic range. Existing
methodologies typically assume using frame-rate 6DoF pose estimated from the
device's visual-inertial odometry system, which may neglect crucial details
necessary for pixel-accurate reconstruction. This study presents two
significant findings. Firstly, in contrast to mainstream work treating RGB
camera as global shutter frame-rate camera, we emphasize the importance of
employing visual-inertial bundle adjustment (VIBA) to calibrate the precise
timestamps and movement of the rolling shutter RGB sensing camera in a high
frequency trajectory format, which ensures an accurate calibration of the
physical properties of the rolling-shutter camera. Secondly, we incorporate a
physical image formation model based into Gaussian Splatting, which effectively
addresses the sensor characteristics, including the rolling-shutter effect of
RGB cameras and the dynamic ranges measured by sensors. Our proposed
formulation is applicable to the widely-used variants of Gaussian Splats
representation. We conduct a comprehensive evaluation of our pipeline using the
open-source Project Aria device under diverse indoor and outdoor lighting
conditions, and further validate it on a Meta Quest3 device. Across all
experiments, we observe a consistent visual enhancement of +1 dB in PSNR by
incorporating VIBA, with an additional +1 dB achieved through our proposed
image formation model. Our complete implementation, evaluation datasets, and
recording profile are available at
http://www.projectaria.com/photoreal-reconstruction/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted to SIGGRAPH Conference Paper 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sounding that Object: Interactive Object-Aware Image to Audio Generation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingle Li, Baihe Huang, Xiaobin Zhuang, Dongya Jia, Jiawei Chen, Yuping Wang, Zhuo Chen, Gopala Anumanchipalli, Yuxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually
  Impaired via GRPO with LLM-as-Follower Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhao, Siqi Wang, Jing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation instruction generation for visually impaired (VI) individuals
(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses
on producing precise, in-situ, step-by-step navigation instructions that are
practically usable by VI users. Concretely, we propose LaF-GRPO
(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate
rewards guiding the Vision-Language Model (VLM) post-training. This enhances
instruction usability while reducing costly real-world data needs. To
facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced
benchmark. It provides diverse navigation scenarios with accurate spatial
coordinates, supporting detailed, open-ended in-situ instruction generation.
Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative
metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542
vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and
benchmark are available at
\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformer-based Ultrasound-to-Speech Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Ibrahimov, Zainkó Csaba, Gábor Gosztolya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have shown promising potential for ultrasound-to-speech
conversion task towards Silent Speech Interfaces. In this work, we applied two
Conformer-based DNN architectures (Base and one with bi-LSTM) for this task.
Speaker-specific models were trained on the data of four speakers from the
Ultrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized
to audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN
baseline, objective measurements (MSE and mel cepstral distortion) showed no
statistically significant improvement for either model. However, a MUSHRA
listening test revealed that Conformer with bi-LSTM provided better perceptual
quality, while Conformer Base matched the performance of the baseline along
with a 3x faster training time due to its simpler architecture. These findings
suggest that Conformer-based models, especially the Conformer with bi-LSTM,
offer a promising alternative to CNNs for ultrasound-to-speech conversion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplArt: Articulation Estimation and Part-Level Reconstruction with 3D
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Lin, Jiading Fang, Muhammad Zubair Irshad, Vitor Campagnolo Guizilini, Rares Andrei Ambrus, Greg Shakhnarovich, Matthew R. Walter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/ripl/splart</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far Are We from Predicting Missing Modalities with Foundation
  Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzhou Ke, Yi Xie, Xiaoli Wang, Guoqing Chao, Bo Wang, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models have demonstrated impressive capabilities across
diverse tasks. However, their potential as plug-and-play solutions for missing
modality prediction remains underexplored. To investigate this, we categorize
existing approaches into three representative paradigms, encompassing a total
of 42 model variants, and conduct a comprehensive evaluation in terms of
prediction accuracy and adaptability to downstream tasks. Our analysis reveals
that current foundation models often fall short in two critical aspects: (i)
fine-grained semantic extraction from the available modalities, and (ii) robust
validation of generated modalities. These limitations lead to suboptimal and,
at times, misaligned predictions. To address these challenges, we propose an
agentic framework tailored for missing modality prediction. This framework
dynamically formulates modality-aware mining strategies based on the input
context, facilitating the extraction of richer and more discriminative semantic
features. In addition, we introduce a \textit{self-refinement mechanism}, which
iteratively verifies and enhances the quality of generated modalities through
internal feedback. Experimental results show that our method reduces FID for
missing image prediction by at least 14% and MER for missing text prediction by
at least 10% compared to baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReactDiff: Latent Diffusion for Facial Reaction Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14151v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14151v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the audio-visual clip of the speaker, facial reaction generation aims
to predict the listener's facial reactions. The challenge lies in capturing the
relevance between video and audio while balancing appropriateness, realism, and
diversity. While prior works have mostly focused on uni-modal inputs or
simplified reaction mappings, recent approaches such as PerFRDiff have explored
multi-modal inputs and the one-to-many nature of appropriate reaction mappings.
In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework
that uniquely integrates a Multi-Modality Transformer with conditional
diffusion in the latent space for enhanced reaction generation. Unlike existing
methods, ReactDiff leverages intra- and inter-class attention for fine-grained
multi-modal interaction, while the latent diffusion process between the encoder
and decoder enables diverse yet contextually appropriate outputs. Experimental
results demonstrate that ReactDiff significantly outperforms existing
approaches, achieving a facial reaction correlation of 0.26 and diversity score
of 0.094 while maintaining competitive realism. The code is open-sourced at
\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-03T00:00:00Z">2025-06-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">30</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quake: Adaptive Indexing for Vector Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Mohoney, Devesh Sarda, Mengze Tang, Shihabur Rahman Chowdhury, Anil Pacaci, Ihab F. Ilyas, Theodoros Rekatsinas, Shivaram Venkataraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector search, the task of finding the k-nearest neighbors of
high-dimensional vectors, underpins many machine learning applications,
including recommendation systems and information retrieval. However, existing
approximate nearest neighbor (ANN) methods perform poorly under dynamic, skewed
workloads where data distributions evolve. We introduce Quake, an adaptive
indexing system that maintains low latency and high recall in such
environments. Quake employs a hierarchical partitioning scheme that adjusts to
updates and changing access patterns, guided by a cost model that predicts
query latency based on partition sizes and access frequencies. Quake also
dynamically optimizes query execution parameters to meet recall targets using a
novel recall estimation model. Furthermore, Quake utilizes optimized query
processing, leveraging NUMA-aware parallelism for improved memory bandwidth
utilization. To evaluate Quake, we prepare a Wikipedia vector search workload
and develop a workload generator to create vector search workloads with
configurable access patterns. Our evaluation shows that on dynamic workloads,
Quake achieves query latency reductions of 1.5-22x and update latency
reductions of 6-83x compared to state-of-the-art indexes SVS, DiskANN, HNSW,
and SCANN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DistRAG: Towards Distance-Based Spatial Reasoning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real world tasks where Large Language Models (LLMs) can be used require
spatial reasoning, like Point of Interest (POI) recommendation and itinerary
planning. However, on their own LLMs lack reliable spatial reasoning
capabilities, especially about distances. To address this problem, we develop a
novel approach, DistRAG, that enables an LLM to retrieve relevant spatial
information not explicitly learned during training. Our method encodes the
geodesic distances between cities and towns in a graph and retrieves a context
subgraph relevant to the question. Using this technique, our method enables an
LLM to answer distance-based reasoning questions that it otherwise cannot
answer. Given the vast array of possible places an LLM could be asked about,
DistRAG offers a flexible first step towards providing a rudimentary `world
model' to complement the linguistic knowledge held in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Reusability in Recommender Systems: The Case for <span class="highlight-title">Dataset</span>- and
  Task-Independent Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Kurniawan Wijaya, Xinyang Shao, Gonzalo Fiz Pontiveros, Edoardo D'Amico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are pivotal in delivering personalized experiences across
industries, yet their adoption and scalability remain hindered by the need for
extensive dataset- and task-specific configurations. Existing systems often
require significant manual intervention, domain expertise, and engineering
effort to adapt to new datasets or tasks, creating barriers to entry and
limiting reusability. In contrast, recent advancements in large language models
(LLMs) have demonstrated the transformative potential of reusable systems,
where a single model can handle diverse tasks without significant
reconfiguration. Inspired by this paradigm, we propose the Dataset- and
Task-Independent Recommender System (DTIRS), a framework aimed at maximizing
the reusability of recommender systems while minimizing barriers to entry.
Unlike LLMs, which achieve task generalization directly, DTIRS focuses on
eliminating the need to rebuild or reconfigure recommendation pipelines for
every new dataset or task, even though models may still need retraining on new
data. By leveraging the novel Dataset Description Language (DsDL), DTIRS
enables standardized dataset descriptions and explicit task definitions,
allowing autonomous feature engineering, model selection, and optimization.
This paper introduces the concept of DTIRS and establishes a roadmap for
transitioning from Level-1 automation (dataset-agnostic but task-specific
systems) to Level-2 automation (fully dataset- and task-independent systems).
Achieving this paradigm would maximize code reusability and lower barriers to
adoption. We discuss key challenges, including the trade-offs between
generalization and specialization, computational overhead, and scalability,
while presenting DsDL as a foundational tool for this vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Rankings and Personalized Recommendations in Marketplaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Besbes, Yash Kanoria, Akshit Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individuals often navigate several options with incomplete knowledge of their
own preferences. Information provisioning tools such as public rankings and
personalized recommendations have become central to helping individuals make
choices, yet their value proposition under different marketplace environments
remains unexplored. This paper studies a stylized model to explore the impact
of these tools in two marketplace settings: uncapacitated supply, where items
can be selected by any number of agents, and capacitated supply, where each
item is constrained to be matched to a single agent. We model the agents
utility as a weighted combination of a common term which depends only on the
item, reflecting the item's population level quality, and an idiosyncratic
term, which depends on the agent item pair capturing individual specific
tastes. Public rankings reveal the common term, while personalized
recommendations reveal both terms. In the supply unconstrained settings, both
public rankings and personalized recommendations improve welfare, with their
relative value determined by the degree of preference heterogeneity. Public
rankings are effective when preferences are relatively homogeneous, while
personalized recommendations become critical as heterogeneity increases. In
contrast, in supply constrained settings, revealing just the common term, as
done by public rankings, provides limited benefit since the total common value
available is limited by capacity constraints, whereas personalized
recommendations, by revealing both common and idiosyncratic terms,
significantly enhance welfare by enabling agents to match with items they
idiosyncratically value highly. These results illustrate the interplay between
supply constraints and preference heterogeneity in determining the
effectiveness of information provisioning tools, offering insights for their
design and deployment in diverse settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Information Retrieval to Enhance Spoken Language
  Understanding <span class="highlight-title">Prompt</span>s in Few-Shot Learning <span class="chip">INTERSPEECH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference paper accepted to INTERSPEECH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and
  <span class="highlight-title">Prompt</span>-Based Approaches to Depression Symptom Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diogo A. P. Nunes, Eugénio Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token and Span Classification for Entity Recognition in French
  Historical Encyclopedias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Moncla, Hédi Zeghidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepShop: A Benchmark for Deep Research Shopping Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, Xiuying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web agents for online shopping have shown great promise in automating user
interactions across e-commerce platforms. Benchmarks for assessing such agents
do not reflect the complexity of real-world shopping scenarios, as they often
consist of overly simple queries with deterministic paths, such as "Find iPhone
15." Real shopping scenarios are inherently more layered, involving
multi-dimensional product attributes, search filters, and user-specific sorting
preferences. To address this gap, we introduce DeepShop, a benchmark designed
to evaluate web agents in complex and realistic online shopping environments.
DeepShop comprises three key components. (1) Query diversity evolution:
Starting from real user queries, we generate diverse queries across five
popular online shopping domains. (2) Query complexity evolution: We further
evolve these queries to increase complexity, considering product attributes,
search filters, and sorting preferences, and classify them into three levels:
easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and
holistic evaluation: We propose an automated evaluation framework that assesses
agent performance in terms of fine-grained aspects (product attributes, search
filters, and sorting preferences) and reports the overall success rate through
holistic evaluation. We conduct a systematic evaluation of retrieval-augmented
generation (RAG) methods, web agents, and deep research systems. Results show
that RAG struggles with complex queries due to its lack of web interaction,
while other methods face significant challenges with filters and sorting
preferences, leading to low overall success rates. We also perform
cross-category, complexity-based evaluations and error analyses to support the
advancement of deep research shopping agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining social relations and interaction data in Recommender System
  with Graph Convolution Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin T. Tran, Vaclav Snasel, Loc Tan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recommender system is an important subject in the field of data mining,
where the item rating information from users is exploited and processed to make
suitable recommendations with all other users. The recommender system creates
convenience for e-commerce users and stimulates the consumption of items that
are suitable for users. In addition to e-commerce, a recommender system is also
used to provide recommendations on books to read, movies to watch, courses to
take or websites to visit. Similarity between users is an important impact for
recommendation, which could be calculated from the data of past user ratings of
the item by methods of collaborative filtering, matrix factorization or
singular vector decomposition. In the development of graph data mining
techniques, the relationships between users and items can be represented by
matrices from which collaborative filtering could be done with the larger
database, more accurate and faster in calculation. All these data can be
represented graphically and mined by today's highly developed graph neural
network models. On the other hand, users' social friendship data also influence
consumption habits because recommendations from friends will be considered more
carefully than information sources. However, combining a user's friend
influence and the similarity between users whose similar shopping habits is
challenging. Because the information is noisy and it affects each particular
data set in different ways. In this study, we present the input data processing
method to remove outliers which are single reviews or users with little
interaction with the items; the next proposed model will combine the social
relationship data and the similarity in the rating history of users to improve
the accuracy and recall of the recommender system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTCS: Effective Unsupervised Temporal Community Search with <span class="highlight-title">Pre-train</span>ing
  of Temporal Dynamics and Subgraph Knowledge <span class="chip">SIGIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhang, Yankai Chen, Yingli Zhou, Yucan Guo, Xiaolin Han, Chenhao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world applications, the evolving relationships between entities
can be modeled as temporal graphs, where each edge has a timestamp representing
the interaction time.
  As a fundamental problem in graph analysis, {\it community search (CS)} in
temporal graphs has received growing attention but exhibits two major
limitations: (1) Traditional methods typically require predefined subgraph
structures, which are not always known in advance. (2) Learning-based methods
struggle to capture temporal interaction information. To fill this research
gap, in this paper, we propose an effective \textbf{U}nsupervised
\textbf{T}emporal \textbf{C}ommunity \textbf{S}earch with pre-training of
temporal dynamics and subgraph knowledge model (\textbf{\model}).
\model~contains two key stages: offline pre-training and online search. In the
first stage, we introduce multiple learning objectives to facilitate the
pre-training process in the unsupervised learning setting. In the second stage,
we identify a candidate subgraph and compute community scores using the
pre-trained node representations and a novel scoring mechanism to determine the
final community members. Experiments on five real-world datasets demonstrate
the effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR'25 short paper track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yankai Chen, Yue Que, Xinni Zhang, Chen Ma, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning vectorized embeddings is fundamental to many recommender systems for
user-item matching. To enable efficient online inference, representation
binarization, which embeds latent features into compact binary sequences, has
recently shown significant promise in optimizing both memory usage and
computational overhead. However, existing approaches primarily focus on
numerical quantization, neglecting the associated information loss, which often
results in noticeable performance degradation. To address these issues, we
study the problem of graph representation binarization for efficient
collaborative filtering. Our findings indicate that explicitly mitigating
information loss at various stages of embedding binarization has a significant
positive impact on performance. Building on these insights, we propose an
enhanced framework, BiGeaR++, which specifically leverages supervisory signals
from pseudo-positive samples, incorporating both real item data and latent
embedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a
fine-grained inference distillation mechanism and an effective embedding sample
synthesis approach. Empirical evaluations across five real-world datasets
demonstrate that the new designs in BiGeaR++ work seamlessly well with other
modules, delivering substantial improvements of around 1%-10% over BiGeaR and
thus achieving state-of-the-art performance compared to the competing methods.
Our implementation is available at https://github.com/QueYork/BiGeaR-SS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Named Entity Recognition Models for Russian Cultural News
  Texts: From <span class="highlight-title">BERT</span> to LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Levchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Information Retrieval with a Monolingual Knowledge Base <span class="chip">SIGIR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Zhuang, Aman Gupta, Anurag Beniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted at GENNEXT@SIGIR25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NextQuill: Causal Preference Modeling for Enhancing LLM Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Zhao, Juntao You, Yang Zhang, Wenjie Wang, Hong Cheng, Fuli Feng, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalizing large language models (LLMs) for individual users has become
increasingly important as they are progressively integrated into real-world
applications to support users' daily lives. However, existing personalization
approaches often fail to distinguish which components of model predictions and
training data truly reflect user preferences, leading to superficial
personalization alignment. In this paper, we introduce NextQuill, a novel LLM
personalization alignment framework grounded in causal preference modeling. We
approach personalization from a causal perspective, treating both model
predictions and ground-truth data generation as outcomes influenced by user
preferences, along with other factors. We define the true preference effect as
the causal impact of user history (which reflects preferences) on each token
prediction or data generation instance, estimated through causal intervention
techniques. Building on this insight, NextQuill introduces two complementary
alignment strategies: (1) aligning model-internal causal preference effects on
predictions with those reflected in ground-truth data, rather than
indiscriminately fitting predictions, and (2) focusing on fitting
preference-bearing tokens identified via ground-truth data preference effects,
rather than treating all tokens uniformly. By integrating these strategies,
NextQuill shifts the alignment process toward learning from causal preference
effects, facilitating more effective and personalized adaptation. Experiments
across multiple personalization benchmarks demonstrate that NextQuill
significantly improves personalization quality, offering a principled, causal
foundation for LLM personalization. Our codes are available on
https://github.com/juntaoyou/NextQuill.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense
  Retrievers <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong effectiveness and
robustness while fine-tuned as dense retrievers. However, their large parameter
size brings significant inference time computational challenges, including high
encoding costs for large-scale corpora and increased query latency, limiting
their practical deployment. While smaller retrievers offer better efficiency,
they often fail to generalize effectively with limited supervised fine-tuning
data. In this work, we introduce DRAMA, a training framework that leverages
LLMs to train smaller generalizable dense retrievers. In particular, we adopt
pruned LLMs as the backbone and train on diverse LLM-augmented data in a
single-stage contrastive learning setup. Experiments show that DRAMA offers
better multilingual and long-context capabilities than traditional
encoder-based retrievers, and achieves strong performance across multiple tasks
and languages. These highlight the potential of connecting the training of
smaller retrievers with the growing advancements in LLMs, bridging the gap
between efficiency and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotative Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06256v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06256v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles L. A. Clarke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces annotative indexing, a novel framework that unifies and
generalizes traditional inverted indexes, column stores, object stores, and
graph databases. As a result, annotative indexing can provide the underlying
indexing framework for databases that support retrieval augmented generation,
knowledge graphs, entity retrieval, semi-structured data, and ranked retrieval.
While we primarily focus on human language data in the form of text, annotative
indexing is sufficiently general to support a range of other datatypes, and we
provide examples of SQL-like queries over a JSON store that includes numbers
and dates. Taking advantage of the flexibility of annotative indexing, we also
demonstrate a fully dynamic annotative index incorporating support for ACID
properties of transactions with hundreds of multiple concurrent readers and
writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/claclark/Cottontail</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoisonArena: Uncovering Competing Poisoning Attacks in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12574v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12574v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems, widely used to improve the
factual grounding of large language models (LLMs), are increasingly vulnerable
to poisoning attacks, where adversaries inject manipulated content into the
retriever's corpus. While prior research has predominantly focused on
single-attacker settings, real-world scenarios often involve multiple,
competing attackers with conflicting objectives. In this work, we introduce
PoisonArena, the first benchmark to systematically study and evaluate competing
poisoning attacks in RAG. We formalize the multi-attacker threat model, where
attackers vie to control the answer to the same query using mutually exclusive
misinformation. PoisonArena leverages the Bradley-Terry model to quantify each
method's competitive effectiveness in such adversarial environments. Through
extensive experiments on the Natural Questions and MS MARCO datasets, we
demonstrate that many attack strategies successful in isolation fail under
competitive pressure. Our findings highlight the limitations of conventional
evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore
the need for competitive evaluation to assess real-world attack robustness.
PoisonArena provides a standardized framework to benchmark and develop future
attack and defense strategies under more realistic, multi-adversary conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://poison-arena.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond <span class="highlight-title">Prompt</span> Engineering: Robust Behavior Control in LLMs via Steering
  Target Atoms <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Podcast P<span class="highlight-title">review</span> Generation: From Expert Models to LLM-Based
  Systems <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Winstead Zhu, Ann Clifton, Azin Ghazimatin, Edgar Tanaka, Edward Ronan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering and evaluating long-form talk content such as videos and podcasts
poses a significant challenge for users, as it requires a considerable time
investment. Previews offer a practical solution by providing concise snippets
that showcase key moments of the content, enabling users to make more informed
and confident choices. We propose an LLM-based approach for generating podcast
episode previews and deploy the solution at scale, serving hundreds of
thousands of podcast previews in a real-world application. Comprehensive
offline evaluations and online A/B testing demonstrate that LLM-generated
previews consistently outperform a strong baseline built on top of various ML
expert models, showcasing a significant reduction in the need for meticulous
feature engineering. The offline results indicate notable enhancements in
understandability, contextual clarity, and interest level, and the online A/B
test shows a 4.6% increase in user engagement with preview content, along with
a 5x boost in processing efficiency, offering a more streamlined and performant
solution compared to the strong baseline of feature-engineered expert models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted at ACL 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Lexical and Semantic Vector Search Methods When Classifying
  Medical Documents <span class="chip">KR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Harris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification is a common AI problem, and vector search is a typical
solution. This transforms a given body of text into a numerical representation,
known as an embedding, and modern improvements to vector search focus on
optimising speed and predictive accuracy. This is often achieved through neural
methods that aim to learn language semantics. However, our results suggest that
these are not always the best solution. Our task was to classify
rigidly-structured medical documents according to their content, and we found
that using off-the-shelf semantic vector search produced slightly worse
predictive accuracy than creating a bespoke lexical vector search model, and
that it required significantly more time to execute. These findings suggest
that traditional methods deserve to be contenders in the information retrieval
toolkit, despite the prevalence and success of neural models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This project was funded by a UKRI grant, number: 10048265</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACLR: A Scalable and Efficient Retrieval-based Method for Industrial
  Product Attribute Value Identification <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03835v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03835v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product Attribute Value Identification (PAVI) involves identifying attribute
values from product profiles, a key task for improving product search,
recommendation, and business analytics on e-commerce platforms. However,
existing PAVI methods face critical challenges, such as inferring implicit
values, handling out-of-distribution (OOD) values, and producing normalized
outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive
Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR
formulates PAVI as an information retrieval task by encoding product profiles
and candidate values into embeddings and retrieving values based on their
similarity. It leverages contrastive training with taxonomy-aware hard negative
sampling and employs adaptive inference with dynamic thresholds. TACLR offers
three key advantages: (1) it effectively handles implicit and OOD values while
producing normalized outputs; (2) it scales to thousands of categories, tens of
thousands of attributes, and millions of values; and (3) it supports efficient
inference for high-load industrial deployment. Extensive experiments on
proprietary and public datasets validate the effectiveness and efficiency of
TACLR. Further, it has been successfully deployed on the real-world e-commerce
platform Xianyu, processing millions of product listings daily with frequently
updated, large-scale attribute taxonomies. We release the code to facilitate
reproducibility and future research at https://github.com/SuYindu/TACLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for
  Graph-RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Huang, Shiqi Zhang, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval
in Large Language Model (LLM)-based question answering. It is particularly
useful in domains such as biomedicine, law, and political science, where
retrieval often requires multi-hop reasoning over proprietary documents. Some
existing Graph-RAG systems construct KNN graphs based on text chunk relevance,
but this coarse-grained approach fails to capture entity relationships within
texts, leading to sub-par retrieval and generation quality. To address this,
recent solutions leverage LLMs to extract entities and relationships from text
chunks, constructing triplet-based knowledge graphs. However, this approach
incurs significant indexing costs, especially for large document collections.
  To ensure a good result accuracy while reducing the indexing cost, we propose
KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small
set of key text chunks and leverages an LLM to construct a knowledge graph
skeleton. It then builds a text-keyword bipartite graph from all text chunks,
serving as a lightweight alternative to a full knowledge graph. During
retrieval, KET-RAG searches both structures: it follows the local search
strategy of existing Graph-RAG systems on the skeleton while mimicking this
search on the bipartite graph to improve retrieval quality. We evaluate 13
solutions on three real-world datasets, demonstrating that KET-RAG outperforms
all competitors in indexing cost, retrieval effectiveness, and generation
quality. Notably, it achieves comparable or superior retrieval quality to
Microsoft's Graph-RAG while reducing indexing costs by over an order of
magnitude. Additionally, it improves the generation quality by up to 32.4%
while lowering indexing costs by around 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Dialogue State Tracking through Combinatorial Search for
  In-Context Examples <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haesung Pyun, Yoonah Park, Yohan Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Driven E-Commerce Marketing Content Optimization: Balancing
  Creativity and Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPOT-Trip: Dual-Preference Driven Out-of-Town Trip Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Liu, Hao Miao, Guojiang Shen, Yan Zhao, Xiangjie Kong, Ivan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-town trip recommendation aims to generate a sequence of Points of
Interest (POIs) for users traveling from their hometowns to previously
unvisited regions based on personalized itineraries, e.g., origin, destination,
and trip duration. Modeling the complex user preferences--which often exhibit a
two-fold nature of static and dynamic interests--is critical for effective
recommendations. However, the sparsity of out-of-town check-in data presents
significant challenges in capturing such user preferences. Meanwhile, existing
methods often conflate the static and dynamic preferences, resulting in
suboptimal performance. In this paper, we for the first time systematically
study the problem of out-of-town trip recommendation. A novel framework
SPOT-Trip is proposed to explicitly learns the dual static-dynamic user
preferences. Specifically, to handle scarce data, we construct a POI attribute
knowledge graph to enrich the semantic modeling of users' hometown and
out-of-town check-ins, enabling the static preference modeling through
attribute relation-aware aggregation. Then, we employ neural ordinary
differential equations (ODEs) to capture the continuous evolution of latent
dynamic user preferences and innovatively combine a temporal point process to
describe the instantaneous probability of each preference behavior. Further, a
static-dynamic fusion module is proposed to merge the learned static and
dynamic user preferences. Extensive experiments on real data offer insight into
the effectiveness of the proposed solutions, showing that SPOT-Trip achieves
performance improvement by up to 17.01%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic
  Iterative Reasoning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding information from visually rich documents remains a significant
challenge for traditional Retrieval-Augmented Generation (RAG) methods.
Existing benchmarks predominantly focus on image-based question answering (QA),
overlooking the fundamental challenges of efficient retrieval, comprehension,
and reasoning within dense visual documents. To bridge this gap, we introduce
ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich
documents requiring complex reasoning. Based on it, we identify key limitations
in current RAG approaches: (i) purely visual retrieval methods struggle to
effectively integrate both textual and visual features, and (ii) previous
approaches often allocate insufficient reasoning tokens, limiting their
effectiveness. To address these challenges, we propose ViDoRAG, a novel
multi-agent RAG framework tailored for complex reasoning across visual
documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy
to effectively handle multi-modal retrieval. To further elicit the model's
reasoning capabilities, we introduce an iterative agent workflow incorporating
exploration, summarization, and reflection, providing a framework for
investigating test-time scaling in RAG domains. Extensive experiments on
ViDoSeek validate the effectiveness and generalization of our approach.
Notably, ViDoRAG outperforms existing methods by over 10% on the competitive
ViDoSeek benchmark. The code is available at
https://github.com/Alibaba-NLP/ViDoRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven effective in integrating
external knowledge into large language models (LLMs) for solving
question-answer (QA) tasks. The state-of-the-art RAG approaches often use the
graph data as the external data since they capture the rich semantic
information and link relationships between entities. However, existing
graph-based RAG approaches cannot accurately identify the relevant information
from the graph and also consume large numbers of tokens in the online retrieval
process. To address these issues, we introduce a novel graph-based RAG
approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by
augmenting the question using attributed communities, and also introducing a
novel LLM-based hierarchical clustering method. To retrieve the most relevant
information from the graph for the question, we build a novel hierarchical
index structure for the attributed communities and develop an effective online
retrieval method. Experimental results demonstrate that ArchRAG outperforms
existing methods in both accuracy and token cost. Moreover, ArchRAG has been
successfully applied to domain knowledge QA in Huawei Cloud Computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving
  Cloud-Device Collaboration <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyi Zhang, Pengyue Jia, Xianneng Li, Derong Xu, Maolin Wang, Yichao Wang, Zhaocheng Du, Huifeng Guo, Yong Liu, Ruiming Tang, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)
for handling public user queries and on-device Small Language Models (SLMs) for
processing private user data, collectively forming a powerful and
privacy-preserving solution. However, existing approaches often fail to fully
leverage the scalable problem-solving capabilities of on-cloud LLMs while
underutilizing the advantage of on-device SLMs in accessing and processing
personalized data. This leads to two interconnected issues: 1) Limited
utilization of the problem-solving capabilities of on-cloud LLMs, which fail to
align with personalized user-task needs, and 2) Inadequate integration of user
data into on-device SLM responses, resulting in mismatches in contextual user
information.
  In this paper, we propose a Leader-Subordinate Retrieval framework for
Privacy-preserving cloud-device collaboration (LSRP), a novel solution that
bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM
through a dynamic selection of task-specific leader strategies named as
user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the
data advantages of on-device SLMs through small model feedback Direct
Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the
on-device SLM. Experiments on two datasets demonstrate that LSRP consistently
outperforms state-of-the-art baselines, significantly improving question-answer
relevance and personalization, while preserving user privacy through efficient
on-device retrieval. Our code is available at:
https://github.com/Applied-Machine-Learning-Lab/LSRP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HASH-RAG: Bridging Deep Hashing with Retriever for Efficient, Fine
  Retrieval and Augmented Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16133v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16133v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyu Guo, Xunlei Chen, Qiyang Xia, Zhaokun Wang, Jie Ou, Libo Qin, Shunyu Yao, Wenhong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) encounters efficiency challenges when
scaling to massive knowledge bases while preserving contextual relevance. We
propose Hash-RAG, a framework that integrates deep hashing techniques with
systematic optimizations to address these limitations. Our queries directly
learn binary hash codes from knowledgebase code, eliminating intermediate
feature extraction steps, and significantly reducing storage and computational
overhead. Building upon this hash-based efficient retrieval framework, we
establish the foundation for fine-grained chunking. Consequently, we design a
Prompt-Guided Chunk-to-Context (PGCC) module that leverages retrieved
hash-indexed propositions and their original document segments through prompt
engineering to enhance the LLM's contextual awareness. Experimental evaluations
on NQ, TriviaQA, and HotpotQA datasets demonstrate that our approach achieves a
90% reduction in retrieval time compared to conventional methods while
maintaining considerate recall performance. Additionally, The proposed system
outperforms retrieval/non-retrieval baselines by 1.4-4.3% in EM scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPR: Empowering Generation with Graph-<span class="highlight-title">Pretrain</span>ed Retriever 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Wang, Zongyu Wu, Yuan Zhong, Xiang Zhang, Suhang Wang, Fenglong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph retrieval-augmented generation (GRAG) places high demands on
graph-specific retrievers. However, existing retrievers often rely on language
models pretrained on plain text, limiting their effectiveness due to domain
misalignment and structure ignorance. To address these challenges, we propose
GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR
aligns natural language questions with relevant subgraphs through LLM-guided
graph augmentation and employs a structure-aware objective to learn
fine-grained retrieval strategies. Experiments on two datasets, three LLM
backbones, and five baselines show that GPR consistently improves both
retrieval quality and downstream generation, demonstrating its effectiveness as
a robust retrieval solution for GRAG.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through
  Audio-Visual Alignment with Cascaded Cross-<span class="highlight-title">Transformer</span> <span class="chip">INTERSPEECH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orchid Chetia Phukan, Mohd Mujtaba Akhtar,  Girish, Swarup Ranjan Behera, Abu Osama Siddiqui, Sarthak Jain, Priyabrata Mallick, Jaya Sai Kiran Patibandla, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As video-sharing platforms have grown over the past decade, child viewership
has surged, increasing the need for precise detection of harmful content like
violence or explicit scenes. Malicious users exploit moderation systems by
embedding unsafe content in minimal frames to evade detection. While prior
research has focused on visual cues and advanced such fine-grained detection,
audio features remain underexplored. In this study, we embed audio cues with
visual for fine-grained child harmful content detection and introduce SNIFR, a
novel framework for effective alignment. SNIFR employs a transformer encoder
for intra-modality interaction, followed by a cascaded cross-transformer for
inter-modality alignment. Our approach achieves superior performance over
unimodal and baseline fusion methods, setting a new state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Source Attribution of Singing Voice Deepfake with Multimodal
  Foundation Models <span class="chip">INTERSPEECH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Priyabrata Mallick, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce the task of singing voice deepfake source
attribution (SVDSA). We hypothesize that multimodal foundation models (MMFMs)
such as ImageBind, LanguageBind will be most effective for SVDSA as they are
better equipped for capturing subtle source-specific characteristics-such as
unique timbre, pitch manipulation, or synthesis artifacts of each singing voice
deepfake source due to their cross-modality pre-training. Our experiments with
MMFMs, speech foundation models and music foundation models verify the
hypothesis that MMFMs are the most effective for SVDSA. Furthermore, inspired
from related research, we also explore fusion of foundation models (FMs) for
improved SVDSA. To this end, we propose a novel framework, COFFE which employs
Chernoff Distance as novel loss function for effective fusion of FMs. Through
COFFE with the symphony of MMFMs, we attain the topmost performance in
comparison to all the individual FMs and baseline fusion methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IllumiCraft: Unified Geometry and Illumination Diffusion for
  Controllable Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Ronald Clark, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although diffusion-based models can generate high-quality and high-resolution
video sequences from textual or image inputs, they lack explicit integration of
geometric cues when controlling scene lighting and visual appearance across
frames. To address this limitation, we propose IllumiCraft, an end-to-end
diffusion framework accepting three complementary inputs: (1)
high-dynamic-range (HDR) video maps for detailed lighting control; (2)
synthetically relit frames with randomized illumination changes (optionally
paired with a static background reference image) to provide appearance cues;
and (3) 3D point tracks that capture precise 3D geometry information. By
integrating the lighting, appearance, and geometry cues within a unified
diffusion architecture, IllumiCraft generates temporally coherent videos
aligned with user-defined prompts. It supports background-conditioned and
text-conditioned video relighting and provides better fidelity than existing
controllable video generation methods. Project Page:
https://yuanze-lin.me/IllumiCraft_page
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition
  Query 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Yuan Gao, Linfeng Li, Xian Wang, Qi Xu, Hang Song, Lingdong Kong, Ran Zhou, Yi Zeng, Yidong Cai, Botian Jiang, Shilin Xu, Jiajun Zhang, Minghui Qiu, Xiangtai Li, Tianshu Yang, Siliang Tang, Juncheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval is crucial for modern applications yet remains
underexplored in current research. Existing datasets are limited to single
languages, single images, or singular retrieval conditions, often failing to
fully exploit the expressive capacity of visual information as evidenced by
maintained performance when images are replaced with captions. However,
practical retrieval scenarios frequently involve interleaved multi-condition
queries with multiple images. Hence, this paper introduces MERIT, the first
multilingual dataset for interleaved multi-condition semantic retrieval,
comprising 320,000 queries with 135,000 products in 5 languages, covering 7
distinct product categories. Extensive experiments on MERIT identify existing
models's limitation: focusing solely on global semantic information while
neglecting specific conditional elements in queries. Consequently, we propose
Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by
integrating embedding reconstruction to preserve fine-grained conditional
elements and contrastive learning to extract comprehensive global semantics.
Experiments demonstrate that Coral achieves a 45.9% performance improvement
over conventional approaches on MERIT, with strong generalization capabilities
validated across 8 established retrieval benchmarks. Collectively, our
contributions - a novel dataset, identification of critical limitations in
existing approaches, and an innovative fine-tuning framework - establish a
foundation for future research in interleaved multi-condition semantic
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; Project Page, Code, and Dataset at:
  https://merit-2025.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Wang, Chunlei Zhang, Hangting Chen, Zhou Zhao, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable TTS models with natural language prompts often lack the ability
for fine-grained control and face a scarcity of high-quality data. We propose a
two-stage style-controllable TTS system with language models, utilizing a
quantized masked-autoencoded style-rich representation as an intermediary. In
the first stage, an autoregressive transformer is used for the conditional
generation of these style-rich tokens from text and control signals. The second
stage generates codec tokens from both text and sampled style-rich tokens.
Experiments show that training the first-stage model on extensive datasets
enhances the content robustness of the two-stage model as well as control
capabilities over multiple attributes. By selectively combining discrete labels
and speaker embeddings, we explore fully controlling the speaker's timbre and
other stylistic information, and adjusting attributes like emotion for a
specified speaker. Audio samples are available at
https://style-ar-tts.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic mapping from static labels: remote sensing dynamic sample
  generation with temporal-spectral embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yuan, Shuang Chen, Tianwu Lin, Jie Wang, Peng Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate remote sensing geographic mapping depends heavily on representative
and timely sample data. However, rapid changes in land surface dynamics
necessitate frequent updates, quickly rendering previously collected samples
obsolete and imposing significant labor demands for continuous manual updates.
In this study, we aim to address this problem by dynamic sample generation
using existing single-date static labeled samples. We introduce TasGen, a
two-stage automated framework to automatically generate dynamic samples,
designed to simultaneously model spectral and temporal dependencies in
time-series remote sensing imagery via temporal-spectral embedding, capturing
land surface changes without additional manual annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech
  Generation in Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice Conversion (VC) modifies speech to match a target speaker while
preserving linguistic content. Traditional methods usually extract speaker
information directly from speech while neglecting the explicit utilization of
linguistic content. Since VC fundamentally involves disentangling speaker
identity from linguistic content, leveraging structured semantic features could
enhance conversion performance. However, previous attempts to incorporate
semantic features into VC have shown limited effectiveness, motivating the
integration of explicit text modeling. We propose StarVC, a unified
autoregressive VC framework that first predicts text tokens before synthesizing
acoustic features. The experiments demonstrate that StarVC outperforms
conventional VC methods in preserving both linguistic content (i.e., WER and
CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found
at: https://thuhcsi.github.io/StarVC/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, Accepted by Interspeech 2025, Demo:
  https://thuhcsi.github.io/StarVC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trusted Fake Audio Detection Based on Dirichlet Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Ding, Junxiao Xue, Cong Wang, Hao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous development of deep learning-based speech conversion and
speech synthesis technologies, the cybersecurity problem posed by fake audio
has become increasingly serious. Previously proposed models for defending
against fake audio have attained remarkable performance. However, they all fall
short in modeling the trustworthiness of the decisions made by the models
themselves. Based on this, we put forward a plausible fake audio detection
approach based on the Dirichlet distribution with the aim of enhancing the
reliability of fake audio detection. Specifically, we first generate evidence
through a neural network. Uncertainty is then modeled using the Dirichlet
distribution. By modeling the belief distribution with the parameters of the
Dirichlet distribution, an estimate of uncertainty can be obtained for each
decision. Finally, the predicted probabilities and corresponding uncertainty
estimates are combined to form the final opinion. On the ASVspoof series
dataset (i.e., ASVspoof 2019 LA, ASVspoof 2021 LA, and DF), we conduct a number
of comparison experiments to verify the excellent performance of the proposed
model in terms of accuracy, robustness, and trustworthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EyeNavGS: A 6-DoF Navigation <span class="highlight-title">Dataset</span> and Record-n-Replay Software for
  Real-World 3DGS Scenes in VR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is an emerging media representation that
reconstructs real-world 3D scenes in high fidelity, enabling
6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,
developing and evaluating 3DGS-enabled applications and optimizing their
rendering performance, require realistic user navigation data. Such data is
currently unavailable for photorealistic 3DGS reconstructions of real-world
scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available
6-DoF navigation dataset featuring traces from 46 participants exploring twelve
diverse, real-world 3DGS scenes. The dataset was collected at two sites, using
the Meta Quest Pro headsets, recording the head pose and eye gaze data for each
rendered frame during free world standing 6-DoF navigation. For each of the
twelve scenes, we performed careful scene initialization to correct for scene
tilt and scale, ensuring a perceptually-comfortable VR experience. We also
release our open-source SIBR viewer software fork with record-and-replay
functionalities and a suite of utility tools for data processing, conversion,
and visualization. The EyeNavGS dataset and its accompanying software tools
provide valuable resources for advancing research in 6-DoF viewport prediction,
adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The
EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Jailbreak Attack for Image Generation Models via Editing Step
  by Step <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Kuiyi Gao, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based image generation models, such as Stable Diffusion and DALL-E 3,
hold significant potential in content creation and publishing workflows, making
them the focus in recent years. Despite their remarkable capability to generate
diverse and vivid images, considerable efforts are being made to prevent the
generation of harmful content, such as abusive, violent, or pornographic
material. To assess the safety of existing models, we introduce a novel
jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises
image generation models through a step-by-step editing process. Specifically,
for malicious queries that cannot bypass the safeguards with a single prompt,
we intentionally decompose the query into multiple sub-queries. The image
generation models are then prompted to generate and iteratively edit images
based on these sub-queries. To evaluate the effectiveness of our CoJ attack
method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine
safety scenarios, three types of editing operations, and three editing
elements. Experiments on four widely-used image generation services provided by
GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack
method can successfully bypass the safeguards of models for over 60% cases,
which significantly outperforms other jailbreaking methods (i.e., 14%).
Further, to enhance these models' safety against our CoJ attack method, we also
propose an effective prompting-based method, Think Twice Prompting, that can
successfully defend over 95% of CoJ attack. We release our dataset and code to
facilitate the AI safety research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can't See the Forest for the Trees: Benchmarking Multimodal Safety
  Awareness for Multimodal LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have expanded the capabilities of
traditional language models by enabling interaction through both text and
images. However, ensuring the safety of these models remains a significant
challenge, particularly in accurately identifying whether multimodal content is
safe or unsafe-a capability we term safety awareness. In this paper, we
introduce MMSafeAware, the first comprehensive multimodal safety awareness
benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500
carefully curated image-prompt pairs. MMSafeAware includes both unsafe and
over-safety subsets to assess models abilities to correctly identify unsafe
content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine
widely used MLLMs using MMSafeAware reveals that current models are not
sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies
36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further
explore three methods to improve safety awareness-prompting-based approaches,
visual contrastive decoding, and vision-centric reasoning fine-tuning-but find
that none achieve satisfactory performance. Our findings highlight the profound
challenges in developing MLLMs with robust safety awareness, underscoring the
need for further research in this area. All the code and data will be publicly
available to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-TAME: Trainable Attention Mechanism for Explaining Convolutional
  Networks and Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development and adoption of Vision Transformers and other deep-learning
architectures for image classification tasks has been rapid. However, the
"black box" nature of neural networks is a barrier to adoption in applications
where explainability is essential. While some techniques for generating
explanations have been proposed, primarily for Convolutional Neural Networks,
adapting such techniques to the new paradigm of Vision Transformers is
non-trivial. This paper presents T-TAME, Transformer-compatible Trainable
Attention Mechanism for Explanations, a general methodology for explaining deep
neural networks used in image classification tasks. The proposed architecture
and training technique can be easily applied to any convolutional or Vision
Transformer-like neural network, using a streamlined training approach. After
training, explanation maps can be computed in a single forward pass; these
explanation maps are comparable to or outperform the outputs of computationally
expensive perturbation-based explainability techniques, achieving SOTA
performance. We apply T-TAME to three popular deep learning classifier
architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet
dataset, and we demonstrate improvements over existing state-of-the-art
explainability methods. A detailed analysis of the results and an ablation
study provide insights into how the T-TAME design choices affect the quality of
the generated explanation maps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inclusion 2024 Global Multimedia Deepfake Detection Challenge: Towards
  Multi-dimensional Face Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Weize Gao, Changtao Miao, Man Luo, Jianshu Li, Wenzhong Deng, Zhe Li, Bingyu Hu, Weibin Yao, Yunfeng Diao, Wenbo Zhou, Tao Gong, Qi Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the Global Multimedia Deepfake Detection held
concurrently with the Inclusion 2024. Our Multimedia Deepfake Detection aims to
detect automatic image and audio-video manipulations including but not limited
to editing, synthesis, generation, Photoshop,etc. Our challenge has attracted
1500 teams from all over the world, with about 5000 valid result submission
counts. We invite the top 20 teams to present their solutions to the challenge,
from which the top 3 teams are awarded prizes in the grand finale. In this
paper, we present the solutions from the top 3 teams of the two tracks, to
boost the research work in the field of image and audio-video forgery
detection. The methodologies developed through the challenge will contribute to
the development of next-generation deepfake detection systems and we encourage
participants to open source their methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Inclusion 2024 Global Multimedia Deepfake Detection Competition Top
  Team Technical Report</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-02T00:00:00Z">2025-06-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entity Image and Mixed-Modal Image Retrieval <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian-Ioan Blaga, Paul Suganthan, Sahil Dua, Krishna Srinivasan, Enrique Alfonseca, Peter Dornbach, Tom Duerig, Imed Zitouni, Zhe Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in multimodal learning, challenging benchmarks for
mixed-modal image retrieval that combines visual and textual information are
lacking. This paper introduces a novel benchmark to rigorously evaluate image
retrieval that demands deep cross-modal contextual understanding. We present
two new datasets: the Entity Image Dataset (EI), providing canonical images for
Wikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived
from the WIT dataset. The MMIR benchmark features two challenging query types
requiring models to ground textual descriptions in the context of provided
visual entities: single entity-image queries (one entity image with descriptive
text) and multi-entity-image queries (multiple entity images with relational
text). We empirically validate the benchmark's utility as both a training
corpus and an evaluation set for mixed-modal retrieval. The quality of both
datasets is further affirmed through crowd-sourced human annotations. The
datasets are accessible through the GitHub page:
https://github.com/google-research-datasets/wit-retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransAct V2: Lifelong User Action Sequence Modeling on Pinterest
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Xia, Saurabh Vishwas Joshi, Kousik Rajesh, Kangnan Li, Yangyi Lu, Nikil Pancha, Dhruvil Deven Badani, Jiajing Xu, Pong Eksombatchai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user action sequences has become a popular focus in industrial
recommendation system research, particularly for Click-Through Rate (CTR)
prediction tasks. However, industry-scale CTR models often rely on short user
sequences, limiting their ability to capture long-term behavior. Additionally,
these models typically lack an integrated action-prediction task within a
point-wise ranking framework, reducing their predictive power. They also rarely
address the infrastructure challenges involved in efficiently serving
large-scale sequential models. In this paper, we introduce TransAct V2, a
production model for Pinterest's Homefeed ranking system, featuring three key
innovations: (1) leveraging very long user sequences to improve CTR
predictions, (2) integrating a Next Action Loss function for enhanced user
action forecasting, and (3) employing scalable, low-latency deployment
solutions tailored to handle the computational demands of extended user action
sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-like Preference Profiling in Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyu Ouyang, Qianlong Wen, Chunhui Zhang, Yanfang Ye, Soroush Vosoughi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems aspire to profile users by interpreting
their interaction histories, echoing how humans make decisions by weighing
experience, relative preference strength, and situational relevance. Yet,
existing large language model (LLM)-based recommenders often fall short of
mimicking the flexible, context-aware decision strategies humans exhibit,
neglecting the structured, dynamic, and context-aware mechanisms fundamental to
human behaviors. To bridge this gap, we propose RecPO, a preference
optimization framework that models structured feedback and contextual delay to
emulate human-like prioritization in sequential recommendation RecPO exploits
adaptive reward margins based on inferred preference hierarchies and temporal
signals, enabling the model to favor immediately relevant items and to
distinguish between varying degrees of preference and aversion. Extensive
experiments across five real-world datasets demonstrate that RecPO not only
yields performance gains over state-of-the-art baselines, but also mirrors key
characteristics of human decision-making: favoring timely satisfaction,
maintaining coherent preferences, and exercising discernment under shifting
contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE)
  Using Embeddings and Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madan Krishnamurthy, Daniel Korn, Melissa A Haendel, Christopher J Mungall, Anne E Thessen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLoSS: Generative Language Models with Semantic Search for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Acharya, Aleksandr V. Petrov, Juba Ziani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Generative Low-rank language model with Semantic Search (GLoSS), a
generative recommendation framework that combines large language models with
dense retrieval for sequential recommendation. Unlike prior methods such as
GPT4Rec, which rely on lexical matching via BM25, GLoSS uses semantic search to
retrieve relevant items beyond lexical matching. For query generation, we
employ 4-bit quantized LlaMA-3 models fine-tuned with low-rank adaptation
(LoRA), enabling efficient training and inference on modest hardware. We
evaluate GLoSS on three real-world Amazon review datasets: Beauty, Toys, and
Sports, and find that it achieves state-of-the-art performance. Compared to
traditional ID-based baselines, GLoSS improves Recall@5 by 33.3%, 52.8%, and
15.2%, and NDCG@5 by 30.0%, 42.6%, and 16.1%, respectively. It also outperforms
LLM-based recommenders such as P5, GPT4Rec, LlamaRec and E4SRec with Recall@5
gains of 4.3%, 22.8%, and 29.5%. Additionally, user segment evaluations show
that GLoSS performs particularly well for cold-start users in the Amazon Toys
and Sports datasets, and benefits from longer user histories in Amazon Beauty
dataset, demonstrating robustness across different levels of interaction
lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and model checkpoints are publicly available
  at:https://github.com/krishnacharya/GLoSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Getting almost all the bits from a quantum random access code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Hsuan Lin, Ronald de Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A quantum random access code (QRAC) is a map $x\mapsto\rho_x$ that encodes
$n$-bit strings $x$ into $m$-qubit quantum states $\rho_x$, in a way that
allows us to recover any one bit of $x$ with success probability $\geq p$. The
measurement on $\rho_x$ that is used to recover, say, $x_1$ may destroy all the
information about the other bits; this is in fact what happens in the
well-known QRAC that encodes $n=2$ bits into $m=1$ qubits. Does this generalize
to large $n$, i.e., could there exist QRACs that are so "obfuscated" that one
cannot get much more than one bit out of them? Here we show that this is not
the case: for every QRAC there exists a measurement that (with high
probability) recovers the full $n$-bit string $x$ up to small Hamming distance,
even for the worst-case $x$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages LaTeX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting
  Out-of-Distribution Corpora Using GradNormIR <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayoon Ko, Jinyoung Kim, Sohyeon Kim, Jinhyuk Kim, Jaehoon Lee, Seonghak Song, Minyoung Lee, Gunhee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers encode texts into embeddings to efficiently retrieve
relevant documents from large databases in response to user queries. However,
real-world corpora continually evolve, leading to a shift from the original
training distribution of the retriever. Without timely updates or retraining,
indexing newly emerging documents can degrade retrieval performance for future
queries. Thus, identifying when a dense retriever requires an update is
critical for maintaining robust retrieval systems. In this paper, we propose a
novel task of predicting whether a corpus is out-of-distribution (OOD) relative
to a dense retriever before indexing. Addressing this task allows us to
proactively manage retriever updates, preventing potential retrieval failures.
We introduce GradNormIR, an unsupervised approach that leverages gradient norms
to detect OOD corpora effectively. Experiments on the BEIR benchmark
demonstrate that GradNormIR enables timely updates of dense retrievers in
evolving document collections, significantly enhancing retrieval robustness and
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CiteEval: Principle-Driven Citation Evaluation for Source Attribution <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumo Xu, Peng Qi, Jifan Chen, Kunlun Liu, Rujun Han, Lan Liu, Bonan Min, Vittorio Castelli, Arshit Gupta, Zhiguo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation quality is crucial in information-seeking systems, directly
influencing trust and the effectiveness of information access. Current
evaluation frameworks, both human and automatic, mainly rely on Natural
Language Inference (NLI) to assess binary or ternary supportiveness from cited
sources, which we argue is a suboptimal proxy for citation evaluation. In this
work we introduce CiteEval, a citation evaluation framework driven by
principles focusing on fine-grained citation assessment within a broad context,
encompassing not only the cited sources but the full retrieval context, user
query, and generated text. Guided by the proposed framework, we construct
CiteBench, a multi-domain benchmark with high-quality human annotations on
citation quality. To enable efficient evaluation, we further develop
CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation
with human judgments. Experiments across diverse systems demonstrate
CiteEval-Auto's superior ability to capture the multifaceted nature of
citations compared to existing metrics, offering a principled and scalable
approach to evaluate and improve model-generated citations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAM: Generative Recommendation via Semantic-aware Multi-granular Late
  Fusion <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunkyung Lee, Minjin Choi, Eunseong Choi, Hye-young Kim, Jongwuk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation is an emerging paradigm that leverages the
extensive knowledge of large language models by formulating recommendations
into a text-to-text generation task. However, existing studies face two key
limitations in (i) incorporating implicit item relationships and (ii) utilizing
rich yet lengthy item information. To address these challenges, we propose a
Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM),
introducing two synergistic innovations. First, we design semantic-to-lexical
translation to encode implicit hierarchical and collaborative item
relationships into the vocabulary space of LLMs. Second, we present
multi-granular late fusion to integrate rich semantics efficiently with minimal
information loss. It employs separate encoders for multi-granular prompts,
delaying the fusion until the decoding stage. Experiments on four benchmark
datasets show that GRAM outperforms eight state-of-the-art generative
recommendation models, achieving significant improvements of 11.5-16.0% in
Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at
https://github.com/skleee/GRAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Stickers, Big Meanings: A Multilingual Sticker Semantic
  Understanding <span class="highlight-title">Dataset</span> with a Gamified Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stickers, though small, are a highly condensed form of visual expression,
ubiquitous across messaging platforms and embraced by diverse cultures,
genders, and age groups. Despite their popularity, sticker retrieval remains an
underexplored task due to the significant human effort and subjectivity
involved in constructing high-quality sticker query datasets. Although large
language models (LLMs) excel at general NLP tasks, they falter when confronted
with the nuanced, intangible, and highly specific nature of sticker query
generation.
  To address this challenge, we propose a threefold solution. First, we
introduce Sticktionary, a gamified annotation framework designed to gather
diverse, high-quality, and contextually resonant sticker queries. Second, we
present StickerQueries, a multilingual sticker query dataset containing 1,115
English and 615 Chinese queries, annotated by over 60 contributors across 60+
hours. Lastly, Through extensive quantitative and qualitative evaluation, we
demonstrate that our approach significantly enhances query generation quality,
retrieval accuracy, and semantic understanding in the sticker domain. To
support future research, we publicly release our multilingual dataset along
with two fine-tuned query generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engram Memory Encoding and Retrieval: A Neurocomputational Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Szelogowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial research into the biological basis of memory, the precise
mechanisms by which experiences are encoded, stored, and retrieved in the brain
remain incompletely understood. A growing body of evidence supports the engram
theory, which posits that sparse populations of neurons undergo lasting
physical and biochemical changes to support long-term memory. Yet, a
comprehensive computational framework that integrates biological findings with
mechanistic models remains elusive. This work synthesizes insights from
cellular neuroscience and computational modeling to address key challenges in
engram research: how engram neurons are identified and manipulated; how
synaptic plasticity mechanisms contribute to stable memory traces; and how
sparsity promotes efficient, interference-resistant representations. Relevant
computational approaches -- such as sparse regularization, engram gating, and
biologically inspired architectures like Sparse Distributed Memory and spiking
neural networks -- are also examined. Together, these findings suggest that
memory efficiency, capacity, and stability emerge from the interaction of
plasticity and sparsity constraints. By integrating neurobiological and
computational perspectives, this paper provides a comprehensive theoretical
foundation for engram research and proposes a roadmap for future inquiry into
the mechanisms underlying memory, with implications for the diagnosis and
treatment of memory-related disorders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argument-Centric Causal Intervention Method for Mitigating Bias in
  Cross-Document Event Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Yao, Wenzhong Yang, Yabo Yin, Fuyuan Wei, Hongzhen Lv, Jiaren Peng, Liejun Wang, Xiaoming Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in
natural language processing (NLP) that seeks to determine whether event
mentions across multiple documents refer to the same real-world occurrence.
However, current CD-ECR approaches predominantly rely on trigger features
within input mention pairs, which induce spurious correlations between
surface-level lexical features and coreference relationships, impairing the
overall performance of the models. To address this issue, we propose a novel
cross-document event coreference resolution method based on Argument-Centric
Causal Intervention (ACCI). Specifically, we construct a structural causal
graph to uncover confounding dependencies between lexical triggers and
coreference labels, and introduce backdoor-adjusted interventions to isolate
the true causal effect of argument semantics. To further mitigate spurious
correlations, ACCI integrates a counterfactual reasoning module that quantifies
the causal influence of trigger word perturbations, and an argument-aware
enhancement module to promote greater sensitivity to semantically grounded
information. In contrast to prior methods that depend on costly data
augmentation or heuristic-based filtering, ACCI enables effective debiasing in
a unified end-to-end framework without altering the underlying training
procedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of
88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The
implementation and materials are available at https://github.com/era211/ACCI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Entity Association Mining Framework for Knowledge Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshika Rawal, Abhijeet Kumar, Mridul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting useful signals or pattern to support important business decisions
for example analyzing investment product traction and discovering customer
preference, risk monitoring etc. from unstructured text is a challenging task.
Capturing interaction of entities or concepts and association mining is a
crucial component in text mining, enabling information extraction and reasoning
over and knowledge discovery from text. Furthermore, it can be used to enrich
or filter knowledge graphs to guide exploration processes, descriptive
analytics and uncover hidden stories in the text. In this paper, we introduce a
domain independent pipeline i.e., generalized framework to enable document
filtering, entity extraction using various sources (or techniques) as plug-ins
and association mining to build any text mining business use-case and
quantitatively define a scoring metric for ranking purpose. The proposed
framework has three major components a) Document filtering: filtering
documents/text of interest from massive amount of texts b) Configurable entity
extraction pipeline: include entity extraction techniques i.e., i) DBpedia
Spotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or
dictionary) based c) Association Relationship Mining: To generates
co-occurrence graph to analyse potential relationships among entities,
concepts. Further, co-occurrence count based frequency statistics provide a
holistic window to observe association trends or buzz rate in specific business
context. The paper demonstrates the usage of framework as fundamental building
box in two financial use-cases namely brand product discovery and vendor risk
monitoring. We aim that such framework will remove duplicated effort, minimize
the development effort, and encourage reusability and rapid prototyping in
association mining business applications for institutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Business Analytics and Intelligence Conference, IIM
  Bengaluru</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Next POI Recommendation with Semantic ID <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Wang, Yuxi Huang, Shen Gao, Yifan Wang, Chengrui Huang, Shuo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-of-interest (POI) recommendation systems aim to predict the next
destinations of user based on their preferences and historical check-ins.
Existing generative POI recommendation methods usually employ random numeric
IDs for POIs, limiting the ability to model semantic relationships between
similar locations. In this paper, we propose Generative Next POI Recommendation
with Semantic ID (GNPR-SID), an LLM-based POI recommendation model with a novel
semantic POI ID (SID) representation method that enhances the semantic
understanding of POI modeling. There are two key components in our GNPR-SID:
(1) a Semantic ID Construction module that generates semantically rich POI IDs
based on semantic and collaborative features, and (2) a Generative POI
Recommendation module that fine-tunes LLMs to predict the next POI using these
semantic IDs. By incorporating user interaction patterns and POI semantic
features into the semantic ID generation, our method improves the
recommendation accuracy and generalization of the model. To construct
semantically related SIDs, we propose a POI quantization method based on
residual quantized variational autoencoder, which maps POIs into a discrete
semantic space. We also propose a diversity loss to ensure that SIDs are
uniformly distributed across the semantic space. Extensive experiments on three
benchmark datasets demonstrate that GNPR-SID substantially outperforms
state-of-the-art methods, achieving up to 16% improvement in recommendation
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, the paper has been accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeGraph: Synthetic Benchmark <span class="highlight-title">Dataset</span>s for Robust Time-Series Causal
  Discovery <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hasan Ferdous, Emam Hossain, Md Osman Gani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust causal discovery in time series datasets depends on reliable benchmark
datasets with known ground-truth causal relationships. However, such datasets
remain scarce, and existing synthetic alternatives often overlook critical
temporal properties inherent in real-world data, including nonstationarity
driven by trends and seasonality, irregular sampling intervals, and the
presence of unobserved confounders. To address these challenges, we introduce
TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets
that systematically incorporates both linear and nonlinear dependencies while
modeling key temporal characteristics such as trends, seasonal effects, and
heterogeneous noise patterns. Each dataset is accompanied by a fully specified
causal graph featuring varying densities and diverse noise distributions and is
provided in two versions: one including unobserved confounders and one without,
thereby offering extensive coverage of real-world complexity while preserving
methodological neutrality. We further demonstrate the utility of TimeGraph
through systematic evaluations of state-of-the-art causal discovery algorithms
including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and
metrics. Our experiments reveal significant variations in algorithmic
performance under realistic temporal conditions, underscoring the need for
robust synthetic benchmarks in the fair and transparent assessment of causal
discovery methods. The complete TimeGraph suite, including dataset generation
scripts, evaluation metrics, and recommended experimental protocols, is freely
available to facilitate reproducible research and foster community-driven
advancements in time-series causal discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, accepted at KDD 2025 (Datasets and Benchmarks
  Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Platform for Investigating Public Health Content with Efficient
  Concern Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Li, Rickard Stureborg, Bhuwan Dhingra, Jun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent rise in online content expressing concerns with public health
initiatives has contributed to already stalled uptake of preemptive measures
globally. Future public health efforts must attempt to understand such content,
what concerns it may raise among readers, and how to effectively respond to it.
To this end, we present ConcernScope, a platform that uses a teacher-student
framework for knowledge transfer between large language models and light-weight
classifiers to quickly and effectively identify the health concerns raised in a
text corpus. The platform allows uploading massive files directly,
automatically scraping specific URLs, and direct text editing. ConcernScope is
built on top of a taxonomy of public health concerns. Intended for public
health officials, we demonstrate several applications of this platform: guided
data exploration to find useful examples of common concerns found in online
community datasets, identification of trends in concerns through an example
time series analysis of 186,000 samples, and finding trends in topic frequency
before and after significant events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational
  Knowledge Bases <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala, Christos Faloutsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a semi-structured knowledge base (SKB), where text documents are
interconnected by relations, how can we effectively retrieve relevant
information to answer user questions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language models (LLMs) in question
answering; while Graph RAG (GRAG) uses structured knowledge bases as its
knowledge source. However, many questions require both textual and relational
information from SKB - referred to as "hybrid" questions - which complicates
the retrieval process and underscores the need for a hybrid retrieval method
that leverages both information. In this paper, through our empirical analysis,
we identify key insights that show why existing methods may struggle with
hybrid question answering (HQA) over SKB. Based on these insights, we propose
HybGRAG for HQA consisting of a retriever bank and a critic module, with the
following advantages: (1) Agentic, it automatically refines the output by
incorporating feedback from the critic module, (2) Adaptive, it solves hybrid
questions requiring both textual and relational information with the retriever
bank, (3) Interpretable, it justifies decision making with intuitive refinement
path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In
experiments on the STaRK benchmark, HybGRAG achieves significant performance
gains, with an average relative improvement in Hit@1 of 51%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collapse of Dense Retrievers: Short, Early, and Literal Biases
  Outranking Factual Evidence <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Fayyaz, Ali Modarressi, Hinrich Schuetze, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrieval models are commonly used in Information Retrieval (IR)
applications, such as Retrieval-Augmented Generation (RAG). Since they often
serve as the first step in these systems, their robustness is critical to avoid
downstream failures. In this work, we repurpose a relation extraction dataset
(e.g., Re-DocRED) to design controlled experiments that quantify the impact of
heuristic biases, such as a preference for shorter documents, on retrievers
like Dragon+ and Contriever. We uncover major vulnerabilities, showing
retrievers favor shorter documents, early positions, repeated entities, and
literal matches, all while ignoring the answer's presence! Notably, when
multiple biases combine, models exhibit catastrophic performance degradation,
selecting the answer-containing document in less than 10% of cases over a
synthetic biased document without the answer. Furthermore, we show that these
biases have direct consequences for downstream applications like RAG, where
retrieval-preferred documents can mislead LLMs, resulting in a 34% performance
drop than providing no documents at all.
https://huggingface.co/datasets/mohsenfayyaz/ColDeR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask in Any Modality: A Comprehensive <span class="highlight-title">Survey</span> on Multimodal
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) suffer from hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information for improved factual grounding. With advances in multimodal
learning, Multimodal RAG extends this approach by incorporating multiple
modalities such as text, images, audio, and video to enhance the generated
outputs. However, cross-modal alignment and reasoning introduce unique
challenges beyond those in unimodal RAG. This survey offers a structured and
comprehensive analysis of Multimodal RAG systems, covering datasets,
benchmarks, metrics, evaluation, methodologies, and innovations in retrieval,
fusion, augmentation, and generation. We review training strategies, robustness
enhancements, loss functions, and agent-based approaches, while also exploring
the diverse Multimodal RAG scenarios. In addition, we outline open challenges
and future directions to guide research in this evolving field. This survey
lays the foundation for developing more capable and reliable AI systems that
effectively leverage multimodal dynamic external knowledge bases. All resources
are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository:
  https://github.com/llm-lab-org/Multimodal-RAG-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Needle: A Generative AI-Powered Multi-modal Database for Answering
  Complex Natural Language Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Erfanian, Mohsen Dehghankar, Abolfazl Asudeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal datasets, like those involving images, often miss the detailed
descriptions that properly capture the rich information encoded in each item.
This makes answering complex natural language queries a major challenge in this
domain. In particular, unlike the traditional nearest neighbor search, where
the tuples and the query are represented as points in a single metric space,
these settings involve queries and tuples embedded in fundamentally different
spaces, making the traditional query answering methods inapplicable. Existing
literature addresses this challenge for image datasets through vector
representations jointly trained on natural language and images. This technique,
however, underperforms for complex queries due to various reasons.
  This paper takes a step towards addressing this challenge by introducing a
Generative-based Monte Carlo method that utilizes foundation models to generate
synthetic samples that capture the complexity of the natural language query and
represent it in the same metric space as the multi-modal data.
  Following this method, we propose Needle, a database for image data
retrieval. Instead of relying on contrastive learning or metadata-searching
approaches, our system is based on synthetic data generation to capture the
complexities of natural language queries. Our system is open-source and ready
for deployment, designed to be easily adopted by researchers and developers.
The comprehensive experiments on various benchmark datasets verify that this
system significantly outperforms state-of-the-art text-to-image retrieval
methods in the literature. Any foundation model and embedder can be easily
integrated into Needle to improve the performance, piggybacking on the
advancements in these technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGA-V1: Unifying Online Advertising with End-to-End Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Qiu, Ze Wang, Fan Zhang, Zuowu Zheng, Jile Zhu, Jiangke Fan, Teng Zhang, Haitao Wang, Yongkang Wang, Xingxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern industrial advertising systems commonly employ Multi-stage Cascading
Architectures (MCA) to balance computational efficiency with ranking accuracy.
However, this approach presents two fundamental challenges: (1) performance
inconsistencies arising from divergent optimization targets and capability
differences between stages, and (2) failure to account for advertisement
externalities - the complex interactions between candidate ads during ranking.
These limitations ultimately compromise system effectiveness and reduce
platform profitability. In this paper, we present EGA-V1, an end-to-end
generative architecture that unifies online advertising ranking as one model.
EGA-V1 replaces cascaded stages with a single model to directly generate
optimal ad sequences from the full candidate ad corpus in location-based
services (LBS). The primary challenges associated with this approach stem from
high costs of feature processing and computational bottlenecks in modeling
externalities of large-scale candidate pools. To address these challenges,
EGA-V1 introduces an algorithm and engine co-designed hybrid feature service to
decouple user and ad feature processing, reducing latency while preserving
expressiveness. To efficiently extract intra- and cross-sequence mutual
information, we propose RecFormer with an innovative cluster-attention
mechanism as its core architectural component. Furthermore, we propose a
bi-stage training strategy that integrates pre-training with reinforcement
learning-based post-training to meet sophisticated platform and advertising
objectives. Extensive offline evaluations on public benchmarks and large-scale
online A/B testing on industrial advertising platform have demonstrated the
superior performance of EGA-V1 over state-of-the-art MCAs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerSRV: Personalized Sticker Retrieval with Vision-Language Model <span class="chip">WWW '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instant Messaging is a popular means for daily communication, allowing users
to send text and stickers. As the saying goes, "a picture is worth a thousand
words", so developing an effective sticker retrieval technique is crucial for
enhancing user experience. However, existing sticker retrieval methods rely on
labeled data to interpret stickers, and general-purpose Vision-Language Models
(VLMs) often struggle to capture the unique semantics of stickers.
Additionally, relevant-based sticker retrieval methods lack personalization,
creating a gap between diverse user expectations and retrieval results. To
address these, we propose the Personalized Sticker Retrieval with
Vision-Language Model framework, namely PerSRV, structured into offline
calculations and online processing modules. The online retrieval part follows
the paradigm of relevant recall and personalized ranking, supported by the
offline pre-calculation parts, which are sticker semantic understanding,
utility evaluation and personalization modules. Firstly, for sticker-level
semantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate
human-like sticker semantics, complemented by textual content extracted from
figures and historical interaction queries. Secondly, we investigate three
crowd-sourcing metrics for sticker utility evaluation. Thirdly, we cluster
style centroids based on users' historical interactions to achieve personal
preference modeling. Finally, we evaluate our proposed PerSRV method on a
public sticker retrieval dataset from WeChat, containing 543,098 candidates and
12,568 interactions. Experimental results show that PerSRV significantly
outperforms existing methods in multi-modal sticker retrieval. Additionally,
our fine-tuned VLM delivers notable improvements in sticker semantic
understandings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRACode: LoRA Adapters for Code Embeddings <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saumya Chaturvedi, Aman Chadha, Laurent Bindschaedler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code embeddings are essential for semantic code search; however, current
approaches often struggle to capture the precise syntactic and contextual
nuances inherent in code. Open-source models such as CodeBERT and UniXcoder
exhibit limitations in scalability and efficiency, while high-performing
proprietary systems impose substantial computational costs. We introduce a
parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to
construct task-specific adapters for code retrieval. Our approach reduces the
number of trainable parameters to less than two percent of the base model,
enabling rapid fine-tuning on extensive code corpora (2 million samples in 25
minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in
Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code
search tasks across multiple programming languages. Distinction in task-wise
and language-wise adaptation helps explore the sensitivity of code retrieval
for syntactical and linguistic variations. To foster research in this area, we
make our code and pre-trained models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Deep Learning for Code (DL4C) Workshop at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12499v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12499v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Likes and Dislikes in Personalized Generative Explainable
  Recommendation <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on explainable recommendation generally frames the task as a
standard text generation problem, and evaluates models simply based on the
textual similarity between the predicted and ground-truth explanations.
However, this approach fails to consider one crucial aspect of the systems:
whether their outputs accurately reflect the users' (post-purchase) sentiments,
i.e., whether and why they would like and/or dislike the recommended items. To
shed light on this issue, we introduce new datasets and evaluation methods that
focus on the users' sentiments. Specifically, we construct the datasets by
explicitly extracting users' positive and negative opinions from their
post-purchase reviews using an LLM, and propose to evaluate systems based on
whether the generated explanations 1) align well with the users' sentiments,
and 2) accurately identify both positive and negative opinions of users on the
target items. We benchmark several recent models on our datasets and
demonstrate that achieving strong performance on existing metrics does not
ensure that the generated explanations align well with the users' sentiments.
Lastly, we find that existing models can provide more sentiment-aware
explanations when the users' (predicted) ratings for the target items are
directly fed into the models as input. The datasets and benchmark
implementation are available at: https://github.com/jchanxtarov/sent_xrec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted for presentation at The Web
  Conference (WWW) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reassessing Large Language Model Boolean Query Generation for Systematic
  <span class="highlight-title">Review</span>s <span class="chip">SIGIR-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic reviews are comprehensive literature reviews that address highly
focused research questions and represent the highest form of evidence in
medicine. A critical step in this process is the development of complex Boolean
queries to retrieve relevant literature. Given the difficulty of manually
constructing these queries, recent efforts have explored Large Language Models
(LLMs) to assist in their formulation. One of the first studies,Wang et al.,
investigated ChatGPT for this task, followed by Staudinger et al., which
evaluated multiple LLMs in a reproducibility study. However, the latter
overlooked several key aspects of the original work, including (i) validation
of generated queries, (ii) output formatting constraints, and (iii) selection
of examples for chain-of-thought (Guided) prompting. As a result, its findings
diverged significantly from the original study. In this work, we systematically
reproduce both studies while addressing these overlooked factors. Our results
show that query effectiveness varies significantly across models and prompt
designs, with guided query formulation benefiting from well-chosen seed
studies. Overall, prompt design and model selection are key drivers of
successful query formulation. Our findings provide a clearer understanding of
LLMs' potential in Boolean query generation and highlight the importance of
model- and prompt-specific optimisations. The complex nature of systematic
reviews adds to challenges in both developing and reproducing methods but also
highlights the importance of reproducibility studies in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in SIGIR-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Retrieval with Evidence Curation for Open-Domain Financial
  Question Answering on Standardized Documents <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Choe, Jihoon Kim, Woohwan Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) based large language models (LLMs) are
widely used in finance for their excellent performance on knowledge-intensive
tasks. However, standardized documents (e.g., SEC filing) share similar formats
such as repetitive boilerplate texts, and similar table structures. This
similarity forces traditional RAG methods to misidentify near-duplicate text,
leading to duplicate retrieval that undermines accuracy and completeness. To
address these issues, we propose the Hierarchical Retrieval with Evidence
Curation (HiREC) framework. Our approach first performs hierarchical retrieval
to reduce confusion among similar texts. It first retrieve related documents
and then selects the most relevant passages from the documents. The evidence
curation process removes irrelevant passages. When necessary, it automatically
generates complementary queries to collect missing information. To evaluate our
approach, we construct and release a Large-scale Open-domain Financial (LOFin)
question answering benchmark that includes 145,897 SEC documents and 1,595
question-answer pairs. Our code and data are available at
https://github.com/deep-over/LOFin-bench-HiREC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for
  Auto-Generating Chemical Process and Instrumentation Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoDA: Modulation Adapter for Fine-Grained Visual Grounding in
  Instructional MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wayner Barrios, Andrés Villa, Juan León Alcázar, SouYoung Jin, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Models (MLLMs) have demonstrated
impressive performance on instruction-following tasks by integrating pretrained
visual encoders with large language models (LLMs). However, existing approaches
often struggle to ground fine-grained visual concepts in complex scenes. In
this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective
module designed to refine pre-aligned visual features through
instruction-guided modulation. Our approach follows the standard LLaVA training
protocol, consisting of a two-stage process: (1) aligning image features to the
LLMs input space via a frozen vision encoder and adapter layers, and (2)
refining those features using the MoDA adapter during the instructional tuning
stage. MoDA employs a Transformer-based cross-attention mechanism to generate a
modulation mask over the aligned visual tokens, thereby emphasizing
semantically relevant embedding dimensions based on the language instruction.
The modulated features are then passed to the LLM for autoregressive language
generation. Our experimental evaluation shows that MoDA improves visual
grounding and generates more contextually appropriate responses, demonstrating
its effectiveness as a general-purpose enhancement for image-based MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSCodec Studio: A Modular Framework for Gaussian Splat Compression <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Li, Chengzhen Wu, Hao Li, Xiang Gao, Yiyi Liao, Lu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting and its extension to 4D dynamic scenes enable
photorealistic, real-time rendering from real-world captures, positioning
Gaussian Splats (GS) as a promising format for next-generation immersive media.
However, their high storage requirements pose significant challenges for
practical use in sharing, transmission, and storage. Despite various studies
exploring GS compression from different perspectives, these efforts remain
scattered across separate repositories, complicating benchmarking and the
integration of best practices. To address this gap, we present GSCodec Studio,
a unified and modular framework for GS reconstruction, compression, and
rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction
methods and GS compression techniques as modular components, facilitating
flexible combinations and comprehensive comparisons. By integrating best
practices from community research and our own explorations, GSCodec Studio
supports the development of compact representation and compression solutions
for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,
achieving competitive rate-distortion performance in static and dynamic GS
compression. The code for our framework is publicly available at
https://github.com/JasonLSC/GSCodec_Studio , to advance the research on
Gaussian Splats compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repository of the project: https://github.com/JasonLSC/GSCodec_Studio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Stickers, Big Meanings: A Multilingual Sticker Semantic
  Understanding <span class="highlight-title">Dataset</span> with a Gamified Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stickers, though small, are a highly condensed form of visual expression,
ubiquitous across messaging platforms and embraced by diverse cultures,
genders, and age groups. Despite their popularity, sticker retrieval remains an
underexplored task due to the significant human effort and subjectivity
involved in constructing high-quality sticker query datasets. Although large
language models (LLMs) excel at general NLP tasks, they falter when confronted
with the nuanced, intangible, and highly specific nature of sticker query
generation.
  To address this challenge, we propose a threefold solution. First, we
introduce Sticktionary, a gamified annotation framework designed to gather
diverse, high-quality, and contextually resonant sticker queries. Second, we
present StickerQueries, a multilingual sticker query dataset containing 1,115
English and 615 Chinese queries, annotated by over 60 contributors across 60+
hours. Lastly, Through extensive quantitative and qualitative evaluation, we
demonstrate that our approach significantly enhances query generation quality,
retrieval accuracy, and semantic understanding in the sticker domain. To
support future research, we publicly release our multilingual dataset along
with two fine-tuned query generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned
  Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Srinivas Menon, Raj Prakash Gohil, Kumud Tripathi, Pankaj Wasnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker recognition models face challenges in multi-lingual settings due to
the entanglement of linguistic information within speaker embeddings. The
overlap between vocal traits such as accent, vocal anatomy, and a language's
phonetic structure complicates separating linguistic and speaker information.
Disentangling these components can significantly improve speaker recognition
accuracy. To this end, we propose a novel disentanglement learning strategy
that integrates joint learning through prefix-tuned cross-attention. This
approach is particularly effective when speakers switch between languages.
Experimental results show the model generalizes across monolingual and
multi-lingual settings, including unseen languages. Notably, the proposed model
improves the equal error rate across multiple datasets, highlighting its
ability to separate language information from speaker embeddings and enhance
recognition in diverse linguistic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2025, Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Stage Lighting Control: Is it a Rule-Driven Process or
  Generative Task? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhao, Dian Jin, Zijing Zhou, Xiaoyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stage lighting plays an essential role in live music performances,
influencing the engaging experience of both musicians and audiences. Given the
high costs associated with hiring or training professional lighting engineers,
Automatic Stage Lighting Control (ASLC) has gained increasing attention.
However, most existing approaches only classify music into limited categories
and map them to predefined light patterns, resulting in formulaic and
monotonous outcomes that lack rationality. To address this issue, this paper
presents an end-to-end solution that directly learns from experienced lighting
engineers -- Skip-BART. To the best of our knowledge, this is the first work to
conceptualize ASLC as a generative task rather than merely a classification
problem. Our method modifies the BART model to take audio music as input and
produce light hue and value (intensity) as output, incorporating a novel skip
connection mechanism to enhance the relationship between music and light within
the frame grid.We validate our method through both quantitative analysis and an
human evaluation, demonstrating that Skip-BART outperforms conventional
rule-based methods across all evaluation metrics and shows only a limited gap
compared to real lighting engineers.Specifically, our method yields a p-value
of 0.72 in a statistical comparison based on human evaluations with human
lighting engineers, suggesting that the proposed approach closely matches human
lighting engineering performance. To support further research, we have made our
self-collected dataset, code, and trained model parameters available at
https://github.com/RS2002/Skip-BART .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUDI: A Multimodal Biomedical <span class="highlight-title">Dataset</span> for Understanding Pharmacodynamic
  Drug-Drug Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung-Lam Ngo, Ba-Hoang Tran, Duy-Cat Can, Trung-Hieu Do, Oliver Y. Chén, Hoang-Quynh Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the interaction between different drugs (drug-drug interaction
or DDI) is critical for ensuring patient safety and optimizing therapeutic
outcomes. Existing DDI datasets primarily focus on textual information,
overlooking multimodal data that reflect complex drug mechanisms. In this
paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for
Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark
learning methods to study it. In brief, MUDI provides a comprehensive
multimodal representation of drugs by combining pharmacological text, chemical
formulas, molecular structure graphs, and images across 310,532 annotated drug
pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to
effectively evaluate machine-learning based generalization, MUDI consists of
unseen drug pairs in the test set. We evaluate benchmark models using both late
fusion voting and intermediate fusion strategies. All data, annotations,
evaluation scripts, and baselines are released under an open research license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Sparsity for Effective and Efficient Music Performance Question
  Answering <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Diao, Tianzhen Yang, Chunhui Zhang, Weiyi Wu, Ming Cheng, Jiang Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music performances, characterized by dense and continuous audio as well as
seamless audio-visual integration, present unique challenges for multimodal
scene understanding and reasoning. Recent Music Performance Audio-Visual
Question Answering (Music AVQA) datasets have been proposed to reflect these
challenges, highlighting the continued need for more effective integration of
audio-visual representations in complex question answering. However, existing
Music AVQA methods often rely on dense and unoptimized representations, leading
to inefficiencies in the isolation of key information, the reduction of
redundancy, and the prioritization of critical samples. To address these
challenges, we introduce Sparsify, a sparse learning framework specifically
designed for Music AVQA. It integrates three sparsification strategies into an
end-to-end pipeline and achieves state-of-the-art performance on the Music AVQA
datasets. In addition, it reduces training time by 28.32% compared to its fully
trained dense counterpart while maintaining accuracy, demonstrating clear
efficiency gains. To further improve data efficiency, we propose a key-subset
selection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0
training data and retains 70-80% of full-data performance across models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of the 63rd Annual Meeting of the
  Association for Computational Linguistics (ACL 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEA: Low-Resource Safety Alignment for Multimodal Large Language Models
  via Synthetic Embeddings <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12562v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12562v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have serious security
vulnerabilities.While safety alignment using multimodal datasets consisting of
text and data of additional modalities can effectively enhance MLLM's security,
it is costly to construct these datasets. Existing low-resource security
alignment methods, including textual alignment, have been found to struggle
with the security risks posed by additional modalities. To address this, we
propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes
embeddings of additional modality through gradient updates to expand textual
datasets. This enables multimodal safety alignment training even when only
textual data is available. Extensive experiments on image, video, and
audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding
on a single RTX3090 GPU within 24 seconds. SEA significantly improves the
security of MLLMs when faced with threats from additional modalities. To assess
the security risks introduced by video and audio, we also introduced a new
benchmark called VA-SafetyBench. High attack success rates across multiple
MLLMs validate its challenge. Our code and data will be available at
https://github.com/ZeroNLP/SEA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ola: Pushing the Frontiers of Omni-Modal Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04328v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04328v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models, particularly following GPT-4o, have
sparked increasing interest in developing omni-modal models capable of
understanding more modalities. While some open-source alternatives have
emerged, there is still a notable lag behind specialized single-modality models
in performance. In this paper, we present Ola, an Omni-modal Language model
that achieves competitive performance across image, video, and audio
understanding compared to specialized counterparts, pushing the frontiers of
the omni-modal language model to a large extent. We conduct a comprehensive
exploration of architectural design, data curation, and training strategies
essential for building a robust omni-modal model. Ola incorporates advanced
visual understanding and audio recognition capabilities through several
critical and effective improvements over mainstream baselines. Moreover, we
rethink inter-modal relationships during omni-modal training, emphasizing
cross-modal alignment with video as a central bridge, and propose a progressive
training pipeline that begins with the most distinct modalities and gradually
moves towards closer modality alignment. Extensive experiments demonstrate that
Ola surpasses existing open omni-modal LLMs across all modalities while
achieving highly competitive performance compared to state-of-the-art
specialized models of similar sizes. We aim to make Ola a fully open omni-modal
understanding solution to advance future research in this emerging field. Model
weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I see what you mean: Co-Speech Gestures for Reference Resolution in
  Multimodal Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esam Ghaleb, Bulat Khaertdinov, Aslı Özyürek, Raquel Fernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In face-to-face interaction, we use multiple modalities, including speech and
gestures, to communicate information and resolve references to objects.
However, how representational co-speech gestures refer to objects remains
understudied from a computational perspective. In this work, we address this
gap by introducing a multimodal reference resolution task centred on
representational gestures, while simultaneously tackling the challenge of
learning robust gesture embeddings. We propose a self-supervised pre-training
approach to gesture representation learning that grounds body movements in
spoken language. Our experiments show that the learned embeddings align with
expert annotations and have significant predictive power. Moreover, reference
resolution accuracy further improves when (1) using multimodal gesture
representations, even when speech is unavailable at inference time, and (2)
leveraging dialogue history. Overall, our findings highlight the complementary
roles of gesture and speech in reference resolution, offering a step towards
more naturalistic models of human-machine interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoSMM: A Robust and Secure Multi-Modal Watermarking Framework for
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZhongLi Fang, Yu Xie, Ping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current image watermarking technologies are predominantly categorized into
text watermarking techniques and image steganography; however, few methods can
simultaneously handle text and image-based watermark data, which limits their
applicability in complex digital environments. This paper introduces an
innovative multi-modal watermarking approach, drawing on the concept of vector
discretization in encoder-based vector quantization. By constructing adjacency
matrices, the proposed method enables the transformation of text watermarks
into robust image-based representations, providing a novel multi-modal
watermarking paradigm for image generation applications. Additionally, this
study presents a newly designed image restoration module to mitigate image
degradation caused by transmission losses and various noise interferences,
thereby ensuring the reliability and integrity of the watermark. Experimental
results validate the robustness of the method under multiple noise attacks,
providing a secure, scalable, and efficient solution for digital image
copyright protection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12499v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12499v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Transferability of Multimodal Adversarial Samples for
  Vision-Language <span class="highlight-title">Pre-train</span>ing Models with Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12636v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12636v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youze Wang, Wenbo Hu, Yinpeng Dong, Hanwang Zhang, Hang Su, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of visual and textual data in Vision-Language Pre-training
(VLP) models is crucial for enhancing vision-language understanding. However,
the adversarial robustness of these models, especially in the alignment of
image-text features, has not yet been sufficiently explored. In this paper, we
introduce a novel gradient-based multimodal adversarial attack method,
underpinned by contrastive learning, to improve the transferability of
multimodal adversarial samples in VLP models. This method concurrently
generates adversarial texts and images within imperceptive perturbation,
employing both image-text and intra-modal contrastive loss. We evaluate the
effectiveness of our approach on image-text retrieval and visual entailment
tasks, using publicly available datasets in a black-box setting. Extensive
experiments indicate a significant advancement over existing single-modal
transfer-based adversarial attack methods and current multimodal adversarial
attack approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Multimedia Generated by Large AI Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00045v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00045v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large AI Models (LAIMs), particularly diffusion
models and large language models, has marked a new era where AI-generated
multimedia is increasingly integrated into various aspects of daily life.
Although beneficial in numerous fields, this content presents significant
risks, including potential misuse, societal disruptions, and ethical concerns.
Consequently, detecting multimedia generated by LAIMs has become crucial, with
a marked rise in related research. Despite this, there remains a notable gap in
systematic surveys that focus specifically on detecting LAIM-generated
multimedia. Addressing this, we provide the first survey to comprehensively
cover existing research on detecting multimedia (such as text, images, videos,
audio, and multimodal content) created by LAIMs. Specifically, we introduce a
novel taxonomy for detection methods, categorized by media modality, and
aligned with two perspectives: pure detection (aiming to enhance detection
performance) and beyond detection (adding attributes like generalizability,
robustness, and interpretability to detectors). Additionally, we have presented
a brief overview of generation mechanisms, public datasets, online detection
tools, and evaluation metrics to provide a valuable resource for researchers
and practitioners in this field. Most importantly, we offer a focused analysis
from a social media perspective to highlight their broader societal impact.
Furthermore, we identify current challenges in detection and propose directions
for future research that address unexplored, ongoing, and emerging issues in
detecting multimedia generated by LAIMs. Our aim for this survey is to fill an
academic gap and contribute to global AI security efforts, helping to ensure
the integrity of information in the digital realm. The project link is
https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-01T00:00:00Z">2025-06-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative
  Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maruf Ahmed Mridul, Ian Sloyan, Aparna Gupta, Oshani Seneviratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) are
reshaping how AI systems extract and organize information from unstructured
text. A key challenge is designing AI methods that can incrementally extract,
structure, and validate information while preserving hierarchical and
contextual relationships. We introduce CDMizer, a template-driven, LLM, and
RAG-based framework for structured text transformation. By leveraging
depth-based retrieval and hierarchical generation, CDMizer ensures a
controlled, modular process that aligns generated outputs with predefined
schema. Its template-driven approach guarantees syntactic correctness, schema
adherence, and improved scalability, addressing key limitations of direct
generation methods. Additionally, we propose an LLM-powered evaluation
framework to assess the completeness and accuracy of structured
representations. Demonstrated in the transformation of Over-the-Counter (OTC)
financial derivative contracts into the Common Domain Model (CDM), CDMizer
establishes a scalable foundation for AI-driven document understanding,
structured synthesis, and automated validation in broader contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap: From Ad-hoc to Proactive Search in Conversations <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Meng, Francesco Tonolini, Fengran Mo, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proactive search in conversations (PSC) aims to reduce user effort in
formulating explicit queries by proactively retrieving useful relevant
information given conversational context. Previous work in PSC either directly
uses this context as input to off-the-shelf ad-hoc retrievers or further
fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on
short and concise queries, while the PSC input is longer and noisier. This
input mismatch between ad-hoc search and PSC limits retrieval quality. While
fine-tuning on PSC data helps, its benefits remain constrained by this input
gap. In this work, we propose Conv2Query, a novel conversation-to-query
framework that adapts ad-hoc retrievers to PSC by bridging the input gap
between ad-hoc search and PSC. Conv2Query maps conversational context into
ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc
retrievers or for further fine-tuning on PSC data. Extensive experiments on two
PSC datasets show that Conv2Query significantly improves ad-hoc retrievers'
performance, both when used directly and after fine-tuning on PSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AliBoost: Ecological Boosting Framework in Alibaba Platform <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijie Shen, Yuanchen Bei, Zihong Huang, Jialin Zhu, Keqin Xu, Boya Du, Jiawei Tang, Yuning Jiang, Feiran Huang, Xiao Huang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maintaining a healthy ecosystem in billion-scale online platforms is
challenging, as users naturally gravitate toward popular items, leaving cold
and less-explored items behind. This ''rich-get-richer'' phenomenon hinders the
growth of potentially valuable cold items and harms the platform's ecosystem.
Existing cold-start models primarily focus on improving initial recommendation
performance for cold items but fail to address users' natural preference for
popular content. In this paper, we introduce AliBoost, Alibaba's ecological
boosting framework, designed to complement user-oriented natural
recommendations and foster a healthier ecosystem. AliBoost incorporates a
tiered boosting structure and boosting principles to ensure high-potential
items quickly gain exposure while minimizing disruption to low-potential items.
To achieve this, we propose the Stacking Fine-Tuning Cold Predictor to enhance
the foundation CTR model's performance on cold items for accurate CTR and
potential prediction. AliBoost then employs an Item-oriented Bidding Boosting
mechanism to deliver cold items to the most suitable users while balancing
boosting speed with user-personalized preferences. Over the past six months,
AliBoost has been deployed across Alibaba's mainstream platforms, successfully
cold-starting over a billion new items and increasing both clicks and GMV of
cold items by over 60% within 180 days. Extensive online analysis and A/B
testing demonstrate the effectiveness of AliBoost in addressing ecological
challenges, offering new insights into the design of billion-scale recommender
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, accepted by KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaker: Removing Shortcut Cues with User Clustering for Single-slot
  Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Yue Zheng, Yujing Zhang, Yan Feng, Zhe Wang, Xiaowei Shi, An You, Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a single-slot recommendation system, users are only exposed to one item at
a time, and the system cannot collect user feedback on multiple items
simultaneously. Therefore, only pointwise modeling solutions can be adopted,
focusing solely on modeling the likelihood of clicks or conversions for items
by users to learn user-item preferences, without the ability to capture the
ranking information among different items directly. However, since user-side
information is often much more abundant than item-side information, the model
can quickly learn the differences in user intrinsic tendencies, which are
independent of the items they are exposed to. This can cause these intrinsic
tendencies to become a shortcut bias for the model, leading to insufficient
mining of the most concerned user-item preferences. To solve this challenge, we
introduce the Breaker model. Breaker integrates an auxiliary task of user
representation clustering with a multi-tower structure for cluster-specific
preference modeling. By clustering user representations, we ensure that users
within each cluster exhibit similar characteristics, which increases the
complexity of the pointwise recommendation task on the user side. This forces
the multi-tower structure with cluster-driven parameter learning to better
model user-item preferences, ultimately eliminating shortcut biases related to
user intrinsic tendencies. In terms of training, we propose a delayed parameter
update mechanism to enhance training stability and convergence, enabling
end-to-end joint training of the auxiliary clustering and classification tasks.
Both offline and online experiments demonstrate that our method surpasses the
baselines. It has already been deployed and is actively serving tens of
millions of users daily on Meituan, one of the most popular e-commerce
platforms for services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Inequality of LLM Fact-Checking over Geographic Regions
  with Agent and Retrieval models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Coelho, Shujaat Mirza, Yuyuan Cui, Christina Pöpper, Damon McCoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is a potentially useful application of Large Language Models
(LLMs) to combat the growing dissemination of disinformation. However, the
performance of LLMs varies across geographic regions. In this paper, we
evaluate the factual accuracy of open and private models across a diverse set
of regions and scenarios.
  Using a dataset containing 600 fact-checked statements balanced across six
global regions we examine three experimental setups of fact-checking a
statement: (1) when just the statement is available, (2) when an LLM-based
agent with Wikipedia access is utilized, and (3) as a best case scenario when a
Retrieval-Augmented Generation (RAG) system provided with the official fact
check is employed. Our findings reveal that regardless of the scenario and LLM
used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global
North perform substantially better than those from the Global South.
Furthermore, this gap is broadened for the more realistic case of a Wikipedia
agent-based system, highlighting that overly general knowledge bases have a
limited ability to address region-specific nuances. These results underscore
the urgent need for better dataset balancing and robust retrieval strategies to
enhance LLM fact-checking capabilities, particularly in geographically diverse
contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Pairwise Learning-To-Rank At Airbnb 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09795v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09795v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malay Haldar, Daochen Zha, Huiji Gao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are three fundamental asks from a ranking algorithm: it should scale to
handle a large number of items, sort items accurately by their utility, and
impose a total order on the items for logical consistency. But here's the
catch-no algorithm can achieve all three at the same time. We call this
limitation the SAT theorem for ranking algorithms. Given the dilemma, how can
we design a practical system that meets user needs? Our current work at Airbnb
provides an answer, with a working solution deployed at scale. We start with
pairwise learning-to-rank (LTR) models-the bedrock of search ranking tech
stacks today. They scale linearly with the number of items ranked and perform
strongly on metrics like NDCG by learning from pairwise comparisons. They are
at a sweet spot of performance vs. cost, making them an ideal choice for
several industrial applications. However, they have a drawback-by ignoring
interactions between items, they compromise on accuracy. To improve accuracy,
we create a "true" pairwise LTR model-one that captures interactions between
items during pairwise comparisons. But accuracy comes at the expense of
scalability and total order, and we discuss strategies to counter these
challenges. For greater accuracy, we take each item in the search result, and
compare it against the rest of the items along two dimensions: (1) Superiority:
How strongly do searchers prefer the given item over the remaining ones? (2)
Similarity: How similar is the given item to all the other items? This forms
the basis of our "all-pairwise" LTR framework, which factors in interactions
across all items at once. Looking at items on the search result page all
together-superiority and similarity combined-gives us a deeper understanding of
what searchers truly want. We quantify the resulting improvements in searcher
experience through offline and online experiments at Airbnb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Yambda-5B -- A Large-Scale Multi-modal <span class="highlight-title">Dataset</span> for Ranking And Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Ploshkin, V. Tytskiy, A. Pismenny, V. Baikalov, E. Taychinov, A. Permiakov, D. Burlakov, E. Krofto, N. Savushkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Yambda-5B, a large-scale open dataset sourced from the Yandex
Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of LLM $\times$ DATA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please refer to the paper list at:
  https://github.com/weAIDB/awesome-data-llm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Resource-Efficient Streaming of Large-Scale Medical Image
  <span class="highlight-title">Dataset</span>s for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Eliot Siegel, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale medical imaging datasets have accelerated deep learning (DL) for
medical image analysis. However, the large scale of these datasets poses a
challenge for researchers, resulting in increased storage and bandwidth
requirements for hosting and accessing them. Since different researchers have
different use cases and require different resolutions or formats for DL, it is
neither feasible to anticipate every researcher's needs nor practical to store
data in multiple resolutions and formats. To that end, we propose the Medical
Image Streaming Toolkit (MIST), a format-agnostic database that enables
streaming of medical images at different resolutions and formats from a single
high-resolution copy. We evaluated MIST across eight popular, large-scale
medical imaging datasets spanning different body parts, modalities, and
formats. Our results showed that our framework reduced the storage and
bandwidth requirements for hosting and downloading datasets without impacting
image quality. We demonstrate that MIST addresses the challenges posed by
large-scale medical imaging datasets by building a data-efficient and
format-agnostic database to meet the diverse needs of researchers and reduce
barriers to DL research in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures, 10 tables, accepted to MIDL'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGA-V2: An End-to-end Generative Framework for Industrial Advertising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17549v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17549v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuowu Zheng, Ze Wang, Fan Yang, Jiangke Fan, Teng Zhang, Yongkang Wang, Xingxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional online industrial advertising systems suffer from the limitations
of multi-stage cascaded architectures, which often discard high-potential
candidates prematurely and distribute decision logic across disconnected
modules. While recent generative recommendation approaches provide end-to-end
solutions, they fail to address critical advertising requirements of key
components for real-world deployment, such as explicit bidding, creative
selection, ad allocation, and payment computation. To bridge this gap, we
introduce End-to-End Generative Advertising (EGA-V2), the first unified
framework that holistically models user interests, point-of-interest (POI) and
creative generation, ad allocation, and payment optimization within a single
generative model. Our approach employs hierarchical tokenization and
multi-token prediction to jointly generate POI recommendations and ad
creatives, while a permutation-aware reward model and token-level bidding
strategy ensure alignment with both user experiences and advertiser objectives.
Additionally, we decouple allocation from payment using a differentiable
ex-post regret minimization mechanism, guaranteeing approximate incentive
compatibility at the POI level. Through extensive offline evaluations we
demonstrate that EGA-V2 significantly outperforms traditional cascaded systems
in both performance and practicality. Our results highlight its potential as a
pioneering fully generative advertising solution, paving the way for
next-generation industrial ad systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Descriptor: C++ Self-Admitted Technical Debt <span class="highlight-title">Dataset</span> (CppSATD) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc Pham, Murali Sridharan, Matteo Esposito, Valentina Lenarduzzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In software development, technical debt (TD) refers to suboptimal
implementation choices made by the developers to meet urgent deadlines and
limited resources, posing challenges for future maintenance. Self-Admitted
Technical Debt (SATD) is a sub-type of TD, representing specific TD instances
``openly admitted'' by the developers and often expressed through source code
comments. Previous research on SATD has focused predominantly on the Java
programming language, revealing a significant gap in cross-language SATD. Such
a narrow focus limits the generalizability of existing findings as well as SATD
detection techniques across multiple programming languages. Our work addresses
such limitation by introducing CppSATD, a dedicated C++ SATD dataset,
comprising over 531,000 annotated comments and their source code contexts. Our
dataset can serve as a foundation for future studies that aim to develop SATD
detection methods in C++, generalize the existing findings to other languages,
or contribute novel insights to cross-language SATD research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecLM: Recommendation Instruction Tuning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed
$\underline{Rec}$ommendation $\underline{L}$anguage $\underline{M}$odel (RecLM)
enhances the capture of user preference diversity through a carefully designed
reinforcement learning reward function that facilitates self-augmentation of
language models. Comprehensive evaluations demonstrate significant advantages
of our approach across various settings, and its plug-and-play compatibility
with state-of-the-art recommender systems results in notable performance
enhancements. The implementation of our RecLM framework is publicly available
at: https://github.com/HKUDS/RecLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CKnowEdit: A New Chinese Knowledge Editing <span class="highlight-title">Dataset</span> for Linguistics,
  Facts, and Logic Error Correction in LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05806v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05806v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese, as a linguistic system rich in depth and complexity, is
characterized by distinctive elements such as ancient poetry, proverbs, idioms,
and other cultural constructs. However, current Large Language Models (LLMs)
face limitations in these specialized domains, highlighting the need for the
development of comprehensive datasets that can assess, continuously update, and
progressively improve these culturally-grounded linguistic competencies through
targeted training optimizations. To address this gap, we introduce CKnowEdit,
the first-ever Chinese knowledge editing dataset designed to correct
linguistic, factual, and logical errors in LLMs. We collect seven types of
knowledge from a wide range of sources, including classical texts, idioms, and
content from Baidu Tieba Ruozhiba, taking into account the unique polyphony,
antithesis, and logical structures inherent in the Chinese language. By
analyzing this dataset, we highlight the challenges current LLMs face in
mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques reveals opportunities to advance the correction of Chinese
knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025; project website is available at
  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Structural-and-Textual Retrieval over Text-rich Graph
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjia Lei, Haoyu Han, Ryan A. Rossi, Franck Dernoncourt, Nedim Lipka, Mahantesh M Halappanavar, Jiliang Tang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for
answering queries by providing textual and structural knowledge. However,
current retrieval methods often retrieve these two types of knowledge in
isolation without considering their mutual reinforcement and some hybrid
methods even bypass structural retrieval entirely after neighboring
aggregation. To fill in this gap, we propose a Mixture of
Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge
via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR
generates textual planning graphs delineating the logic for answering queries.
Following planning graphs, in the Reasoning stage, MoR interweaves structural
traversal and textual matching to obtain candidates from TG-KBs. In the
Organizing stage, MoR further reranks fetched candidates based on their
structural trajectory. Extensive experiments demonstrate the superiority of MoR
in harmonizing structural and textual retrieval with insights, including uneven
retrieving performance across different query logics and the benefits of
integrating structural trajectories for candidate reranking. Our code is
available at https://github.com/Yoega/MoR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyang Liu, Junlin Li, Yinjun Wu, Zhen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Multi-Vector Retrieval (MVR) has achieved the state of the art on
many information retrieval (IR) tasks, its performance highly depends on how to
decompose queries into smaller pieces, say phrases or tokens. However,
optimizing query decomposition for MVR performance is not end-to-end
differentiable. Even worse, jointly solving this problem and training the
downstream retrieval-based systems, say RAG systems could be highly
inefficient. To overcome these challenges, we propose Performance-Oriented
Query Decomposer (POQD), a novel query decomposition framework for MVR. POQD
leverages one LLM for query decomposition and searches the optimal prompt with
an LLM-based optimizer. We further propose an end-to-end training algorithm to
alternatively optimize the prompt for query decomposition and the downstream
models. This algorithm can achieve superior MVR performance at a reasonable
training cost as our theoretical analysis suggests. POQD can be integrated
seamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented
Generation (RAG) systems. Extensive empirical studies on representative
RAG-based QA tasks show that POQD outperforms existing query decomposition
strategies in both retrieval performance and end-to-end QA accuracy. POQD is
available at https://github.com/PKU-SDS-lab/POQD-ICML25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iola Walker: A Mobile Footfall Detection System for Music Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Will James
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project is the first of several experiments composing music that changes
in response to biosignals. The system is dubbed "iola walker" in reference to a
common polyrhythm, the hemiola. A listener goes for a walk, and the Iola Walker
app detects their walking pace. Iola Walker picks up footfalls using a
foot-mounted accelerometer, processing the signals in real time using a
recurrent neural network in an Android app. The Android app outputs a MIDI
event for each footfall. The iola walker player, which might be a VST running
in a DAW, plays the version of the next music passage with underlying
polyrhythms closest to the listener's walking pace.
  This paper documents the process of training the model to detect the
footfalls in real time. The model is trained on accelerometer data from an
Mbient Labs foot-mounted IMU at 200~Hz, with the ground truth for footfalls
annotated by pressing the volume-up button on the Android device when the foot
hits the ground. To collect training data, I walked around my neighborhood
clicking the volume-up button each time my foot hit the ground. Several methods
were tried for detecting footfalls in real time from sensor data, including
ones based on digital signal processing techniques and traditional machine
learning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengze Li, Yangle Liu, Jieming Ma, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camera Trajectory Generation: A Comprehensive <span class="highlight-title">Survey</span> of Methods,
  Metrics, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Dehghanian, Pouya Ardekhani, Amir Vahedi, Hamid Beigy, Hamid R. Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera trajectory generation is a cornerstone in computer graphics, robotics,
virtual reality, and cinematography, enabling seamless and adaptive camera
movements that enhance visual storytelling and immersive experiences. Despite
its growing prominence, the field lacks a systematic and unified survey that
consolidates essential knowledge and advancements in this domain. This paper
addresses this gap by providing the first comprehensive review of the field,
covering from foundational definitions to advanced methodologies. We introduce
the different approaches to camera representation and present an in-depth
review of available camera trajectory generation models, starting with
rule-based approaches and progressing through optimization-based techniques,
machine learning advancements, and hybrid methods that integrate multiple
strategies. Additionally, we gather and analyze the metrics and datasets
commonly used for evaluating camera trajectory systems, offering insights into
how these tools measure performance, aesthetic quality, and practical
applicability. Finally, we highlight existing limitations, critical gaps in
current research, and promising opportunities for investment and innovation in
the field. This paper not only serves as a foundational resource for
researchers entering the field but also paves the way for advancing adaptive,
efficient, and creative camera trajectory systems across diverse applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiverse Through Deepfakes: The MultiFakeVerse <span class="highlight-title">Dataset</span> of
  Person-Centric Visual and Conceptual Manipulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parul Gupta, Shreya Ghosh, Tom Gedeon, Thanh-Toan Do, Abhinav Dhall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of GenAI technology over the past few years has
significantly contributed towards highly realistic deepfake content generation.
Despite ongoing efforts, the research community still lacks a large-scale and
reasoning capability driven deepfake benchmark dataset specifically tailored
for person-centric object, context and scene manipulations. In this paper, we
address this gap by introducing MultiFakeVerse, a large scale person-centric
deepfake dataset, comprising 845,286 images generated through manipulation
suggestions and image manipulations both derived from vision-language models
(VLM). The VLM instructions were specifically targeted towards modifications to
individuals or contextual elements of a scene that influence human perception
of importance, intent, or narrative. This VLM-driven approach enables semantic,
context-aware alterations such as modifying actions, scenes, and human-object
interactions rather than synthetic or low-level identity swaps and
region-specific edits that are common in existing datasets. Our experiments
reveal that current state-of-the-art deepfake detection models and human
observers struggle to detect these subtle yet meaningful manipulations. The
code and dataset are available on
\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG
  Alignment via Large Language Model and Contrastive Learning on ChineseEEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one
of the earliest open-vocabulary EEG-to-text generation frameworks tailored for
Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact
pretrained language model (MiniLM), our architecture aligns multichannel brain
signals with natural language representations via masked pretraining and
contrastive learning. Using a subset of the ChineseEEG dataset, where each
sentence contains approximately ten Chinese characters aligned with 128-channel
EEG recorded at 256 Hz, we segment EEG into per-character embeddings and
predict full sentences in a zero-shot setting. The decoder is trained with
teacher forcing and padding masks to accommodate variable-length sequences.
Evaluation on over 1,500 training-validation sentences and 300 held-out test
samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%.
While syntactic fluency remains a challenge, our findings demonstrate the
feasibility of non-phonetic, cross-modal language decoding from EEG. This work
opens a new direction in multilingual brain-to-text research and lays the
foundation for future cognitive-language interfaces in Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak
  Threats for Large Audio Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Cheng, Erjia Xiao, Jing Shao, Yichi Wang, Le Yang, Chao Shen, Philip Torr, Jindong Gu, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate impressive zero-shot performance
across a wide range of natural language processing tasks. Integrating various
modality encoders further expands their capabilities, giving rise to Multimodal
Large Language Models (MLLMs) that process not only text but also visual and
auditory modality inputs. However, these advanced capabilities may also pose
significant security risks, as models can be exploited to generate harmful or
inappropriate content through jailbreak attack. While prior work has
extensively explored how manipulating textual or visual modality inputs can
circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific
Jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored.
To address this gap, we introduce \textbf{Jailbreak-AudioBench}, which consists
of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox
supports not only text-to-audio conversion but also various editing techniques
for injecting audio hidden semantics. The curated Dataset provides diverse
explicit and implicit jailbreak audio examples in both original and edited
forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and
establish the most comprehensive Jailbreak benchmark to date for audio
modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing
future research on LALMs safety alignment by enabling the in-depth exposure of
more powerful jailbreak threats, such as query-based audio editing, and by
facilitating the development of effective defense mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADS-Edit: A Multimodal Knowledge Editing <span class="highlight-title">Dataset</span> for Autonomous Driving
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Wang, Jizhan Fang, Xiang Chen, Bozhong Tian, Ziwen Xu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Multimodal Models (LMMs) have shown promise in
Autonomous Driving Systems (ADS). However, their direct application to ADS is
hindered by challenges such as misunderstanding of traffic knowledge, complex
road conditions, and diverse states of vehicle. To address these challenges, we
propose the use of Knowledge Editing, which enables targeted modifications to a
model's behavior without the need for full retraining. Meanwhile, we introduce
ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,
which includes various real-world scenarios, multiple data types, and
comprehensive evaluation metrics. We conduct comprehensive experiments and
derive several interesting conclusions. We hope that our work will contribute
to the further advancement of knowledge editing applications in the field of
autonomous driving. Code and data are available in
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-31T00:00:00Z">2025-05-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pitfalls in Evaluating Language Model Forecasters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Paleka, Shashwat Goel, Jonas Geiping, Florian Tramèr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently been applied to forecasting tasks,
with some works claiming these systems match or exceed human performance. In
this paper, we argue that, as a community, we should be careful about such
conclusions as evaluating LLM forecasters presents unique challenges. We
identify two broad categories of issues: (1) difficulty in trusting evaluation
results due to many forms of temporal leakage, and (2) difficulty in
extrapolating from evaluation performance to real-world forecasting. Through
systematic analysis and concrete examples from prior work, we demonstrate how
evaluation flaws can raise concerns about current and future performance
claims. We argue that more rigorous evaluation methodologies are needed to
confidently assess the forecasting abilities of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Question Semantic Space for Dynamic Retrieval-Augmented
  Multi-hop Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Ye, Lang Yu, Zhikai Lei, Qin Chen, Jie Zhou, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is usually integrated into large
language models (LLMs) to mitigate hallucinations and knowledge obsolescence.
Whereas,conventional one-step retrieve-and-read methods are insufficient for
multi-hop question answering, facing challenges of retrieval semantic
mismatching and the high cost in handling interdependent subquestions. In this
paper, we propose Optimizing Question Semantic Space for Dynamic
Retrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of
three key modules: (1) the Question Decomposition Module (QDM), which
decomposes multi-hop questions into fine-grained subquestions; (2) the
Subquestion Dependency Optimizer Module (SDOM), which models the interdependent
relations of subquestions for better understanding; and (3) the Dynamic Passage
Retrieval Module (DPRM), which aligns subquestions with relevant passages by
optimizing the semantic embeddings. Experimental results across various
benchmarks demonstrate that Q-DREAM significantly outperforms existing RAG
methods, achieving state-of-the-art performance in both in-domain and
out-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency
while maintaining high accuracy compared with recent baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DV365: Extremely Long User History Modeling at Instagram <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Lyu, Devashish Tyagi, Yihang Yang, Ziwei Li, Ajay Somani, Karthikeyan Shanmugasundaram, Nikola Andrejevic, Ferdi Adeputra, Curtis Zeng, Arun K. Singh, Maxime Ransan, Sagar Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long user history is highly valuable signal for recommendation systems, but
effectively incorporating it often comes with high cost in terms of data center
power consumption and GPU. In this work, we chose offline embedding over
end-to-end sequence length optimization methods to enable extremely long user
sequence modeling as a cost-effective solution, and propose a new user
embedding learning strategy, multi-slicing and summarization, that generates
highly generalizable user representation of user's long-term stable interest.
History length we encoded in this embedding is up to 70,000 and on average
40,000. This embedding, named as DV365, is proven highly incremental on top of
advanced attentive user sequence models deployed in Instagram. Produced by a
single upstream foundational model, it is launched in 15 different models
across Instagram and Threads with significant impact, and has been production
battle-proven for >1 year since our first launch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGKDD 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K-order Ranking Preference Optimization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Cai, Chongming Gao, Yang Zhang, Wentao Shi, Jizhi Zhang, Keqin Bao, Qifan Wang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To adapt large language models (LLMs) to ranking tasks, existing list-wise
methods, represented by list-wise Direct Preference Optimization (DPO), focus
on optimizing partial-order or full-order list ranking consistency for LLMs to
enhance their ranking abilities. However, we argue that optimizing top-K
ranking consistency could be more appropriate for real-world applications.
There are two main reasons: (1) users are typically concerned with only the
top-K results, making top-K ranking more important, and (2) tail items often
lack precise feedback, making top-K ranking more reliable. Based on this, we
propose K-order Ranking Preference Optimization (KPO) by extending the DPO's
Plackett-Luce model to accommodate top-K rankings. Additionally, recognizing
that the number of important items can vary across queries, we extend KPO to
dynamically determine appropriate K for different samples and introduce a
curriculum learning strategy to boost training efficiency. Extensive
experiments demonstrate the effectiveness of KPO, highlighting its high sample
efficiency and robustness to noise. The code is available at
https://github.com/Lanyu0303/KPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting General-Purpose Embedding Models to Private <span class="highlight-title">Dataset</span>s Using
  Keyword-based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubai Wei, Jiale Han, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models play a cornerstone role in AI applications, such as
retrieval-augmented generation (RAG). While general-purpose text embedding
models demonstrate strong performance on generic retrieval benchmarks, their
effectiveness diminishes when applied to private datasets (e.g.,
company-specific proprietary data), which often contain specialized terminology
and lingo. In this work, we introduce BMEmbed, a novel method for adapting
general-purpose text embedding models to private datasets. By leveraging the
well-established keyword-based retrieval technique (BM25), we construct
supervisory signals from the ranking of keyword-based retrieval results to
facilitate model adaptation. We evaluate BMEmbed across a range of domains,
datasets, and models, showing consistent improvements in retrieval performance.
Moreover, we provide empirical insights into how BM25-based signals contribute
to improving embeddings by fostering alignment and uniformity, highlighting the
value of this approach in adapting models to domain-specific data. We release
the source code available at https://github.com/BaileyWei/BMEmbed for the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://github.com/BaileyWei/BMEmbed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Refine: Enhancing Few-Shot Performance on Low-Resource Indic
  Languages with Example Selection from Related Example Banks <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated impressive few-shot
learning capabilities through in-context learning (ICL). However, ICL
performance is highly dependent on the choice of few-shot demonstrations,
making the selection of the most optimal examples a persistent research
challenge. This issue is further amplified in low-resource Indic languages,
where the scarcity of ground-truth data complicates the selection process. In
this work, we propose PromptRefine, a novel Alternating Minimization approach
for example selection that improves ICL performance on low-resource Indic
languages. PromptRefine leverages auxiliary example banks from related
high-resource Indic languages and employs multi-task learning techniques to
align language-specific retrievers, enabling effective cross-language
retrieval. Additionally, we incorporate diversity in the selected examples to
enhance generalization and reduce bias. Through comprehensive evaluations on
four text generation tasks -- Cross-Lingual Question Answering, Multilingual
Question Answering, Machine Translation, and Cross-Lingual Summarization using
state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and
Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms
existing frameworks for retrieving examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ It's High Time: A <span class="highlight-title">Survey</span> of Temporal Information Retrieval and Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Avishek Anand, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time plays a critical role in how information is generated, retrieved, and
interpreted. In this survey, we provide a comprehensive overview of Temporal
Information Retrieval and Temporal Question Answering, two research areas aimed
at handling and understanding time-sensitive information. As the amount of
time-stamped content from sources like news articles, web archives, and
knowledge bases increases, systems must address challenges such as detecting
temporal intent, normalizing time expressions, ordering events, and reasoning
over evolving or ambiguous facts. These challenges are critical across many
dynamic and time-sensitive domains, from news and encyclopedias to science,
history, and social media. We review both traditional approaches and modern
neural methods, including those that use transformer models and Large Language
Models (LLMs). We also review recent advances in temporal language modeling,
multi-hop reasoning, and retrieval-augmented generation (RAG), alongside
benchmark datasets and evaluation strategies that test temporal robustness,
recency awareness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Multi-Hop Document Retrieval Through Intermediate
  Representations <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaen Lin, Jingyu Liu, Yingbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) encounters challenges when addressing
complex queries, particularly multi-hop questions. While several methods tackle
multi-hop queries by iteratively generating internal queries and retrieving
external documents, these approaches are computationally expensive. In this
paper, we identify a three-stage information processing pattern in LLMs during
layer-by-layer reasoning, consisting of extraction, processing, and subsequent
extraction steps. This observation suggests that the representations in
intermediate layers contain richer information compared to those in other
layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike
prior methods that focus on generating new internal queries, L-RAG leverages
intermediate representations from the middle layers, which capture next-hop
information, to retrieve external knowledge. L-RAG achieves performance
comparable to multi-step approaches while maintaining inference overhead
similar to that of standard RAG. Experimental results show that L-RAG
outperforms existing RAG methods on open-domain multi-hop question-answering
datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is
available in https://anonymous.4open.science/r/L-RAG-ADD5/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TestNUC: Enhancing Test-Time Computing Approaches and Scaling through
  Neighboring Unlabeled Data Consistency <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time computing approaches, which leverage additional computational
resources during inference, have been proven effective in enhancing large
language model performance. This work introduces a novel, linearly scaling
approach, TestNUC, that improves test-time predictions by leveraging the local
consistency of neighboring unlabeled data-it classifies an input instance by
considering not only the model's prediction on that instance but also on
neighboring unlabeled instances. We evaluate TestNUC across eight diverse
datasets, spanning intent classification, topic mining, domain discovery, and
emotion detection, demonstrating its consistent superiority over baseline
methods such as standard prompting and self-consistency. Furthermore, TestNUC
can be seamlessly integrated with existing test-time computing approaches,
substantially boosting their performance. Our analysis reveals that TestNUC
scales effectively with increasing amounts of unlabeled data and performs
robustly across different embedding models, making it practical for real-world
applications. Our code is available at https://github.com/HenryPengZou/TestNUC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Generation In Large Model Era: A <span class="highlight-title">Survey</span> <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Jinghao Zhang, Alireza Salemi, Xinting Hu, Wenjie Wang, Fuli Feng, Hamed Zamani, Xiangnan He, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large models, content generation is gradually shifting to
Personalized Generation (PGen), tailoring content to individual preferences and
needs. This paper presents the first comprehensive survey on PGen,
investigating existing research in this rapidly growing field. We conceptualize
PGen from a unified perspective, systematically formalizing its key components,
core objectives, and abstract workflows. Based on this unified perspective, we
propose a multi-level taxonomy, offering an in-depth review of technical
advancements, commonly used datasets, and evaluation metrics across multiple
modalities, personalized contexts, and tasks. Moreover, we envision the
potential applications of PGen and highlight open challenges and promising
directions for future exploration. By bridging PGen research across multiple
modalities, this survey serves as a valuable resource for fostering knowledge
sharing and interdisciplinary collaboration, ultimately contributing to a more
personalized digital landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExPerT: Effective and Explainable Evaluation of Personalized Long-Form
  Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Julian Killingback, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating personalized text generated by large language models (LLMs) is
challenging, as only the LLM user, i.e., prompt author, can reliably assess the
output, but re-engaging the same individuals across studies is infeasible. This
paper addresses the challenge of evaluating personalized text generation by
introducing ExPerT, an explainable reference-based evaluation framework. ExPerT
leverages an LLM to extract atomic aspects and their evidence from the
generated and reference texts, match the aspects, and evaluate their alignment
based on content and writing style -- two key attributes in personalized text
generation. Additionally, ExPerT generates detailed, fine-grained explanations
for every step of the evaluation process, enhancing transparency and
interpretability. Our experiments demonstrate that ExPerT achieves a 7.2%
relative improvement in alignment with human judgments compared to the
state-of-the-art text generation evaluation methods. Furthermore, human
evaluators rated the usability of ExPerT's explanations at 4.7 out of 5,
highlighting its effectiveness in making evaluation decisions more
interpretable.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene Detection Policies and Keyframe Extraction Strategies for
  Large-Scale Video Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilii Korolkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust scene segmentation and keyframe extraction are essential preprocessing
steps in video understanding pipelines, supporting tasks such as indexing,
summarization, and semantic retrieval. However, existing methods often lack
generalizability across diverse video types and durations. We present a
unified, adaptive framework for automatic scene detection and keyframe
selection that handles formats ranging from short-form media to long-form
films, archival content, and surveillance footage. Our system dynamically
selects segmentation policies based on video length: adaptive thresholding for
short videos, hybrid strategies for mid-length ones, and interval-based
splitting for extended recordings. This ensures consistent granularity and
efficient processing across domains. For keyframe selection, we employ a
lightweight module that scores sampled frames using a composite metric of
sharpness, luminance, and temporal spread, avoiding complex saliency models
while ensuring visual relevance. Designed for high-throughput workflows, the
system is deployed in a commercial video analysis platform and has processed
content from media, education, research, and security domains. It offers a
scalable and interpretable solution suitable for downstream applications such
as UI previews, embedding pipelines, and content filtering. We discuss
practical implementation details and outline future enhancements, including
audio-aware segmentation and reinforcement-learned frame scoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures, submitted as a preprint. ArXiv preprint only,
  not submitted to a journal yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEED: A Benchmark <span class="highlight-title">Dataset</span> for Sequential Facial Attribute Editing with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yule Zhu, Ping Liu, Zhedong Zheng, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently enabled precise and photorealistic facial
editing across a wide range of semantic attributes. Beyond single-step
modifications, a growing class of applications now demands the ability to
analyze and track sequences of progressive edits, such as stepwise changes to
hair, makeup, or accessories. However, sequential editing introduces
significant challenges in edit attribution and detection robustness, further
complicated by the lack of large-scale, finely annotated benchmarks tailored
explicitly for this task. We introduce SEED, a large-scale Sequentially Edited
facE Dataset constructed via state-of-the-art diffusion models. SEED contains
over 90,000 facial images with one to four sequential attribute modifications,
generated using diverse diffusion-based editing pipelines (LEdits, SDXL, SD3).
Each image is annotated with detailed edit sequences, attribute masks, and
prompts, facilitating research on sequential edit tracking, visual provenance
analysis, and manipulation robustness assessment. To benchmark this task, we
propose FAITH, a frequency-aware transformer-based model that incorporates
high-frequency cues to enhance sensitivity to subtle sequential changes.
Comprehensive experiments, including systematic comparisons of multiple
frequency-domain methods, demonstrate the effectiveness of FAITH and the unique
challenges posed by SEED. SEED offers a challenging and flexible resource for
studying progressive diffusion-based edits at scale. Dataset and code will be
publicly released at: https://github.com/Zeus1037/SEED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixing Yang, Xulong Tang, Ziqiao Peng, Yuxuan Hu, Jun He, Hongyan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music-driven 3D dance generation has attracted increasing attention in recent
years, with promising applications in choreography, virtual reality, and
creative content creation. Previous research has generated promising realistic
dance movement from audio signals. However, traditional methods underutilize
genre conditioning, often treating it as auxiliary modifiers rather than core
semantic drivers. This oversight compromises music-motion synchronization and
disrupts dance genre continuity, particularly during complex rhythmic
transitions, thereby leading to visually unsatisfactory effects. To address the
challenge, we propose MEGADance, a novel architecture for music-driven 3D dance
generation. By decoupling choreographic consistency into dance generality and
genre specificity, MEGADance demonstrates significant dance quality and strong
genre controllability. It consists of two stages: (1) High-Fidelity Dance
Quantization Stage (HFDQ), which encodes dance motions into a latent
representation by Finite Scalar Quantization (FSQ) and reconstructs them with
kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage
(GADG), which maps music into the latent representation by synergistic
utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid
backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate
the state-of-the-art performance of MEGADance both qualitatively and
quantitatively. Code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2505.14222</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Many Challenges of Human-Like Agents in Virtual Game Environments <span class="chip">AAMAS-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Swiechowski, Dominik Slezak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-like agents are an increasingly important topic in games and beyond.
Believable non-player characters enhance the gaming experience by improving
immersion and providing entertainment. They also offer players the opportunity
to engage with AI entities that can function as opponents, teachers, or
cooperating partners. Additionally, in games where bots are prohibited -- and
even more so in non-game environments -- there is a need for methods capable of
identifying whether digital interactions occur with bots or humans. This leads
to two fundamental research questions: (1) how to model and implement
human-like AI, and (2) how to measure its degree of human likeness.
  This article offers two contributions. The first one is a survey of the most
significant challenges in implementing human-like AI in games (or any virtual
environment featuring simulated agents, although this article specifically
focuses on games). Thirteen such challenges, both conceptual and technical, are
discussed in detail. The second is an empirical study performed in a tactical
video game that addresses the research question: "Is it possible to distinguish
human players from bots (AI agents) based on empirical data?" A
machine-learning approach using a custom deep recurrent convolutional neural
network is presented. We hypothesize that the more challenging it is to create
human-like AI for a given game, the easier it becomes to develop a method for
distinguishing humans from AI-driven players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the 24th International Conference on Autonomous
  Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23,
  Detroit, Michigan, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co-Reinforcement Learning for Unified Multimodal Understanding and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a pioneering exploration of reinforcement learning (RL)
via group relative policy optimization for unified multimodal large language
models (ULMs), aimed at simultaneously reinforcing generation and understanding
capabilities. Through systematic pilot studies, we uncover the significant
potential of ULMs to enable the synergistic co-evolution of dual capabilities
within a shared policy optimization framework. Building on this insight, we
introduce CoRL, a co-reinforcement learning framework comprising a unified RL
stage for joint optimization and a refined RL stage for task-specific
enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves
average improvements of 7% on three text-to-image generation datasets and 23%
on nine multimodal understanding benchmarks. These results demonstrate the
effectiveness of CoRL and highlight the substantial benefit of reinforcement
learning in facilitating cross-task synergy and optimization for ULMs. Code is
available at https://github.com/mm-vl/ULM-R1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Driven Multimodal Feature Learning Framework for Apparent
  Personality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangsheng Wang, Chengwei Ye, Huanzhen Zhang, Linuo Xu, Shuyan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting personality traits automatically has become a challenging problem
in computer vision. This paper introduces an innovative multimodal feature
learning framework for personality analysis in short video clips. For visual
processing, we construct a facial graph and design a Geo-based two-stream
network incorporating an attention mechanism, leveraging both Graph
Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture
static facial expressions. Additionally, ResNet18 and VGGFace networks are
employed to extract global scene and facial appearance features at the frame
level. To capture dynamic temporal information, we integrate a BiGRU with a
temporal attention module for extracting salient frame representations. To
enhance the model's robustness, we incorporate the VGGish CNN for audio-based
features and XLM-Roberta for text-based features. Finally, a multimodal channel
attention mechanism is introduced to integrate different modalities, and a
Multi-Layer Perceptron (MLP) regression model is used to predict personality
traits. Experimental results confirm that our proposed framework surpasses
existing state-of-the-art approaches in performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article contains serious scientific errors and cannot be
  corrected by updating the preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-30T00:00:00Z">2025-05-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">27</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACE: A Fine-grained Reference Free Evaluator for Conversational
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hideaki Joko, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A systematic, reliable, and low-cost evaluation of Conversational Recommender
Systems (CRSs) remains an open challenge. Existing automatic CRS evaluation
methods are proven insufficient for evaluating the dynamic nature of
recommendation conversations. This work proposes FACE: a Fine-grained,
Aspect-based Conversation Evaluation method that provides evaluation scores for
diverse turn and dialogue level qualities of recommendation conversations. FACE
is reference-free and shows strong correlation with human judgments, achieving
system correlation of 0.9 and turn/dialogue-level of 0.5, outperforming
state-of-the-art CRS evaluation methods by a large margin. Additionally, unlike
existing LLM-based methods that provide single uninterpretable scores, FACE
provides insights into the system performance and enables identifying and
locating problems within conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer
  Mapping for Natural Disaster Damage Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Karimi, Maryam Rahnemoonfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural disasters usually affect vast areas and devastate infrastructures.
Performing a timely and efficient response is crucial to minimize the impact on
affected communities, and data-driven approaches are the best choice. Visual
question answering (VQA) models help management teams to achieve in-depth
understanding of damages. However, recently published models do not possess the
ability to answer open-ended questions and only select the best answer among a
predefined list of answers. If we want to ask questions with new additional
possible answers that do not exist in the predefined list, the model needs to
be fin-tuned/retrained on a new collected and annotated dataset, which is a
time-consuming procedure. In recent years, large-scale Vision-Language Models
(VLMs) have earned significant attention. These models are trained on extensive
datasets and demonstrate strong performance on both unimodal and multimodal
vision/language downstream tasks, often without the need for fine-tuning. In
this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and
investigate the performance of on post-disaster FloodNet dataset. Since the
proposed method takes advantage of zero-shot learning, it can be applied on new
datasets without fine-tuning. In addition, ZeShot-VQA is able to process and
generate answers that has been not seen during the training procedure, which
demonstrates its flexibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2025 IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curate, Connect, Inquire: A System for Findable Accessible Interoperable
  and Reusable (FAIR) Human-Robot Centered <span class="highlight-title">Dataset</span>s <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingru Zhou, Sadanand Modak, Yao-Cheng Chan, Zhiyun Deng, Luis Sentis, Maria Esteva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of AI in robotics has amplified the need for high-quality,
reusable datasets, particularly in human-robot interaction (HRI) and
AI-embedded robotics. While more robotics datasets are being created, the
landscape of open data in the field is uneven. This is due to a lack of
curation standards and consistent publication practices, which makes it
difficult to discover, access, and reuse robotics data. To address these
challenges, this paper presents a curation and access system with two main
contributions: (1) a structured methodology to curate, publish, and integrate
FAIR (Findable, Accessible, Interoperable, Reusable) human-centered robotics
datasets; and (2) a ChatGPT-powered conversational interface trained with the
curated datasets metadata and documentation to enable exploration, comparison
robotics datasets and data retrieval using natural language. Developed based on
practical experience curating datasets from robotics labs within Texas Robotics
at the University of Texas at Austin, the system demonstrates the value of
standardized curation and persistent publication of robotics data. The system's
evaluation suggests that access and understandability of human-robotics data
are significantly improved. This work directly aligns with the goals of the
HCRL @ ICRA 2025 workshop and represents a step towards more human-centered
access to data for embodied AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages (excluding references), 8 pages (including references); 5
  figures; accepted to the ICRA 2025 Workshop on Human-Centered Robot Learning
  in the Era of Big Data and Large Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The World As Large Language Models See It: Exploring the reliability of
  LLMs in representing geographical features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omid Reza Abbasi, Franz Welscher, Georg Weinberger, Johannes Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to evolve, questions about their
trustworthiness in delivering factual information have become increasingly
important. This concern also applies to their ability to accurately represent
the geographic world. With recent advancements in this field, it is relevant to
consider whether and to what extent LLMs' representations of the geographical
world can be trusted. This study evaluates the performance of GPT-4o and Gemini
2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and
reverse geocoding. In the geocoding task, both models exhibited systematic and
random errors in estimating the coordinates of St. Anne's Column in Innsbruck,
Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash
demonstrating more precision but a significant systematic offset. For elevation
estimation, both models tended to underestimate elevations across Austria,
though they captured overall topographical trends, and Gemini 2.0 Flash
performed better in eastern regions. The reverse geocoding task, which involved
identifying Austrian federal states from coordinates, revealed that Gemini 2.0
Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating
better consistency across regions. Despite these findings, neither model
achieved an accurate reconstruction of Austria's federal states, highlighting
persistent misclassifications. The study concludes that while LLMs can
approximate geographic information, their accuracy and reliability are
inconsistent, underscoring the need for fine-tuning with geographical
information to enhance their utility in GIScience and Geoinformatics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Retrieval Strategies to Capture Interdisciplinary Scientific
  Research: A Bibliometric Evaluation of the Integration of Neuroscience and
  Computer Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malena Mendez Isla, Agustin Mauro, Diego Kozlowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interdisciplinary scientific research is increasingly important in knowledge
production, funding policies, and academic discussions on scholarly
communication. While many studies focus on interdisciplinary corpora defined a
priori - usually through keyword-based searches within assumed
interdisciplinary domains - few explore interdisciplinarity as an emergent
intersection between two distinct fields. Thus, methodological proposals for
building databases at the intersection of two fields of knowledge are scarce.
The goal of this article is to develop and compare different strategies for
defining an interdisciplinary corpus between two bodies of knowledge. As a case
study, we focus on the intersection between neuroscience and computer science.
To this end, we develop and compare four retrieval strategies, two of them
based on keywords and two based on citation and reference patterns. Our results
show that keyword-based strategies provide both better precision and recall.
While we focus on comparing strategies for the study of the intersection
between the fields of neuroscience and computer science, this proposed
methodological reflection is applicable to a wide range of interdisciplinary
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaMP-QA: A Benchmark for Personalized Long-form Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization is essential for question answering systems that are
user-centric. Despite its importance, personalization in answer generation has
been relatively underexplored. This is mainly due to lack of resources for
training and evaluating personalized question answering systems. We address
this gap by introducing LaMP-QA -- a benchmark designed for evaluating
personalized long-form answer generation. The benchmark covers questions from
three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal
Development, and (3) Society & Culture, encompassing over 45 subcategories in
total. To assess the quality and potential impact of the LaMP-QA benchmark for
personalized question answering, we conduct comprehensive human and automatic
evaluations, to compare multiple evaluation strategies for evaluating generated
personalized responses and measure their alignment with human preferences.
Furthermore, we benchmark a number of non-personalized and personalized
approaches based on open-source and proprietary large language models (LLMs).
Our results show that incorporating the personalized context provided leads to
performance improvements of up to 39%. The benchmark is publicly released to
support future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gated Multimodal Graph Learning for Personalized Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibei Liu, Yuanzhe Zhang, Xiang Li, Yunbo Liu, Chengwei Feng, Hao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommendation has emerged as a promising solution to alleviate
the cold-start and sparsity problems in collaborative filtering by
incorporating rich content information, such as product images and textual
descriptions. However, effectively integrating heterogeneous modalities into a
unified recommendation framework remains a challenge. Existing approaches often
rely on fixed fusion strategies or complex architectures , which may fail to
adapt to modality quality variance or introduce unnecessary computational
overhead.
  In this work, we propose RLMultimodalRec, a lightweight and modular
recommendation framework that combines graph-based user modeling with adaptive
multimodal item encoding. The model employs a gated fusion module to
dynamically balance the contribution of visual and textual modalities, enabling
fine-grained and content-aware item representations. Meanwhile, a two-layer
LightGCN encoder captures high-order collaborative signals by propagating
embeddings over the user-item interaction graph without relying on nonlinear
transformations.
  We evaluate our model on a real-world dataset from the Amazon product domain.
Experimental results demonstrate that RLMultimodalRec consistently outperforms
several competitive baselines, including collaborative filtering, visual-aware,
and multimodal GNN-based methods. The proposed approach achieves significant
improvements in top-K recommendation metrics while maintaining scalability and
interpretability, making it suitable for practical deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context is Gold to find the Gold Passage: Evaluating and Training
  Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Conti, Manuel Faysse, Gautier Viaud, Antoine Bosselut, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A limitation of modern document retrieval embedding methods is that they
typically encode passages (chunks) from the same documents independently, often
overlooking crucial contextual information from the rest of the document that
could greatly improve individual chunk representations.
  In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a
benchmark designed to evaluate retrieval models on their ability to leverage
document-wide context. Our results show that state-of-the-art embedding models
struggle in retrieval scenarios where context is required. To address this
limitation, we propose InSeNT (In-sequence Negative Training), a novel
contrastive post-training approach which combined with late chunking pooling
enhances contextual representation learning while preserving computational
efficiency. Our method significantly improves retrieval quality on ConTEB
without sacrificing base model performance. We further find chunks embedded
with our method are more robust to suboptimal chunking strategies and larger
retrieval corpus sizes. We open-source all artifacts at
https://github.com/illuin-tech/contextual-embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding
  based on Guided Space Transformation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingchaojie Feng, Yiqun Sun, Yandong Sun, Minfeng Zhu, Qiang Huang, Anthony K. H. Tung, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate an important task named instruction-following
text embedding, which generates dynamic text embeddings that adapt to user
instructions, highlighting specific attributes of text. Despite recent
advancements, existing approaches suffer from significant computational
overhead, as they require re-encoding the entire corpus for each new
instruction. To address this challenge, we propose GSTransform, a novel
instruction-following text embedding framework based on Guided Space
Transformation. Our key observation is that instruction-relevant information is
inherently encoded in generic embeddings but remains underutilized. Instead of
repeatedly encoding the corpus for each instruction, GSTransform is a
lightweight transformation mechanism that adapts pre-computed embeddings in
real time to align with user instructions, guided by a small amount of text
data with instruction-focused label annotation. We conduct extensive
experiments on three instruction-awareness downstream tasks across nine
real-world datasets, demonstrating that GSTransform improves
instruction-following text embedding quality over state-of-the-art methods
while achieving dramatic speedups of 6~300x in real-time processing on
large-scale datasets. The source code is available at
https://github.com/YingchaojieFeng/GSTransform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Discrete Memristor-Coupled Heterogeneous Dual-Neuron Model and
  Its Application in Multi-Scenario Image Encryption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zou, Mengjiao Wang, Xinan Zhang, Herbert Ho-Ching Iu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating brain functions using neural networks is an important area of
research. Recently, discrete memristor-coupled neurons have attracted
significant attention, as memristors effectively mimic synaptic behavior, which
is essential for learning and memory. This highlights the biological relevance
of such models. This study introduces a discrete memristive heterogeneous
dual-neuron network (MHDNN). The stability of the MHDNN is analyzed with
respect to initial conditions and a range of neuronal parameters. Numerical
simulations demonstrate complex dynamical behaviors. Various neuronal firing
patterns are investigated under different coupling strengths, and
synchronization phenomena between neurons are explored. The MHDNN is
implemented and validated on the STM32 hardware platform. An image encryption
algorithm based on the MHDNN is proposed, along with two hardware platforms
tailored for multi-scenario police image encryption. These solutions enable
real-time and secure transmission of police data in complex environments,
reducing hacking risks and enhancing system security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE INTERNET OF THINGS JOURNAL(IOTJ)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Scaling of Robustness and Effectiveness in Dense Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness and Effectiveness are critical aspects of developing dense
retrieval models for real-world applications. It is known that there is a
trade-off between the two. Recent work has addressed scaling laws of
effectiveness in dense retrieval, revealing a power-law relationship between
effectiveness and the size of models and data. Does robustness follow scaling
laws too? If so, can scaling improve both robustness and effectiveness
together, or do they remain locked in a trade-off?
  To answer these questions, we conduct a comprehensive experimental study. We
find that:(i) Robustness, including out-of-distribution and adversarial
robustness, also follows a scaling law.(ii) Robustness and effectiveness
exhibit different scaling patterns, leading to significant resource costs when
jointly improving both. Given these findings, we shift to the third factor that
affects model performance, namely the optimization strategy, beyond the model
size and data size. We find that: (i) By fitting different optimization
strategies, the joint performance of robustness and effectiveness traces out a
Pareto frontier. (ii) When the optimization strategy strays from Pareto
efficiency, the joint performance scales in a sub-optimal direction. (iii) By
adjusting the optimization weights to fit the Pareto efficiency, we can achieve
Pareto training, where the scaling of joint performance becomes most efficient.
Even without requiring additional resources, Pareto training is comparable to
the performance of scaling resources several times under optimization
strategies that overly prioritize either robustness or effectiveness. Finally,
we demonstrate that our findings can help deploy dense retrieval models in
real-world applications that scale efficiently and are balanced for robustness
and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGS3: A Multi-Granularity <span class="highlight-title">Self-Supervised</span> Code Search Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Junfeng Kang, Qi Liu, Liyang He, Zheng Zhang, Yunhao Sha, Linbo Zhu, Zhenya Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of enhancing software reusability and developer productivity,
code search has emerged as a key area, aimed at retrieving code snippets
relevant to functionalities based on natural language queries. Despite
significant progress in self-supervised code pre-training utilizing the vast
amount of code data in repositories, existing methods have primarily focused on
leveraging contrastive learning to align natural language with function-level
code snippets. These studies have overlooked the abundance of fine-grained
(such as block-level and statement-level) code snippets prevalent within the
function-level code snippets, which results in suboptimal performance across
all levels of granularity. To address this problem, we first construct a
multi-granularity code search dataset called MGCodeSearchNet, which contains
536K+ pairs of natural language and code snippets. Subsequently, we introduce a
novel Multi-Granularity Self-Supervised contrastive learning code Search
framework (MGS$^{3}$}). First, MGS$^{3}$ features a Hierarchical
Multi-Granularity Representation module (HMGR), which leverages syntactic
structural relationships for hierarchical representation and aggregates
fine-grained information into coarser-grained representations. Then, during the
contrastive learning phase, we endeavor to construct positive samples of the
same granularity for fine-grained code, and introduce in-function negative
samples for fine-grained code. Finally, we conduct extensive experiments on
code search benchmarks across various granularities, demonstrating that the
framework exhibits outstanding performance in code search tasks of multiple
granularities. These experiments also showcase its model-agnostic nature and
compatibility with existing pre-trained code representation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Guidance of Multi-Turn Conversation in Industrial Search <span class="chip">ACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Xiao Li, Li Gao, Yiding Liu, Xiaoyang Wang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Large Language Models (LLMs) has significantly advanced
multi-turn conversation systems, emphasizing the need for proactive guidance to
enhance users' interactions. However, these systems face challenges in
dynamically adapting to shifts in users' goals and maintaining low latency for
real-time interactions. In the Baidu Search AI assistant, an industrial-scale
multi-turn search system, we propose a novel two-phase framework to provide
proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning
(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal
shifts and provides goal-relevant contextual information. G-SFT also
incorporates scalable knowledge transfer to distill insights from LLMs into a
lightweight model for real-time interaction. The second phase, Click-oriented
Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically
constructs preference pairs from user click signals, and proactively improves
click-through rates through more engaging guidance. This dual-phase
architecture achieves complementary objectives: G-SFT ensures accurate goal
tracking, while C-RL optimizes interaction quality through click signal-driven
reinforcement learning. Extensive experiments demonstrate that our framework
achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and
25.28% CTR in online deployment (149.06% relative improvement), while reducing
inference latency by 69.55% through scalable knowledge distillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL'25 (Industry)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Graph Masked Contrastive Learning for Robust
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sang, Yu Wang, Yiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous graph neural networks (HGNNs) have demonstrated their
superiority in exploiting auxiliary information for recommendation tasks.
However, graphs constructed using meta-paths in HGNNs are usually too dense and
contain a large number of noise edges. The propagation mechanism of HGNNs
propagates even small amounts of noise in a graph to distant neighboring nodes,
thereby affecting numerous node embeddings. To address this limitation, we
introduce a novel model, named Masked Contrastive Learning (MCL), to enhance
recommendation robustness to noise. MCL employs a random masking strategy to
augment the graph via meta-paths, reducing node sensitivity to specific
neighbors and bolstering embedding robustness. Furthermore, MCL employs
contrastive cross-view on a Heterogeneous Information Network (HIN) from two
perspectives: one-hop neighbors and meta-path neighbors. This approach acquires
embeddings capturing both local and high-order structures simultaneously for
recommendation. Empirical evaluations on three real-world datasets confirm the
superiority of our approach over existing recommendation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Automatic Sampling of User Behaviors for Sequential Recommender
  Systems <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00388v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00388v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Mingyue Cheng, Zhiding Liu, Junzhe Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) have gained increasing popularity due to
their remarkable proficiency in capturing dynamic user preferences. In the
current setup of SRS, a common configuration is to uniformly consider each
historical behavior as a positive interaction. However, this setting has the
potential to yield sub-optimal performance as each individual item often have a
different impact on shaping the user's interests. Hence, in this paper, we
propose a novel automatic sampling framework for sequential recommendation,
named AutoSAM, to non-uniformly treat historical behaviors. Specifically,
AutoSAM extends the conventional SRS framework by integrating an extra sampler
to intelligently discern the skew distribution of the raw input, and then
sample informative sub-sets to build more generalizable SRS. To tackle the
challenges posed by non differentiable sampling actions and to introduce
multiple decision factors for sampling, we further design a novel reinforcement
learning based method to guide the training of the sampler. Furthermore, we
theoretically devise multi-objective sampling rewards including \textit{Future
Prediction} and \textit{Sequence Perplexity}, and then optimize the whole
framework in an end-to-end manner by combining the policy gradient. We conduct
extensive experiments on benchmark recommendation models and four real-world
datasets. The experimental results demonstrate the effectiveness of the
proposed AutoSAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Knowledge Graphs to harvest <span class="highlight-title">dataset</span>s for efficient CLIP model
  training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training high-quality CLIP models typically requires enormous datasets, which
limits the development of domain-specific models -- especially in areas that
even the largest CLIP models do not cover well -- and drives up training costs.
This poses challenges for scientific research that needs fine-grained control
over the training procedure of CLIP models. In this work, we show that by
employing smart web search strategies enhanced with knowledge graphs, a robust
CLIP model can be trained from scratch with considerably less data.
Specifically, we demonstrate that an expert foundation model for living
organisms can be built using just 10M images. Moreover, we introduce EntityNet,
a dataset comprising 33M images paired with 46M text descriptions, which
enables the training of a generic CLIP model in significantly reduced time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeAR: Generation Augmented Retrieval <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liu, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Furu Wei, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document retrieval techniques are essential for developing large-scale
information systems. The common approach involves using a bi-encoder to compute
the semantic similarity between a query and documents. However, the scalar
similarity often fail to reflect enough information, hindering the
interpretation of retrieval results. In addition, this process primarily
focuses on global semantics, overlooking the finer-grained semantic
relationships between the query and the document's content. In this paper, we
introduce a novel method, $\textbf{Ge}$neration $\textbf{A}$ugmented
$\textbf{R}$etrieval ($\textbf{GeAR}$), which not only improves the global
document-query similarity through contrastive learning, but also integrates
well-designed fusion and decoding modules. This enables GeAR to generate
relevant context within the documents based on a given query, facilitating
learning to retrieve local fine-grained information. Furthermore, when used as
a retriever, GeAR does not incur any additional computational cost over
bi-encoders. GeAR exhibits competitive retrieval performance across diverse
scenarios and tasks. Moreover, qualitative analysis and the results generated
by GeAR provide novel insights into the interpretation of retrieval results.
The code, data, and models will be released at
\href{https://github.com/microsoft/LMOps}{https://github.com/microsoft/LMOps}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMIE: Combining MLLM Insights with External Evidence for Explainable
  Out-of-Context Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxiao Li, Jiaying Wu, Canyuan He, Wei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond KAN: Introducing KarSein for Adaptive High-Order Feature
  Interaction Modeling in CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08713v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08713v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiao Shi, Wujiang Xu, Haimin Zhang, Qiang Wu, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling high-order feature interactions is crucial for click-through rate
(CTR) prediction, and traditional approaches often predefine a maximum
interaction order and rely on exhaustive enumeration of feature combinations up
to this predefined order. This framework heavily relies on prior domain
knowledge to define interaction scope and entails high computational costs from
enumeration. Conventional CTR models face a trade-off between improving
representation through complex high-order feature interactions and reducing
computational inefficiencies associated with these processes. To address this
dual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse
Efficient Interaction Network (KarSein). Drawing inspiration from the learnable
activation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages
this mechanism to adaptively transform low-order basic features into high-order
feature interactions, offering a novel approach to feature interaction
modeling. KarSein extends the capabilities of KAN by introducing a more
efficient architecture that significantly reduces computational costs while
accommodating two-dimensional embedding vectors as feature inputs. Furthermore,
it overcomes the limitation of KAN's its inability to spontaneously capture
multiplicative relationships among features.
  Extensive experiments highlight the superiority of KarSein, demonstrating its
ability to surpass not only the vanilla implementation of KAN in CTR predictio
but also other baseline methods. Remarkably, KarSein achieves exceptional
predictive accuracy while maintaining a highly compact parameter size and
minimal computational overhead. As the first attempt to apply KAN in the CTR
domain, this work introduces KarSein as a novel solution for modeling complex
feature interactions, underscoring its transformative potential in advancing
CTR prediction task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>draft paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Gold Standard <span class="highlight-title">Dataset</span> for the <span class="highlight-title">Review</span>er Assignment Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Stelmakh, John Wieting, Sarina Xi, Graham Neubig, Nihar B. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many peer-review venues are using algorithms to assign submissions to
reviewers. The crux of such automated approaches is the notion of the
"similarity score" -- a numerical estimate of the expertise of a reviewer in
reviewing a paper -- and many algorithms have been proposed to compute these
scores. However, these algorithms have not been subjected to a principled
comparison, making it difficult for stakeholders to choose the algorithm in an
evidence-based manner. The key challenge in comparing existing algorithms and
developing better algorithms is the lack of publicly available gold-standard
data. We address this challenge by collecting a novel dataset of similarity
scores that we release to the research community. Our dataset consists of 477
self-reported expertise scores provided by 58 researchers who evaluated their
expertise in reviewing papers they have read previously.
  Using our dataset, we compare several widely used similarity algorithms and
offer key insights. First, all algorithms exhibit significant error, with
misranking rates between 12%-30% in easier cases and 36%-43% in harder ones.
Second, most specialized algorithms are designed to work with titles and
abstracts of papers, and in this regime the SPECTER2 algorithm performs best.
Interestingly, classical TF-IDF matches SPECTER2 in accuracy when given access
to full submission texts. In contrast, off-the-shelf LLMs lag behind
specialized approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The RAG Paradox: A Black-Box Attack Exploiting Unintentional
  Vulnerabilities in Retrieval-Augmented Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Choi, Jinsoo Kim, Sukmin Cho, Soyeong Jeong, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing adoption of retrieval-augmented generation (RAG) systems,
various attack methods have been proposed to degrade their performance.
However, most existing approaches rely on unrealistic assumptions in which
external attackers have access to internal components such as the retriever. To
address this issue, we introduce a realistic black-box attack based on the RAG
paradox, a structural vulnerability arising from the system's effort to enhance
trust by revealing both the retrieved documents and their sources to users.
This transparency enables attackers to observe which sources are used and how
information is phrased, allowing them to craft poisoned documents that are more
likely to be retrieved and upload them to the identified sources. Moreover, as
RAG systems directly provide retrieved content to users, these documents must
not only be retrievable but also appear natural and credible to maintain user
confidence in the search results. Unlike prior work that focuses solely on
improving document retrievability, our attack method explicitly considers both
retrievability and user trust in the retrieved content. Both offline and online
experiments demonstrate that our method significantly degrades system
performance without internal access, while generating natural-looking poisoned
documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nested Hash Layer: A Plug-and-play Module for Multiple-length Hash Code
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang He, Yuren Zhang, Rui Li, Zhenya Huang, Runze Wu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep supervised hashing is essential for efficient storage and search in
large-scale image retrieval. Traditional deep supervised hashing models
generate single-length hash codes, but this creates a trade-off between
efficiency and effectiveness for different code lengths. To find the optimal
length for a task, multiple models must be trained, increasing time and
computation. Furthermore, relationships between hash codes of different lengths
are often ignored. To address these issues, we propose the Nested Hash Layer
(NHL), a plug-and-play module for deep supervised hashing models. NHL generates
hash codes of multiple lengths simultaneously in a nested structure. To resolve
optimization conflicts from multiple learning objectives, we introduce a
dominance-aware dynamic weighting strategy to adjust gradients. Additionally,
we propose a long-short cascade self-distillation method, where long hash codes
guide the learning of shorter ones, improving overall code quality. Experiments
indicate that the NHL achieves an overall training speed improvement of
approximately 5 to 8 times across various deep supervised hashing models and
enhances the average performance of these models by about 3.4%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLTW: Joint Improved Graph <span class="highlight-title">Transformer</span> and LLM via Three-Word Language
  for Knowledge Graph Completion <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2025(Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consensus-aware Contrastive Learning for Group Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyoung Kim, Dongjun Lee, Jaekwang Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group recommendation aims to provide personalized item suggestions to a group
of users by reflecting their collective preferences. A fundamental challenge in
this task is deriving a consensus that adequately represents the diverse
interests of individual group members. Despite advancements made by deep
learning-based models, existing approaches still struggle in two main areas:
(1) Capturing consensus in small-group settings, which are more prevalent in
real-world applications, and (2) Balancing individual preferences with overall
group performance, particularly in hypergraph-based methods that tend to
emphasize group accuracy at the expense of personalization. To address these
challenges, we introduce a Consensus-aware Contrastive Learning for Group
Recommendation (CoCoRec) that models group consensus through contrastive
learning. CoCoRec utilizes a transformer encoder to jointly learn user and
group representations, enabling richer modeling of intra-group dynamics.
Additionally, the contrastive objective helps reduce overfitting from
high-frequency user interactions, leading to more robust and representative
group embeddings. Experiments conducted on four benchmark datasets show that
CoCoRec consistently outperforms state-of-the-art baselines in both individual
and group recommendation scenarios, highlighting the effectiveness of
consensus-aware contrastive learning in group recommendation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepResearchGym: A Free, Transparent, and Reproducible Evaluation
  Sandbox for Deep Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie Callan, João Magalhães, Bruno Martins, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep research systems represent an emerging class of agentic information
retrieval methods that generate comprehensive and well-supported reports to
complex queries. However, most existing frameworks rely on dynamic commercial
search APIs, which pose reproducibility and transparency challenges in addition
to their cost. To address these limitations, we introduce DeepResearchGym, an
open-source sandbox that combines a reproducible search API with a rigorous
evaluation protocol for benchmarking deep research systems. The API indexes
large-scale public web corpora, namely ClueWeb22 and FineWeb, using a
state-of-the-art dense retriever and approximate nearest neighbor search via
DiskANN. It achieves lower latency than popular commercial APIs while ensuring
stable document rankings across runs, and is freely available for research use.
To evaluate deep research systems' outputs, we extend the Researchy Questions
benchmark with automatic metrics through LLM-as-a-judge assessments to measure
alignment with users' information needs, retrieval faithfulness, and report
quality. Experimental results show that systems integrated with DeepResearchGym
achieve performance comparable to those using commercial APIs, with performance
rankings remaining consistent across evaluation metrics. A human evaluation
study further confirms that our automatic protocol aligns with human
preferences, validating the framework's ability to help support controlled
assessment of deep research systems. Our code and API documentation are
available at https://www.deepresearchgym.ai.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Starbucks-v2: Improved Training for 2D Matryoshka Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13230v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13230v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Shuai Wang, Fabio Zheng, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D Matryoshka training enables a single embedding model to generate
sub-network representations across different layers and embedding dimensions,
offering adaptability to diverse computational and task constraints. However,
its effectiveness remains well below that of individually trained models of
equivalent sizes. To address this, we propose Starbucks, a new training
strategy for Matryoshka-style embedding models that combines structured
fine-tuning with masked autoencoder (MAE) pre-training. During fine-tuning, we
compute the loss over a fixed set of layer-dimension pairs, from small to
large, which significantly improves performance over randomly sampled
sub-networks and matches that of separately trained models. Our MAE-based
pre-training further enhances the representation quality of sub-networks,
providing a stronger backbone for downstream tasks. Experiments on both
in-domain (semantic similarity and passage retrieval) and out-of-domain (BEIR)
benchmarks show that Starbucks consistently outperforms 2D Matryoshka models
and matches or exceeds the performance of individually trained models, while
maintaining high efficiency and adaptability. Ablation studies confirm our loss
design choices, the impact of SMAE pre-training and demonstrate the
applicability of Starbucks across backbones. We further show that depth- and
width-wise Starbucks variants capture complementary information, and that their
hybridization yields additional performance gains with minimal latency overhead
due to parallelization. Code available at https://github.com/ielab/Starbucks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated Version of Starbucks, add (1) Generalisation to E5 model (2)
  Out-of-domain zero-shot effectiveness (3) Propose Depth-wise Starbucks and
  Hybrid-Starbucks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Vulnerability of Applying Retrieval-Augmented Generation within
  Knowledge-Intensive Application Domains <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has been empirically shown to enhance
the performance of large language models (LLMs) in knowledge-intensive domains
such as healthcare, finance, and legal contexts. Given a query, RAG retrieves
relevant documents from a corpus and integrates them into the LLMs' generation
process. In this study, we investigate the adversarial robustness of RAG,
focusing specifically on examining the retrieval system. First, across 225
different setup combinations of corpus, retriever, query, and targeted
information, we show that retrieval systems are vulnerable to universal
poisoning attacks in medical Q\&A. In such attacks, adversaries generate
poisoned documents containing a broad spectrum of targeted information, such as
personally identifiable information. When these poisoned documents are inserted
into a corpus, they can be accurately retrieved by any users, as long as
attacker-specified queries are used. To understand this vulnerability, we
discovered that the deviation from the query's embedding to that of the
poisoned document tends to follow a pattern in which the high similarity
between the poisoned document and the query is retained, thereby enabling
precise retrieval. Based on these findings, we develop a new detection-based
defense to ensure the safe use of RAG. Through extensive experiments spanning
various Q\&A domains, we observed that our proposed method consistently
achieves excellent detection rates in nearly all cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis
  Optimization for Speech Multi-Metric Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Shi, Yifan Cheng, Bo-Hao Su, Hye-jin Shim, Jinchuan Tian, Samuele Cornell, Yiwen Zhao, Siddhant Arora, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech signal analysis poses significant challenges, particularly in tasks
such as speech quality evaluation and profiling, where the goal is to predict
multiple perceptual and objective metrics. For instance, metrics like PESQ
(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective
Intelligibility), and MOS (Mean Opinion Score) each capture different aspects
of speech quality. However, these metrics often have different scales,
assumptions, and dependencies, making joint estimation non-trivial. To address
these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based
Hypothesis Optimization), a chain-based, versatile evaluation system for speech
assessment grounded in autoregressive dependency modeling. ARECHO is
distinguished by three key innovations: (1) a comprehensive speech information
tokenization pipeline; (2) a dynamic classifier chain that explicitly captures
inter-metric dependencies; and (3) a two-step confidence-oriented decoding
algorithm that enhances inference reliability. Experiments demonstrate that
ARECHO significantly outperforms the baseline framework across diverse
evaluation scenarios, including enhanced speech analysis, speech generation
evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency
modeling improves interpretability by capturing inter-metric relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Video Generation via Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Rawal, Suryansh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned diffusion models have emerged as powerful tools for
high-quality video generation. However, enabling Interactive Video Generation
(IVG), where users control motion elements such as object trajectory, remains
challenging. Recent training-free approaches introduce attention masking to
guide trajectory, but this often degrades perceptual quality. We identify two
key failure modes in these methods, both of which we interpret as domain shift
problems, and propose solutions inspired by domain adaptation. First, we
attribute the perceptual degradation to internal covariate shift induced by
attention masking, as pretrained models are not trained to handle masked
attention. To address this, we propose mask normalization, a pre-normalization
layer designed to mitigate this shift via distribution matching. Second, we
address initialization gap, where the randomly sampled initial noise does not
align with IVG conditioning, by introducing a temporal intrinsic diffusion
prior that enforces spatio-temporal consistency at each denoising step.
Extensive qualitative and quantitative evaluations demonstrate that mask
normalization and temporal intrinsic denoising improve both perceptual quality
and trajectory control over the existing state-of-the-art IVG techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISMAF: Intrinsic-Social Modality Alignment and Fusion for Multimodal
  Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yu, Xiang Li, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid dissemination of rumors on social media highlights the urgent need
for automatic detection methods to safeguard societal trust and stability.
While existing multimodal rumor detection models primarily emphasize capturing
consistency between intrinsic modalities (e.g., news text and images), they
often overlook the intricate interplay between intrinsic and social modalities.
This limitation hampers the ability to fully capture nuanced relationships that
are crucial for a comprehensive understanding. Additionally, current methods
struggle with effectively fusing social context with textual and visual
information, resulting in fragmented interpretations. To address these
challenges, this paper proposes a novel Intrinsic-Social Modality Alignment and
Fusion (ISMAF) framework for multimodal rumor detection. ISMAF first employs a
cross-modal consistency alignment strategy to align complex interactions
between intrinsic and social modalities. It then leverages a mutual learning
approach to facilitate collaborative refinement and integration of
complementary information across modalities. Finally, an adaptive fusion
mechanism is incorporated to dynamically adjust the contribution of each
modality, tackling the complexities of three-modality fusion. Extensive
experiments on both English and Chinese real-world multimedia datasets
demonstrate that ISMAF consistently outperforms state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General
  Vision-Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang, Zheng Lian, Xiaojiang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion understanding in videos aims to accurately recognize and interpret
individuals' emotional states by integrating contextual, visual, textual, and
auditory cues. While Large Multimodal Models (LMMs) have demonstrated
significant progress in general vision-language (VL) tasks, their performance
in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on
emotion-related tasks often leads to catastrophic forgetting, hindering their
ability to generalize across diverse tasks. To address these challenges, we
present Emotion-Qwen, a tailored multimodal framework designed to enhance both
emotion understanding and general VL reasoning. Emotion-Qwen incorporates a
sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,
which dynamically routes inputs to balance emotion-specific and general-purpose
processing. The model is pre-trained in a three-stage pipeline on large-scale
general and emotional image datasets to support robust multimodal
representations. Furthermore, we construct the Video Emotion Reasoning (VER)
dataset, comprising more than 40K bilingual video clips with fine-grained
descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning
capability. Experimental results demonstrate that Emotion-Qwen achieves
state-of-the-art performance on multiple emotion recognition benchmarks, while
maintaining competitive results on general VL tasks. Code and models are
available at https://github.com/24DavidHuang/Emotion-Qwen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMIE: Combining MLLM Insights with External Evidence for Explainable
  Out-of-Context Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxiao Li, Jiaying Wu, Canyuan He, Wei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ISDrama: Immersive Spatial Drama Generation through Multimodal <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal immersive spatial drama generation focuses on creating continuous
multi-speaker binaural speech with dramatic prosody based on multimodal
prompts, with potential applications in AR, VR, and others. This task requires
simultaneous modeling of spatial information and dramatic prosody based on
multimodal inputs, with high data collection costs. To the best of our
knowledge, our work is the first attempt to address these challenges. We
construct MRSDrama, the first multimodal recorded spatial drama dataset,
containing binaural drama audios, scripts, videos, geometric poses, and textual
prompts. Then, we propose ISDrama, the first immersive spatial drama generation
model through multimodal prompting. ISDrama comprises these primary components:
1) Multimodal Pose Encoder, based on contrastive learning, considering the
Doppler effect caused by moving speakers to extract unified pose information
from multimodal prompts. 2) Immersive Drama Transformer, a flow-based
mamba-transformer model that generates high-quality drama, incorporating
Drama-MOE to select proper experts for enhanced prosody and pose control. We
also design a context-consistent classifier-free guidance strategy to
coherently generate complete drama. Experimental results show that ISDrama
outperforms baseline models on objective and subjective metrics. The demos and
dataset are available at https://aaronz345.github.io/ISDramaDemo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionTalk: An Interactive Chinese Multimodal Emotion <span class="highlight-title">Dataset</span> With Rich
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, emotion recognition plays a critical role in applications
such as human-computer interaction, mental health monitoring, and sentiment
analysis. While datasets for emotion analysis in languages such as English have
proliferated, there remains a pressing need for high-quality, comprehensive
datasets tailored to the unique linguistic, cultural, and multimodal
characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an
interactive Chinese multimodal emotion dataset with rich annotations. This
dataset provides multimodal information from 19 actors participating in dyadic
conversational settings, incorporating acoustic, visual, and textual
modalities. It includes 23.6 hours of speech (19,250 utterances), annotations
for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,
fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,
neutral, weakly positive, and positive) and 4-dimensional speech captions
(speaker, speaking style, emotion and overall). The dataset is well-suited for
research on unimodal and multimodal emotion recognition, missing modality
challenges, and speech captioning tasks. To our knowledge, it represents the
first high-quality and versatile Chinese dialogue multimodal emotion dataset,
which is a valuable contribution to research on cross-cultural emotion analysis
and recognition. Additionally, we conduct experiments on EmotionTalk to
demonstrate the effectiveness and quality of the dataset. It will be
open-source and freely available for all academic purposes. The dataset and
codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-06-07T05:28:50.652252297Z">
            2025-06-07 05:28:50 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
