{"2025-06-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.05345v1","updated":"2025-06-05T17:59:55Z","published":"2025-06-05T17:59:55Z","title":"Inference-Time Hyper-Scaling with KV Cache Compression","summary":"  Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.\n","authors":["Adrian Łańcucki","Konrad Staniszewski","Piotr Nawrot","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2506.05345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05346v1","updated":"2025-06-05T17:59:55Z","published":"2025-06-05T17:59:55Z","title":"Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets","summary":"  Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.\n","authors":["Lei Hsiung","Tianyu Pang","Yung-Chen Tang","Linyue Song","Tsung-Yi Ho","Pin-Yu Chen","Yaoqing Yang"],"pdf_url":"https://arxiv.org/pdf/2506.05346v1.pdf","comment":"Project Page: https://hsiung.cc/llm-similarity-risk/"},{"id":"http://arxiv.org/abs/2506.05339v1","updated":"2025-06-05T17:59:32Z","published":"2025-06-05T17:59:32Z","title":"Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases\n  in Preference Models","summary":"  Language models serve as proxies for human preference judgements in alignment\nand evaluation, yet they exhibit systematic miscalibration, prioritizing\nsuperficial patterns over substantive qualities. This bias manifests as\noverreliance on features like length, structure, and style, leading to issues\nlike reward hacking and unreliable evaluations. Evidence suggests these biases\noriginate in artifacts in human training data. In this work, we systematically\ninvestigate the relationship between training data biases and preference model\nmiscalibration across five idiosyncratic features of language model\ngenerations: length, structure, jargon, sycophancy and vagueness. Using\ncontrolled counterfactual pairs, we first quantify the extent to which\npreference models favor responses with magnified biases (skew), finding this\npreference occurs in >60% of instances, and model preferences show high\nmiscalibration (~40%) compared to human preferences. Notably, bias features\nonly show mild negative correlations to human preference labels (mean r_human =\n-0.12) but show moderately strong positive correlations with labels from a\nstrong reward model (mean r_model = +0.36), suggesting that models may overrely\non spurious cues. To mitigate these issues, we propose a simple post-training\nmethod based on counterfactual data augmentation (CDA) using synthesized\ncontrastive examples. Finetuning models with CDA reduces average miscalibration\nfrom 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%,\nwhile maintaining overall RewardBench performance, showing that targeted\ndebiasing is effective for building reliable preference models.\n","authors":["Anirudh Bharadwaj","Chaitanya Malaviya","Nitish Joshi","Mark Yatskar"],"pdf_url":"https://arxiv.org/pdf/2506.05339v1.pdf","comment":"Code and data available at\n  https://github.com/anirudhb123/preference-model-biases"},{"id":"http://arxiv.org/abs/2506.05334v1","updated":"2025-06-05T17:59:26Z","published":"2025-06-05T17:59:26Z","title":"Search Arena: Analyzing Search-Augmented LLMs","summary":"  Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.\n","authors":["Mihran Miroyan","Tsung-Han Wu","Logan King","Tianle Li","Jiayi Pan","Xinyan Hu","Wei-Lin Chiang","Anastasios N. Angelopoulos","Trevor Darrell","Narges Norouzi","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2506.05334v1.pdf","comment":"Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k"},{"id":"http://arxiv.org/abs/2506.05333v1","updated":"2025-06-05T17:59:24Z","published":"2025-06-05T17:59:24Z","title":"Kinetics: Rethinking Test-Time Scaling Laws","summary":"  We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.\n","authors":["Ranajoy Sadhukhan","Zhuoming Chen","Haizhong Zheng","Yang Zhou","Emma Strubell","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16400v3","updated":"2025-06-05T17:59:12Z","published":"2025-05-22T08:50:47Z","title":"AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning","summary":"  Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.\n","authors":["Yang Chen","Zhuolin Yang","Zihan Liu","Chankyu Lee","Peng Xu","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2505.16400v3.pdf","comment":"Add pass@1024 evaluation results for LiveCodeBench v6. We release the\n  models at:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485"},{"id":"http://arxiv.org/abs/2506.05332v1","updated":"2025-06-05T17:59:04Z","published":"2025-06-05T17:59:04Z","title":"Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding","summary":"  Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.\n","authors":["Jingyang Lin","Jialian Wu","Ximeng Sun","Ze Wang","Jiang Liu","Yusheng Su","Xiaodong Yu","Hao Chen","Jiebo Luo","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2506.05332v1.pdf","comment":"Project page: https://videomarathon.github.io/"},{"id":"http://arxiv.org/abs/2412.03782v3","updated":"2025-06-05T17:58:57Z","published":"2024-12-05T00:05:11Z","title":"The broader spectrum of in-context learning","summary":"  The ability of language models to learn a task from a few examples in context\nhas generated substantial interest. Here, we provide a perspective that\nsituates this type of supervised few-shot learning within a much broader\nspectrum of meta-learned in-context learning. Indeed, we suggest that any\ndistribution of sequences in which context non-trivially decreases loss on\nsubsequent predictions can be interpreted as eliciting a kind of in-context\nlearning. We suggest that this perspective helps to unify the broad set of\nin-context abilities that language models exhibit -- such as adapting to tasks\nfrom instructions or role play, or extrapolating time series. This perspective\nalso sheds light on potential roots of in-context learning in lower-level\nprocessing of linguistic dependencies (e.g. coreference or parallel\nstructures). Finally, taking this perspective highlights the importance of\ngeneralization, which we suggest can be studied along several dimensions: not\nonly the ability to learn something novel, but also flexibility in learning\nfrom different presentations, and in applying what is learned. We discuss\nbroader connections to past literature in meta-learning and goal-conditioned\nagents, and other perspectives on learning and adaptation. We close by\nsuggesting that research on in-context learning should consider this broader\nspectrum of in-context capabilities and types of generalization.\n","authors":["Andrew Kyle Lampinen","Stephanie C. Y. Chan","Aaditya K. Singh","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2412.03782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05316v1","updated":"2025-06-05T17:55:43Z","published":"2025-06-05T17:55:43Z","title":"Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay","summary":"  Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.\n","authors":["Yifan Sun","Jingyan Shen","Yibin Wang","Tianyu Chen","Zhendong Wang","Mingyuan Zhou","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05314v1","updated":"2025-06-05T17:55:23Z","published":"2025-06-05T17:55:23Z","title":"Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models","summary":"  Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.\n","authors":["Taha Entesari","Arman Hatami","Rinat Khaziev","Anil Ramakrishna","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2506.05314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.24102v2","updated":"2025-06-05T17:55:07Z","published":"2025-03-31T13:56:03Z","title":"Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?","summary":"  Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advances in Large\nLanguage Models (LLMs) and Neural Machine Translation have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates current\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\nlimitations in LRL translation capability. We also explore alternative data\nsources, including news articles and bilingual dictionaries, and demonstrate\nhow knowledge distillation from large pre-trained teacher models can\nsignificantly improve the performance of small LLMs on LRL translation tasks.\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\ndifferent fine-tuning configurations, providing practical insights on optimal\ndata scale, training efficiency, and the preservation of generalization\ncapabilities of models under study.\n","authors":["Yewei Song","Lujun Li","Cedric Lothritz","Saad Ezzini","Lama Sleem","Niccolo Gentile","Radu State","Tegawendé F. Bissyandé","Jacques Klein"],"pdf_url":"https://arxiv.org/pdf/2503.24102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05309v1","updated":"2025-06-05T17:53:44Z","published":"2025-06-05T17:53:44Z","title":"Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games","summary":"  LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.\n","authors":["Niv Eckhaus","Uri Berger","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2506.05309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05305v1","updated":"2025-06-05T17:52:30Z","published":"2025-06-05T17:52:30Z","title":"ProRefine: Inference-time Prompt Refinement with Textual Feedback","summary":"  Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.\n","authors":["Deepak Pandita","Tharindu Cyril Weerasooriya","Ankit Parag Shah","Christopher M. Homan","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2506.05305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24875v2","updated":"2025-06-05T17:51:58Z","published":"2025-05-30T17:59:48Z","title":"ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL","summary":"  Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.\n","authors":["Yu Zhang","Yunqi Li","Yifan Yang","Rui Wang","Yuqing Yang","Dai Qi","Jianmin Bao","Dongdong Chen","Chong Luo","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.24875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04119v3","updated":"2025-06-05T17:37:25Z","published":"2024-12-05T12:37:27Z","title":"GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice\n  Question Answering","summary":"  Pre-trained Language Models (PLMs) have shown remarkable performances in\nrecent years, setting a new paradigm for NLP research and industry. The legal\ndomain has received some attention from the NLP community partly due to its\ntextual nature. Some tasks from this domain are represented by\nquestion-answering (QA) tasks. This work explores the legal domain\nMultiple-Choice QA (MCQA) for a low-resource language. The contribution of this\nwork is multi-fold. We first introduce JuRO, the first openly available\nRomanian legal MCQA dataset, comprising three different examinations and a\nnumber of 10,836 total questions. Along with this dataset, we introduce CROL,\nan organized corpus of laws that has a total of 93 distinct documents with\ntheir modifications from 763 time spans, that we leveraged in this work for\nInformation Retrieval (IR) techniques. Moreover, we are the first to propose\nLaw-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is\nderived from the aforementioned corpus. Lastly, we propose a novel approach for\nMCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive\nresults with generally accepted SOTA methods and even exceeds them in most\nsettings.\n","authors":["Cristian-George Crăciun","Răzvan-Alexandru Smădu","Dumitru-Clementin Cercel","Mihaela-Claudia Cercel"],"pdf_url":"https://arxiv.org/pdf/2412.04119v3.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.05278v1","updated":"2025-06-05T17:33:02Z","published":"2025-06-05T17:33:02Z","title":"Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning","summary":"  Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.\n","authors":["Nan Huo","Jinyang Li","Bowen Qin","Ge Qu","Xiaolong Li","Xiaodong Li","Chenhao Ma","Reynold Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.05278v1.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.00038v2","updated":"2025-06-05T17:10:34Z","published":"2025-02-25T08:41:25Z","title":"From Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors","summary":"  Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.\n","authors":["Yu Yan","Sheng Sun","Zenghao Duan","Teli Liu","Min Liu","Zhiyi Yin","Jiangyu Lei","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2503.00038v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2412.12145"},{"id":"http://arxiv.org/abs/2412.10510v3","updated":"2025-06-05T17:10:20Z","published":"2024-12-13T19:11:18Z","title":"DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts","summary":"  The proliferation of disinformation demands reliable and scalable\nfact-checking solutions. We present Dynamic Evidence-based FAct-checking with\nMultimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for\nopen-domain, text-image claim verification. DEFAME operates in a six-stage\nprocess, dynamically selecting the tools and search depth to extract and\nevaluate textual and visual evidence. Unlike prior approaches that are\ntext-only, lack explainability, or rely solely on parametric knowledge, DEFAME\nperforms end-to-end verification, accounting for images in claims and evidence\nwhile generating structured, multimodal reports. Evaluation on the popular\nbenchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all\nprevious methods, establishing itself as the new state-of-the-art fact-checking\nsystem for uni- and multimodal fact-checking. Moreover, we introduce a new\nmultimodal benchmark, ClaimReview2024+, featuring claims after the knowledge\ncutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms\nthe GPT-4o baselines, showing temporal generalizability and the potential for\nreal-time fact-checking.\n","authors":["Tobias Braun","Mark Rothermel","Marcus Rohrbach","Anna Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2412.10510v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05243v1","updated":"2025-06-05T17:02:52Z","published":"2025-06-05T17:02:52Z","title":"CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection","summary":"  A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme.\n","authors":["Ron Eliav","Arie Cattan","Eran Hirsch","Shahaf Bassan","Elias Stengel-Eskin","Mohit Bansal","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2506.05243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05235v1","updated":"2025-06-05T16:54:41Z","published":"2025-06-05T16:54:41Z","title":"Towards a Unified System of Representation for Continuity and\n  Discontinuity in Natural Language","summary":"  Syntactic discontinuity is a grammatical phenomenon in which a constituent is\nsplit into more than one part because of the insertion of an element which is\nnot part of the constituent. This is observed in many languages across the\nworld such as Turkish, Russian, Japanese, Warlpiri, Navajo, Hopi, Dyirbal,\nYidiny etc. Different formalisms/frameworks in current linguistic theory\napproach the problem of discontinuous structures in different ways. Each\nframework/formalism has widely been viewed as an independent and non-converging\nsystem of analysis. In this paper, we propose a unified system of\nrepresentation for both continuity and discontinuity in structures of natural\nlanguages by taking into account three formalisms, in particular, Phrase\nStructure Grammar (PSG) for its widely used notion of constituency, Dependency\nGrammar (DG) for its head-dependent relations, and Categorial Grammar (CG) for\nits focus on functor-argument relations. We attempt to show that discontinuous\nexpressions as well as continuous structures can be analysed through a unified\nmathematical derivation incorporating the representations of linguistic\nstructure in these three grammar formalisms.\n","authors":["Ratna Kandala","Prakash Mondal"],"pdf_url":"https://arxiv.org/pdf/2506.05235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05233v1","updated":"2025-06-05T16:50:23Z","published":"2025-06-05T16:50:23Z","title":"MesaNet: Sequence Modeling by Locally Optimal Test-Time Training","summary":"  Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.\n","authors":["Johannes von Oswald","Nino Scherrer","Seijin Kobayashi","Luca Versari","Songlin Yang","Maximilian Schlegel","Kaitlin Maile","Yanick Schimpf","Oliver Sieberling","Alexander Meulemans","Rif A. Saurous","Guillaume Lajoie","Charlotte Frenkel","Razvan Pascanu","Blaise Agüera y Arcas","João Sacramento"],"pdf_url":"https://arxiv.org/pdf/2506.05233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05229v1","updated":"2025-06-05T16:43:48Z","published":"2025-06-05T16:43:48Z","title":"Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts","summary":"  Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.\n","authors":["Danil Sivtsov","Ivan Rodkin","Gleb Kuzmin","Yuri Kuratov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2506.05229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05227v1","updated":"2025-06-05T16:42:45Z","published":"2025-06-05T16:42:45Z","title":"Improving Low-Resource Morphological Inflection via Self-Supervised\n  Objectives","summary":"  Self-supervised objectives have driven major advances in NLP by leveraging\nlarge-scale unlabeled data, but such resources are scarce for many of the\nworld's languages. Surprisingly, they have not been explored much for\ncharacter-level tasks, where smaller amounts of data have the potential to be\nbeneficial. We investigate the effectiveness of self-supervised auxiliary tasks\nfor morphological inflection -- a character-level task highly relevant for\nlanguage documentation -- in extremely low-resource settings, training\nencoder-decoder transformers for 19 languages and 13 auxiliary objectives.\nAutoencoding yields the best performance when unlabeled data is very limited,\nwhile character masked language modeling (CMLM) becomes more effective as data\navailability increases. Though objectives with stronger inductive biases\ninfluence model predictions intuitively, they rarely outperform standard CMLM.\nHowever, sampling masks based on known morpheme boundaries consistently\nimproves performance, highlighting a promising direction for low-resource\nmorphological modeling.\n","authors":["Adam Wiemerslage","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2506.05227v1.pdf","comment":"ACL 2025 main"},{"id":"http://arxiv.org/abs/2506.03147v3","updated":"2025-06-05T16:41:40Z","published":"2025-06-03T17:59:33Z","title":"UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation","summary":"  Although existing unified models achieve strong performance in\nvision-language understanding and text-to-image generation, they remain limited\nin addressing image perception and manipulation -- capabilities increasingly\ndemanded in practical applications. Recently, OpenAI introduced the powerful\nGPT-4o-Image model, which showcases advanced capabilities in comprehensive\nimage perception and manipulation, sparking widespread interest. Through\ncarefully designed experiments, we observe that GPT-4o-Image likely relies on\nsemantic encoders rather than VAEs for feature extraction, despite VAEs being\ncommonly regarded as crucial for image manipulation tasks. Inspired by this\ninsight, we propose UniWorld-V1, a unified generative framework built upon\nsemantic features extracted from powerful multimodal large language models and\ncontrastive semantic encoders. Using only 2.7M training data, UniWorld-V1\nachieves impressive performance across diverse tasks, including image\nunderstanding, generation, manipulation, and perception. We fully open-source\nthe UniWorld-V1 framework, including model weights, training and evaluation\nscripts, and datasets to promote reproducibility and further research.\n","authors":["Bin Lin","Zongjian Li","Xinhua Cheng","Yuwei Niu","Yang Ye","Xianyi He","Shenghai Yuan","Wangbo Yu","Shaodong Wang","Yunyang Ge","Yatian Pang","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2506.03147v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07301v2","updated":"2025-06-05T16:34:24Z","published":"2025-01-13T13:10:16Z","title":"The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning","summary":"  Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.\n","authors":["Zhenru Zhang","Chujie Zheng","Yangzhen Wu","Beichen Zhang","Runji Lin","Bowen Yu","Dayiheng Liu","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05214v1","updated":"2025-06-05T16:28:12Z","published":"2025-06-05T16:28:12Z","title":"Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph\n  Contrastive Learning","summary":"  Graph Neural Networks (GNNs) often suffer from degree bias in node\nclassification tasks, where prediction performance varies across nodes with\ndifferent degrees. Several approaches, which adopt Graph Contrastive Learning\n(GCL), have been proposed to mitigate this bias. However, the limited number of\npositive pairs and the equal weighting of all positives and negatives in GCL\nstill lead to low-degree nodes acquiring insufficient and noisy information.\nThis paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to\nmitigate degree bias. It adds more positive pairs by leveraging node labels and\nadaptively weights positive and negative pairs based on their learning\nhardness. In addition, we develop an experimental framework named SHARP to\nextend HAR to a broader range of scenarios. Both our theoretical analysis and\nexperiments validate the effectiveness of SHARP. The experimental results\nacross four datasets show that SHARP achieves better performance against\nbaselines at both global and degree levels.\n","authors":["Jingyu Hu","Hongbo Bo","Jun Hong","Xiaowei Liu","Weiru Liu"],"pdf_url":"https://arxiv.org/pdf/2506.05214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05213v1","updated":"2025-06-05T16:27:49Z","published":"2025-06-05T16:27:49Z","title":"LLM-First Search: Self-Guided Exploration of the Solution Space","summary":"  Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.\n","authors":["Nathan Herr","Tim Rocktäschel","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2506.05213v1.pdf","comment":"9 main pages, 2 figures, 2 tables, 36 appendix pages"},{"id":"http://arxiv.org/abs/2406.02524v4","updated":"2025-06-05T16:22:36Z","published":"2024-06-04T17:42:21Z","title":"CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks","summary":"  Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.\n","authors":["Maciej Besta","Lorenzo Paleari","Marcin Copik","Robert Gerstenberger","Ales Kubicek","Piotr Nyczyk","Patrick Iff","Eric Schreiber","Tanja Srindran","Tomasz Lehmann","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.02524v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05209v1","updated":"2025-06-05T16:21:30Z","published":"2025-06-05T16:21:30Z","title":"The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text","summary":"  Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.\n","authors":["Nikhil Kandpal","Brian Lester","Colin Raffel","Sebastian Majstorovic","Stella Biderman","Baber Abbasi","Luca Soldaini","Enrico Shippole","A. Feder Cooper","Aviya Skowron","John Kirchenbauer","Shayne Longpre","Lintang Sutawika","Alon Albalak","Zhenlin Xu","Guilherme Penedo","Loubna Ben Allal","Elie Bakouch","John David Pressman","Honglu Fan","Dashiell Stander","Guangyu Song","Aaron Gokaslan","Tom Goldstein","Brian R. Bartoldson","Bhavya Kailkhura","Tyler Murray"],"pdf_url":"https://arxiv.org/pdf/2506.05209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23224v2","updated":"2025-06-05T16:19:56Z","published":"2025-05-29T08:14:40Z","title":"MMBoundary: Advancing MLLM Knowledge Boundary Awareness through\n  Reasoning Step Confidence Calibration","summary":"  In recent years, multimodal large language models (MLLMs) have made\nsignificant progress but continue to face inherent challenges in multimodal\nreasoning, which requires multi-level (e.g., perception, reasoning) and\nmulti-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior\nwork on estimating model confidence tends to focus on the overall response for\ntraining and calibration, but fails to assess confidence in each reasoning\nstep, leading to undesirable hallucination snowballing. In this work, we\npresent MMBoundary, a novel framework that advances the knowledge boundary\nawareness of MLLMs through reasoning step confidence calibration. To achieve\nthis, we propose to incorporate complementary textual and cross-modal\nself-rewarding signals to estimate confidence at each step of the MLLM\nreasoning process. In addition to supervised fine-tuning MLLM on this set of\nself-rewarded confidence estimation signal for initial confidence expression\nwarm-up, we introduce a reinforcement learning stage with multiple reward\nfunctions for further aligning model knowledge and calibrating confidence at\neach reasoning step, enhancing reasoning chain self-correction. Empirical\nresults show that MMBoundary significantly outperforms existing methods across\ndiverse domain datasets and metrics, achieving an average of 7.5% reduction in\nmultimodal confidence calibration errors and up to 8.3% improvement in task\nperformance.\n","authors":["Zhitao He","Sandeep Polisetty","Zhiyuan Fan","Yuchen Huang","Shujin Wu","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2505.23224v2.pdf","comment":"18 pages, ACL 2025"},{"id":"http://arxiv.org/abs/2506.05205v1","updated":"2025-06-05T16:17:24Z","published":"2025-06-05T16:17:24Z","title":"RELIC: Evaluating Compositional Instruction Following via Language\n  Recognition","summary":"  Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions.\n","authors":["Jackson Petty","Michael Y. Hu","Wentao Wang","Shauli Ravfogel","William Merrill","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2506.05205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18053v2","updated":"2025-06-05T16:13:05Z","published":"2025-04-25T03:54:24Z","title":"DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.\n","authors":["Jianyu Liu","Hangyu Guo","Ranjie Duan","Xingyuan Bu","Yancheng He","Shilong Li","Hui Huang","Jiaheng Liu","Yucheng Wang","Chenchen Jing","Xingwei Qu","Xiao Zhang","Yingshui Tan","Yanan Wu","Jihao Gu","Yangguang Li","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.18053v2.pdf","comment":"[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM"},{"id":"http://arxiv.org/abs/2506.05188v1","updated":"2025-06-05T16:02:07Z","published":"2025-06-05T16:02:07Z","title":"Counterfactual reasoning: an analysis of in-context emergence","summary":"  Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .\n","authors":["Moritz Miller","Bernhard Schölkopf","Siyuan Guo"],"pdf_url":"https://arxiv.org/pdf/2506.05188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05085v3","updated":"2025-06-05T15:57:36Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.\n","authors":["Maciej Besta","Ales Kubicek","Robert Gerstenberger","Marcin Chrapek","Roman Niggli","Patrik Okanovic","Yi Zhu","Patrick Iff","Michal Podstawski","Lucas Weitzendorf","Mingyuan Chi","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.05085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05176v1","updated":"2025-06-05T15:49:48Z","published":"2025-06-05T15:49:48Z","title":"Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models","summary":"  In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.\n","authors":["Yanzhao Zhang","Mingxin Li","Dingkun Long","Xin Zhang","Huan Lin","Baosong Yang","Pengjun Xie","An Yang","Dayiheng Liu","Junyang Lin","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.05176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06854v2","updated":"2025-06-05T15:48:54Z","published":"2025-02-07T17:23:48Z","title":"Can Large Language Models Understand Intermediate Representations in\n  Compilers?","summary":"  Intermediate Representations (IRs) play a critical role in compiler design\nand program analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. In this paper, we present an explorative empirical study\nevaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,\nDeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,\nwe assess model performance across four core tasks: control flow graph\nreconstruction, decompilation, code summarization, and execution reasoning.\nWhile LLMs exhibit competence in parsing IR syntax and identifying high-level\nstructures, they consistently struggle with instruction-level reasoning,\nespecially in control flow reasoning, loop handling, and dynamic execution.\nCommon failure modes include misinterpreting branching instructions, omitting\ncritical operations, and relying on heuristic reasoning rather than precise\ninstruction-level logic. Our findings highlight the need for IR-specific\nenhancements in LLM design. We recommend fine-tuning on structured IR datasets\nand integrating control-flow-sensitive architectures to improve model\neffectiveness. All experimental data and source code are publicly available at\n","authors":["Hailong Jiang","Jianfeng Zhu","Yao Wan","Bo Fang","Hongyu Zhang","Ruoming Jin","Qiang Guan"],"pdf_url":"https://arxiv.org/pdf/2502.06854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17394v2","updated":"2025-06-05T15:45:00Z","published":"2025-02-24T18:20:42Z","title":"SNaRe: Domain-aware Data Generation for Low-Resource Event Detection","summary":"  Event Detection (ED) -- the task of identifying event mentions from natural\nlanguage text -- is critical for enabling reasoning in highly specialized\ndomains such as biomedicine, law, and epidemiology. Data generation has proven\nto be effective in broadening its utility to wider applications without\nrequiring expensive expert annotations. However, when existing generation\napproaches are applied to specialized domains, they struggle with label noise,\nwhere annotations are incorrect, and domain drift, characterized by a\ndistributional mismatch between generated sentences and the target domain. To\naddress these issues, we introduce SNaRe, a domain-aware synthetic data\ngeneration framework composed of three components: Scout, Narrator, and\nRefiner. Scout extracts triggers from unlabeled target domain data and curates\na high-quality domain-specific trigger list using corpus-level statistics to\nmitigate domain drift. Narrator, conditioned on these triggers, generates\nhigh-quality domain-aligned sentences, and Refiner identifies additional event\nmentions, ensuring high annotation quality. Experimentation on three diverse\ndomain ED datasets reveals how SNaRe outperforms the best baseline, achieving\naverage F1 gains of 3-7% in the zero-shot/few-shot settings and 4-20% F1\nimprovement for multilingual generation. Analyzing the generated trigger hit\nrate and human evaluation substantiates SNaRe's stronger annotation quality and\nreduced domain drift.\n","authors":["Tanmay Parekh","Yuxuan Dong","Lucas Bandarkar","Artin Kim","I-Hung Hsu","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2502.17394v2.pdf","comment":"Under review at ACL ARR May 2025"},{"id":"http://arxiv.org/abs/2506.05167v1","updated":"2025-06-05T15:43:49Z","published":"2025-06-05T15:43:49Z","title":"ECoRAG: Evidentiality-guided Compression for Long Context RAG","summary":"  Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\n\\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing\nretrieved documents based on evidentiality, ensuring whether answer generation\nis supported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.\n","authors":["Yeonseok Jeong","Jinsu Kim","Dohyeon Lee","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.05167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05166v1","updated":"2025-06-05T15:43:34Z","published":"2025-06-05T15:43:34Z","title":"Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective","summary":"  Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.\n","authors":["Bhavik Chandna","Zubair Bashir","Procheta Sen"],"pdf_url":"https://arxiv.org/pdf/2506.05166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23827v2","updated":"2025-06-05T15:41:26Z","published":"2025-05-28T06:43:16Z","title":"ValueSim: Generating Backstories to Model Individual Value Systems","summary":"  As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time.\n","authors":["Bangde Du","Ziyi Ye","Zhijing Wu","Jankowska Monika","Shuqi Zhu","Qingyao Ai","Yujia Zhou","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2505.23827v2.pdf","comment":"8 pages main paper + 13 pages appendix, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.00837v2","updated":"2025-06-05T15:41:25Z","published":"2025-02-02T16:18:44Z","title":"Explainability in Practice: A Survey of Explainable NLP Across Various\n  Domains","summary":"  Natural Language Processing (NLP) has become a cornerstone in many critical\nsectors, including healthcare, finance, and customer relationship management.\nThis is especially true with the development and use of advanced models such as\nGPT-based architectures and BERT, which are widely used in decision-making\nprocesses. However, the black-box nature of these advanced NLP models has\ncreated an urgent need for transparency and explainability. This review\nexplores explainable NLP (XNLP) with a focus on its practical deployment and\nreal-world applications, examining its implementation and the challenges faced\nin domain-specific contexts. The paper underscores the importance of\nexplainability in NLP and provides a comprehensive perspective on how XNLP can\nbe designed to meet the unique demands of various sectors, from healthcare's\nneed for clear insights to finance's emphasis on fraud detection and risk\nassessment. Additionally, this review aims to bridge the knowledge gap in XNLP\nliterature by offering a domain-specific exploration and discussing\nunderrepresented areas such as real-world applicability, metric evaluation, and\nthe role of human interaction in model assessment. The paper concludes by\nsuggesting future research directions that could enhance the understanding and\nbroader application of XNLP.\n","authors":["Hadi Mohammadi","Ayoub Bagheri","Anastasia Giachanou","Daniel L. Oberski"],"pdf_url":"https://arxiv.org/pdf/2502.00837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05154v1","updated":"2025-06-05T15:34:15Z","published":"2025-06-05T15:34:15Z","title":"Knowledgeable-r1: Policy Optimization for Knowledge Exploration in\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) is a mainstream method for improving\nperformance on knowledge-intensive tasks. However,current RAG systems often\nplace too much emphasis on retrieved contexts. This can lead to reliance on\ninaccurate sources and overlook the model's inherent knowledge, especially when\ndealing with misleading or excessive information. To resolve this imbalance, we\npropose Knowledgeable-r1 that using joint sampling and define multi policy\ndistributions in knowledge capability exploration to stimulate large language\nmodels'self-integrated utilization of parametric and contextual knowledge.\nExperiments show that Knowledgeable-r1 significantly enhances robustness and\nreasoning accuracy in both parameters and contextual conflict tasks and general\nRAG tasks, especially outperforming baselines by 17.07% in counterfactual\nscenarios and demonstrating consistent gains across RAG tasks. Our code are\navailable at https://github.com/lcy80366872/ knowledgeable-r1.\n","authors":["Chenyu Lin","Yilin Wen","Du Su","Fei Sun","Muhan Chen","Chenfu Bao","Zhonghou Lv"],"pdf_url":"https://arxiv.org/pdf/2506.05154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05146v1","updated":"2025-06-05T15:27:16Z","published":"2025-06-05T15:27:16Z","title":"CIVET: Systematic Evaluation of Understanding in VLMs","summary":"  While Vision-Language Models (VLMs) have achieved competitive performance in\nvarious tasks, their comprehension of the underlying structure and semantics of\na scene remains understudied. To investigate the understanding of VLMs, we\nstudy their capability regarding object properties and relations in a\ncontrolled and interpretable manner. To this scope, we introduce CIVET, a novel\nand extensible framework for systematiC evaluatIon Via controllEd sTimuli.\nCIVET addresses the lack of standardized systematic evaluation for assessing\nVLMs' understanding, enabling researchers to test hypotheses with statistical\nrigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of\nstimuli, free from annotation noise, dataset-specific biases, and uncontrolled\nscene complexity. Our findings reveal that 1) current VLMs can accurately\nrecognize only a limited set of basic object properties; 2) their performance\nheavily depends on the position of the object in the scene; 3) they struggle to\nunderstand basic relations among objects. Furthermore, a comparative evaluation\nwith human annotators reveals that VLMs still fall short of achieving\nhuman-level accuracy.\n","authors":["Massimo Rizzoli","Simone Alghisi","Olha Khomyn","Gabriel Roccabruna","Seyed Mahed Mousavi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2506.05146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05142v1","updated":"2025-06-05T15:24:33Z","published":"2025-06-05T15:24:33Z","title":"Do Large Language Models Judge Error Severity Like Humans?","summary":"  Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models.\n","authors":["Diege Sun","Guanyi Chen","Fan Zhao","Xiaorong Cheng","Tingting He"],"pdf_url":"https://arxiv.org/pdf/2506.05142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05140v1","updated":"2025-06-05T15:22:47Z","published":"2025-06-05T15:22:47Z","title":"AudioLens: A Closer Look at Auditory Attribute Perception of Large\n  Audio-Language Models","summary":"  Understanding the internal mechanisms of large audio-language models (LALMs)\nis crucial for interpreting their behavior and improving performance. This work\npresents the first in-depth analysis of how LALMs internally perceive and\nrecognize auditory attributes. By applying vocabulary projection on three\nstate-of-the-art LALMs, we track how attribute information evolves across\nlayers and token positions. We find that attribute information generally\ndecreases with layer depth when recognition fails, and that resolving\nattributes at earlier layers correlates with better accuracy. Moreover, LALMs\nheavily rely on querying auditory inputs for predicting attributes instead of\naggregating necessary information in hidden states at attribute-mentioning\npositions. Based on our findings, we demonstrate a method to enhance LALMs. Our\nresults offer insights into auditory attribute processing, paving the way for\nfuture improvements.\n","authors":["Chih-Kai Yang","Neo Ho","Yi-Jyun Lee","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2506.05140v1.pdf","comment":"8 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.05136v1","updated":"2025-06-05T15:21:05Z","published":"2025-06-05T15:21:05Z","title":"Information Locality as an Inductive Bias for Neural Language Models","summary":"  Inductive biases are inherent in every machine learning system, shaping how\nmodels generalize from finite data. In the case of neural language models\n(LMs), debates persist as to whether these biases align with or diverge from\nhuman processing constraints. To address this issue, we propose a quantitative\nframework that allows for controlled investigations into the nature of these\nbiases. Within our framework, we introduce $m$-local entropy$\\unicode{x2013}$an\ninformation-theoretic measure derived from average lossy-context\nsurprisal$\\unicode{x2013}$that captures the local uncertainty of a language by\nquantifying how effectively the $m-1$ preceding symbols disambiguate the next\nsymbol. In experiments on both perturbed natural language corpora and languages\ndefined by probabilistic finite-state automata (PFSAs), we show that languages\nwith higher $m$-local entropy are more difficult for Transformer and LSTM LMs\nto learn. These results suggest that neural LMs, much like humans, are highly\nsensitive to the local statistical structure of a language.\n","authors":["Taiga Someya","Anej Svete","Brian DuSell","Timothy J. O'Donnell","Mario Giulianelli","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2506.05136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20779v3","updated":"2025-06-05T15:20:59Z","published":"2025-05-27T06:36:04Z","title":"CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature","summary":"  A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.com/noy-sternlicht/CHIMERA-KB\n","authors":["Noy Sternlicht","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2505.20779v3.pdf","comment":"Project page: https://noy-sternlicht.github.io/CHIMERA-Web"},{"id":"http://arxiv.org/abs/2506.05128v1","updated":"2025-06-05T15:16:14Z","published":"2025-06-05T15:16:14Z","title":"DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning","summary":"  Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.\n","authors":["Tanmay Parekh","Kartik Mehta","Ninareh Mehrabi","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2506.05128v1.pdf","comment":"Submitted at ACL ARR May 2025"},{"id":"http://arxiv.org/abs/2504.02234v2","updated":"2025-06-05T15:12:55Z","published":"2025-04-03T03:01:26Z","title":"LLM Social Simulations Are a Promising Research Method","summary":"  Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted this method. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a review of\nempirical comparisons between LLMs and human research subjects, commentaries on\nthe topic, and related work. We identify promising directions, including\ncontext-rich prompting and fine-tuning with social science datasets. We believe\nthat LLM social simulations can already be used for pilot and exploratory\nstudies, and more widespread use may soon be possible with rapidly advancing\nLLM capabilities. Researchers should prioritize developing conceptual models\nand iterative evaluations to make the best use of new AI systems.\n","authors":["Jacy Reese Anthis","Ryan Liu","Sean M. Richardson","Austin C. Kozlowski","Bernard Koch","James Evans","Erik Brynjolfsson","Michael Bernstein"],"pdf_url":"https://arxiv.org/pdf/2504.02234v2.pdf","comment":"Published at ICML 2025"},{"id":"http://arxiv.org/abs/2506.05121v1","updated":"2025-06-05T15:09:23Z","published":"2025-06-05T15:09:23Z","title":"The NTNU System at the S&I Challenge 2025 SLA Open Track","summary":"  A recent line of research on spoken language assessment (SLA) employs neural\nmodels such as BERT and wav2vec 2.0 (W2V) to evaluate speaking proficiency\nacross linguistic and acoustic modalities. Although both models effectively\ncapture features relevant to oral competence, each exhibits modality-specific\nlimitations. BERT-based methods rely on ASR transcripts, which often fail to\ncapture prosodic and phonetic cues for SLA. In contrast, W2V-based methods\nexcel at modeling acoustic features but lack semantic interpretability. To\novercome these limitations, we propose a system that integrates W2V with Phi-4\nmultimodal large language model (MLLM) through a score fusion strategy. The\nproposed system achieves a root mean square error (RMSE) of 0.375 on the\nofficial test set of the Speak & Improve Challenge 2025, securing second place\nin the competition. For comparison, the RMSEs of the top-ranked, third-ranked,\nand official baseline systems are 0.364, 0.384, and 0.444, respectively.\n","authors":["Hong-Yun Lin","Tien-Hong Lo","Yu-Hsuan Fang","Jhen-Ke Lin","Chung-Chun Wang","Hao-Chien Lu","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05121v1.pdf","comment":"submitted to the ISCA SLaTE-2025 Workshop"},{"id":"http://arxiv.org/abs/2505.20277v2","updated":"2025-06-05T14:53:28Z","published":"2025-05-26T17:55:06Z","title":"OmniCharacter: Towards Immersive Role-Playing Agents with Seamless\n  Speech-Language Personality Interaction","summary":"  Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.\n","authors":["Haonan Zhang","Run Luo","Xiong Liu","Yuchuan Wu","Ting-En Lin","Pengpeng Zeng","Qiang Qu","Feiteng Fang","Min Yang","Lianli Gao","Jingkuan Song","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2505.20277v2.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.05107v1","updated":"2025-06-05T14:52:28Z","published":"2025-06-05T14:52:28Z","title":"CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework\n  for Misleading Text Detection on Social Media","summary":"  Misleading text detection on social media platforms is a critical research\narea, as these texts can lead to public misunderstanding, social panic and even\neconomic losses. This paper proposes a novel framework - CL-ISR (Contrastive\nLearning and Implicit Stance Reasoning), which combines contrastive learning\nand implicit stance reasoning, to improve the detection accuracy of misleading\ntexts on social media. First, we use the contrastive learning algorithm to\nimprove the model's learning ability of semantic differences between truthful\nand misleading texts. Contrastive learning could help the model to better\ncapture the distinguishing features between different categories by\nconstructing positive and negative sample pairs. This approach enables the\nmodel to capture distinguishing features more effectively, particularly in\nlinguistically complicated situations. Second, we introduce the implicit stance\nreasoning module, to explore the potential stance tendencies in the text and\ntheir relationships with related topics. This method is effective for\nidentifying content that misleads through stance shifting or emotional\nmanipulation, because it can capture the implicit information behind the text.\nFinally, we integrate these two algorithms together to form a new framework,\nCL-ISR, which leverages the discriminative power of contrastive learning and\nthe interpretive depth of stance reasoning to significantly improve detection\neffect.\n","authors":["Tianyi Huang","Zikun Cui","Cuiqianhe Du","Chia-En Chiang"],"pdf_url":"https://arxiv.org/pdf/2506.05107v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.03198v2","updated":"2025-06-05T14:35:42Z","published":"2024-05-28T04:36:15Z","title":"The Impossibility of Fair LLMs","summary":"  The rise of general-purpose artificial intelligence (AI) systems,\nparticularly large language models (LLMs), has raised pressing moral questions\nabout how to reduce bias and ensure fairness at scale. Researchers have\ndocumented a sort of \"bias\" in the significant correlations between\ndemographics (e.g., race, gender) in LLM prompts and responses, but it remains\nunclear how LLM fairness could be evaluated with more rigorous definitions,\nsuch as group fairness or fair representations. We analyze a variety of\ntechnical fairness frameworks and find inherent challenges in each that make\nthe development of a fair LLM intractable. We show that each framework either\ndoes not logically extend to the general-purpose AI context or is infeasible in\npractice, primarily due to the large amounts of unstructured training data and\nthe many potential combinations of human populations, use cases, and sensitive\nattributes. These inherent challenges would persist for general-purpose AI,\nincluding LLMs, even if empirical challenges, such as limited participatory\ninput and limited measurement methods, were overcome. Nonetheless, fairness\nwill remain an important type of model evaluation, and there are still\npromising research directions, particularly the development of standards for\nthe responsibility of LLM developers, context-specific evaluations, and methods\nof iterative, participatory, and AI-assisted evaluation that could scale\nfairness across the diverse contexts of modern human-AI interaction.\n","authors":["Jacy Anthis","Kristian Lum","Michael Ekstrand","Avi Feller","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2406.03198v2.pdf","comment":"Published in ACL 2025"},{"id":"http://arxiv.org/abs/2402.12649v3","updated":"2025-06-05T14:35:00Z","published":"2024-02-20T01:49:15Z","title":"Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation","summary":"  Standard benchmarks of bias and fairness in large language models (LLMs)\nmeasure the association between the user attributes stated or implied by a\nprompt and the LLM's short text response, but human-AI interaction increasingly\nrequires long-form and context-specific system output to solve real-world\ntasks. In the commonly studied domain of gender-occupation bias, we test\nwhether these benchmarks are robust to lengthening the LLM responses as a\nmeasure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From\nthe current literature, we adapt three standard bias metrics (neutrality, skew,\nand stereotype) and develop analogous RUTEd evaluations from three contexts of\nreal-world use: children's bedtime stories, user personas, and English language\nlearning exercises. We find that standard bias metrics have no significant\ncorrelation with the more realistic bias metrics. For example, selecting the\nleast biased model based on the standard \"trick tests\" coincides with selecting\nthe least biased model as measured in more realistic use no more than random\nchance. We suggest that there is not yet evidence to justify standard\nbenchmarks as reliable proxies of real-world AI biases, and we encourage\nfurther development of evaluations grounded in particular contexts.\n","authors":["Kristian Lum","Jacy Reese Anthis","Kevin Robinson","Chirag Nagpal","Alexander D'Amour"],"pdf_url":"https://arxiv.org/pdf/2402.12649v3.pdf","comment":"Published in ACL 2025"},{"id":"http://arxiv.org/abs/2506.05087v1","updated":"2025-06-05T14:34:04Z","published":"2025-06-05T14:34:04Z","title":"Interpretable Multimodal Framework for Human-Centered Street Assessment:\n  Integrating Visual-Language Models for Perceptual Urban Diagnostics","summary":"  While objective street metrics derived from imagery or GIS have become\nstandard in urban analytics, they remain insufficient to capture subjective\nperceptions essential to inclusive urban design. This study introduces a novel\nMultimodal Street Evaluation Framework (MSEF) that fuses a vision transformer\n(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable\ndual-output assessment of streetscapes. Leveraging over 15,000 annotated\nstreet-view images from Harbin, China, we fine-tune the framework using LoRA\nand P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1\nscore of 0.84 on objective features and 89.3 percent agreement with aggregated\nresident perceptions, validated across stratified socioeconomic geographies.\nBeyond classification accuracy, MSEF captures context-dependent contradictions:\nfor instance, informal commerce boosts perceived vibrancy while simultaneously\nreducing pedestrian comfort. It also identifies nonlinear and semantically\ncontingent patterns -- such as the divergent perceptual effects of\narchitectural transparency across residential and commercial zones -- revealing\nthe limits of universal spatial heuristics. By generating natural-language\nrationales grounded in attention mechanisms, the framework bridges sensory data\nwith socio-affective inference, enabling transparent diagnostics aligned with\nSDG 11. This work offers both methodological innovation in urban perception\nmodeling and practical utility for planning systems seeking to reconcile\ninfrastructural precision with lived experience.\n","authors":["HaoTian Lan"],"pdf_url":"https://arxiv.org/pdf/2506.05087v1.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.05080v1","updated":"2025-06-05T14:28:48Z","published":"2025-06-05T14:28:48Z","title":"Parking, Perception, and Retail: Street-Level Determinants of Community\n  Vitality in Harbin","summary":"  The commercial vitality of community-scale streets in Chinese cities is\nshaped by complex interactions between vehicular accessibility, environmental\nquality, and pedestrian perception. This study proposes an interpretable,\nimage-based framework to examine how street-level features -- including parked\nvehicle density, greenery, cleanliness, and street width -- impact retail\nperformance and user satisfaction in Harbin, China. Leveraging street view\nimagery and a multimodal large language model (VisualGLM-6B), we construct a\nCommunity Commercial Vitality Index (CCVI) from Meituan and Dianping data and\nanalyze its relationship with spatial attributes extracted via GPT-4-based\nperception modeling. Our findings reveal that while moderate vehicle presence\nmay enhance commercial access, excessive on-street parking -- especially in\nnarrow streets -- erodes walkability and reduces both satisfaction and\nshop-level pricing. In contrast, streets with higher perceived greenery and\ncleanliness show significantly greater satisfaction scores but only weak\nassociations with pricing. Street width moderates the effects of vehicle\npresence, underscoring the importance of spatial configuration. These results\ndemonstrate the value of integrating AI-assisted perception with urban\nmorphological analysis to capture non-linear and context-sensitive drivers of\ncommercial success. This study advances both theoretical and methodological\nfrontiers by highlighting the conditional role of vehicle activity in\nneighborhood commerce and demonstrating the feasibility of multimodal AI for\nperceptual urban diagnostics. The implications extend to urban design, parking\nmanagement, and scalable planning tools for community revitalization.\n","authors":["HaoTian Lan"],"pdf_url":"https://arxiv.org/pdf/2506.05080v1.pdf","comment":"22 pages,5 figures"},{"id":"http://arxiv.org/abs/2506.05073v1","updated":"2025-06-05T14:19:48Z","published":"2025-06-05T14:19:48Z","title":"Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection\n  through Intent Differentiation and Emoji Interpretation","summary":"  Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .\n","authors":["Soumitra Ghosh","Gopendra Vikram Singh"," Shambhavi","Sabarna Choudhury","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2506.05073v1.pdf","comment":"To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025 Main)"},{"id":"http://arxiv.org/abs/2506.05070v1","updated":"2025-06-05T14:18:21Z","published":"2025-06-05T14:18:21Z","title":"RIVAL: Reinforcement Learning with Iterative and Adversarial\n  Optimization for Machine Translation","summary":"  Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines.\n","authors":["Tianjiao Li","Mengran Yu","Chenyu Shi","Yanjun Zhao","Xiaojing Liu","Qiang Zhang","Qi Zhang","Xuanjing Huang","Jiayin Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14284v2","updated":"2025-06-05T14:17:05Z","published":"2024-06-20T13:09:29Z","title":"Leveraging LLMs for Bangla Grammar Error Correction:Error\n  Categorization, Synthetic Data, and Model Evaluation","summary":"  Large Language Models (LLMs) perform exceedingly well in Natural Language\nUnderstanding (NLU) tasks for many languages including English. However,\ndespite being the fifth most-spoken language globally, Grammatical Error\nCorrection (GEC) in Bangla remains underdeveloped. In this work, we investigate\nhow LLMs can be leveraged for improving Bangla GEC. For that, we first do an\nextensive categorization of 12 error classes in Bangla, and take a survey of\nnative Bangla speakers to collect real-world errors. We next devise a\nrule-based noise injection method to create grammatically incorrect sentences\ncorresponding to correct ones. The Vaiyakarana dataset, thus created, consists\nof 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then\nused to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show\nthat instruction-tuning with \\name improves GEC performance of LLMs by 3-7\npercentage points as compared to the zero-shot setting, and makes them achieve\nhuman-like performance in grammatical error identification. Humans, though,\nremain superior in error correction.\n","authors":["Pramit Bhattacharyya","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2406.14284v2.pdf","comment":"Accepted at ACL Findings, 2025"},{"id":"http://arxiv.org/abs/2502.12171v2","updated":"2025-06-05T14:16:46Z","published":"2025-02-13T10:33:58Z","title":"GoRA: Gradient-driven Adaptive Low Rank Adaptation","summary":"  Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings.\n","authors":["Haonan He","Peng Ye","Yuchen Ren","Yuan Yuan","Luyang Zhou","Shucun Ju","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05068v1","updated":"2025-06-05T14:13:54Z","published":"2025-06-05T14:13:54Z","title":"Does It Make Sense to Speak of Introspection in Large Language Models?","summary":"  Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown ``creative'' writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.\n","authors":["Iulia Comşa","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2506.05068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05062v1","updated":"2025-06-05T14:06:51Z","published":"2025-06-05T14:06:51Z","title":"Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation","summary":"  We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that have previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task.\n","authors":["Noy Sternlicht","Ariel Gera","Roy Bar-Haim","Tom Hope","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2506.05062v1.pdf","comment":"Code: https://github.com/noy-sternlicht/Debatable-Intelligence"},{"id":"http://arxiv.org/abs/2503.00847v3","updated":"2025-06-05T14:03:53Z","published":"2025-03-02T10:49:10Z","title":"Argument Summarization and its Evaluation in the Era of Large Language\n  Models","summary":"  Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum. We also show that among the four LLMs integrated in (i) and (ii),\nQwen-3-32B, despite having the fewest parameters, performs best, even\nsurpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms.\n","authors":["Moritz Altemeyer","Steffen Eger","Johannes Daxenberger","Yanran Chen","Tim Altendorf","Philipp Cimiano","Benjamin Schiller"],"pdf_url":"https://arxiv.org/pdf/2503.00847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05057v1","updated":"2025-06-05T14:02:12Z","published":"2025-06-05T14:02:12Z","title":"TALL -- A Trainable Architecture for Enhancing LLM Performance in\n  Low-Resource Languages","summary":"  Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains.\n","authors":["Moshe Ofer","Orel Zamler","Amos Azaria"],"pdf_url":"https://arxiv.org/pdf/2506.05057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13438v2","updated":"2025-06-05T13:55:06Z","published":"2025-05-19T17:58:44Z","title":"Optimizing Anytime Reasoning via Budget Relative Policy Optimization","summary":"  Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.\n","authors":["Penghui Qi","Zichen Liu","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2505.13438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05038v1","updated":"2025-06-05T13:42:39Z","published":"2025-06-05T13:42:39Z","title":"Automatic Robustness Stress Testing of LLMs as Mathematical Problem\n  Solvers","summary":"  Large language models (LLMs) have achieved distinguished performance on\nvarious reasoning-intensive tasks. However, LLMs might still face the\nchallenges of robustness issues and fail unexpectedly in some simple reasoning\ntasks. Previous works evaluate the LLM robustness with hand-crafted templates\nor a limited set of perturbation rules, indicating potential data contamination\nin pre-training or fine-tuning datasets. In this work, inspired by stress\ntesting in software engineering, we propose a novel framework, Automatic\nRobustness Checker (AR-Checker), to generate mathematical problem variants that\nmaintain the semantic meanings of the original one but might fail the LLMs. The\nAR-Checker framework generates mathematical problem variants through\nmulti-round parallel streams of LLM-based rewriting and verification. Our\nframework can generate benchmark variants dynamically for each LLM, thus\nminimizing the risk of data contamination. Experiments on GSM8K and MATH-500\ndemonstrate the strong performance of AR-Checker on mathematical tasks. We also\nevaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,\nand CommonsenseQA, where it also achieves strong performance, further proving\nthe effectiveness of AR-Checker.\n","authors":["Yutao Hou","Zeguan Xiao","Fei Yu","Yihan Jiang","Xuetao Wei","Hailiang Huang","Yun Chen","Guanhua Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09117v3","updated":"2025-06-05T13:34:42Z","published":"2025-03-12T07:08:54Z","title":"GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs","summary":"  Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. It motivates\nthis paper to explore enhanced unlearning schemes that can mitigate this\ntrade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an\nimproved framework that regulates the directions of gradient updates during the\nunlearning procedure such that their side impacts on other, unrelated responses\ncan be minimized. GRU is easy and general to implement, demonstrating practical\neffectiveness across a variety of well-established unlearning benchmarks.\n","authors":["Yue Wang","Qizhou Wang","Feng Liu","Wei Huang","Yali Du","Xiaojiang Du","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2503.09117v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04055v3","updated":"2025-06-05T13:25:43Z","published":"2024-10-05T06:28:54Z","title":"Self-Correction is More than Refinement: A Learning Framework for Visual\n  and Language Reasoning Tasks","summary":"  While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement.\n","authors":["Jiayi He","Hehai Lin","Qingyun Wang","Yi Fung","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2410.04055v3.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.05017v1","updated":"2025-06-05T13:25:28Z","published":"2025-06-05T13:25:28Z","title":"Controlling Summarization Length Through EOS Token Weighting","summary":"  Controlling the length of generated text can be crucial in various\ntext-generation tasks, including summarization. Existing methods often require\ncomplex model alterations, limiting compatibility with pre-trained models. We\naddress these limitations by developing a simple approach for controlling the\nlength of automatic text summaries by increasing the importance of correctly\npredicting the EOS token in the cross-entropy loss computation. The proposed\nmethodology is agnostic to architecture and decoding algorithms and orthogonal\nto other inference-time techniques to control the generation length. We tested\nit with encoder-decoder and modern GPT-style LLMs, and show that this method\ncan control generation length, often without affecting the quality of the\nsummary.\n","authors":["Zeno Belligoli","Emmanouil Stergiadis","Eran Fainman","Ilya Gusev"],"pdf_url":"https://arxiv.org/pdf/2506.05017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05010v1","updated":"2025-06-05T13:20:50Z","published":"2025-06-05T13:20:50Z","title":"ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development","summary":"  We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.\n","authors":["Zhenran Xu","Xue Yang","Yiyu Wang","Qingli Hu","Zijiao Wu","Longyue Wang","Weihua Luo","Kaifu Zhang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05010v1.pdf","comment":"ACL 2025 Demo. Github: https://github.com/AIDC-AI/ComfyUI-Copilot"},{"id":"http://arxiv.org/abs/2502.13063v2","updated":"2025-06-05T13:20:09Z","published":"2025-02-18T17:08:45Z","title":"Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity","summary":"  A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.\n","authors":["Yuri Kuratov","Mikhail Arkhipov","Aydar Bulatov","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2502.13063v2.pdf","comment":"ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2505.21091v2","updated":"2025-06-05T13:12:53Z","published":"2025-05-27T12:19:08Z","title":"Position is Power: System Prompts as a Mechanism of Bias in Large\n  Language Models (LLMs)","summary":"  System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.\n","authors":["Anna Neumann","Elisabeth Kirsten","Muhammad Bilal Zafar","Jatinder Singh"],"pdf_url":"https://arxiv.org/pdf/2505.21091v2.pdf","comment":"Forthcoming in Proceedings of ACM FAccT 2025"},{"id":"http://arxiv.org/abs/2506.05000v1","updated":"2025-06-05T13:10:24Z","published":"2025-06-05T13:10:24Z","title":"SCOP: Evaluating the Comprehension Process of Large Language Models from\n  a Cognitive View","summary":"  Despite the great potential of large language models(LLMs) in machine\ncomprehension, it is still disturbing to fully count on them in real-world\nscenarios. This is probably because there is no rational explanation for\nwhether the comprehension process of LLMs is aligned with that of experts. In\nthis paper, we propose SCOP to carefully examine how LLMs perform during the\ncomprehension process from a cognitive view. Specifically, it is equipped with\na systematical definition of five requisite skills during the comprehension\nprocess, a strict framework to construct testing data for these skills, and a\ndetailed analysis of advanced open-sourced and closed-sourced LLMs using the\ntesting data. With SCOP, we find that it is still challenging for LLMs to\nperform an expert-level comprehension process. Even so, we notice that LLMs\nshare some similarities with experts, e.g., performing better at comprehending\nlocal information than global information. Further analysis reveals that LLMs\ncan be somewhat unreliable -- they might reach correct answers through flawed\ncomprehension processes. Based on SCOP, we suggest that one direction for\nimproving LLMs is to focus more on the comprehension process, ensuring all\ncomprehension skills are thoroughly developed during training.\n","authors":["Yongjie Xiao","Hongru Liang","Peixin Qin","Yao Zhang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2506.05000v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2004.14535 by other authors"},{"id":"http://arxiv.org/abs/2506.04997v1","updated":"2025-06-05T13:06:01Z","published":"2025-06-05T13:06:01Z","title":"Towards Storage-Efficient Visual Document Retrieval: An Empirical Study\n  on Reducing Patch-Level Embeddings","summary":"  Despite the strong performance of ColPali/ColQwen2 in Visualized Document\nRetrieval (VDR), it encodes each page into multiple patch-level embeddings and\nleads to excessive memory usage. This empirical study investigates methods to\nreduce patch embeddings per page at minimum performance degradation. We\nevaluate two token-reduction strategies: token pruning and token merging.\nRegarding token pruning, we surprisingly observe that a simple random strategy\noutperforms other sophisticated pruning methods, though still far from\nsatisfactory. Further analysis reveals that pruning is inherently unsuitable\nfor VDR as it requires removing certain page embeddings without query-specific\ninformation. Turning to token merging (more suitable for VDR), we search for\nthe optimal combinations of merging strategy across three dimensions and\ndevelop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance\nwith only 11.8% of original memory usage, and preserves 94.6% effectiveness at\n2.8% memory footprint. We expect our empirical findings and resulting\nLight-ColPali/ColQwen2 offer valuable insights and establish a competitive\nbaseline for future research towards efficient VDR.\n","authors":["Yubo Ma","Jinsong Li","Yuhang Zang","Xiaobao Wu","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Haodong Duan","Jiaqi Wang","Yixin Cao","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2506.04997v1.pdf","comment":"Accepted by ACL 2025 findings"},{"id":"http://arxiv.org/abs/2506.03785v2","updated":"2025-06-05T13:01:33Z","published":"2025-06-04T09:46:43Z","title":"Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons","summary":"  Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.\n","authors":["Isik Baran Sandan","Tu Anh Dinh","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2506.03785v2.pdf","comment":"Accepted to GEM @ ACL 2025"},{"id":"http://arxiv.org/abs/2506.04981v1","updated":"2025-06-05T12:53:20Z","published":"2025-06-05T12:53:20Z","title":"Better Semi-supervised Learning for Multi-domain ASR Through Incremental\n  Retraining and Data Filtering","summary":"  Fine-tuning pretrained ASR models for specific domains is challenging when\nlabeled data is scarce. But unlabeled audio and labeled data from related\ndomains are often available. We propose an incremental semi-supervised learning\npipeline that first integrates a small in-domain labeled set and an auxiliary\ndataset from a closely related domain, achieving a relative improvement of 4%\nover no auxiliary data. Filtering based on multi-model consensus or named\nentity recognition (NER) is then applied to select and iteratively refine\npseudo-labels, showing slower performance saturation compared to random\nselection. Evaluated on the multi-domain Wow call center and Fisher English\ncorpora, it outperforms single-step fine-tuning. Consensus-based filtering\noutperforms other methods, providing up to 22.3% relative improvement on Wow\nand 24.8% on Fisher over single-step fine-tuning with random selection. NER is\nthe second-best filter, providing competitive performance at a lower\ncomputational cost.\n","authors":["Andres Carofilis","Pradeep Rangappa","Srikanth Madikeri","Shashi Kumar","Sergio Burdisso","Jeena Prakash","Esau Villatoro-Tello","Petr Motlicek","Bidisha Sharma","Kadri Hacioglu","Shankar Venkatesan","Saurabh Vyas","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2506.04981v1.pdf","comment":"Accepted at Interspeech 2025, Netherlands"},{"id":"http://arxiv.org/abs/2506.02672v2","updated":"2025-06-05T12:44:51Z","published":"2025-06-03T09:18:33Z","title":"EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving","summary":"  We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.\n","authors":["Shihan Dou","Ming Zhang","Chenhao Huang","Jiayi Chen","Feng Chen","Shichun Liu","Yan Liu","Chenxiao Liu","Cheng Zhong","Zongzhang Zhang","Tao Gui","Chao Xin","Wei Chengzhi","Lin Yan","Qi Zhang","Yonghui Wu","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2506.02672v2.pdf","comment":"47 pages, 24 figures"},{"id":"http://arxiv.org/abs/2506.04965v1","updated":"2025-06-05T12:41:20Z","published":"2025-06-05T12:41:20Z","title":"From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced\n  Algorithm Exams and Pave the Way for Editorial Generation","summary":"  This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education.\n","authors":["Adrian Marius Dumitran","Theodor-Pierre Moroianu","Vasile Paul Alexe"],"pdf_url":"https://arxiv.org/pdf/2506.04965v1.pdf","comment":"15 pages Pre-print Paper accepted to ITS 2025"},{"id":"http://arxiv.org/abs/2504.17934v2","updated":"2025-06-05T12:40:53Z","published":"2025-04-24T20:51:20Z","title":"Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered\n  GUI Agents","summary":"  The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation.\n","authors":["Chaoran Chen","Zhiping Zhang","Ibrahim Khalilov","Bingcan Guo","Simret A Gebreegziabher","Yanfang Ye","Ziang Xiao","Yaxing Yao","Tianshi Li","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2504.17934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23404v2","updated":"2025-06-05T12:24:01Z","published":"2025-05-29T12:50:57Z","title":"Adaptive Jailbreaking Strategies Based on the Semantic Understanding\n  Capabilities of Large Language Models","summary":"  Adversarial attacks on Large Language Models (LLMs) via jailbreaking\ntechniques-methods that circumvent their built-in safety and ethical\nconstraints-have emerged as a critical challenge in AI security. These attacks\ncompromise the reliability of LLMs by exploiting inherent weaknesses in their\ncomprehension capabilities. This paper investigates the efficacy of\njailbreaking strategies that are specifically adapted to the diverse levels of\nunderstanding exhibited by different LLMs. We propose the Adaptive Jailbreaking\nStrategies Based on the Semantic Understanding Capabilities of Large Language\nModels, a novel framework that classifies LLMs into Type I and Type II\ncategories according to their semantic comprehension abilities. For each\ncategory, we design tailored jailbreaking strategies aimed at leveraging their\nvulnerabilities to facilitate successful attacks. Extensive experiments\nconducted on multiple LLMs demonstrate that our adaptive strategy markedly\nimproves the success rate of jailbreaking. Notably, our approach achieves an\nexceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)\n","authors":["Mingyu Yu","Wei Wang","Yanjie Wei","Sujuan Qin"],"pdf_url":"https://arxiv.org/pdf/2505.23404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15821v2","updated":"2025-06-05T12:05:19Z","published":"2025-02-20T03:01:08Z","title":"Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action\n  Analysis with Cross-Category Generalization","summary":"  Sustainability reports are key for evaluating companies' environmental,\nsocial and governance, ESG performance, but their content is increasingly\nobscured by greenwashing - sustainability claims that are misleading,\nexaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack\nrobustness against greenwashing risks, often extracting insights that reflect\nmisleading or exaggerated sustainability claims rather than objective ESG\nperformance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis\nwith Cross-Category Generalization, as a novel dataset to improve the\nrobustness of ESG analysis amid the prevalence of greenwashing. By explicitly\nlinking sustainability aspects with their associated actions, A3CG facilitates\na more fine-grained and transparent evaluation of sustainability claims,\nensuring that insights are grounded in verifiable actions rather than vague or\nmisleading rhetoric. Additionally, A3CG emphasizes cross-category\ngeneralization. This ensures robust model performance in aspect-action analysis\neven when companies change their reports to selectively favor certain\nsustainability areas. Through experiments on A3CG, we analyze state-of-the-art\nsupervised models and LLMs, uncovering their limitations and outlining key\ndirections for future research.\n","authors":["Keane Ong","Rui Mao","Deeksha Varshney","Erik Cambria","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2502.15821v2.pdf","comment":"Proceedings of the Association for Computational Linguistics Main\n  Conference (ACL 2025)"},{"id":"http://arxiv.org/abs/2506.04929v1","updated":"2025-06-05T12:02:01Z","published":"2025-06-05T12:02:01Z","title":"ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT","summary":"  Neural Machine Translation (NMT) has improved translation by using\nTransformer-based models, but it still struggles with word ambiguity and\ncontext. This problem is especially important in domain-specific applications,\nwhich often have problems with unclear sentences or poor data quality. Our\nresearch explores how adding information to models can improve translations in\nthe context of e-commerce data. To this end we create ConECT -- a new\nCzech-to-Polish e-commerce product translation dataset coupled with images and\nproduct metadata consisting of 11,400 sentence pairs. We then investigate and\ncompare different methods that are applicable to context-aware translation. We\ntest a vision-language model (VLM), finding that visual context aids\ntranslation quality. Additionally, we explore the incorporation of contextual\ninformation into text-to-text models, such as the product's category path or\nimage descriptions. The results of our study demonstrate that the incorporation\nof contextual information leads to an improvement in the quality of machine\ntranslation. We make the new dataset publicly available.\n","authors":["Mikołaj Pokrywka","Wojciech Kusa","Mieszko Rutkowski","Mikołaj Koszowski"],"pdf_url":"https://arxiv.org/pdf/2506.04929v1.pdf","comment":"Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)"},{"id":"http://arxiv.org/abs/2505.19430v2","updated":"2025-06-05T11:59:20Z","published":"2025-05-26T02:41:50Z","title":"Deriving Strategic Market Insights with Large Language Models: A\n  Benchmark for Forward Counterfactual Generation","summary":"  Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research.\n","authors":["Keane Ong","Rui Mao","Deeksha Varshney","Paul Pu Liang","Erik Cambria","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2505.19430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04920v1","updated":"2025-06-05T11:53:04Z","published":"2025-06-05T11:53:04Z","title":"Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback","summary":"  Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.\n","authors":["Junior Cedric Tonga","KV Aditya Srivatsa","Kaushal Kumar Maurya","Fajri Koto","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2506.04920v1.pdf","comment":"Preprint, in submission"},{"id":"http://arxiv.org/abs/2506.04915v1","updated":"2025-06-05T11:52:08Z","published":"2025-06-05T11:52:08Z","title":"A Practitioner's Guide to Building ASR Models for Low-Resource\n  Languages: A Case Study on Scottish Gaelic","summary":"  An effective approach to the development of ASR systems for low-resource\nlanguages is to fine-tune an existing multilingual end-to-end model. When the\noriginal model has been trained on large quantities of data from many\nlanguages, fine-tuning can be effective with limited training data, even when\nthe language in question was not present in the original training data. The\nfine-tuning approach has been encouraged by the availability of public-domain\nE2E models and is widely believed to lead to state-of-the-art results. This\npaper, however, challenges that belief. We show that an approach combining\nhybrid HMMs with self-supervised models can yield substantially better\nperformance with limited training data. This combination allows better\nutilisation of all available speech and text data through continued\nself-supervised pre-training and semi-supervised training. We benchmark our\napproach on Scottish Gaelic, achieving WER reductions of 32% relative over our\nbest fine-tuned Whisper model.\n","authors":["Ondřej Klejch","William Lamb","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2506.04915v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.20445v3","updated":"2025-06-05T11:49:15Z","published":"2025-05-26T18:38:59Z","title":"In-context Language Learning for Endangered Languages in Speech\n  Recognition","summary":"  With approximately 7,000 languages spoken worldwide, current large language\nmodels (LLMs) support only a small subset. Prior research indicates LLMs can\nlearn new languages for certain tasks without supervised data. We extend this\ninvestigation to speech recognition, investigating whether LLMs can learn\nunseen, low-resource languages through in-context learning (ICL). With\nexperiments on four diverse endangered languages that LLMs have not been\ntrained on, we find that providing more relevant text samples enhances\nperformance in both language modelling and Automatic Speech Recognition (ASR)\ntasks. Furthermore, we show that the probability-based approach outperforms the\ntraditional instruction-based approach in language learning. Lastly, we show\nICL enables LLMs to achieve ASR performance that is comparable to or even\nsurpasses dedicated language models trained specifically for these languages,\nwhile preserving the original capabilities of the LLMs.\n","authors":["Zhaolin Li","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2505.20445v3.pdf","comment":"Interspeech2025"},{"id":"http://arxiv.org/abs/2505.07608v2","updated":"2025-06-05T11:49:09Z","published":"2025-05-12T14:30:11Z","title":"MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining","summary":"  We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.\n","authors":["LLM-Core Xiaomi"," :","Bingquan Xia","Bowen Shen"," Cici","Dawei Zhu","Di Zhang","Gang Wang","Hailin Zhang","Huaqiu Liu","Jiebao Xiao","Jinhao Dong","Liang Zhao","Peidian Li","Peng Wang","Shihua Yu","Shimao Chen","Weikun Wang","Wenhan Ma","Xiangwei Deng","Yi Huang","Yifan Song","Zihan Jiang","Bowen Ye","Can Cai","Chenhong He","Dong Zhang","Duo Zhang","Guoan Wang","Hao Tian","Haochen Zhao","Heng Qu","Hongshen Xu","Jun Shi","Kainan Bao","Kai Fang","Kang Zhou","Kangyang Zhou","Lei Li","Menghang Zhu","Nuo Chen","Qiantong Wang","Shaohui Liu","Shicheng Li","Shuhao Gu","Shuhuai Ren","Shuo Liu","Sirui Deng","Weiji Zhuang","Weiwei Lv","Wenyu Yang","Xin Zhang","Xing Yong","Xing Zhang","Xingchen Song","Xinzhe Xu","Xu Wang","Yihan Yan","Yu Tu","Yuanyuan Tian","Yudong Wang","Yue Yu","Zhenru Lin","Zhichao Song","Zihao Yue"],"pdf_url":"https://arxiv.org/pdf/2505.07608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04913v1","updated":"2025-06-05T11:47:10Z","published":"2025-06-05T11:47:10Z","title":"Dissecting Long Reasoning Models: An Empirical Study","summary":"  Despite recent progress in training long-context reasoning models via\nreinforcement learning (RL), several open questions and counterintuitive\nbehaviors remain. This work focuses on three key aspects: (1) We systematically\nanalyze the roles of positive and negative samples in RL, revealing that\npositive samples mainly facilitate data fitting, whereas negative samples\nsignificantly enhance generalization and robustness. Interestingly, training\nsolely on negative samples can rival standard RL training performance. (2) We\nidentify substantial data inefficiency in group relative policy optimization,\nwhere over half of the samples yield zero advantage. To address this, we\nexplore two straightforward strategies, including relative length rewards and\noffline sample injection, to better leverage these data and enhance reasoning\nefficiency and capability. (3) We investigate unstable performance across\nvarious reasoning models and benchmarks, attributing instability to uncertain\nproblems with ambiguous outcomes, and demonstrate that multiple evaluation runs\nmitigate this issue.\n","authors":["Yongyu Mu","Jiali Zeng","Bei Li","Xinyan Guan","Fandong Meng","Jie Zhou","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.04913v1.pdf","comment":"Working in process"},{"id":"http://arxiv.org/abs/2506.04909v1","updated":"2025-06-05T11:44:19Z","published":"2025-06-05T11:44:19Z","title":"When Thinking LLMs Lie: Unveiling the Strategic Deception in\n  Representations of Reasoning Models","summary":"  The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.\n","authors":["Kai Wang","Yihao Zhang","Meng Sun"],"pdf_url":"https://arxiv.org/pdf/2506.04909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04907v1","updated":"2025-06-05T11:41:05Z","published":"2025-06-05T11:41:05Z","title":"Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning\n  Blind Spots","summary":"  Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.\n","authors":["Alex Pan","Mary-Anne Williams"],"pdf_url":"https://arxiv.org/pdf/2506.04907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04894v1","updated":"2025-06-05T11:20:37Z","published":"2025-06-05T11:20:37Z","title":"ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive\n  Programming Contests","summary":"  With the significant progress of large reasoning models in complex coding and\nreasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are\ninsufficient to evaluate the coding capabilities of large language models\n(LLMs) in real competition environments. Moreover, current evaluation metrics\nsuch as Pass@K fail to capture the reflective abilities of reasoning models. To\naddress these challenges, we propose \\textbf{ICPC-Eval}, a top-level\ncompetitive coding benchmark designed to probing the frontiers of LLM\nreasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent\nICPC contests held in various regions of the world, offering three key\ncontributions: 1) A challenging realistic ICPC competition scenario, featuring\na problem type and difficulty distribution consistent with actual contests. 2)\nA robust test case generation method and a corresponding local evaluation\ntoolkit, enabling efficient and accurate local evaluation. 3) An effective\ntest-time scaling evaluation metric, Refine@K, which allows iterative repair of\nsolutions based on execution feedback. The results underscore the significant\nchallenge in evaluating complex reasoning abilities: top-tier reasoning models\nlike DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their\nin-context reasoning potential when compared to non-reasoning counterparts.\nFurthermore, despite recent advancements in code generation, these models still\nlag behind top-performing human teams. We release the benchmark at:\nhttps://github.com/RUCAIBox/Slow_Thinking_with_LLMs\n","authors":["Shiyi Xu","Yiwen Hu","Yingqian Min","Zhipeng Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2506.04894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05946v2","updated":"2025-06-05T11:13:42Z","published":"2025-05-09T10:43:37Z","title":"Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency\n  and Domain Knowledge","summary":"  In this technical report, we empirically investigate the relationship between\nlinguistic fluency and domain knowledge in the context of continual learning\nwith large language models (LLMs). Specifically, we enhance the linguistic\nfluency of the Gemma2 LLM for the Lithuanian language by autoregressively\npretraining its full parameter set on the first 10\\% of the Lithuanian language\ncomponent of the CulturaX dataset. To prevent catastrophic forgetting of the\nmodel's existing domain knowledge, we apply Elastic Weight Consolidation (EWC),\nleveraging Fisher information estimated using data from the Massive Multitask\nLanguage Understanding (MMLU) benchmark. In the post-training evaluations, we\nassess linguistic fluency through perplexity and evaluate domain knowledge\nusing accuracy on a suite of language understanding benchmarks, including\nARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both\nEnglish and Lithuanian. The empirical results demonstrate that EWC not only\nmitigates catastrophic forgetting by preserving the model's performance in\nterms of both linguistic fluency and domain knowledge but also improves or\nmaintains these capabilities for the newly added Lithuanian language. These\nfindings highlight the potential for more efficient adaptation of\ngeneral-purpose LLMs to under-represented languages without requiring access to\nthe original training data. The accompanying codebase is openly accessible at\nhttps://github.com/Neurotechnology/LLM_EWC.\n","authors":["Vytenis Šliogeris","Povilas Daniušis","Artūras Nakvosas"],"pdf_url":"https://arxiv.org/pdf/2505.05946v2.pdf","comment":"9 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.04887v1","updated":"2025-06-05T11:10:14Z","published":"2025-06-05T11:10:14Z","title":"Evaluating the Effectiveness of Linguistic Knowledge in Pretrained\n  Language Models: A Case Study of Universal Dependencies","summary":"  Universal Dependencies (UD), while widely regarded as the most successful\nlinguistic framework for cross-lingual syntactic representation, remains\nunderexplored in terms of its effectiveness. This paper addresses this gap by\nintegrating UD into pretrained language models and assesses if UD can improve\ntheir performance on a cross-lingual adversarial paraphrase identification\ntask. Experimental results show that incorporation of UD yields significant\nimprovements in accuracy and $F_1$ scores, with average gains of 3.85\\% and\n6.08\\% respectively. These enhancements reduce the performance gap between\npretrained models and large language models in some language pairs, and even\noutperform the latter in some others. Furthermore, the UD-based similarity\nscore between a given language and English is positively correlated to the\nperformance of models in that language. Both findings highlight the validity\nand potential of UD in out-of-domain tasks.\n","authors":["Wenxi Li"],"pdf_url":"https://arxiv.org/pdf/2506.04887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00975v2","updated":"2025-06-05T11:09:58Z","published":"2025-06-01T12:01:40Z","title":"NTPP: Generative Speech Language Modeling for Dual-Channel Spoken\n  Dialogue via Next-Token-Pair Prediction","summary":"  Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications.\n","authors":["Qichao Wang","Ziqiao Meng","Wenqian Cui","Yifei Zhang","Pengcheng Wu","Bingzhe Wu","Irwin King","Liang Chen","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.00975v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2504.07749v2","updated":"2025-06-05T10:51:26Z","published":"2025-04-10T13:44:55Z","title":"NorEval: A Norwegian Language Understanding and Generation Evaluation\n  Benchmark","summary":"  This paper introduces NorEval, a new and comprehensive evaluation suite for\nlarge-scale standardized benchmarking of Norwegian generative language models\n(LMs). NorEval consists of 24 high-quality human-created datasets -- of which\nfive are created from scratch. In contrast to existing benchmarks for\nNorwegian, NorEval covers a broad spectrum of task categories targeting\nNorwegian language understanding and generation, establishes human baselines,\nand focuses on both of the official written standards of the Norwegian\nlanguage: Bokm{\\aa}l and Nynorsk. All our datasets and a collection of over 100\nhuman-written prompts are integrated into LM Evaluation Harness, ensuring\nflexible and reproducible evaluation. We describe the NorEval design and\npresent the results of benchmarking 19 open-source pre-trained and\ninstruction-tuned LMs for Norwegian in various scenarios. Our benchmark,\nevaluation framework, and annotation materials are publicly available.\n","authors":["Vladislav Mikhailov","Tita Enstad","David Samuel","Hans Christian Farsethås","Andrey Kutuzov","Erik Velldal","Lilja Øvrelid"],"pdf_url":"https://arxiv.org/pdf/2504.07749v2.pdf","comment":"Accepted for Findings of the Association for Computational\n  Linguistics: ACL 2025"},{"id":"http://arxiv.org/abs/2505.22251v2","updated":"2025-06-05T10:40:05Z","published":"2025-05-28T11:39:59Z","title":"Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in\n  Large Language Models for Speech Recognition","summary":"  Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\ncontamination impact, LLMs trained with/without contamination are compared. A\ncontaminated LLM is more likely to generate test sentences it has seen during\ntraining. Then, speech recognisers based on LLMs are compared. They show only\nsubtle error rate differences if the LLM is contaminated, but assign\nsignificantly higher probabilities to transcriptions seen during LLM training.\nResults show that LLM outputs can be biased by tiny amounts of data\ncontamination, highlighting the importance of evaluating LLM-based speech\nsystems with held-out data.\n","authors":["Yuan Tseng","Titouan Parcollet","Rogier van Dalen","Shucong Zhang","Sourav Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2505.22251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01271v2","updated":"2025-06-05T10:34:56Z","published":"2024-12-02T08:38:19Z","title":"MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages\n  with Negligible Cost","summary":"  In this work, we explore a cost-effective framework for multilingual image\ngeneration. We find that, unlike models tuned on high-quality images with\nmultilingual annotations, leveraging text encoders pre-trained on widely\navailable, noisy Internet image-text pairs significantly enhances data\nefficiency in text-to-image (T2I) generation across multiple languages.Based on\nthis insight, we introduce MuLan, Multi-Language adapter, a lightweight\nlanguage adapter with fewer than 20M parameters, trained alongside a frozen\ntext encoder and image diffusion model. Compared to previous multilingual T2I\nmodels, this framework offers: (1) Cost efficiency. Using readily accessible\nEnglish data and off-the-shelf multilingual text encoders minimizes the\ntraining cost; (2) High performance. Achieving comparable generation\ncapabilities in over 110 languages with CLIP similarity scores nearly matching\nthose in English (39.57 for English vs. 39.61 for other languages); and (3)\nBroad applicability. Seamlessly integrating with compatible community tools\nlike LoRA, LCM, ControlNet, and IP-Adapter, expanding its potential use cases.\n","authors":["Sen Xing","Muyan Zhong","Zeqiang Lai","Liangchen Li","Jiawen Liu","Yaohui Wang","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.01271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04855v1","updated":"2025-06-05T10:24:08Z","published":"2025-06-05T10:24:08Z","title":"Prompting LLMs: Length Control for Isometric Machine Translation","summary":"  In this study, we explore the effectiveness of isometric machine translation\nacross multiple language pairs (En$\\to$De, En$\\to$Fr, and En$\\to$Es) under the\nconditions of the IWSLT Isometric Shared Task 2022. Using eight open-source\nlarge language models (LLMs) of varying sizes, we investigate how different\nprompting strategies, varying numbers of few-shot examples, and demonstration\nselection influence translation quality and length control. We discover that\nthe phrasing of instructions, when aligned with the properties of the provided\ndemonstrations, plays a crucial role in controlling the output length. Our\nexperiments show that LLMs tend to produce shorter translations only when\npresented with extreme examples, while isometric demonstrations often lead to\nthe models disregarding length constraints. While few-shot prompting generally\nenhances translation quality, further improvements are marginal across 5, 10,\nand 20-shot settings. Finally, considering multiple outputs allows to notably\nimprove overall tradeoff between the length and quality, yielding\nstate-of-the-art performance for some language pairs.\n","authors":["Dávid Javorský","Ondřej Bojar","François Yvon"],"pdf_url":"https://arxiv.org/pdf/2506.04855v1.pdf","comment":"Accepted to IWSLT 2025"},{"id":"http://arxiv.org/abs/2506.04851v1","updated":"2025-06-05T10:21:49Z","published":"2025-06-05T10:21:49Z","title":"Multiple-Choice Question Generation Using Large Language Models:\n  Methodology and Educator Insights","summary":"  Integrating Artificial Intelligence (AI) in educational settings has brought\nnew learning approaches, transforming the practices of both students and\neducators. Among the various technologies driving this transformation, Large\nLanguage Models (LLMs) have emerged as powerful tools for creating educational\nmaterials and question answering, but there are still space for new\napplications. Educators commonly use Multiple-Choice Questions (MCQs) to assess\nstudent knowledge, but manually generating these questions is\nresource-intensive and requires significant time and cognitive effort. In our\nopinion, LLMs offer a promising solution to these challenges. This paper\npresents a novel comparative analysis of three widely known LLMs - Llama 2,\nMistral, and GPT-3.5 - to explore their potential for creating informative and\nchallenging MCQs. In our approach, we do not rely on the knowledge of the LLM,\nbut we inject the knowledge into the prompt to contrast the hallucinations,\ngiving the educators control over the test's source text, too. Our experiment\ninvolving 21 educators shows that GPT-3.5 generates the most effective MCQs\nacross several known metrics. Additionally, it shows that there is still some\nreluctance to adopt AI in the educational field. This study sheds light on the\npotential of LLMs to generate MCQs and improve the educational experience,\nproviding valuable insights for the future.\n","authors":["Giorgio Biancini","Alessio Ferrato","Carla Limongelli"],"pdf_url":"https://arxiv.org/pdf/2506.04851v1.pdf","comment":"Copyright ACM 2024. This is the author's version of the work. It is\n  posted here for your personal use. Not for redistribution. The definitive\n  Version of Record was published in Adjunct Proceedings of the 32nd ACM\n  Conference on User Modeling, Adaptation and Personalization (UMAP Adjunct\n  '24), http://dx.doi.org/10.1145/3631700.3665233"},{"id":"http://arxiv.org/abs/2503.03417v3","updated":"2025-06-05T10:17:20Z","published":"2025-03-05T11:47:32Z","title":"When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits","summary":"  Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on claim matching systems that use sentence embedding models\nto retrieve relevant fact-checks. However, as users interact with claims\nonline, they often introduce edits, and it remains unclear whether current\nembedding models used in retrieval are robust to such edits. To investigate\nthis, we introduce a perturbation framework that generates valid and natural\nclaim variations, enabling us to assess the robustness of a wide-range of\nsentence embedding models in a multi-stage retrieval pipeline and evaluate the\neffectiveness of various mitigation approaches. Our evaluation reveals that\nstandard embedding models exhibit notable performance drops on edited claims,\nwhile LLM-distilled embedding models offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps to reduce the performance\ndrop, it cannot fully compensate for first-stage retrieval gaps. To address\nthese retrieval gaps, we evaluate train- and inference-time mitigation\napproaches, demonstrating that they can improve in-domain robustness by up to\n17 percentage points and boost out-of-domain generalization by 10 percentage\npoints. Overall, our findings provide practical improvements to claim-matching\nsystems, enabling more reliable fact-checking of evolving misinformation. Code\nand data are available at\nhttps://github.com/JabezNzomo99/claim-matching-robustness.\n","authors":["Jabez Magomere","Emanuele La Malfa","Manuel Tonneau","Ashkan Kazemi","Scott Hale"],"pdf_url":"https://arxiv.org/pdf/2503.03417v3.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.04848v1","updated":"2025-06-05T10:16:15Z","published":"2025-06-05T10:16:15Z","title":"MockConf: A Student Interpretation Dataset: Analysis, Word- and\n  Span-level Alignment and Baselines","summary":"  In simultaneous interpreting, an interpreter renders a source speech into\nanother language with a very short lag, much sooner than sentences are\nfinished. In order to understand and later reproduce this dynamic and complex\ntask automatically, we need dedicated datasets and tools for analysis,\nmonitoring, and evaluation, such as parallel speech corpora, and tools for\ntheir automatic annotation. Existing parallel corpora of translated texts and\nassociated alignment algorithms hardly fill this gap, as they fail to model\nlong-range interactions between speech segments or specific types of\ndivergences (e.g., shortening, simplification, functional generalization)\nbetween the original and interpreted speeches. In this work, we introduce\nMockConf, a student interpreting dataset that was collected from Mock\nConferences run as part of the students' curriculum. This dataset contains 7\nhours of recordings in 5 European languages, transcribed and aligned at the\nlevel of spans and words. We further implement and release InterAlign, a modern\nweb-based annotation tool for parallel word and span annotations on long\ninputs, suitable for aligning simultaneous interpreting. We propose metrics for\nthe evaluation and a baseline for automatic alignment. Dataset and tools are\nreleased to the community.\n","authors":["Dávid Javorský","Ondřej Bojar","François Yvon"],"pdf_url":"https://arxiv.org/pdf/2506.04848v1.pdf","comment":"Accepted to ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2410.12656v4","updated":"2025-06-05T10:08:39Z","published":"2024-10-16T15:17:20Z","title":"Evaluating Morphological Compositional Generalization in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n","authors":["Mete Ismayilzada","Defne Circi","Jonne Sälevä","Hale Sirin","Abdullatif Köksal","Bhuwan Dhingra","Antoine Bosselut","Duygu Ataman","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2410.12656v4.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2505.20354v2","updated":"2025-06-05T09:59:09Z","published":"2025-05-26T06:25:43Z","title":"Rethinking Text-based Protein Understanding: Retrieval or LLM?","summary":"  In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM.\n","authors":["Juntong Wu","Zijing Liu","He Cao","Hao Li","Bin Feng","Zishan Shu","Ke Yu","Li Yuan","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2505.20354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04832v1","updated":"2025-06-05T09:54:04Z","published":"2025-06-05T09:54:04Z","title":"Joint Evaluation of Answer and Reasoning Consistency for Hallucination\n  Detection in Large Reasoning Models","summary":"  Large Reasoning Models (LRMs) extend large language models with explicit,\nmulti-step reasoning traces to enhance transparency and performance on complex\ntasks. However, these reasoning traces can be redundant or logically\ninconsistent, making them a new source of hallucination that is difficult to\ndetect. Existing hallucination detection methods focus primarily on\nanswer-level uncertainty and often fail to detect hallucinations or logical\ninconsistencies arising from the model's reasoning trace. This oversight is\nparticularly problematic for LRMs, where the explicit thinking trace is not\nonly an important support to the model's decision-making process but also a key\nsource of potential hallucination. To this end, we propose RACE (Reasoning and\nAnswer Consistency Evaluation), a novel framework specifically tailored for\nhallucination detection in LRMs. RACE operates by extracting essential\nreasoning steps and computing four diagnostic signals: inter-sample consistency\nof reasoning traces, entropy-based answer uncertainty, semantic alignment\nbetween reasoning and answers, and internal coherence of reasoning. This joint\nanalysis enables fine-grained hallucination detection even when the final\nanswer appears correct. Experiments across datasets and different LLMs\ndemonstrate that RACE outperforms existing hallucination detection baselines,\noffering a robust and generalizable solution for evaluating LRMs. Our code is\navailable at: https://github.com/bebr2/RACE.\n","authors":["Changyue Wang","Weihang Su","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2506.04832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04831v1","updated":"2025-06-05T09:54:01Z","published":"2025-06-05T09:54:01Z","title":"From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health\n  Trajectories with LLMs","summary":"  Healthcare systems face significant challenges in managing and interpreting\nvast, heterogeneous patient data for personalized care. Existing approaches\noften focus on narrow use cases with a limited feature space, overlooking the\ncomplex, longitudinal interactions needed for a holistic understanding of\npatient health. In this work, we propose a novel approach to patient pathway\nmodeling by transforming diverse electronic health record (EHR) data into a\nstructured representation and designing a holistic pathway prediction model,\nEHR2Path, optimized to predict future health trajectories. Further, we\nintroduce a novel summary mechanism that embeds long-term temporal context into\ntopic-specific summary tokens, improving performance over text-only models,\nwhile being much more token-efficient. EHR2Path demonstrates strong performance\nin both next time-step prediction and longitudinal simulation, outperforming\ncompetitive baselines. It enables detailed simulations of patient trajectories,\ninherently targeting diverse evaluation tasks, such as forecasting vital signs,\nlab test results, or length-of-stay, opening a path towards predictive and\npersonalized healthcare.\n","authors":["Chantal Pellegrini","Ege Özsoy","David Bani-Harouni","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2506.04831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19536v2","updated":"2025-06-05T09:50:13Z","published":"2025-05-26T05:54:48Z","title":"FlowCut: Rethinking Redundancy via Information Flow for Efficient\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut\n","authors":["Jintao Tong","Wenwei Jin","Pengda Qin","Anqi Li","Yixiong Zou","Yuhong Li","Yuhua Li","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2505.19536v2.pdf","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2506.04824v1","updated":"2025-06-05T09:43:28Z","published":"2025-06-05T09:43:28Z","title":"A Reasoning-Based Approach to Cryptic Crossword Clue Solving","summary":"  Cryptic crossword clues are challenging language tasks for which new test\nsets are released daily by major newspapers on a global basis. Each cryptic\nclue contains both the definition of the answer to be placed in the crossword\ngrid (in common with regular crosswords), and 'wordplay' that proves that the\nanswer is correct (i.e. a human solver can be confident that an answer is\ncorrect without needing crossing words as confirmation). This work describes an\nLLM-based reasoning system built from open-licensed components that solves\ncryptic clues by (i) hypothesising answers; (ii) proposing wordplay\nexplanations; and (iii) using a verifier system that operates on codified\nreasoning steps. Overall, this system establishes a new state-of-the-art\nperformance on the challenging Cryptonite dataset of clues from The Times and\nThe Telegraph newspapers in the UK. Because each proved solution is expressed\nin Python, interpretable wordplay reasoning for proven answers is available for\ninspection.\n","authors":["Martin Andrews","Sam Witteveen"],"pdf_url":"https://arxiv.org/pdf/2506.04824v1.pdf","comment":"9 page paper plus Appendices. Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.04822v1","updated":"2025-06-05T09:41:09Z","published":"2025-06-05T09:41:09Z","title":"Evaluating Vision-Language and Large Language Models for Automated\n  Student Assessment in Indonesian Classrooms","summary":"  Although vision-language and large language models (VLM and LLM) offer\npromising opportunities for AI-driven educational assessment, their\neffectiveness in real-world classroom settings, particularly in\nunderrepresented educational contexts, remains underexplored. In this study, we\nevaluated the performance of a state-of-the-art VLM and several LLMs on 646\nhandwritten exam responses from grade 4 students in six Indonesian schools,\ncovering two subjects: Mathematics and English. These sheets contain more than\n14K student answers that span multiple choice, short answer, and essay\nquestions. Assessment tasks include grading these responses and generating\npersonalized feedback. Our findings show that the VLM often struggles to\naccurately recognize student handwriting, leading to error propagation in\ndownstream LLM grading. Nevertheless, LLM-generated feedback retains some\nutility, even when derived from imperfect input, although limitations in\npersonalization and contextual relevance persist.\n","authors":["Nurul Aisyah","Muhammad Dehan Al Kautsar","Arif Hidayat","Raqib Chowdhury","Fajri Koto"],"pdf_url":"https://arxiv.org/pdf/2506.04822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04811v1","updated":"2025-06-05T09:34:42Z","published":"2025-06-05T09:34:42Z","title":"Design of intelligent proofreading system for English translation based\n  on CNN and BERT","summary":"  Since automatic translations can contain errors that require substantial\nhuman post-editing, machine translation proofreading is essential for improving\nquality. This paper proposes a novel hybrid approach for robust proofreading\nthat combines convolutional neural networks (CNN) with Bidirectional Encoder\nRepresentations from Transformers (BERT). In order to extract semantic\ninformation from phrases and expressions, CNN uses a variety of convolution\nkernel filters to capture local n-gram patterns. In the meanwhile, BERT creates\ncontext-rich representations of whole sequences by utilizing stacked\nbidirectional transformer encoders. Using BERT's attention processes, the\nintegrated error detection component relates tokens to spot translation\nirregularities including word order problems and omissions. The correction\nmodule then uses parallel English-German alignment and GRU decoder models in\nconjunction with translation memory to propose logical modifications that\nmaintain original meaning. A unified end-to-end training process optimized for\npost-editing performance is applied to the whole pipeline. The multi-domain\ncollection of WMT and the conversational dialogues of Open-Subtitles are two of\nthe English-German parallel corpora used to train the model. Multiple loss\nfunctions supervise detection and correction capabilities. Experiments attain a\n90% accuracy, 89.37% F1, and 16.24% MSE, exceeding recent proofreading\ntechniques by over 10% overall. Comparative benchmarking demonstrates\nstate-of-the-art performance in identifying and coherently rectifying\nmistranslations and omissions.\n","authors":["Feijun Liu","Huifeng Wang","Kun Wang","Yizhen Wang"],"pdf_url":"https://arxiv.org/pdf/2506.04811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04810v1","updated":"2025-06-05T09:34:12Z","published":"2025-06-05T09:34:12Z","title":"Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and\n  Supervision Study","summary":"  Logical reasoning is a core capability for many applications of large\nlanguage models (LLMs), yet existing benchmarks often rely solely on\nfinal-answer accuracy, failing to capture the quality and structure of the\nreasoning process. We propose FineLogic, a fine-grained evaluation framework\nthat assesses logical reasoning across three dimensions: overall benchmark\naccuracy, stepwise soundness, and representation-level alignment. In addition,\nto better understand how reasoning capabilities emerge, we conduct a\ncomprehensive study on the effects of supervision format during fine-tuning. We\nconstruct four supervision styles (one natural language and three symbolic\nvariants) and train LLMs under each. Our findings reveal that natural language\nsupervision yields strong generalization even on out-of-distribution and\nlong-context tasks, while symbolic reasoning styles promote more structurally\nsound and atomic inference chains. Further, our representation-level probing\nshows that fine-tuning primarily improves reasoning behaviors through\nstep-by-step generation, rather than enhancing shortcut prediction or\ninternalized correctness. Together, our framework and analysis provide a more\nrigorous and interpretable lens for evaluating and improving logical reasoning\nin LLMs.\n","authors":["Yujun Zhou","Jiayi Ye","Zipeng Ling","Yufei Han","Yue Huang","Haomin Zhuang","Zhenwen Liang","Kehan Guo","Taicheng Guo","Xiangqi Wang","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24347v2","updated":"2025-06-05T09:26:39Z","published":"2025-05-30T08:40:49Z","title":"Fewer Hallucinations, More Verification: A Three-Stage LLM-Based\n  Framework for ASR Error Correction","summary":"  Automatic Speech Recognition (ASR) error correction aims to correct\nrecognition errors while preserving accurate text. Although traditional\napproaches demonstrate moderate effectiveness, LLMs offer a paradigm that\neliminates the need for training and labeled data. However, directly using LLMs\nwill encounter hallucinations problem, which may lead to the modification of\nthe correct text. To address this problem, we propose the Reliable LLM\nCorrection Framework (RLLM-CF), which consists of three stages: (1) error\npre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)\nreasoning process verification. The advantage of our method is that it does not\nrequire additional information or fine-tuning of the model, and ensures the\ncorrectness of the LLM correction under multi-pass programming. Experiments on\nAISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by\nour framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.\n","authors":["Yangui Fang","Baixu Cheng","Jing Peng","Xu Li","Yu Xi","Chengwei Zhang","Guohui Zhong"],"pdf_url":"https://arxiv.org/pdf/2505.24347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04788v1","updated":"2025-06-05T09:14:41Z","published":"2025-06-05T09:14:41Z","title":"Towards LLM-Centric Multimodal Fusion: A Survey on Integration\n  Strategies and Techniques","summary":"  The rapid progress of Multimodal Large Language Models(MLLMs) has transformed\nthe AI landscape. These models combine pre-trained LLMs with various modality\nencoders. This integration requires a systematic understanding of how different\nmodalities connect to the language backbone. Our survey presents an LLM-centric\nanalysis of current approaches. We examine methods for transforming and\naligning diverse modal inputs into the language embedding space. This addresses\na significant gap in existing literature. We propose a classification framework\nfor MLLMs based on three key dimensions. First, we examine architectural\nstrategies for modality integration. This includes both the specific\nintegration mechanisms and the fusion level. Second, we categorize\nrepresentation learning techniques as either joint or coordinate\nrepresentations. Third, we analyze training paradigms, including training\nstrategies and objective functions. By examining 125 MLLMs developed between\n2021 and 2025, we identify emerging patterns in the field. Our taxonomy\nprovides researchers with a structured overview of current integration\ntechniques. These insights aim to guide the development of more robust\nmultimodal integration strategies for future models built on pre-trained\nfoundations.\n","authors":["Jisu An","Junseok Lee","Jeoungeun Lee","Yongseok Son"],"pdf_url":"https://arxiv.org/pdf/2506.04788v1.pdf","comment":"18 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.04779v1","updated":"2025-06-05T09:09:36Z","published":"2025-06-05T09:09:36Z","title":"MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning\n  Benchmark","summary":"  Speech inherently contains rich acoustic information that extends far beyond\nthe textual language. In real-world spoken language understanding, effective\ninterpretation often requires integrating semantic meaning (e.g., content),\nparalinguistic features (e.g., emotions, speed, pitch) and phonological\ncharacteristics (e.g., prosody, intonation, rhythm), which are embedded in\nspeech. While recent multimodal Speech Large Language Models (SpeechLLMs) have\ndemonstrated remarkable capabilities in processing audio information, their\nability to perform fine-grained perception and complex reasoning in natural\nspeech remains largely unexplored. To address this gap, we introduce MMSU, a\ncomprehensive benchmark designed specifically for understanding and reasoning\nin spoken language. MMSU comprises 5,000 meticulously curated\naudio-question-answer triplets across 47 distinct tasks. To ground our\nbenchmark in linguistic theory, we systematically incorporate a wide range of\nlinguistic phenomena, including phonetics, prosody, rhetoric, syntactics,\nsemantics, and paralinguistics. Through a rigorous evaluation of 14 advanced\nSpeechLLMs, we identify substantial room for improvement in existing models,\nhighlighting meaningful directions for future optimization. MMSU establishes a\nnew standard for comprehensive assessment of spoken language understanding,\nproviding valuable insights for developing more sophisticated human-AI speech\ninteraction systems. MMSU benchmark is available at\nhttps://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available\nat https://github.com/dingdongwang/MMSU_Bench.\n","authors":["Dingdong Wang","Jincenzi Wu","Junan Li","Dongchao Yang","Xueyuan Chen","Tianhua Zhang","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2506.04779v1.pdf","comment":"MMSU benchmark is available at\n  https://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available\n  at https://github.com/dingdongwang/MMSU_Bench"},{"id":"http://arxiv.org/abs/2506.04774v1","updated":"2025-06-05T09:06:59Z","published":"2025-06-05T09:06:59Z","title":"Fine-Grained Interpretation of Political Opinions in Large Language\n  Models","summary":"  Studies of LLMs' political opinions mainly rely on evaluations of their\nopen-ended responses. Recent work indicates that there is a misalignment\nbetween LLMs' responses and their internal intentions. This motivates us to\nprobe LLMs' internal mechanisms and help uncover their internal political\nstates. Additionally, we found that the analysis of LLMs' political opinions\noften relies on single-axis concepts, which can lead to concept confounds. In\nthis work, we extend the single-axis to multi-dimensions and apply\ninterpretable representation engineering techniques for more transparent LLM\npolitical concept learning. Specifically, we designed a four-dimensional\npolitical learning framework and constructed a corresponding dataset for\nfine-grained political concept vector learning. These vectors can be used to\ndetect and intervene in LLM internals. Experiments are conducted on eight\nopen-source LLMs with three representation engineering techniques. Results show\nthese vectors can disentangle political concept confounds. Detection tasks\nvalidate the semantic meaning of the vectors and show good generalization and\nrobustness in OOD settings. Intervention Experiments show these vectors can\nintervene in LLMs to generate responses with different political leanings.\n","authors":["Jingyu Hu","Mengyue Yang","Mengnan Du","Weiru Liu"],"pdf_url":"https://arxiv.org/pdf/2506.04774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04772v1","updated":"2025-06-05T09:00:23Z","published":"2025-06-05T09:00:23Z","title":"Identifying Reliable Evaluation Metrics for Scientific Text Revision","summary":"  Evaluating text revision in scientific writing remains a challenge, as\ntraditional metrics such as ROUGE and BERTScore primarily focus on similarity\nrather than capturing meaningful improvements. In this work, we analyse and\nidentify the limitations of these metrics and explore alternative evaluation\nmethods that better align with human judgments. We first conduct a manual\nannotation study to assess the quality of different revisions. Then, we\ninvestigate reference-free evaluation metrics from related NLP domains.\nAdditionally, we examine LLM-as-a-judge approaches, analysing their ability to\nassess revisions with and without a gold reference. Our results show that LLMs\neffectively assess instruction-following but struggle with correctness, while\ndomain-specific metrics provide complementary insights. We find that a hybrid\napproach combining LLM-as-a-judge evaluation and task-specific metrics offers\nthe most reliable assessment of revision quality.\n","authors":["Léane Jourdan","Florian Boudin","Richard Dufour","Nicolas Hernandez"],"pdf_url":"https://arxiv.org/pdf/2506.04772v1.pdf","comment":"Accepted to ACL 2025 main"},{"id":"http://arxiv.org/abs/2505.16142v2","updated":"2025-06-05T08:57:42Z","published":"2025-05-22T02:36:36Z","title":"Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via\n  Reinforcement Learning","summary":"  Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Jia Gu","Zihao Wei","Jingcheng Deng","Feiyang Pan","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.16142v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2506.04762v1","updated":"2025-06-05T08:45:48Z","published":"2025-06-05T08:45:48Z","title":"GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner\n  for Query Expansion in Information Retrieval","summary":"  Large language models (LLMs)-based query expansion for information retrieval\naugments queries with generated hypothetical documents with LLMs. However, its\nperformance relies heavily on the scale of the language models (LMs),\nnecessitating larger, more advanced LLMs. This approach is costly,\ncomputationally intensive, and often has limited accessibility. To address\nthese limitations, we introduce GOLFer - Smaller LMs-Generated Documents\nHallucination Filter & Combiner - a novel method leveraging smaller open-source\nLMs for query expansion. GOLFer comprises two modules: a hallucination filter\nand a documents combiner. The former detects and removes non-factual and\ninconsistent sentences in generated documents, a common issue with smaller LMs,\nwhile the latter combines the filtered content with the query using a weight\nvector to balance their influence. We evaluate GOLFer alongside dominant\nLLM-based query expansion methods on three web search and ten low-resource\ndatasets. Experimental results demonstrate that GOLFer consistently outperforms\nother methods using smaller LMs, and maintains competitive performance against\nmethods using large-size LLMs, demonstrating its effectiveness.\n","authors":["Lingyuan Liu","Mengxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04760v1","updated":"2025-06-05T08:44:34Z","published":"2025-06-05T08:44:34Z","title":"Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using\n  Large Language Model-based Query Expansion","summary":"  Large Language Models (LLMs) have shown potential in generating hypothetical\ndocuments for query expansion, thereby enhancing information retrieval\nperformance. However, the efficacy of this method is highly dependent on the\nquality of the generated documents, which often requires complex prompt\nstrategies and the integration of advanced dense retrieval techniques. This can\nbe both costly and computationally intensive. To mitigate these limitations, we\nexplore the use of zero-shot LLM-based query expansion to improve sparse\nretrieval, particularly for learned sparse retrievers. We introduce a novel\nfusion ranking framework, Exp4Fuse, which enhances the performance of sparse\nretrievers through an indirect application of zero-shot LLM-based query\nexpansion. Exp4Fuse operates by simultaneously considering two retrieval\nroutes-one based on the original query and the other on the LLM-augmented\nquery. It then generates two ranked lists using a sparse retriever and fuses\nthem using a modified reciprocal rank fusion method. We conduct extensive\nevaluations of Exp4Fuse against leading LLM-based query expansion methods and\nadvanced retrieval techniques on three MS MARCO-related datasets and seven\nlow-resource datasets. Experimental results reveal that Exp4Fuse not only\nsurpasses existing LLM-based query expansion methods in enhancing sparse\nretrievers but also, when combined with advanced sparse retrievers, achieves\nSOTA results on several benchmarks. This highlights the superior performance\nand effectiveness of Exp4Fuse in improving query expansion for sparse\nretrieval.\n","authors":["Lingyuan Liu","Mengxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22353v4","updated":"2025-06-05T08:39:20Z","published":"2025-03-28T11:49:56Z","title":"Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions","summary":"  Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nand coherent behavior across multiple rounds of user interaction. This paper\nintroduces a comprehensive framework for evaluating and improving LLM response\nconsistency, making three key contributions. Code and data are available at:\nhttps://github.com/yubol-bobo/MT-Consistency. First, we introduce\nPosition-Weighted Consistency (PWC), a metric designed to capture both the\nimportance of early-stage stability and recovery patterns in multi-turn\ninteractions. Second, we present MT-Consistency, a carefully curated benchmark\ndataset spanning diverse domains and difficulty levels, specifically designed\nto evaluate LLM consistency under various challenging follow-up scenarios.\nThird, we introduce Confidence-Aware Response Generation (CARG), a framework\nthat significantly improves response stability by explicitly integrating\ninternal model confidence scores during the generation process. Experimental\nresults demonstrate that CARG significantly improves response stability without\nsacrificing accuracy, offering a practical path toward more dependable LLM\nbehavior in critical, real-world deployments.\n","authors":["Yubo Li","Yidi Miao","Xueying Ding","Ramayya Krishnan","Rema Padman"],"pdf_url":"https://arxiv.org/pdf/2503.22353v4.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.22944v4","updated":"2025-06-05T08:25:12Z","published":"2024-10-30T12:01:48Z","title":"Focus On This, Not That! Steering LLMs with Adaptive Feature\n  Specification","summary":"  Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs), such models often leverage spurious or biased features learnt\nfrom their training data and can become misaligned, leading to undesired\nbehaviours. While existing techniques can steer model behaviour at\ninference-time, they are often post-hoc and do not embed steering as an\nintrinsic model feature. In this work, we introduce Focus Instruction Tuning\n(FIT), which trains LLMs to condition their responses by focusing on specific\nfeatures whilst ignoring others, leading to different behaviours based on what\nfeatures are specified. Across diverse benchmarks, we demonstrate that FIT: (i)\nsuccessfully steers behaviour at inference time; (ii) increases robustness by\namplifying core task signals and down-weighting spurious cues; (iii) mitigates\nsocial bias by suppressing demographic attributes; and (iv) generalises under\ndistribution shifts and to previously unseen focus features. FIT therefore\noffers a lightweight, intrinsic mechanism for building more robust, fair, and\neasily controllable LLMs.\n","authors":["Tom A. Lamb","Adam Davies","Alasdair Paren","Philip H. S. Torr","Francesco Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.22944v4.pdf","comment":"36pages, 19 figures"},{"id":"http://arxiv.org/abs/2505.19426v2","updated":"2025-06-05T08:20:31Z","published":"2025-05-26T02:37:26Z","title":"The Role of Diversity in In-Context Learning for Large Language Models","summary":"  In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection.\n","authors":["Wenyang Xiao","Haoyu Zhao","Lingxiao Huang"],"pdf_url":"https://arxiv.org/pdf/2505.19426v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2506.04739v1","updated":"2025-06-05T08:17:55Z","published":"2025-06-05T08:17:55Z","title":"Lifelong Evolution: Collaborative Learning between Large and Small\n  Language Models for Continuous Emergent Fake News Detection","summary":"  The widespread dissemination of fake news on social media has significantly\nimpacted society, resulting in serious consequences. Conventional deep learning\nmethodologies employing small language models (SLMs) suffer from extensive\nsupervised training requirements and difficulties adapting to evolving news\nenvironments due to data scarcity and distribution shifts. Large language\nmodels (LLMs), despite robust zero-shot capabilities, fall short in accurately\ndetecting fake news owing to outdated knowledge and the absence of suitable\ndemonstrations. In this paper, we propose a novel Continuous Collaborative\nEmergent Fake News Detection (C$^2$EFND) framework to address these challenges.\nThe C$^2$EFND framework strategically leverages both LLMs' generalization power\nand SLMs' classification expertise via a multi-round collaborative learning\nframework. We further introduce a lifelong knowledge editing module based on a\nMixture-of-Experts architecture to incrementally update LLMs and a replay-based\ncontinue learning method to ensure SLMs retain prior knowledge without\nretraining entirely. Extensive experiments on Pheme and Twitter16 datasets\ndemonstrate that C$^2$EFND significantly outperforms existed methods,\neffectively improving detection accuracy and adaptability in continuous\nemergent fake news scenarios.\n","authors":["Ziyi Zhou","Xiaoming Zhang","Litian Zhang","Yibo Zhang","Zhenyu Guan","Chaozhuo Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2506.04739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18213v5","updated":"2025-06-05T08:11:43Z","published":"2024-07-25T17:26:41Z","title":"Scaling Trends in Language Model Robustness","summary":"  Increasing model size has unlocked a dazzling array of capabilities in modern\nlanguage models. At the same time, even frontier models remain vulnerable to\njailbreaks and prompt injections, despite concerted efforts to make them\nrobust. As both attack and defense gain access to more compute, and as models\nbecome larger, what happens to robustness? We argue that to answer this\nquestion requires a \\emph{scaling} approach, which we employ in an extensive\nstudy of language model robustness across several classification tasks, model\nfamilies, and adversarial attacks. We find that in the absence of explicit\nsafety training, larger models are not consistently more robust; however, scale\nimproves sample efficiency in adversarial training, though it worsens compute\nefficiency. Further, we find that increasing attack compute smoothly improves\nattack success rate against both undefended and adversarially trained models.\nFinally, after exploring robustness transfer across attacks and threat models,\nwe combine attack and defense scaling rates to study the offense-defense\nbalance. We find that while attack scaling outpaces adversarial training across\nall models studied, larger adversarially trained models might give defense the\nadvantage in the long run. These results underscore the utility of the scaling\nlens, and provide a paradigm for evaluating future attacks and defenses on\nfrontier models.\n","authors":["Nikolaus Howe","Ian McKenzie","Oskar Hollinsworth","Michał Zajac","Tom Tseng","Aaron Tucker","Pierre-Luc Bacon","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2407.18213v5.pdf","comment":"59 pages; updated to ICML version"},{"id":"http://arxiv.org/abs/2411.11055v3","updated":"2025-06-05T08:09:22Z","published":"2024-11-17T12:32:44Z","title":"FastDraft: How to Train Your Draft","summary":"  Speculative Decoding has gained popularity as an effective technique for\naccelerating the auto-regressive inference process of Large Language Models.\nHowever, Speculative Decoding entirely relies on the availability of efficient\ndraft models, which are often lacking for many existing language models due to\na stringent constraint of vocabulary compatibility. In this work we introduce\nFastDraft, a novel and efficient approach for pre-training and aligning a draft\nmodel to any large language model by incorporating efficient pre-training,\nfollowed by fine-tuning over synthetic datasets generated by the target model.\nWe demonstrate FastDraft by training two highly parameter efficient drafts for\nthe popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able\nto produce a draft model with approximately 10 billion tokens on a single\nserver with 8 Intel$^\\circledR$ Gaudi$^\\circledR$ 2 accelerators in under 24\nhours. Our results show that the draft model achieves impressive results in key\nmetrics of acceptance rate, block efficiency and up to 3x memory bound speed up\nwhen evaluated on code completion and up to 2x in summarization, text\ncompletion and instruction tasks. We validate our theoretical findings through\nbenchmarking on the latest Intel$^\\circledR$ Core$^{\\tiny \\text{TM}}$ Ultra,\nachieving a wall-clock time speedup of up to 2x, indicating a significant\nreduction in runtime. Due to its high quality, FastDraft unlocks large language\nmodels inference on AI-PC and other edge-devices.\n","authors":["Ofir Zafrir","Igor Margulis","Dorin Shteyman","Shira Guskin","Guy Boudoukh"],"pdf_url":"https://arxiv.org/pdf/2411.11055v3.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2506.04734v1","updated":"2025-06-05T08:09:11Z","published":"2025-06-05T08:09:11Z","title":"Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design","summary":"  Reasoning models represented by the Deepseek-R1-Distill series have been\nwidely adopted by the open-source community due to their strong performance in\nmathematics, science, programming, and other domains. However, our study\nreveals that their benchmark evaluation results are subject to significant\nfluctuations caused by various factors. Subtle differences in evaluation\nconditions can lead to substantial variations in results. Similar phenomena are\nobserved in other open-source inference models fine-tuned based on the\nDeepseek-R1-Distill series, as well as in the QwQ-32B model, making their\nclaimed performance improvements difficult to reproduce reliably. Therefore, we\nadvocate for the establishment of a more rigorous paradigm for model\nperformance evaluation and present our empirical assessments of the\nDeepseek-R1-Distill series models.\n","authors":["Lin Sun","Weihong Lin","Jinzhu Wu","Yongfu Zhu","Xiaoqi Jian","Guangxiang Zhao","Change Jia","Linglin Zhang","Sai-er Hu","Yuhan Wu","Xiangzheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01481v2","updated":"2025-06-05T07:55:56Z","published":"2023-04-04T02:54:04Z","title":"The Vector Grounding Problem","summary":"  The remarkable performance of large language models (LLMs) on complex\nlinguistic tasks has sparked debate about their capabilities. Unlike humans,\nthese models learn language solely from textual data without directly\ninteracting with the world. Yet they generate seemingly meaningful text on\ndiverse topics. This achievement has renewed interest in the classical `Symbol\nGrounding Problem' -- the question of whether the internal representations and\noutputs of symbolic AI systems can possess intrinsic meaning that is not\nparasitic on external interpretation. Although modern LLMs compute over vectors\nrather than symbols, an analogous problem arises for these systems, which we\ncall the Vector Grounding Problem. This paper has two main goals. First, we\ndistinguish five main notions of grounding that are often conflated in the\nliterature, and argue that only one of them, which we call referential\ngrounding, is relevant to the Vector Grounding Problem. Second, drawing on\nphilosophical theories of representational content, we provide two arguments\nfor the claim that LLMs and related systems can achieve referential grounding:\n(1) through preference fine-tuning methods that explicitly establish\nworld-involving functions, and (2) through pre-training alone, which in limited\ndomains may select for internal states with world-involving content, as\nmechanistic interpretability research suggests. Through these pathways, LLMs\ncan establish connections to the world sufficient for intrinsic meaning. One\npotentially surprising implication of our discussion is that that multimodality\nand embodiment are neither necessary nor sufficient to overcome the Grounding\nProblem.\n","authors":["Dimitri Coelho Mollo","Raphaël Millière"],"pdf_url":"https://arxiv.org/pdf/2304.01481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04721v1","updated":"2025-06-05T07:51:23Z","published":"2025-06-05T07:51:23Z","title":"SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through\n  Combat","summary":"  We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs\nthrough competition and combat. To complement a single model's lack of\ndiversity in generation and biases in evaluation, multiple LLMs form a \"sparta\ntribe\" to compete against each other in fulfilling instructions while serving\nas judges for the competition of others. For each iteration, one instruction\nand two models are selected for a duel, the other models evaluate the two\nresponses, and their evaluation scores are aggregated through a adapted\nelo-ranking based reputation system, where winners/losers of combat gain/lose\nweight in evaluating others. The peer-evaluated combat results then become\npreference pairs where the winning response is preferred over the losing one,\nand all models learn from these preferences at the end of each iteration.\nSPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative\nand collective competition process. Extensive experiments demonstrate that\nSPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines\nacross 10 out of 12 tasks and datasets with 7.0% average improvement. Further\nanalysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen\ntasks and leverages the expertise diversity of participating models to produce\nmore logical, direct and informative outputs.\n","authors":["Yuru Jiang","Wenxuan Ding","Shangbin Feng","Greg Durrett","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2506.04721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04714v1","updated":"2025-06-05T07:38:01Z","published":"2025-06-05T07:38:01Z","title":"IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech\n  translation","summary":"  This paper presents the submission of IIITH-BUT to the IWSLT 2025 shared task\non speech translation for the low-resource Bhojpuri-Hindi language pair. We\nexplored the impact of hyperparameter optimisation and data augmentation\ntechniques on the performance of the SeamlessM4T model fine-tuned for this\nspecific task. We systematically investigated a range of hyperparameters\nincluding learning rate schedules, number of update steps, warm-up steps, label\nsmoothing, and batch sizes; and report their effect on translation quality. To\naddress data scarcity, we applied speed perturbation and SpecAugment and\nstudied their effect on translation quality. We also examined the use of\ncross-lingual signal through joint training with Marathi and Bhojpuri speech\ndata. Our experiments reveal that careful selection of hyperparameters and the\napplication of simple yet effective augmentation techniques significantly\nimprove performance in low-resource settings. We also analysed the translation\nhypotheses to understand various kinds of errors that impacted the translation\nquality in terms of BLEU.\n","authors":["Bhavana Akkiraju","Aishwarya Pothula","Santosh Kesiraju","Anil Kumar Vuppala"],"pdf_url":"https://arxiv.org/pdf/2506.04714v1.pdf","comment":"Paper is accepted to IWSLT2025"},{"id":"http://arxiv.org/abs/2506.04711v1","updated":"2025-06-05T07:35:55Z","published":"2025-06-05T07:35:55Z","title":"LLM-based phoneme-to-grapheme for phoneme-based speech recognition","summary":"  In automatic speech recognition (ASR), phoneme-based multilingual\npre-training and crosslingual fine-tuning is attractive for its high data\nefficiency and competitive results compared to subword-based models. However,\nWeighted Finite State Transducer (WFST) based decoding is limited by its\ncomplex pipeline and inability to leverage large language models (LLMs).\nTherefore, we propose LLM-based phoneme-to-grapheme (LLM-P2G) decoding for\nphoneme-based ASR, consisting of speech-to-phoneme (S2P) and\nphoneme-to-grapheme (P2G). A challenge is that there seems to have information\nloss in cascading S2P and P2G. To address this challenge, we propose two\ntraining strategies: data augmentation with noisy phonemes (DANP), and\nrandomized top-$K$ marginalized (TKM) training and decoding. Our experimental\nresults show that LLM-P2G outperforms WFST-based systems in crosslingual ASR\nfor Polish and German, by relative WER reductions of 3.6% and 6.9%\nrespectively.\n","authors":["Te Ma","Min Bi","Saierdaer Yusuyin","Hao Huang","Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2506.04711v1.pdf","comment":"Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.04708v1","updated":"2025-06-05T07:31:18Z","published":"2025-06-05T07:31:18Z","title":"Accelerated Test-Time Scaling with Model-Free Speculative Sampling","summary":"  Language models have demonstrated remarkable capabilities in reasoning tasks\nthrough test-time scaling techniques like best-of-N sampling and tree search.\nHowever, these approaches often demand substantial computational resources,\ncreating a critical trade-off between performance and efficiency. We introduce\nSTAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative\ndecoding approach that leverages the inherent redundancy in reasoning\ntrajectories to achieve significant acceleration without compromising accuracy.\nOur analysis reveals that reasoning paths frequently reuse similar reasoning\npatterns, enabling efficient model-free token prediction without requiring\nseparate draft models. By introducing stochastic drafting and preserving\nprobabilistic information through a memory-efficient logit-based N-gram module,\ncombined with optimized Gumbel-Top-K sampling and data-driven tree\nconstruction, STAND significantly improves token acceptance rates. Extensive\nevaluations across multiple models and reasoning tasks (AIME-2024,\nGPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference\nlatency by 60-65% compared to standard autoregressive decoding while\nmaintaining accuracy. Furthermore, STAND outperforms state-of-the-art\nspeculative decoding methods by 14-28% in throughput and shows strong\nperformance even in single-trajectory scenarios, reducing inference latency by\n48-58%. As a model-free approach, STAND can be applied to any existing language\nmodel without additional training, being a powerful plug-and-play solution for\naccelerating language model reasoning.\n","authors":["Woomin Song","Saket Dingliwal","Sai Muralidhar Jayanthi","Bhavana Ganesh","Jinwoo Shin","Aram Galstyan","Sravan Babu Bodapati"],"pdf_url":"https://arxiv.org/pdf/2506.04708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09560v3","updated":"2025-06-05T07:22:50Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 24 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9\\% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code and dataset are available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v3.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.04693v1","updated":"2025-06-05T07:15:21Z","published":"2025-06-05T07:15:21Z","title":"Cracking the Code: Enhancing Implicit Hate Speech Detection through\n  Coding Classification","summary":"  The internet has become a hotspot for hate speech (HS), threatening societal\nharmony and individual well-being. While automatic detection methods perform\nwell in identifying explicit hate speech (ex-HS), they struggle with more\nsubtle forms, such as implicit hate speech (im-HS). We tackle this problem by\nintroducing a new taxonomy for im-HS detection, defining six encoding\nstrategies named codetypes. We present two methods for integrating codetypes\ninto im-HS detection: 1) prompting large language models (LLMs) directly to\nclassify sentences based on generated responses, and 2) using LLMs as encoders\nwith codetypes embedded during the encoding process. Experiments show that the\nuse of codetypes improves im-HS detection in both Chinese and English datasets,\nvalidating the effectiveness of our approach across different languages.\n","authors":["Lu Wei","Liangzhi Li","Tong Xiang","Xiao Liu","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2506.04693v1.pdf","comment":"Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025),\n  112-126"},{"id":"http://arxiv.org/abs/2502.11028v2","updated":"2025-06-05T07:14:36Z","published":"2025-02-16T07:46:09Z","title":"Mind the Confidence Gap: Overconfidence, Calibration, and Distractor\n  Effects in Large Language Models","summary":"  Large Language Models (LLMs) show remarkable proficiency in natural language\ntasks, yet their frequent overconfidence-misalignment between predicted\nconfidence and true correctness-poses significant risks in critical\ndecision-making applications. We present a comprehensive analysis on\ncalibration in LLMs across nine LLMs and three factual Question-Answering (QA)\ndatasets, systematically comparing standard free-generation settings against\nstructured distractor-augmented prompts. Our evaluation reveals that explicitly\nincorporating distractors can substantially mitigate miscalibration, achieving\nrelative accuracy improvements up to 460% and ECE reductions up to 90%. Despite\ngeneral trends, we uncover nuanced findings: large RLHF-tuned models display\ninherent calibration strengths but can paradoxically suffer increased\nmiscalibration on easier queries, whereas smaller models benefit\ndisproportionately from distractor prompts but remain significantly\nmiscalibrated. Through detailed analyses across question types, we identify\npersistent calibration failures, particularly in person-based queries. We\nconclude with concrete recommendations-targeted fine-tuning, structured\nprompting, and strategic model choice-to ensure reliable, trustworthy LLM\ndeployments.\n","authors":["Prateek Chhikara"],"pdf_url":"https://arxiv.org/pdf/2502.11028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04689v1","updated":"2025-06-05T07:12:12Z","published":"2025-06-05T07:12:12Z","title":"Recycling the Web: A Method to Enhance Pre-training Data Quality and\n  Quantity for Language Models","summary":"  Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data.\n","authors":["Thao Nguyen","Yang Li","Olga Golovneva","Luke Zettlemoyer","Sewoong Oh","Ludwig Schmidt","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2506.04689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04688v1","updated":"2025-06-05T07:11:36Z","published":"2025-06-05T07:11:36Z","title":"MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models","summary":"  This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.\n","authors":["Gio Paik","Geewook Kim","Jinbae Im"],"pdf_url":"https://arxiv.org/pdf/2506.04688v1.pdf","comment":"ACL Findings 2025"},{"id":"http://arxiv.org/abs/2506.04681v1","updated":"2025-06-05T07:00:31Z","published":"2025-06-05T07:00:31Z","title":"Urania: Differentially Private Insights into AI Use","summary":"  We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.\n","authors":["Daogao Liu","Edith Cohen","Badih Ghazi","Peter Kairouz","Pritish Kamath","Alexander Knop","Ravi Kumar","Pasin Manurangsi","Adam Sealfon","Da Yu","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04679v1","updated":"2025-06-05T06:57:28Z","published":"2025-06-05T06:57:28Z","title":"Normative Conflicts and Shallow AI Alignment","summary":"  The progress of AI systems such as large language models (LLMs) raises\nincreasingly pressing concerns about their safe deployment. This paper examines\nthe value alignment problem for LLMs, arguing that current alignment strategies\nare fundamentally inadequate to prevent misuse. Despite ongoing efforts to\ninstill norms such as helpfulness, honesty, and harmlessness in LLMs through\nfine-tuning based on human preferences, they remain vulnerable to adversarial\nattacks that exploit conflicts between these norms. I argue that this\nvulnerability reflects a fundamental limitation of existing alignment methods:\nthey reinforce shallow behavioral dispositions rather than endowing LLMs with a\ngenuine capacity for normative deliberation. Drawing from on research in moral\npsychology, I show how humans' ability to engage in deliberative reasoning\nenhances their resilience against similar adversarial tactics. LLMs, by\ncontrast, lack a robust capacity to detect and rationally resolve normative\nconflicts, leaving them susceptible to manipulation; even recent advances in\nreasoning-focused LLMs have not addressed this vulnerability. This ``shallow\nalignment'' problem carries significant implications for AI safety and\nregulation, suggesting that current approaches are insufficient for mitigating\npotential harms posed by increasingly capable AI systems.\n","authors":["Raphaël Millière"],"pdf_url":"https://arxiv.org/pdf/2506.04679v1.pdf","comment":"Published in Philosophical Studies"},{"id":"http://arxiv.org/abs/2506.03901v2","updated":"2025-06-05T06:44:23Z","published":"2025-06-04T12:55:59Z","title":"Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of\n  Retrieval Noise Erosion in RAG Systems","summary":"  Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge. However, RAG systems are highly\nsensitive to retrieval noise prevalent in real-world scenarios. Existing\nbenchmarks fail to emulate the complex and heterogeneous noise distributions\nencountered in real-world retrieval environments, undermining reliable\nrobustness assessment. In this paper, we define four categories of retrieval\nnoise based on linguistic properties and noise characteristics, aiming to\nreflect the heterogeneity of noise in real-world scenarios. Building on this,\nwe introduce Magic Mushroom, a benchmark for replicating \"magic mushroom\"\nnoise: contexts that appear relevant on the surface but covertly mislead RAG\nsystems. Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop\nquestion-answer pairs. More importantly, Magic Mushroom enables researchers to\nflexibly configure combinations of retrieval noise according to specific\nresearch objectives or application scenarios, allowing for highly controlled\nevaluation setups. We evaluate LLM generators of varying parameter scales and\nclassic RAG denoising strategies under diverse noise distributions to\ninvestigate their performance dynamics during progressive noise encroachment.\nOur analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions. Magic Mushroom emerges as a promising tool for evaluating and\nadvancing noise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications. The Magic Mushroom benchmark is available at\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.\n","authors":["Yuxin Zhang","Yan Wang","Yongrui Chen","Shenyu Zhang","Xinbang Dai","Sheng Bi","Guilin Qi"],"pdf_url":"https://arxiv.org/pdf/2506.03901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12436v3","updated":"2025-06-05T06:27:54Z","published":"2025-02-18T02:11:41Z","title":"Should I Trust You? Detecting Deception in Negotiations using\n  Counterfactual RL","summary":"  An increasingly common socio-technical problem is people being taken in by\noffers that sound ``too good to be true'', where persuasion and trust shape\ndecision-making. This paper investigates how \\abr{ai} can help detect these\ndeceptive scenarios. We analyze how humans strategically deceive each other in\n\\textit{Diplomacy}, a board game that requires both natural language\ncommunication and strategic reasoning. This requires extracting logical forms\nof proposed agreements in player communications and computing the relative\nrewards of the proposal using agents' value functions. Combined with text-based\nfeatures, this can improve our deception detection. Our method detects human\ndeception with a high precision when compared to a Large Language Model\napproach that flags many true messages as deceptive. Future human-\\abr{ai}\ninteraction tools can build on our methods for deception detection by\ntriggering \\textit{friction} to give users a chance of interrogating suspicious\nproposals.\n","authors":["Wichayaporn Wongkamjan","Yanze Wang","Feng Gu","Denis Peskoff","Jonathan K. Kummerfeld","Jonathan May","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2502.12436v3.pdf","comment":"ACL Findings 2025"},{"id":"http://arxiv.org/abs/2506.01807v2","updated":"2025-06-05T06:11:55Z","published":"2025-06-02T15:52:04Z","title":"Propaganda and Information Dissemination in the Russo-Ukrainian War:\n  Natural Language Processing of Russian and Western Twitter Narratives","summary":"  The conflict in Ukraine has been not only characterised by military\nengagement but also by a significant information war, with social media\nplatforms like X, formerly known as Twitter playing an important role in\nshaping public perception. This article provides an analysis of tweets from\npropaganda accounts and trusted accounts collected from the onset of the war,\nFebruary 2022 until the middle of May 2022 with n=40,000 total tweets. We\nutilise natural language processing and machine learning algorithms to assess\nthe sentiment and identify key themes, topics and narratives across the dataset\nwith human-in-the-loop (HITL) analysis throughout. Our findings indicate\ndistinct strategies in how information is created, spread, and targeted at\ndifferent audiences by both sides. Propaganda accounts frequently employ\nemotionally charged language and disinformation to evoke fear and distrust,\nwhereas other accounts, primarily Western tend to focus on factual reporting\nand humanitarian aspects of the conflict. Clustering analysis reveals groups of\naccounts with similar behaviours, which we suspect indicates the presence of\ncoordinated efforts. This research attempts to contribute to our understanding\nof the dynamics of information warfare and offers techniques for future studies\non social media influence in military conflicts.\n","authors":["Zaur Gouliev"],"pdf_url":"https://arxiv.org/pdf/2506.01807v2.pdf","comment":"7 pages; 6 figures"},{"id":"http://arxiv.org/abs/2506.03519v2","updated":"2025-06-05T06:09:38Z","published":"2025-06-04T03:07:55Z","title":"An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement\n  Learning Injected by Elite Individuals","summary":"  Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.\n","authors":["Yangyang Zhao","Ben Niu","Libo Qin","Shihan Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03519v2.pdf","comment":"Accepted to ACL 2025 (Main Track)"},{"id":"http://arxiv.org/abs/2409.19257v3","updated":"2025-06-05T05:55:19Z","published":"2024-09-28T06:20:20Z","title":"Inducing lexicons of in-group language with socio-temporal context","summary":"  In-group language is an important signifier of group dynamics. This paper\nproposes a novel method for inducing lexicons of in-group language, which\nincorporates its socio-temporal context. Existing methods for lexicon induction\ndo not capture the evolving nature of in-group language, nor the social\nstructure of the community. Using dynamic word and user embeddings trained on\nconversations from online anti-women communities, our approach outperforms\nprior methods for lexicon induction. We develop a test set for the task of\nlexicon induction and a new lexicon of manosphere language, validated by human\nexperts, which quantifies the relevance of each term to a specific\nsub-community at a given point in time. Finally, we present novel insights on\nin-group language which illustrate the utility of this approach.\n","authors":["Christine de Kock"],"pdf_url":"https://arxiv.org/pdf/2409.19257v3.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2506.04652v1","updated":"2025-06-05T05:48:31Z","published":"2025-06-05T05:48:31Z","title":"EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label\n  Speech Emotion Recognition","summary":"  Speech emotion recognition (SER) systems often exhibit gender bias. However,\nthe effectiveness and robustness of existing debiasing methods in such\nmulti-label scenarios remain underexplored. To address this gap, we present\nEMO-Debias, a large-scale comparison of 13 debiasing methods applied to\nmulti-label SER. Our study encompasses techniques from pre-processing,\nregularization, adversarial learning, biased learners, and distributionally\nrobust optimization. Experiments conducted on acted and naturalistic emotion\ndatasets, using WavLM and XLSR representations, evaluate each method under\nconditions of gender imbalance. Our analysis quantifies the trade-offs between\nfairness and accuracy, identifying which approaches consistently reduce gender\nperformance gaps without compromising overall model performance. The findings\nprovide actionable insights for selecting effective debiasing strategies and\nhighlight the impact of dataset distributions.\n","authors":["Yi-Cheng Lin","Huang-Cheng Chou","Yu-Hsuan Li Liang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2506.04652v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.09429v4","updated":"2025-06-05T05:44:18Z","published":"2024-12-12T16:35:05Z","title":"From Intention To Implementation: Automating Biomedical Research via\n  LLMs","summary":"  Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols, on average,\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems.\n","authors":["Yi Luo","Linghang Shi","Yihao Li","Aobo Zhuang","Yeyun Gong","Ling Liu","Chen Lin"],"pdf_url":"https://arxiv.org/pdf/2412.09429v4.pdf","comment":"To appear in SCIENCE CHINA Information Sciences. If you find our work\n  useful, please cite us as: @article{ BioResearcher, author = \"Yi Luo and\n  Linghang Shi and Yihao Li and Aobo Zhuang and Yeyun Gong and Ling Liu and\n  Chen Lin\", title = \"From Intention To Implementation: Automating Biomedical\n  Research via LLMs\", journal = \"SCIENCE CHINA Information Sciences\", year =\n  \"2025\" }"},{"id":"http://arxiv.org/abs/2410.12872v2","updated":"2025-06-05T05:41:21Z","published":"2024-10-14T16:25:48Z","title":"Not All Options Are Created Equal: Textual Option Weighting for\n  Token-Efficient LLM-Based Knowledge Tracing","summary":"  Large Language Models (LLMs) have recently emerged as promising tools for\nknowledge tracing (KT) due to their strong reasoning and generalization\nabilities. While recent LLM-based KT methods have proposed new prompt formats,\nthey struggle to represent the full interaction histories of example learners\nwithin a single prompt during in-context learning (ICL), resulting in limited\nscalability and high computational cost under token constraints. In this work,\nwe present \\textit{LLM-based Option-weighted Knowledge Tracing (LOKT)}, a\nsimple yet effective framework that encodes the interaction histories of\nexample learners in context as \\textit{textual categorical option weights\n(TCOW)}. TCOW are semantic labels (e.g., ``inadequate'') assigned to the\noptions selected by learners when answering questions, enhancing the\ninterpretability of LLMs. Experiments on multiple-choice datasets show that\nLOKT outperforms existing non-LLM and LLM-based KT models in both cold-start\nand warm-start settings. Moreover, LOKT enables scalable and cost-efficient\ninference, achieving strong performance even under strict token constraints.\nOur code is available at\n\\href{https://anonymous.4open.science/r/LOKT_model-3233}{https://anonymous.4open.science/r/LOKT\\_model-3233}.\n","authors":["JongWoo Kim","SeongYeub Chu","Bryan Wong","Mun Yi"],"pdf_url":"https://arxiv.org/pdf/2410.12872v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2506.04108v2","updated":"2025-06-05T05:39:48Z","published":"2025-06-04T16:01:48Z","title":"Rectified Sparse Attention","summary":"  Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.\n","authors":["Yutao Sun","Tianzhu Ye","Li Dong","Yuqing Xia","Jian Chen","Yizhao Gao","Shijie Cao","Jianyong Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2506.04108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04649v1","updated":"2025-06-05T05:31:50Z","published":"2025-06-05T05:31:50Z","title":"Flex-TravelPlanner: A Benchmark for Flexible Planning with Language\n  Agents","summary":"  Real-world planning problems require constant adaptation to changing\nrequirements and balancing of competing constraints. However, current\nbenchmarks for evaluating LLMs' planning capabilities primarily focus on\nstatic, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark\nthat evaluates language models' ability to reason flexibly in dynamic planning\nscenarios. Building on the TravelPlanner dataset~\\citep{xie2024travelplanner},\nwe introduce two novel evaluation settings: (1) sequential constraint\nintroduction across multiple turns, and (2) scenarios with explicitly\nprioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B\nreveals several key findings: models' performance on single-turn tasks poorly\npredicts their ability to adapt plans across multiple turns; constraint\nintroduction order significantly affects performance; and models struggle with\nconstraint prioritization, often incorrectly favoring newly introduced lower\npriority preferences over existing higher-priority constraints. These findings\nhighlight the importance of evaluating LLMs in more realistic, dynamic planning\nscenarios and suggest specific directions for improving model performance on\ncomplex planning tasks. The code and dataset for our framework are publicly\navailable at https://github.com/juhyunohh/FlexTravelBench.\n","authors":["Juhyun Oh","Eunsu Kim","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2506.04649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04642v1","updated":"2025-06-05T05:23:38Z","published":"2025-06-05T05:23:38Z","title":"TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering","summary":"  The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts.\n","authors":["Vinay Joshi","Pratik Prabhanjan Brahma","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2506.04642v1.pdf","comment":"ACL-2025 industry-track accepted"},{"id":"http://arxiv.org/abs/2505.24871v2","updated":"2025-06-05T05:13:46Z","published":"2025-05-30T17:59:38Z","title":"MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.\n","authors":["Yiqing Liang","Jielin Qiu","Wenhao Ding","Zuxin Liu","James Tompkin","Mengdi Xu","Mengzhou Xia","Zhengzhong Tu","Laixi Shi","Jiacheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.24871v2.pdf","comment":"Project Webpage: https://modomodo-rl.github.io/"},{"id":"http://arxiv.org/abs/2506.04635v1","updated":"2025-06-05T05:13:01Z","published":"2025-06-05T05:13:01Z","title":"ViCocktail: Automated Multi-Modal Data Collection for Vietnamese\n  Audio-Visual Speech Recognition","summary":"  Audio-Visual Speech Recognition (AVSR) has gained significant attention\nrecently due to its robustness against noise, which often challenges\nconventional speech recognition systems that rely solely on audio features.\nDespite this advantage, AVSR models remain limited by the scarcity of extensive\ndatasets, especially for most languages beyond English. Automated data\ncollection offers a promising solution. This work presents a practical approach\nto generate AVSR datasets from raw video, refining existing techniques for\nimproved efficiency and accessibility. We demonstrate its broad applicability\nby developing a baseline AVSR model for Vietnamese. Experiments show the\nautomatically collected dataset enables a strong baseline, achieving\ncompetitive performance with robust ASR in clean conditions and significantly\noutperforming them in noisy environments like cocktail parties. This efficient\nmethod provides a pathway to expand AVSR to more languages, particularly\nunder-resourced ones.\n","authors":["Thai-Binh Nguyen","Thi Van Nguyen","Quoc Truong Do","Chi Mai Luong"],"pdf_url":"https://arxiv.org/pdf/2506.04635v1.pdf","comment":"Accepted at Interspeech 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2506.05349v1","updated":"2025-06-05T17:59:58Z","published":"2025-06-05T17:59:58Z","title":"VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos","summary":"  Mathematical reasoning in real-world video settings presents a fundamentally\ndifferent challenge than in static images or text. It requires interpreting\nfine-grained visual information, accurately reading handwritten or digital\ntext, and integrating spoken cues, often dispersed non-linearly over time. In\nsuch multimodal contexts, success hinges not just on perception, but on\nselectively identifying and integrating the right contextual details from a\nrich and noisy stream of content. To this end, we introduce VideoMathQA, a\nbenchmark designed to evaluate whether models can perform such temporally\nextended cross-modal reasoning on videos. The benchmark spans 10 diverse\nmathematical domains, covering videos ranging from 10 seconds to over 1 hour.\nIt requires models to interpret structured visual content, understand\ninstructional narratives, and jointly ground concepts across visual, audio, and\ntextual modalities. We employ graduate-level experts to ensure high quality,\ntotaling over $920$ man-hours of annotation. To reflect real-world scenarios,\nquestions are designed around three core reasoning challenges: direct problem\nsolving, where answers are grounded in the presented question; conceptual\ntransfer, which requires applying learned methods to new problems; and deep\ninstructional comprehension, involving multi-step reasoning over extended\nexplanations and partially worked-out solutions. Each question includes\nmulti-step reasoning annotations, enabling fine-grained diagnosis of model\ncapabilities. Through this benchmark, we highlight the limitations of existing\napproaches and establish a systematic evaluation framework for models that must\nreason, rather than merely perceive, across temporally extended and\nmodality-rich mathematical problem settings. Our benchmark and evaluation code\nare available at: https://mbzuai-oryx.github.io/VideoMathQA\n","authors":["Hanoona Rasheed","Abdelrahman Shaker","Anqi Tang","Muhammad Maaz","Ming-Hsuan Yang","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2506.05349v1.pdf","comment":"VideoMathQA Technical Report"},{"id":"http://arxiv.org/abs/2506.05350v1","updated":"2025-06-05T17:59:58Z","published":"2025-06-05T17:59:58Z","title":"Contrastive Flow Matching","summary":"  Unconditional flow-matching trains diffusion models to transport samples from\na source distribution to a target distribution by enforcing that the flows\nbetween sample pairs are unique. However, in conditional settings (e.g.,\nclass-conditioned models), this uniqueness is no longer guaranteed--flows from\ndifferent conditions may overlap, leading to more ambiguous generations. We\nintroduce Contrastive Flow Matching, an extension to the flow matching\nobjective that explicitly enforces uniqueness across all conditional flows,\nenhancing condition separation. Our approach adds a contrastive objective that\nmaximizes dissimilarities between predicted flows from arbitrary sample pairs.\nWe validate Contrastive Flow Matching by conducting extensive experiments\nacross varying model architectures on both class-conditioned (ImageNet-1k) and\ntext-to-image (CC3M) benchmarks. Notably, we find that training models with\nContrastive Flow Matching (1) improves training speed by a factor of up to 9x,\n(2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9\ncompared to training the same models with flow matching. We release our code\nat: https://github.com/gstoica27/DeltaFM.git.\n","authors":["George Stoica","Vivek Ramanujan","Xiang Fan","Ali Farhadi","Ranjay Krishna","Judy Hoffman"],"pdf_url":"https://arxiv.org/pdf/2506.05350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05348v1","updated":"2025-06-05T17:59:57Z","published":"2025-06-05T17:59:57Z","title":"FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction","summary":"  This paper addresses the challenge of reconstructing dynamic 3D scenes with\ncomplex motions. Some recent works define 3D Gaussian primitives in the\ncanonical space and use deformation fields to map canonical primitives to\nobservation spaces, achieving real-time dynamic view synthesis. However, these\nmethods often struggle to handle scenes with complex motions due to the\ndifficulty of optimizing deformation fields. To overcome this problem, we\npropose FreeTimeGS, a novel 4D representation that allows Gaussian primitives\nto appear at arbitrary time and locations. In contrast to canonical Gaussian\nprimitives, our representation possesses the strong flexibility, thus improving\nthe ability to model dynamic 3D scenes. In addition, we endow each Gaussian\nprimitive with an motion function, allowing it to move to neighboring regions\nover time, which reduces the temporal redundancy. Experiments results on\nseveral datasets show that the rendering quality of our method outperforms\nrecent methods by a large margin.\n","authors":["Yifan Wang","Peishan Yang","Zhen Xu","Jiaming Sun","Zhanhua Zhang","Yong Chen","Hujun Bao","Sida Peng","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.05348v1.pdf","comment":"CVPR 2025; Project page: https://zju3dv.github.io/freetimegs/"},{"id":"http://arxiv.org/abs/2506.05344v1","updated":"2025-06-05T17:59:55Z","published":"2025-06-05T17:59:55Z","title":"SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs","summary":"  Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.\n","authors":["Jiahui Wang","Zuyan Liu","Yongming Rao","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2506.05344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05347v1","updated":"2025-06-05T17:59:55Z","published":"2025-06-05T17:59:55Z","title":"Neural Inverse Rendering from Propagating Light","summary":"  We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes.\n","authors":["Anagh Malik","Benjamin Attal","Andrew Xie","Matthew O'Toole","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2506.05347v1.pdf","comment":"Website: https://anaghmalik.com/InvProp/"},{"id":"http://arxiv.org/abs/2506.05343v1","updated":"2025-06-05T17:59:54Z","published":"2025-06-05T17:59:54Z","title":"ContentV: Efficient Training of Video Generation Models with Limited\n  Compute","summary":"  Recent advances in video generation demand increasingly efficient training\nrecipes to mitigate escalating computational costs. In this report, we present\nContentV, an 8B-parameter text-to-video model that achieves state-of-the-art\nperformance (85.14 on VBench) after training on 256 x 64GB Neural Processing\nUnits (NPUs) for merely four weeks. ContentV generates diverse, high-quality\nvideos across multiple resolutions and durations from text prompts, enabled by\nthree key innovations: (1) A minimalist architecture that maximizes reuse of\npre-trained image generation models for video generation; (2) A systematic\nmulti-stage training strategy leveraging flow matching for enhanced efficiency;\nand (3) A cost-effective reinforcement learning with human feedback framework\nthat improves generation quality without requiring additional human\nannotations. All the code and models are available at:\nhttps://contentv.github.io.\n","authors":["Wenfeng Lin","Renjie Chen","Boyuan Liu","Shiyue Yan","Ruoyu Feng","Jiangchuan Wei","Yichen Zhang","Yimeng Zhou","Chao Feng","Jiao Ran","Qi Wu","Zuotao Liu","Mingyu Guo"],"pdf_url":"https://arxiv.org/pdf/2506.05343v1.pdf","comment":"Project Page: https://contentv.github.io"},{"id":"http://arxiv.org/abs/2506.05342v1","updated":"2025-06-05T17:59:51Z","published":"2025-06-05T17:59:51Z","title":"Refer to Anything with Vision-Language Prompts","summary":"  Recent image segmentation models have advanced to segment images into\nhigh-quality masks for visual entities, and yet they cannot provide\ncomprehensive semantic understanding for complex queries based on both language\nand vision. This limitation reduces their effectiveness in applications that\nrequire user-friendly interactions driven by vision-language prompts. To bridge\nthis gap, we introduce a novel task of omnimodal referring expression\nsegmentation (ORES). In this task, a model produces a group of masks based on\narbitrary prompts specified by text only or text plus reference visual\nentities. To address this new challenge, we propose a novel framework to \"Refer\nto Any Segmentation Mask Group\" (RAS), which augments segmentation models with\ncomplex multimodal interactions and comprehension via a mask-centric large\nmultimodal model. For training and benchmarking ORES models, we create datasets\nMaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by\ntext and reference entities. Through extensive evaluation, we demonstrate\nsuperior performance of RAS on our new ORES task, as well as classic referring\nexpression segmentation (RES) and generalized referring expression segmentation\n(GRES) tasks. Project page: https://Ref2Any.github.io.\n","authors":["Shengcao Cao","Zijun Wei","Jason Kuen","Kangning Liu","Lingzhi Zhang","Jiuxiang Gu","HyunJoon Jung","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05341v1","updated":"2025-06-05T17:59:42Z","published":"2025-06-05T17:59:42Z","title":"Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning","summary":"  Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.\n","authors":["Xingjian Ran","Yixuan Li","Linning Xu","Mulin Yu","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2506.05341v1.pdf","comment":"Project Page: https://directlayout.github.io/"},{"id":"http://arxiv.org/abs/2506.05338v1","updated":"2025-06-05T17:59:30Z","published":"2025-06-05T17:59:30Z","title":"Defurnishing with X-Ray Vision: Joint Removal of Furniture from\n  Panoramas and Mesh","summary":"  We present a pipeline for generating defurnished replicas of indoor spaces\nrepresented as textured meshes and corresponding multi-view panoramic images.\nTo achieve this, we first segment and remove furniture from the mesh\nrepresentation, extend planes, and fill holes, obtaining a simplified\ndefurnished mesh (SDM). This SDM acts as an ``X-ray'' of the scene's underlying\nstructure, guiding the defurnishing process. We extract Canny edges from depth\nand normal images rendered from the SDM. We then use these as a guide to remove\nthe furniture from panorama images via ControlNet inpainting. This control\nsignal ensures the availability of global geometric information that may be\nhidden from a particular panoramic view by the furniture being removed. The\ninpainted panoramas are used to texture the mesh. We show that our approach\nproduces higher quality assets than methods that rely on neural radiance\nfields, which tend to produce blurry low-resolution images, or RGB-D\ninpainting, which is highly susceptible to hallucinations.\n","authors":["Alan Dolhasz","Chen Ma","Dave Gausebeck","Kevin Chen","Gregor Miller","Lucas Hayne","Gunnar Hovden","Azwad Sabik","Olaf Brandt","Mira Slavcheva"],"pdf_url":"https://arxiv.org/pdf/2506.05338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05336v1","updated":"2025-06-05T17:59:29Z","published":"2025-06-05T17:59:29Z","title":"VideoMolmo: Spatio-Temporal Grounding Meets Pointing","summary":"  Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.\n","authors":["Ghazi Shazan Ahmad","Ahmed Heakl","Hanan Gani","Abdelrahman Shaker","Zhiqiang Shen","Ranjay Krishna","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2506.05336v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.05332v1","updated":"2025-06-05T17:59:04Z","published":"2025-06-05T17:59:04Z","title":"Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding","summary":"  Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.\n","authors":["Jingyang Lin","Jialian Wu","Ximeng Sun","Ze Wang","Jiang Liu","Yusheng Su","Xiaodong Yu","Hao Chen","Jiebo Luo","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2506.05332v1.pdf","comment":"Project page: https://videomarathon.github.io/"},{"id":"http://arxiv.org/abs/2506.05331v1","updated":"2025-06-05T17:59:02Z","published":"2025-06-05T17:59:02Z","title":"MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning","summary":"  Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT\n","authors":["Xinyan Chen","Renrui Zhang","Dongzhi Jiang","Aojun Zhou","Shilin Yan","Weifeng Lin","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2506.05331v1.pdf","comment":"Code is released at https://github.com/xinyan-cxy/MINT-CoT"},{"id":"http://arxiv.org/abs/2506.05328v1","updated":"2025-06-05T17:58:33Z","published":"2025-06-05T17:58:33Z","title":"AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs","summary":"  Despite progress in video understanding, current MLLMs struggle with counting\ntasks. Existing benchmarks are limited by short videos, close-set queries, lack\nof clue annotations, and weak multimodal coverage. In this paper, we introduce\nCG-AV-Counting, a manually-annotated clue-grounded counting benchmark with\n1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It\nsupports both black-box and white-box evaluation, serving as a comprehensive\ntestbed for both end-to-end and reasoning-based counting. To explore ways to\nimprove model's counting capability, we propose AV-Reasoner, a model trained\nwith GRPO and curriculum learning to generalize counting ability from related\ntasks. AV-Reasoner achieves state-of-the-art results across multiple\nbenchmarks, demonstrating the effectiveness of reinforcement learning. However,\nexperiments show that on out-of-domain benchmarks, reasoning in the language\nspace fails to bring performance gains. The code and benchmark have been\nrealeased on https://av-reasoner.github.io.\n","authors":["Lidong Lu","Guo Chen","Zhiqi Li","Yicheng Liu","Tong Lu"],"pdf_url":"https://arxiv.org/pdf/2506.05328v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2506.05327v1","updated":"2025-06-05T17:58:23Z","published":"2025-06-05T17:58:23Z","title":"Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting","summary":"  Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)\npipelines by unprojecting them into 3D point clouds for novel view synthesis.\nThis approach offers advantages such as efficient training, the use of known\ncamera poses, and accurate geometry estimation. However, depth discontinuities\nat object boundaries often lead to fragmented or sparse point clouds, degrading\nrendering quality -- a well-known limitation of depth-based representations. To\ntackle this issue, we introduce PM-Loss, a novel regularization loss based on a\npointmap predicted by a pre-trained transformer. Although the pointmap itself\nmay be less accurate than the depth map, it effectively enforces geometric\nsmoothness, especially around object boundaries. With the improved depth map,\nour method significantly improves the feed-forward 3DGS across various\narchitectures and scenes, delivering consistently better rendering results. Our\nproject page: https://aim-uofa.github.io/PMLoss\n","authors":["Duochao Shi","Weijie Wang","Donny Y. Chen","Zeyu Zhang","Jia-Wang Bian","Bohan Zhuang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2506.05327v1.pdf","comment":"Project page: https://aim-uofa.github.io/PMLoss"},{"id":"http://arxiv.org/abs/2506.05318v1","updated":"2025-06-05T17:56:12Z","published":"2025-06-05T17:56:12Z","title":"Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets\n  3D VLMs","summary":"  Remarkable progress in 2D Vision-Language Models (VLMs) has spurred interest\nin extending them to 3D settings for tasks like 3D Question Answering, Dense\nCaptioning, and Visual Grounding. Unlike 2D VLMs that typically process images\nthrough an image encoder, 3D scenes, with their intricate spatial structures,\nallow for diverse model architectures. Based on their encoder design, this\npaper categorizes recent 3D VLMs into 3D object-centric, 2D image-based, and 3D\nscene-centric approaches. Despite the architectural similarity of 3D\nscene-centric VLMs to their 2D counterparts, they have exhibited comparatively\nlower performance compared with the latest 3D object-centric and 2D image-based\napproaches. To understand this gap, we conduct an in-depth analysis, revealing\nthat 3D scene-centric VLMs show limited reliance on the 3D scene encoder, and\nthe pre-train stage appears less effective than in 2D VLMs. Furthermore, we\nobserve that data scaling benefits are less pronounced on larger datasets. Our\ninvestigation suggests that while these models possess cross-modal alignment\ncapabilities, they tend to over-rely on linguistic cues and overfit to frequent\nanswer distributions, thereby diminishing the effective utilization of the 3D\nencoder. To address these limitations and encourage genuine 3D scene\nunderstanding, we introduce a novel 3D Relevance Discrimination QA dataset\ndesigned to disrupt shortcut learning and improve 3D understanding. Our\nfindings highlight the need for advanced evaluation and improved strategies for\nbetter 3D understanding in 3D VLMs.\n","authors":["Haoyuan Li","Yanpeng Zhou","Yufei Gao","Tao Tang","Jianhua Han","Yujie Yuan","Dave Zhenyu Chen","Jiawang Bian","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2506.05318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05317v1","updated":"2025-06-05T17:55:56Z","published":"2025-06-05T17:55:56Z","title":"ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics\n  Estimation","summary":"  Neural rendering has made significant strides in 3D reconstruction and novel\nview synthesis. With the integration with physics, it opens up new\napplications. The inverse problem of estimating physics from visual data,\nhowever, still remains challenging, limiting its effectiveness for applications\nlike physically accurate digital twin creation in robotics and XR. Existing\nmethods that incorporate physics into neural rendering frameworks typically\nrequire dense multi-view videos as input, making them impractical for scalable,\nreal-world use. When presented with sparse multi-view videos, the sequential\noptimization strategy used by existing approaches introduces significant error\naccumulation, e.g., poor initial 3D reconstruction leads to bad material\nparameter estimation in subsequent stages. Instead of sequential optimization,\ndirectly optimizing all parameters at the same time also fails due to the\nhighly non-convex and often non-differentiable nature of the problem. We\npropose ProJo4D, a progressive joint optimization framework that gradually\nincreases the set of jointly optimized parameters guided by their sensitivity,\nleading to fully joint optimization over geometry, appearance, physical state,\nand material property. Evaluations on PAC-NeRF and Spring-Gaus datasets show\nthat ProJo4D outperforms prior work in 4D future state prediction, novel view\nrendering of future state, and material parameter estimation, demonstrating its\neffectiveness in physically grounded 4D scene understanding. For demos, please\nvisit the project webpage: https://daniel03c1.github.io/ProJo4D/\n","authors":["Daniel Rho","Jun Myeong Choi","Biswadip Dey","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2506.05317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05313v1","updated":"2025-06-05T17:55:16Z","published":"2025-06-05T17:55:16Z","title":"MARBLE: Material Recomposition and Blending in CLIP-Space","summary":"  Editing materials of objects in images based on exemplar images is an active\narea of research in computer vision and graphics. We propose MARBLE, a method\nfor performing material blending and recomposing fine-grained material\nproperties by finding material embeddings in CLIP-space and using that to\ncontrol pre-trained text-to-image models. We improve exemplar-based material\nediting by finding a block in the denoising UNet responsible for material\nattribution. Given two material exemplar-images, we find directions in the\nCLIP-space for blending the materials. Further, we can achieve parametric\ncontrol over fine-grained material attributes such as roughness, metallic,\ntransparency, and glow using a shallow network to predict the direction for the\ndesired material attribute change. We perform qualitative and quantitative\nanalysis to demonstrate the efficacy of our proposed method. We also present\nthe ability of our method to perform multiple edits in a single forward pass\nand applicability to painting.\n  Project Page: https://marblecontrol.github.io/\n","authors":["Ta-Ying Cheng","Prafull Sharma","Mark Boss","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2506.05313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05312v1","updated":"2025-06-05T17:54:33Z","published":"2025-06-05T17:54:33Z","title":"Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels","summary":"  Finding correspondences between semantically similar points across images and\nobject instances is one of the everlasting challenges in computer vision. While\nlarge pre-trained vision models have recently been demonstrated as effective\npriors for semantic matching, they still suffer from ambiguities for symmetric\nobjects or repeated object parts. We propose to improve semantic correspondence\nestimation via 3D-aware pseudo-labeling. Specifically, we train an adapter to\nrefine off-the-shelf features using pseudo-labels obtained via 3D-aware\nchaining, filtering wrong labels through relaxed cyclic consistency, and 3D\nspherical prototype mapping constraints. While reducing the need for dataset\nspecific annotations compared to prior work, we set a new state-of-the-art on\nSPair-71k by over 4% absolute gain and by over 7% against methods with similar\nsupervision requirements. The generality of our proposed approach simplifies\nextension of training to other data sources, which we demonstrate in our\nexperiments.\n","authors":["Olaf Dünkel","Thomas Wimmer","Christian Theobalt","Christian Rupprecht","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2506.05312v1.pdf","comment":"Project page: https://genintel.github.io/DIY-SC"},{"id":"http://arxiv.org/abs/2505.24875v2","updated":"2025-06-05T17:51:58Z","published":"2025-05-30T17:59:48Z","title":"ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL","summary":"  Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.\n","authors":["Yu Zhang","Yunqi Li","Yifan Yang","Rui Wang","Yuqing Yang","Dai Qi","Jianmin Bao","Dongdong Chen","Chong Luo","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.24875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05302v1","updated":"2025-06-05T17:51:39Z","published":"2025-06-05T17:51:39Z","title":"Perceive Anything: Recognize, Explain, Caption, and Segment Anything in\n  Images and Videos","summary":"  We present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in\nimages and videos. Our approach extends the powerful segmentation model SAM 2\nby integrating Large Language Models (LLMs), enabling simultaneous object\nsegmentation with the generation of diverse, region-specific semantic outputs,\nincluding categories, label definition, functional explanations, and detailed\ncaptions. A key component, Semantic Perceiver, is introduced to efficiently\ntransform SAM 2's rich visual features, which inherently carry general vision,\nlocalization, and semantic priors into multi-modal tokens for LLM\ncomprehension. To support robust multi-granularity understanding, we also\ndevelop a dedicated data refinement and augmentation pipeline, yielding a\nhigh-quality dataset of 1.5M image and 0.6M video region-semantic annotations,\nincluding novel region-level streaming video caption data. PAM is designed for\nlightweightness and efficiency, while also demonstrates strong performance\nacross a diverse range of region understanding tasks. It runs 1.2-2.4x faster\nand consumes less GPU memory than prior approaches, offering a practical\nsolution for real-world applications. We believe that our effective approach\nwill serve as a strong baseline for future research in region-level visual\nunderstanding.\n","authors":["Weifeng Lin","Xinyu Wei","Ruichuan An","Tianhe Ren","Tingwei Chen","Renrui Zhang","Ziyu Guo","Wentao Zhang","Lei Zhang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2506.05302v1.pdf","comment":"19 pages, 13 figures, Website: https://Perceive-Anything.github.io"},{"id":"http://arxiv.org/abs/2506.05301v1","updated":"2025-06-05T17:51:05Z","published":"2025-06-05T17:51:05Z","title":"SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training","summary":"  Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step.\n","authors":["Jianyi Wang","Shanchuan Lin","Zhijie Lin","Yuxi Ren","Meng Wei","Zongsheng Yue","Shangchen Zhou","Hao Chen","Yang Zhao","Ceyuan Yang","Xuefeng Xiao","Chen Change Loy","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.05301v1.pdf","comment":"Draft Ver. Project page: https://iceclear.github.io/projects/seedvr2/"},{"id":"http://arxiv.org/abs/2506.05297v1","updated":"2025-06-05T17:49:46Z","published":"2025-06-05T17:49:46Z","title":"DM-SegNet: Dual-Mamba Architecture for 3D Medical Image Segmentation\n  with Global Context Modeling","summary":"  Accurate 3D medical image segmentation demands architectures capable of\nreconciling global context modeling with spatial topology preservation. While\nState Space Models (SSMs) like Mamba show potential for sequence modeling,\nexisting medical SSMs suffer from encoder-decoder incompatibility: the\nencoder's 1D sequence flattening compromises spatial structures, while\nconventional decoders fail to leverage Mamba's state propagation. We present\nDM-SegNet, a Dual-Mamba architecture integrating directional state transitions\nwith anatomy-aware hierarchical decoding. The core innovations include a\nquadri-directional spatial Mamba module employing four-directional 3D scanning\nto maintain anatomical spatial coherence, a gated spatial convolution layer\nthat enhances spatially sensitive feature representation prior to state\nmodeling, and a Mamba-driven decoding framework enabling bidirectional state\nsynchronization across scales. Extensive evaluation on two clinically\nsignificant benchmarks demonstrates the efficacy of DM-SegNet: achieving\nstate-of-the-art Dice Similarity Coefficient (DSC) of 85.44% on the Synapse\ndataset for abdominal organ segmentation and 90.22% on the BraTS2023 dataset\nfor brain tumor segmentation.\n","authors":["Hangyu Ji"],"pdf_url":"https://arxiv.org/pdf/2506.05297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05289v1","updated":"2025-06-05T17:45:10Z","published":"2025-06-05T17:45:10Z","title":"AliTok: Towards Sequence Modeling Alignment between Tokenizer and\n  Autoregressive Model","summary":"  Autoregressive image generation aims to predict the next token based on\nprevious ones. However, existing image tokenizers encode tokens with\nbidirectional dependencies during the compression process, which hinders the\neffective modeling by autoregressive models. In this paper, we propose a novel\nAligned Tokenizer (AliTok), which utilizes a causal decoder to establish\nunidirectional dependencies among encoded tokens, thereby aligning the token\nmodeling approach between the tokenizer and autoregressive model. Furthermore,\nby incorporating prefix tokens and employing two-stage tokenizer training to\nenhance reconstruction consistency, AliTok achieves great reconstruction\nperformance while being generation-friendly. On ImageNet-256 benchmark, using a\nstandard decoder-only autoregressive model as the generator with only 177M\nparameters, AliTok achieves a gFID score of 1.50 and an IS of 305.9. When the\nparameter count is increased to 662M, AliTok achieves a gFID score of 1.35,\nsurpassing the state-of-the-art diffusion method with 10x faster sampling\nspeed. The code and weights are available at\nhttps://github.com/ali-vilab/alitok.\n","authors":["Pingyu Wu","Kai Zhu","Yu Liu","Longxiang Tang","Jian Yang","Yansong Peng","Wei Zhai","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2506.05289v1.pdf","comment":"Code: https://github.com/ali-vilab/alitok"},{"id":"http://arxiv.org/abs/2506.05287v1","updated":"2025-06-05T17:44:12Z","published":"2025-06-05T17:44:12Z","title":"EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?","summary":"  The emergence of multimodal large language models (MLLMs) has driven\nbreakthroughs in egocentric vision applications. These applications necessitate\npersistent, context-aware understanding of objects, as users interact with\ntools in dynamic and cluttered environments. However, existing embodied\nbenchmarks primarily focus on static scene exploration, emphasizing object's\nappearance and spatial attributes while neglecting the assessment of dynamic\nchanges arising from users' interactions. To address this gap, we introduce\nEOC-Bench, an innovative benchmark designed to systematically evaluate\nobject-centric embodied cognition in dynamic egocentric scenarios. Specially,\nEOC-Bench features 3,277 meticulously annotated QA pairs categorized into three\ntemporal categories: Past, Present, and Future, covering 11 fine-grained\nevaluation dimensions and 3 visual object referencing types. To ensure thorough\nassessment, we develop a mixed-format human-in-the-loop annotation framework\nwith four types of questions and design a novel multi-scale temporal accuracy\nmetric for open-ended temporal evaluation. Based on EOC-Bench, we conduct\ncomprehensive evaluations of various proprietary, open-source, and object-level\nMLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object\ncognitive capabilities of MLLMs, establishing a robust foundation for\ndeveloping reliable core models for embodied systems.\n","authors":["Yuqian Yuan","Ronghao Dang","Long Li","Wentong Li","Dian Jiao","Xin Li","Deli Zhao","Fan Wang","Wenqiao Zhang","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2506.05287v1.pdf","comment":"32pages"},{"id":"http://arxiv.org/abs/2506.05286v1","updated":"2025-06-05T17:43:27Z","published":"2025-06-05T17:43:27Z","title":"Stable Vision Concept Transformers for Medical Diagnosis","summary":"  Transparency is a paramount concern in the medical field, prompting\nresearchers to delve into the realm of explainable AI (XAI). Among these XAI\nmethods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent\nspace to human-understandable high-level concepts by generating a conceptual\nlayer for extracting conceptual features, which has drawn much attention\nrecently. However, existing methods rely solely on concept features to\ndetermine the model's predictions, which overlook the intrinsic feature\nembeddings within medical images. To address this utility gap between the\noriginal models and concept-based models, we propose Vision Concept Transformer\n(VCT). Furthermore, despite their benefits, CBMs have been found to negatively\nimpact model performance and fail to provide stable explanations when faced\nwith input perturbations, which limits their application in the medical field.\nTo address this faithfulness issue, this paper further proposes the Stable\nVision Concept Transformer (SVCT) based on VCT, which leverages the vision\ntransformer (ViT) as its backbone and incorporates a conceptual layer. SVCT\nemploys conceptual features to enhance decision-making capabilities by fusing\nthem with image features and ensures model faithfulness through the integration\nof Denoised Diffusion Smoothing. Comprehensive experiments on four medical\ndatasets demonstrate that our VCT and SVCT maintain accuracy while remaining\ninterpretable compared to baselines. Furthermore, even when subjected to\nperturbations, our SVCT model consistently provides faithful explanations, thus\nmeeting the needs of the medical field.\n","authors":["Lijie Hu","Songning Lai","Yuan Hua","Shu Yang","Jingfeng Zhang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05286v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.06129 by other authors"},{"id":"http://arxiv.org/abs/2506.05285v1","updated":"2025-06-05T17:43:23Z","published":"2025-06-05T17:43:23Z","title":"RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion","summary":"  3D shape completion has broad applications in robotics, digital twin\nreconstruction, and extended reality (XR). Although recent advances in 3D\nobject and scene completion have achieved impressive results, existing methods\nlack 3D consistency, are computationally expensive, and struggle to capture\nsharp object boundaries. Our work (RaySt3R) addresses these limitations by\nrecasting 3D shape completion as a novel view synthesis problem. Specifically,\ngiven a single RGB-D image and a novel viewpoint (encoded as a collection of\nquery rays), we train a feedforward transformer to predict depth maps, object\nmasks, and per-pixel confidence scores for those query rays. RaySt3R fuses\nthese predictions across multiple query views to reconstruct complete 3D\nshapes. We evaluate RaySt3R on synthetic and real-world datasets, and observe\nit achieves state-of-the-art performance, outperforming the baselines on all\ndatasets by up to 44% in 3D chamfer distance. Project page:\nhttps://rayst3r.github.io\n","authors":["Bardienus P. Duisterhof","Jan Oberst","Bowen Wen","Stan Birchfield","Deva Ramanan","Jeffrey Ichnowski"],"pdf_url":"https://arxiv.org/pdf/2506.05285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05284v1","updated":"2025-06-05T17:42:34Z","published":"2025-06-05T17:42:34Z","title":"Video World Models with Long-term Spatial Memory","summary":"  Emerging world models autoregressively generate video frames in response to\nactions, such as camera movements and text prompts, among other control\nsignals. Due to limited temporal context window sizes, these models often\nstruggle to maintain scene consistency during revisits, leading to severe\nforgetting of previously generated environments. Inspired by the mechanisms of\nhuman memory, we introduce a novel framework to enhancing long-term consistency\nof video world models through a geometry-grounded long-term spatial memory. Our\nframework includes mechanisms to store and retrieve information from the\nlong-term spatial memory and we curate custom datasets to train and evaluate\nworld models with explicitly stored 3D memory mechanisms. Our evaluations show\nimproved quality, consistency, and context length compared to relevant\nbaselines, paving the way towards long-term consistent world generation.\n","authors":["Tong Wu","Shuai Yang","Ryan Po","Yinghao Xu","Ziwei Liu","Dahua Lin","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2506.05284v1.pdf","comment":"Project page: https://spmem.github.io/"},{"id":"http://arxiv.org/abs/2506.05282v1","updated":"2025-06-05T17:36:03Z","published":"2025-06-05T17:36:03Z","title":"Rectified Point Flow: Generic Point Cloud Pose Estimation","summary":"  We introduce Rectified Point Flow, a unified parameterization that formulates\npairwise point cloud registration and multi-part shape assembly as a single\nconditional generative problem. Given unposed point clouds, our method learns a\ncontinuous point-wise velocity field that transports noisy points toward their\ntarget positions, from which part poses are recovered. In contrast to prior\nwork that regresses part-wise poses with ad-hoc symmetry handling, our method\nintrinsically learns assembly symmetries without symmetry labels. Together with\na self-supervised encoder focused on overlapping points, our method achieves a\nnew state-of-the-art performance on six benchmarks spanning pairwise\nregistration and shape assembly. Notably, our unified formulation enables\neffective joint training on diverse datasets, facilitating the learning of\nshared geometric priors and consequently boosting accuracy. Project page:\nhttps://rectified-pointflow.github.io/.\n","authors":["Tao Sun","Liyuan Zhu","Shengyu Huang","Shuran Song","Iro Armeni"],"pdf_url":"https://arxiv.org/pdf/2506.05282v1.pdf","comment":"Project page: https://rectified-pointflow.github.io/"},{"id":"http://arxiv.org/abs/2506.05280v1","updated":"2025-06-05T17:33:41Z","published":"2025-06-05T17:33:41Z","title":"Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian\n  Splatting","summary":"  Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely\non photometric consistency to produce high-quality reconstructions. However, in\nreal-world scenarios, it is challenging to guarantee perfect photometric\nconsistency in acquired images. Appearance codes have been widely used to\naddress this issue, but their modeling capability is limited, as a single code\nis applied to the entire image. Recently, the bilateral grid was introduced to\nperform pixel-wise color mapping, but it is difficult to optimize and constrain\neffectively. In this paper, we propose a novel multi-scale bilateral grid that\nunifies appearance codes and bilateral grids. We demonstrate that this approach\nsignificantly improves geometric accuracy in dynamic, decoupled autonomous\ndriving scene reconstruction, outperforming both appearance codes and bilateral\ngrids. This is crucial for autonomous driving, where accurate geometry is\nimportant for obstacle avoidance and control. Our method shows strong results\nacross four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further\ndemonstrate that the improvement in geometry is driven by the multi-scale\nbilateral grid, which effectively reduces floaters caused by photometric\ninconsistency.\n","authors":["Nan Wang","Yuantao Chen","Lixing Xiao","Weiqing Xiao","Bohan Li","Zhaoxi Chen","Chongjie Ye","Shaocong Xu","Saining Zhang","Ziyang Yan","Pierre Merriaux","Lei Lei","Tianfan Xue","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.05280v1.pdf","comment":"Project page: https://bigcileng.github.io/bilateral-driving; Code:\n  https://github.com/BigCiLeng/bilateral-driving"},{"id":"http://arxiv.org/abs/2506.05274v1","updated":"2025-06-05T17:31:17Z","published":"2025-06-05T17:31:17Z","title":"From Play to Replay: Composed Video Retrieval for Temporally\n  Fine-Grained Videos","summary":"  Composed Video Retrieval (CoVR) retrieves a target video given a query video\nand a modification text describing the intended change. Existing CoVR\nbenchmarks emphasize appearance shifts or coarse event changes and therefore do\nnot test the ability to capture subtle, fast-paced temporal differences. We\nintroduce TF-CoVR, the first large-scale benchmark dedicated to temporally\nfine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K\ntriplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing\non temporal aspect, link each query to a single target segment taken from the\nsame video, limiting practical usefulness. In TF-CoVR, we instead construct\neach <query, modification> pair by prompting an LLM with the label differences\nbetween clips drawn from different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting real-world tasks such\nas sports-highlight generation. To model these temporal dynamics we propose\nTF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video\nencoder on fine-grained action classification to obtain temporally\ndiscriminative embeddings; (ii) align the composed query with candidate videos\nusing contrastive learning. We conduct the first comprehensive study of image,\nvideo, and general multimodal embedding (GME) models on temporally fine-grained\ncomposed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,\nTF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and\nafter fine-tuning raises the state-of-the-art from 19.83 to 25.82.\n","authors":["Animesh Gupta","Jay Parmar","Ishan Rajendrakumar Dave","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2506.05274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05263v1","updated":"2025-06-05T17:24:11Z","published":"2025-06-05T17:24:11Z","title":"Can Foundation Models Generalise the Presentation Attack Detection\n  Capabilities on ID Cards?","summary":"  Nowadays, one of the main challenges in presentation attack detection (PAD)\non ID cards is obtaining generalisation capabilities for a diversity of\ncountries that are issuing ID cards. Most PAD systems are trained on one, two,\nor three ID documents because of privacy protection concerns. As a result, they\ndo not obtain competitive results for commercial purposes when tested in an\nunknown new ID card country. In this scenario, Foundation Models (FM) trained\non huge datasets can help to improve generalisation capabilities. This work\nintends to improve and benchmark the capabilities of FM and how to use them to\nadapt the generalisation on PAD of ID Documents. Different test protocols were\nused, considering zero-shot and fine-tuning and two different ID card datasets.\nOne private dataset based on Chilean IDs and one open-set based on three ID\ncountries: Finland, Spain, and Slovakia. Our findings indicate that bona fide\nimages are the key to generalisation.\n","authors":["Juan E. Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2506.05263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05260v1","updated":"2025-06-05T17:21:16Z","published":"2025-06-05T17:21:16Z","title":"LeanPO: Lean Preference Optimization for Likelihood Alignment in\n  Video-LLMs","summary":"  Most Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin\nbetween a winning response ($y_w$) and a losing response ($y_l$). However, the\nlikelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta\n(y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training,\ninadvertently boosting the probabilities of non-target responses. In this\npaper, we systematically revisit this phenomenon from LLMs to Video-LLMs,\nshowing that it intensifies when dealing with the redundant complexity of video\ncontent. To alleviate the impact of this phenomenon, we propose \\emph{Lean\nPreference Optimization} (LeanPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to\nthe policy model. A key component of LeanPO is the reward-trustworthiness\ncorrelated self-generated preference data pipeline, which carefully infuses\nrelevant prior knowledge into the model while continuously refining the\npreference data via self-reflection. This allows the policy model to obtain\nhigh-quality paired data and accurately estimate the newly defined reward, thus\nmitigating the unintended drop. In addition, we introduce a dynamic label\nsmoothing strategy that mitigates the impact of noise in responses from diverse\nvideo content, preventing the model from overfitting to spurious details.\nExtensive experiments demonstrate that LeanPO significantly enhances the\nperformance of state-of-the-art Video-LLMs, consistently boosting baselines of\nvarying capacities with minimal additional training overhead. Moreover, LeanPO\noffers a simple yet effective solution for aligning Video-LLM preferences with\nhuman trustworthiness, paving the way toward the reliable and efficient\nVideo-LLMs.\n","authors":["Xiaodong Wang","Jinfa Huang","Li Yuan","Peixi Peng"],"pdf_url":"https://arxiv.org/pdf/2506.05260v1.pdf","comment":"Code: https://github.com/Wang-Xiaodong1899/LeanPO"},{"id":"http://arxiv.org/abs/2506.05250v1","updated":"2025-06-05T17:10:29Z","published":"2025-06-05T17:10:29Z","title":"Spatiotemporal Contrastive Learning for Cross-View Video Localization in\n  Unstructured Off-road Terrains","summary":"  Robust cross-view 3-DoF localization in GPS-denied, off-road environments\nremains challenging due to (1) perceptual ambiguities from repetitive\nvegetation and unstructured terrain, and (2) seasonal shifts that significantly\nalter scene appearance, hindering alignment with outdated satellite imagery. To\naddress this, we introduce MoViX, a self-supervised cross-view video\nlocalization framework that learns viewpoint- and season-invariant\nrepresentations while preserving directional awareness essential for accurate\nlocalization. MoViX employs a pose-dependent positive sampling strategy to\nenhance directional discrimination and temporally aligned hard negative mining\nto discourage shortcut learning from seasonal cues. A motion-informed frame\nsampler selects spatially diverse frames, and a lightweight temporal aggregator\nemphasizes geometrically aligned observations while downweighting ambiguous\nones. At inference, MoViX runs within a Monte Carlo Localization framework,\nusing a learned cross-view matching module in place of handcrafted models.\nEntropy-guided temperature scaling enables robust multi-hypothesis tracking and\nconfident convergence under visual ambiguity. We evaluate MoViX on the\nTartanDrive 2.0 dataset, training on under 30 minutes of data and testing over\n12.29 km. Despite outdated satellite imagery, MoViX localizes within 25 meters\nof ground truth 93% of the time, and within 50 meters 100% of the time in\nunseen regions, outperforming state-of-the-art baselines without\nenvironment-specific tuning. We further demonstrate generalization on a\nreal-world off-road dataset from a geographically distinct site with a\ndifferent robot platform.\n","authors":["Zhiyun Deng","Dongmyeong Lee","Amanda Adkins","Jesse Quattrociocchi","Christian Ellis","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2506.05250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10510v3","updated":"2025-06-05T17:10:20Z","published":"2024-12-13T19:11:18Z","title":"DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts","summary":"  The proliferation of disinformation demands reliable and scalable\nfact-checking solutions. We present Dynamic Evidence-based FAct-checking with\nMultimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for\nopen-domain, text-image claim verification. DEFAME operates in a six-stage\nprocess, dynamically selecting the tools and search depth to extract and\nevaluate textual and visual evidence. Unlike prior approaches that are\ntext-only, lack explainability, or rely solely on parametric knowledge, DEFAME\nperforms end-to-end verification, accounting for images in claims and evidence\nwhile generating structured, multimodal reports. Evaluation on the popular\nbenchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all\nprevious methods, establishing itself as the new state-of-the-art fact-checking\nsystem for uni- and multimodal fact-checking. Moreover, we introduce a new\nmultimodal benchmark, ClaimReview2024+, featuring claims after the knowledge\ncutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms\nthe GPT-4o baselines, showing temporal generalizability and the potential for\nreal-time fact-checking.\n","authors":["Tobias Braun","Mark Rothermel","Marcus Rohrbach","Anna Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2412.10510v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05240v1","updated":"2025-06-05T16:59:53Z","published":"2025-06-05T16:59:53Z","title":"Aligning Latent Spaces with Flow Priors","summary":"  This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.\n","authors":["Yizhuo Li","Yuying Ge","Yixiao Ge","Ying Shan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2506.05240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19136v2","updated":"2025-06-05T16:54:25Z","published":"2025-03-24T20:47:51Z","title":"Stochastic Poisson Surface Reconstruction with One Solve using Geometric\n  Gaussian Processes","summary":"  Poisson Surface Reconstruction is a widely-used algorithm for reconstructing\na surface from an oriented point cloud. To facilitate applications where only\npartial surface information is available, or scanning is performed\nsequentially, a recent line of work proposes to incorporate uncertainty into\nthe reconstructed surface via Gaussian process models. The resulting algorithms\nfirst perform Gaussian process interpolation, then solve a set of volumetric\npartial differential equations globally in space, resulting in a\ncomputationally expensive two-stage procedure. In this work, we apply\nrecently-developed techniques from geometric Gaussian processes to combine\ninterpolation and surface reconstruction into a single stage, requiring only\none linear solve per sample. The resulting reconstructed surface samples can be\nqueried locally in space, without the use of problem-dependent volumetric\nmeshes or grids. These capabilities enable one to (a) perform probabilistic\ncollision detection locally around the region of interest, (b) perform ray\ncasting without evaluating points not on the ray's trajectory, and (c) perform\nnext-view planning on a per-ray basis. They also do not requiring one to\napproximate kernel matrix inverses with diagonal matrices as part of\nintermediate computations, unlike prior methods. Results show that our approach\nprovides a cleaner, more-principled, and more-flexible stochastic surface\nreconstruction pipeline.\n","authors":["Sidhanth Holalkere","David S. Bindel","Silvia Sellán","Alexander Terenin"],"pdf_url":"https://arxiv.org/pdf/2503.19136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22769v3","updated":"2025-06-05T16:49:14Z","published":"2025-05-28T18:38:04Z","title":"MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking","summary":"  Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as\nusers naturally change their postures and device orientations. Traditional\ncalibration approaches, like one-off, fail to adapt to these dynamic\nconditions, leading to degraded performance over time. We present MAC-Gaze, a\nMotion-Aware continual Calibration approach that leverages smartphone Inertial\nmeasurement unit (IMU) sensors and continual learning techniques to\nautomatically detect changes in user motion states and update the gaze tracking\nmodel accordingly. Our system integrates a pre-trained visual gaze estimator\nand an IMU-based activity recognition model with a clustering-based hybrid\ndecision-making mechanism that triggers recalibration when motion patterns\ndeviate significantly from previously encountered states. To enable\naccumulative learning of new motion conditions while mitigating catastrophic\nforgetting, we employ replay-based continual learning, allowing the model to\nmaintain performance across previously encountered motion conditions. We\nevaluate our system through extensive experiments on the publicly available\nRGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+\nimages, 800K+ IMU readings), encompassing a wide range of postures under\nvarious motion conditions including sitting, standing, lying, and walking.\nResults demonstrate that our method reduces gaze estimation error by 19.9% on\nRGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to\n1.92 cm) compared to traditional calibration approaches. Our framework provides\na robust solution for maintaining gaze estimation accuracy in mobile scenarios.\n","authors":["Yaxiong Lei","Mingyue Zhao","Yuheng Wang","Shijing He","Yusuke Sugano","Mohamed Khamis","Juan Ye"],"pdf_url":"https://arxiv.org/pdf/2505.22769v3.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.03147v3","updated":"2025-06-05T16:41:40Z","published":"2025-06-03T17:59:33Z","title":"UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation","summary":"  Although existing unified models achieve strong performance in\nvision-language understanding and text-to-image generation, they remain limited\nin addressing image perception and manipulation -- capabilities increasingly\ndemanded in practical applications. Recently, OpenAI introduced the powerful\nGPT-4o-Image model, which showcases advanced capabilities in comprehensive\nimage perception and manipulation, sparking widespread interest. Through\ncarefully designed experiments, we observe that GPT-4o-Image likely relies on\nsemantic encoders rather than VAEs for feature extraction, despite VAEs being\ncommonly regarded as crucial for image manipulation tasks. Inspired by this\ninsight, we propose UniWorld-V1, a unified generative framework built upon\nsemantic features extracted from powerful multimodal large language models and\ncontrastive semantic encoders. Using only 2.7M training data, UniWorld-V1\nachieves impressive performance across diverse tasks, including image\nunderstanding, generation, manipulation, and perception. We fully open-source\nthe UniWorld-V1 framework, including model weights, training and evaluation\nscripts, and datasets to promote reproducibility and further research.\n","authors":["Bin Lin","Zongjian Li","Xinhua Cheng","Yuwei Niu","Yang Ye","Xianyi He","Shenghai Yuan","Wangbo Yu","Shaodong Wang","Yunyang Ge","Yatian Pang","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2506.03147v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05221v1","updated":"2025-06-05T16:38:16Z","published":"2025-06-05T16:38:16Z","title":"SAM-aware Test-time Adaptation for Universal Medical Image Segmentation","summary":"  Universal medical image segmentation using the Segment Anything Model (SAM)\nremains challenging due to its limited adaptability to medical domains.\nExisting adaptations, such as MedSAM, enhance SAM's performance in medical\nimaging but at the cost of reduced generalization to unseen data. Therefore, in\nthis paper, we propose SAM-aware Test-Time Adaptation (SAM-TTA), a\nfundamentally different pipeline that preserves the generalization of SAM while\nimproving its segmentation performance in medical imaging via a test-time\nframework. SAM-TTA tackles two key challenges: (1) input-level discrepancies\ncaused by differences in image acquisition between natural and medical images\nand (2) semantic-level discrepancies due to fundamental differences in object\ndefinition between natural and medical domains (e.g., clear boundaries vs.\nambiguous structures). Specifically, our SAM-TTA framework comprises (1)\nSelf-adaptive Bezier Curve-based Transformation (SBCT), which adaptively\nconverts single-channel medical images into three-channel SAM-compatible inputs\nwhile maintaining structural integrity, to mitigate the input gap between\nmedical and natural images, and (2) Dual-scale Uncertainty-driven Mean Teacher\nadaptation (DUMT), which employs consistency learning to align SAM's internal\nrepresentations to medical semantics, enabling efficient adaptation without\nauxiliary supervision or expensive retraining. Extensive experiments on five\npublic datasets demonstrate that our SAM-TTA outperforms existing TTA\napproaches and even surpasses fully fine-tuned models such as MedSAM in certain\nscenarios, establishing a new paradigm for universal medical image\nsegmentation. Code can be found at https://github.com/JianghaoWu/SAM-TTA.\n","authors":["Jianghao Wu","Yicheng Wu","Yutong Xie","Wenjia Bai","You Zhang","Feilong Tang","Yulong Li","Yasmeen George","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2506.05221v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.05218v1","updated":"2025-06-05T16:34:57Z","published":"2025-06-05T16:34:57Z","title":"MonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm","summary":"  We introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR.\n","authors":["Zhang Li","Yuliang Liu","Qiang Liu","Zhiyin Ma","Ziyang Zhang","Shuo Zhang","Zidun Guo","Jiarui Zhang","Xinyu Wang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2506.05218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05217v1","updated":"2025-06-05T16:33:32Z","published":"2025-06-05T16:33:32Z","title":"DSG-World: Learning a 3D Gaussian World Model from Dual State Videos","summary":"  Building an efficient and physically consistent world model from limited\nobservations is a long standing challenge in vision and robotics. Many existing\nworld modeling pipelines are based on implicit generative models, which are\nhard to train and often lack 3D or physical consistency. On the other hand,\nexplicit 3D methods built from a single state often require multi-stage\nprocessing-such as segmentation, background completion, and inpainting-due to\nocclusions. To address this, we leverage two perturbed observations of the same\nscene under different object configurations. These dual states offer\ncomplementary visibility, alleviating occlusion issues during state transitions\nand enabling more stable and complete reconstruction. In this paper, we present\nDSG-World, a novel end-to-end framework that explicitly constructs a 3D\nGaussian World model from Dual State observations. Our approach builds dual\nsegmentation-aware Gaussian fields and enforces bidirectional photometric and\nsemantic consistency. We further introduce a pseudo intermediate state for\nsymmetric alignment and design collaborative co-pruning trategies to refine\ngeometric completeness. DSG-World enables efficient real-to-simulation transfer\npurely in the explicit Gaussian representation space, supporting high-fidelity\nrendering and object-level scene manipulation without relying on dense\nobservations or multi-stage pipelines. Extensive experiments demonstrate strong\ngeneralization to novel views and scene states, highlighting the effectiveness\nof our approach for real-world 3D reconstruction and simulation.\n","authors":["Wenhao Hu","Xuexiang Wen","Xi Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05210v1","updated":"2025-06-05T16:22:17Z","published":"2025-06-05T16:22:17Z","title":"Towards Vision-Language-Garment Models For Web Knowledge Garment\n  Understanding and Generation","summary":"  Multimodal foundation models have demonstrated strong generalization, yet\ntheir ability to transfer knowledge to specialized domains such as garment\ngeneration remains underexplored. We introduce VLG, a vision-language-garment\nmodel that synthesizes garments from textual descriptions and visual imagery.\nOur experiments assess VLG's zero-shot generalization, investigating its\nability to transfer web-scale reasoning to unseen garment styles and prompts.\nPreliminary results indicate promising transfer capabilities, highlighting the\npotential for multimodal foundation models to adapt effectively to specialized\ndomains like fashion design.\n","authors":["Jan Ackermann","Kiyohiro Nakayama","Guandao Yang","Tong Wu","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2506.05210v1.pdf","comment":"Presented at MMFM CVPRW'25, code available at\n  https://georgenakayama.github.io/AIpparel/"},{"id":"http://arxiv.org/abs/2506.05207v1","updated":"2025-06-05T16:18:32Z","published":"2025-06-05T16:18:32Z","title":"Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal\n  Decoupled Finetuning","summary":"  Recently, breakthroughs in the video diffusion transformer have shown\nremarkable capabilities in diverse motion generations. As for the\nmotion-transfer task, current methods mainly use two-stage Low-Rank Adaptations\n(LoRAs) finetuning to obtain better performance. However, existing\nadaptation-based motion transfer still suffers from motion inconsistency and\ntuning inefficiency when applied to large video diffusion transformers. Naive\ntwo-stage LoRA tuning struggles to maintain motion consistency between\ngenerated and input videos due to the inherent spatial-temporal coupling in the\n3D attention operator. Additionally, they require time-consuming fine-tuning\nprocesses in both stages. To tackle these issues, we propose\nFollow-Your-Motion, an efficient two-stage video motion transfer framework that\nfinetunes a powerful video diffusion transformer to synthesize complex\nmotion.Specifically, we propose a spatial-temporal decoupled LoRA to decouple\nthe attention architecture for spatial appearance and temporal motion\nprocessing. During the second training stage, we design the sparse motion\nsampling and adaptive RoPE to accelerate the tuning speed. To address the lack\nof a benchmark for this field, we introduce MotionBench, a comprehensive\nbenchmark comprising diverse motion, including creative camera motion, single\nobject motion, multiple object motion, and complex human motion. We show\nextensive evaluations on MotionBench to verify the superiority of\nFollow-Your-Motion.\n","authors":["Yue Ma","Yulong Liu","Qiyuan Zhu","Ayden Yang","Kunyu Feng","Xinhua Zhang","Zhifeng Li","Sirui Han","Chenyang Qi","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05207v1.pdf","comment":"project page: https://follow-your-motion.github.io/"},{"id":"http://arxiv.org/abs/2506.05204v1","updated":"2025-06-05T16:17:18Z","published":"2025-06-05T16:17:18Z","title":"OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with\n  Expanded Field-of-View","summary":"  Reconstructing semantic-aware 3D scenes from sparse views is a challenging\nyet essential research direction, driven by the demands of emerging\napplications such as virtual reality and embodied AI. Existing per-scene\noptimization methods require dense input views and incur high computational\ncosts, while generalizable approaches often struggle to reconstruct regions\noutside the input view cone. In this paper, we propose OGGSplat, an open\nGaussian growing method that expands the field-of-view in generalizable 3D\nreconstruction. Our key insight is that the semantic attributes of open\nGaussians provide strong priors for image extrapolation, enabling both semantic\nconsistency and visual plausibility. Specifically, once open Gaussians are\ninitialized from sparse views, we introduce an RGB-semantic consistent\ninpainting module applied to selected rendered views. This module enforces\nbidirectional control between an image diffusion model and a semantic diffusion\nmodel. The inpainted regions are then lifted back into 3D space for efficient\nand progressive Gaussian parameter optimization. To evaluate our method, we\nestablish a Gaussian Outpainting (GO) benchmark that assesses both semantic and\ngenerative quality of reconstructed open-vocabulary scenes. OGGSplat also\ndemonstrates promising semantic-aware scene reconstruction capabilities when\nprovided with two view images captured directly from a smartphone camera.\n","authors":["Yanbo Wang","Ziyi Wang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2506.05204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18053v2","updated":"2025-06-05T16:13:05Z","published":"2025-04-25T03:54:24Z","title":"DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.\n","authors":["Jianyu Liu","Hangyu Guo","Ranjie Duan","Xingyuan Bu","Yancheng He","Shilong Li","Hui Huang","Jiaheng Liu","Yucheng Wang","Chenchen Jing","Xingwei Qu","Xiao Zhang","Yingshui Tan","Yanan Wu","Jihao Gu","Yangguang Li","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.18053v2.pdf","comment":"[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM"},{"id":"http://arxiv.org/abs/2506.05199v1","updated":"2025-06-05T16:11:57Z","published":"2025-06-05T16:11:57Z","title":"Grounding Beyond Detection: Enhancing Contextual Understanding in\n  Embodied 3D Grounding","summary":"  Embodied 3D grounding aims to localize target objects described in human\ninstructions from ego-centric viewpoint. Most methods typically follow a\ntwo-stage paradigm where a trained 3D detector's optimized backbone parameters\nare used to initialize a grounding model. In this study, we explore a\nfundamental question: Does embodied 3D grounding benefit enough from detection?\nTo answer this question, we assess the grounding performance of detection\nmodels using predicted boxes filtered by the target category. Surprisingly,\nthese detection models without any instruction-specific training outperform the\ngrounding models explicitly trained with language instructions. This indicates\nthat even category-level embodied 3D grounding may not be well resolved, let\nalone more fine-grained context-aware grounding. Motivated by this finding, we\npropose DEGround, which shares DETR queries as object representation for both\nDEtection and Grounding and enables the grounding to benefit from basic\ncategory classification and box detection. Based on this framework, we further\nintroduce a regional activation grounding module that highlights\ninstruction-related regions and a query-wise modulation module that\nincorporates sentence-level semantic into the query representation,\nstrengthening the context-aware understanding of language instructions.\nRemarkably, DEGround outperforms state-of-the-art model BIP3D by 7.52\\% at\noverall accuracy on the EmbodiedScan validation set. The source code will be\npublicly available at https://github.com/zyn213/DEGround.\n","authors":["Yani Zhang","Dongming Wu","Hao Shi","Yingfei Liu","Tiancai Wang","Haoqiang Fan","Xingping Dong"],"pdf_url":"https://arxiv.org/pdf/2506.05199v1.pdf","comment":"1st place on embodiedscan"},{"id":"http://arxiv.org/abs/2506.05198v1","updated":"2025-06-05T16:10:47Z","published":"2025-06-05T16:10:47Z","title":"Quantifying Cross-Modality Memorization in Vision-Language Models","summary":"  Understanding what and how neural networks memorize during training is\ncrucial, both from the perspective of unintentional memorization of potentially\nsensitive information and from the standpoint of effective knowledge\nacquisition for real-world, knowledge-intensive tasks. While previous studies\nprimarily investigate memorization within a single modality, such as text\nmemorization in large language models or image memorization in diffusion\nmodels, unified multimodal models are becoming increasingly prevalent in\npractical applications. In this work, we focus on the unique characteristics of\ncross-modality memorization and conduct a systematic study centered on\nvision-language models. To facilitate controlled experiments, we first\nintroduce a synthetic persona dataset comprising diverse synthetic person\nimages and textual descriptions. We quantify factual knowledge memorization and\ncross-modal transferability by training models on a single modality and\nevaluating their performance in the other. Our results reveal that facts\nlearned in one modality transfer to the other, but a significant gap exists\nbetween recalling information in the source and target modalities. Furthermore,\nwe observe that this gap exists across various scenarios, including more\ncapable models, machine unlearning, and the multi-hop case. At the end, we\npropose a baseline method to mitigate this challenge. We hope our study can\ninspire future research on developing more robust multimodal learning\ntechniques to enhance cross-modal transferability.\n","authors":["Yuxin Wen","Yangsibo Huang","Tom Goldstein","Ravi Kumar","Badih Ghazi","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05195v1","updated":"2025-06-05T16:07:22Z","published":"2025-06-05T16:07:22Z","title":"Vision-Based Autonomous MM-Wave Reflector Using ArUco-Driven\n  Angle-of-Arrival Estimation","summary":"  Reliable millimeter-wave (mmWave) communication in non-line-of-sight (NLoS)\nconditions remains a major challenge for both military and civilian operations,\nespecially in urban or infrastructure-limited environments. This paper presents\na vision-aided autonomous reflector system designed to enhance mmWave link\nperformance by dynamically steering signal reflections using a motorized\nmetallic plate. The proposed system leverages a monocular camera to detect\nArUco markers on allied transmitter and receiver nodes, estimate their angles\nof arrival, and align the reflector in real time for optimal signal\nredirection. This approach enables selective beam coverage by serving only\nauthenticated targets with visible markers and reduces the risk of unintended\nsignal exposure. The designed prototype, built on a Raspberry Pi 4 and\nlow-power hardware, operates autonomously without reliance on external\ninfrastructure or GPS. Experimental results at 60\\,GHz demonstrate a 23\\,dB\naverage gain in received signal strength and an 0.89 probability of maintaining\nsignal reception above a target threshold of -65 dB in an indoor environment,\nfar exceeding the static and no-reflector baselines. These results demonstrate\nthe system's potential for resilient and adaptive mmWave connectivity in\ncomplex and dynamic environments.\n","authors":["Josue Marroquin","Nan Inzali","Miles Dillon Lantz","Campbell Freeman","Amod Ashtekar","\\\\Ajinkya Umesh Mulik","Mohammed E Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2506.05195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05191v1","updated":"2025-06-05T16:04:08Z","published":"2025-06-05T16:04:08Z","title":"MokA: Multimodal Low-Rank Adaptation for MLLMs","summary":"  In this paper, we reveal that most current efficient multimodal fine-tuning\nmethods are hindered by a key limitation: they are directly borrowed from LLMs,\noften neglecting the intrinsic differences of multimodal scenarios and even\naffecting the full utilization of all modalities. Inspired by our empirical\nobservation, we argue that unimodal adaptation and cross-modal adaptation are\ntwo essential parts for the effective fine-tuning of MLLMs. From this\nperspective, we propose Multimodal low-rank Adaptation (MokA), a\nmultimodal-aware efficient fine-tuning strategy that takes multimodal\ncharacteristics into consideration. It compresses unimodal information by\nmodality-specific parameters while explicitly enhancing cross-modal\ninteraction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility\nof the proposed method. Ablation studies and efficiency evaluation are also\nconducted to fully asses our method. Overall, we think MokA provides a more\ntargeted solution for efficient adaptation of MLLMs, paving the way for further\nexploration. The project page is at https://gewu-lab.github.io/MokA.\n","authors":["Yake Wei","Yu Miao","Dongzhan Zhou","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2506.05191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05184v1","updated":"2025-06-05T15:56:45Z","published":"2025-06-05T15:56:45Z","title":"Single GPU Task Adaptation of Pathology Foundation Models for Whole\n  Slide Image Analysis","summary":"  Pathology foundation models (PFMs) have emerged as powerful tools for\nanalyzing whole slide images (WSIs). However, adapting these pretrained PFMs\nfor specific clinical tasks presents considerable challenges, primarily due to\nthe availability of only weak (WSI-level) labels for gigapixel images,\nnecessitating multiple instance learning (MIL) paradigm for effective WSI\nanalysis. This paper proposes a novel approach for single-GPU \\textbf{T}ask\n\\textbf{A}daptation of \\textbf{PFM}s (TAPFM) that uses vision transformer\n(\\vit) attention for MIL aggregation while optimizing both for feature\nrepresentations and attention weights. The proposed approach maintains separate\ncomputational graphs for MIL aggregator and the PFM to create stable training\ndynamics that align with downstream task objectives during end-to-end\nadaptation. Evaluated on mutation prediction tasks for bladder cancer and lung\nadenocarcinoma across institutional and TCGA cohorts, TAPFM consistently\noutperforms conventional approaches, with H-Optimus-0 (TAPFM) outperforming the\nbenchmarks. TAPFM effectively handles multi-label classification of actionable\nmutations as well. Thus, TAPFM makes adaptation of powerful pre-trained PFMs\npractical on standard hardware for various clinical applications.\n","authors":["Neeraj Kumar","Swaraj Nanda","Siddharth Singi","Jamal Benhamida","David Kim","Jie-Fu Chen","Amir Momeni-Boroujeni","Gregory M. Goldgof","Gabriele Campanella","Chad Vanderbilt"],"pdf_url":"https://arxiv.org/pdf/2506.05184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05175v1","updated":"2025-06-05T15:49:39Z","published":"2025-06-05T15:49:39Z","title":"Track Any Anomalous Object: A Granular Video Anomaly Detection Pipeline","summary":"  Video anomaly detection (VAD) is crucial in scenarios such as surveillance\nand autonomous driving, where timely detection of unexpected activities is\nessential. Although existing methods have primarily focused on detecting\nanomalous objects in videos -- either by identifying anomalous frames or\nobjects -- they often neglect finer-grained analysis, such as anomalous pixels,\nwhich limits their ability to capture a broader range of anomalies. To address\nthis challenge, we propose a new framework called Track Any Anomalous Object\n(TAO), which introduces a granular video anomaly detection pipeline that, for\nthe first time, integrates the detection of multiple fine-grained anomalous\nobjects into a unified framework. Unlike methods that assign anomaly scores to\nevery pixel, our approach transforms the problem into pixel-level tracking of\nanomalous objects. By linking anomaly scores to downstream tasks such as\nsegmentation and tracking, our method removes the need for threshold tuning and\nachieves more precise anomaly localization in long and complex video sequences.\nExperiments demonstrate that TAO sets new benchmarks in accuracy and\nrobustness. Project page available online.\n","authors":["Yuzhi Huang","Chenxin Li","Haitao Zhang","Zixu Lin","Yunlong Lin","Hengyu Liu","Wuyang Li","Xinyu Liu","Jiechao Gao","Yue Huang","Xinghao Ding","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2506.05175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05169v1","updated":"2025-06-05T15:45:08Z","published":"2025-06-05T15:45:08Z","title":"Through-the-Wall Radar Human Activity Recognition WITHOUT Using Neural\n  Networks","summary":"  After a few years of research in the field of through-the-wall radar (TWR)\nhuman activity recognition (HAR), I found that we seem to be stuck in the\nmindset of training on radar image data through neural network models. The\nearliest related works in this field based on template matching did not require\na training process, and I believe they have never died. Because these methods\npossess a strong physical interpretability and are closer to the basis of\ntheoretical signal processing research. In this paper, I would like to try to\nreturn to the original path by attempting to eschew neural networks to achieve\nthe TWR HAR task and challenge to achieve intelligent recognition as neural\nnetwork models. In detail, the range-time map and Doppler-time map of TWR are\nfirst generated. Then, the initial regions of the human target foreground and\nnoise background on the maps are determined using corner detection method, and\nthe micro-Doppler signature is segmented using the multiphase active contour\nmodel. The micro-Doppler segmentation feature is discretized into a\ntwo-dimensional point cloud. Finally, the topological similarity between the\nresulting point cloud and the point clouds of the template data is calculated\nusing Mapper algorithm to obtain the recognition results. The effectiveness of\nthe proposed method is demonstrated by numerical simulated and measured\nexperiments. The open-source code of this work is released at:\nhttps://github.com/JoeyBGOfficial/Through-the-Wall-Radar-Human-Activity-Recognition-Without-Using-Neural-Networks.\n","authors":["Weicheng Gao"],"pdf_url":"https://arxiv.org/pdf/2506.05169v1.pdf","comment":"15 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2506.05163v1","updated":"2025-06-05T15:40:41Z","published":"2025-06-05T15:40:41Z","title":"FRED: The Florence RGB-Event Drone Dataset","summary":"  Small, fast, and lightweight drones present significant challenges for\ntraditional RGB cameras due to their limitations in capturing fast-moving\nobjects, especially under challenging lighting conditions. Event cameras offer\nan ideal solution, providing high temporal definition and dynamic range, yet\nexisting benchmarks often lack fine temporal resolution or drone-specific\nmotion patterns, hindering progress in these areas. This paper introduces the\nFlorence RGB-Event Drone dataset (FRED), a novel multimodal dataset\nspecifically designed for drone detection, tracking, and trajectory\nforecasting, combining RGB video and event streams. FRED features more than 7\nhours of densely annotated drone trajectories, using 5 different drone models\nand including challenging scenarios such as rain and adverse lighting\nconditions. We provide detailed evaluation protocols and standard metrics for\neach task, facilitating reproducible benchmarking. The authors hope FRED will\nadvance research in high-speed drone perception and multimodal spatiotemporal\nunderstanding.\n","authors":["Gabriele Magrini","Niccolò Marini","Federico Becattini","Lorenzo Berlincioni","Niccolò Biondi","Pietro Pala","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2506.05163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24305v2","updated":"2025-06-05T15:36:46Z","published":"2025-05-30T07:38:46Z","title":"SR3D: Unleashing Single-view 3D Reconstruction for Transparent and\n  Specular Object Grasping","summary":"  Recent advancements in 3D robotic manipulation have improved grasping of\neveryday objects, but transparent and specular materials remain challenging due\nto depth sensing limitations. While several 3D reconstruction and depth\ncompletion approaches address these challenges, they suffer from setup\ncomplexity or limited observation information utilization. To address this,\nleveraging the power of single view 3D object reconstruction approaches, we\npropose a training free framework SR3D that enables robotic grasping of\ntransparent and specular objects from a single view observation. Specifically,\ngiven single view RGB and depth images, SR3D first uses the external visual\nmodels to generate 3D reconstructed object mesh based on RGB image. Then, the\nkey idea is to determine the 3D object's pose and scale to accurately localize\nthe reconstructed object back into its original depth corrupted 3D scene.\nTherefore, we propose view matching and keypoint matching mechanisms,which\nleverage both the 2D and 3D's inherent semantic and geometric information in\nthe observation to determine the object's 3D state within the scene, thereby\nreconstructing an accurate 3D depth map for effective grasp detection.\nExperiments in both simulation and real world show the reconstruction\neffectiveness of SR3D.\n","authors":["Mingxu Zhang","Xiaoqi Li","Jiahui Xu","Kaichen Zhou","Hojin Bae","Yan Shen","Chuyan Xiong","Jiaming Liu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.24305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09080v2","updated":"2025-06-05T15:28:06Z","published":"2025-02-13T08:54:04Z","title":"BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian\n  Primitives for Weakly-Supervised Cross-View Localization","summary":"  This paper addresses the problem of weakly supervised cross-view\nlocalization, where the goal is to estimate the pose of a ground camera\nrelative to a satellite image with noisy ground truth annotations. A common\napproach to bridge the cross-view domain gap for pose estimation is Bird's-Eye\nView (BEV) synthesis. However, existing methods struggle with height ambiguity\ndue to the lack of depth information in ground images and satellite height\nmaps. Previous solutions either assume a flat ground plane or rely on complex\nmodels, such as cross-view transformers. We propose BevSplat, a novel method\nthat resolves height ambiguity by using feature-based Gaussian primitives. Each\npixel in the ground image is represented by a 3D Gaussian with semantic and\nspatial features, which are synthesized into a BEV feature map for relative\npose estimation. Additionally, to address challenges with panoramic query\nimages, we introduce an icosphere-based supervision strategy for the Gaussian\nprimitives. We validate our method on the widely used KITTI and VIGOR datasets,\nwhich include both pinhole and panoramic query images. Experimental results\nshow that BevSplat significantly improves localization accuracy over prior\napproaches.\n","authors":["Qiwei Wang","Shaoxun Wu","Yujiao Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09080v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05146v1","updated":"2025-06-05T15:27:16Z","published":"2025-06-05T15:27:16Z","title":"CIVET: Systematic Evaluation of Understanding in VLMs","summary":"  While Vision-Language Models (VLMs) have achieved competitive performance in\nvarious tasks, their comprehension of the underlying structure and semantics of\na scene remains understudied. To investigate the understanding of VLMs, we\nstudy their capability regarding object properties and relations in a\ncontrolled and interpretable manner. To this scope, we introduce CIVET, a novel\nand extensible framework for systematiC evaluatIon Via controllEd sTimuli.\nCIVET addresses the lack of standardized systematic evaluation for assessing\nVLMs' understanding, enabling researchers to test hypotheses with statistical\nrigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of\nstimuli, free from annotation noise, dataset-specific biases, and uncontrolled\nscene complexity. Our findings reveal that 1) current VLMs can accurately\nrecognize only a limited set of basic object properties; 2) their performance\nheavily depends on the position of the object in the scene; 3) they struggle to\nunderstand basic relations among objects. Furthermore, a comparative evaluation\nwith human annotators reveals that VLMs still fall short of achieving\nhuman-level accuracy.\n","authors":["Massimo Rizzoli","Simone Alghisi","Olha Khomyn","Gabriel Roccabruna","Seyed Mahed Mousavi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2506.05146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11721v2","updated":"2025-06-05T15:25:55Z","published":"2024-08-21T15:51:46Z","title":"Detection-Driven Object Count Optimization for Text-to-Image Diffusion\n  Models","summary":"  Accurately controlling object count in text-to-image generation remains a key\nchallenge. Supervised methods often fail, as training data rarely covers all\ncount variations. Methods that manipulate the denoising process to add or\nremove objects can help; however, they still require labeled data, limit\nrobustness and image quality, and rely on a slow, iterative process.\n  Pre-trained differentiable counting models that rely on soft object density\nsummation exist and could steer generation, but employing them presents three\nmain challenges: (i) they are pre-trained on clean images, making them less\neffective during denoising steps that operate on noisy inputs; (ii) they are\nnot robust to viewpoint changes; and (iii) optimization is computationally\nexpensive, requiring repeated model evaluations per image.\n  We propose a new framework that uses pre-trained object counting techniques\nand object detectors to guide generation. First, we optimize a counting token\nusing an outer-loop loss computed on fully generated images. Second, we\nintroduce a detection-driven scaling term that corrects errors caused by\nviewpoint and proportion shifts, among other factors, without requiring\nbackpropagation through the detection model. Third, we show that the optimized\nparameters can be reused for new prompts, removing the need for repeated\noptimization. Our method provides efficiency through token reuse, flexibility\nvia compatibility with various detectors, and accuracy with improved counting\nacross diverse object categories.\n","authors":["Oz Zafar","Yuval Cohen","Lior Wolf","Idan Schwartz"],"pdf_url":"https://arxiv.org/pdf/2408.11721v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2502.17327v2","updated":"2025-06-05T15:23:33Z","published":"2025-02-24T17:00:36Z","title":"AnyTop: Character Animation Diffusion with Any Topology","summary":"  Generating motion for arbitrary skeletons is a longstanding challenge in\ncomputer graphics, remaining largely unexplored due to the scarcity of diverse\ndatasets and the irregular nature of the data. In this work, we introduce\nAnyTop, a diffusion model that generates motions for diverse characters with\ndistinct motion dynamics, using only their skeletal structure as input. Our\nwork features a transformer-based denoising network, tailored for arbitrary\nskeleton learning, integrating topology information into the traditional\nattention mechanism. Additionally, by incorporating textual joint descriptions\ninto the latent feature representation, AnyTop learns semantic correspondences\nbetween joints across diverse skeletons. Our evaluation demonstrates that\nAnyTop generalizes well, even with as few as three training examples per\ntopology, and can produce motions for unseen skeletons as well. Furthermore,\nour model's latent space is highly informative, enabling downstream tasks such\nas joint correspondence, temporal segmentation and motion editing. Our webpage,\nhttps://anytop2025.github.io/Anytop-page, includes links to videos and code.\n","authors":["Inbar Gat","Sigal Raab","Guy Tevet","Yuval Reshef","Amit H. Bermano","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2502.17327v2.pdf","comment":"SIGGRAPH 2025. Video: https://www.youtube.com/watch?v=NWOdkM5hAbE,\n  Project page: https://anytop2025.github.io/Anytop-page, Code:\n  https://github.com/Anytop2025/Anytop"},{"id":"http://arxiv.org/abs/2405.10723v3","updated":"2025-06-05T15:22:16Z","published":"2024-05-17T12:11:58Z","title":"Eddeep: Fast eddy-current distortion correction for diffusion MRI with\n  deep learning","summary":"  Modern diffusion MRI sequences commonly acquire a large number of volumes\nwith diffusion sensitization gradients of differing strengths or directions.\nSuch sequences rely on echo-planar imaging (EPI) to achieve reasonable scan\nduration. However, EPI is vulnerable to off-resonance effects, leading to\ntissue susceptibility and eddy-current induced distortions. The latter is\nparticularly problematic because it causes misalignment between volumes,\ndisrupting downstream modelling and analysis. The essential correction of eddy\ndistortions is typically done post-acquisition, with image registration.\nHowever, this is non-trivial because correspondence between volumes can be\nseverely disrupted due to volume-specific signal attenuations induced by\nvarying directions and strengths of the applied gradients. This challenge has\nbeen successfully addressed by the popular FSL~Eddy tool but at considerable\ncomputational cost. We propose an alternative approach, leveraging recent\nadvances in image processing enabled by deep learning (DL). It consists of two\nconvolutional neural networks: 1) An image translator to restore correspondence\nbetween images; 2) A registration model to align the translated images. Results\ndemonstrate comparable distortion estimates to FSL~Eddy, while requiring only\nmodest training sample sizes. This work, to the best of our knowledge, is the\nfirst to tackle this problem with deep learning. Together with recently\ndeveloped DL-based susceptibility correction techniques, they pave the way for\nreal-time preprocessing of diffusion MRI, facilitating its wider uptake in the\nclinic.\n","authors":["Antoine Legouhy","Ross Callaghan","Whitney Stee","Philippe Peigneux","Hojjat Azadbakht","Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.10723v3.pdf","comment":"Accepted in MICCAI 2024 conference (without rebuttal). Github repo:\n  https://github.com/CIG-UCL/eddeep"},{"id":"http://arxiv.org/abs/2506.05127v1","updated":"2025-06-05T15:14:32Z","published":"2025-06-05T15:14:32Z","title":"PixCell: A generative foundation model for digital histopathology images","summary":"  The digitization of histology slides has revolutionized pathology, providing\nmassive datasets for cancer diagnosis and research. Contrastive self-supervised\nand vision-language models have been shown to effectively mine large pathology\ndatasets to learn discriminative representations. On the other hand, generative\nmodels, capable of synthesizing realistic and diverse images, present a\ncompelling solution to address unique problems in pathology that involve\nsynthesizing images; overcoming annotated data scarcity, enabling\nprivacy-preserving data sharing, and performing inherently generative tasks,\nsuch as virtual staining. We introduce PixCell, the first diffusion-based\ngenerative foundation model for histopathology. We train PixCell on PanCan-30M,\na vast, diverse dataset derived from 69,184 H\\&E-stained whole slide images\ncovering various cancer types. We employ a progressive training strategy and a\nself-supervision-based conditioning that allows us to scale up training without\nany annotated data. PixCell generates diverse and high-quality images across\nmultiple cancer types, which we find can be used in place of real data to train\na self-supervised discriminative model. Synthetic images shared between\ninstitutions are subject to fewer regulatory barriers than would be the case\nwith real clinical images. Furthermore, we showcase the ability to precisely\ncontrol image generation using a small set of annotated images, which can be\nused for both data augmentation and educational purposes. Testing on a cell\nsegmentation task, a mask-guided PixCell enables targeted data augmentation,\nimproving downstream performance. Finally, we demonstrate PixCell's ability to\nuse H\\&E structural staining to infer results from molecular marker studies; we\nuse this capability to infer IHC staining from H\\&E images. Our trained models\nare publicly released to accelerate research in computational pathology.\n","authors":["Srikar Yellapragada","Alexandros Graikos","Zilinghan Li","Kostas Triaridis","Varun Belagali","Saarthak Kapse","Tarak Nath Nandi","Ravi K Madduri","Prateek Prasanna","Tahsin Kurc","Rajarsi R. Gupta","Joel Saltz","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2506.05127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05119v1","updated":"2025-06-05T15:06:16Z","published":"2025-06-05T15:06:16Z","title":"Practical Manipulation Model for Robust Deepfake Detection","summary":"  Modern deepfake detection models have achieved strong performance even on the\nchallenging cross-dataset task. However, detection performance under non-ideal\nconditions remains very unstable, limiting success on some benchmark datasets\nand making it easy to circumvent detection. Inspired by the move to a more\nreal-world degradation model in the area of image super-resolution, we have\ndeveloped a Practical Manipulation Model (PMM) that covers a larger set of\npossible forgeries. We extend the space of pseudo-fakes by using Poisson\nblending, more diverse masks, generator artifacts, and distractors.\nAdditionally, we improve the detectors' generality and robustness by adding\nstrong degradations to the training images. We demonstrate that these changes\nnot only significantly enhance the model's robustness to common image\ndegradations but also improve performance on standard benchmark datasets.\nSpecifically, we show clear increases of $3.51\\%$ and $6.21\\%$ AUC on the DFDC\nand DFDCP datasets, respectively, over the s-o-t-a LAA backbone. Furthermore,\nwe highlight the lack of robustness in previous detectors and our improvements\nin this regard. Code can be found at https://github.com/BenediktHopf/PMM\n","authors":["Benedikt Hopf","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2506.05119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05108v1","updated":"2025-06-05T14:53:34Z","published":"2025-06-05T14:53:34Z","title":"DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity\n  and Generalization in Text-to-Image Generative Models","summary":"  Recent advances in text-to-image (T2I) models have achieved impressive\nquality and consistency. However, this has come at the cost of representation\ndiversity. While automatic evaluation methods exist for benchmarking model\ndiversity, they either require reference image datasets or lack specificity\nabout the kind of diversity measured, limiting their adaptability and\ninterpretability. To address this gap, we introduce the Does-it/Can-it\nframework, DIM-CIM, a reference-free measurement of default-mode diversity\n(\"Does\" the model generate images with expected attributes?) and generalization\ncapacity (\"Can\" the model generate diverse attributes for a particular\nconcept?). We construct the COCO-DIMCIM benchmark, which is seeded with COCO\nconcepts and captions and augmented by a large language model. With\nCOCO-DIMCIM, we find that widely-used models improve in generalization at the\ncost of default-mode diversity when scaling from 1.5B to 8.1B parameters.\nDIMCIM also identifies fine-grained failure cases, such as attributes that are\ngenerated with generic prompts but are rarely generated when explicitly\nrequested. Finally, we use DIMCIM to evaluate the training data of a T2I model\nand observe a correlation of 0.85 between diversity in training images and\ndefault-mode diversity. Our work provides a flexible and interpretable\nframework for assessing T2I model diversity and generalization, enabling a more\ncomprehensive understanding of model performance.\n","authors":["Revant Teotia","Candace Ross","Karen Ullrich","Sumit Chopra","Adriana Romero-Soriano","Melissa Hall","Matthew J. Muckley"],"pdf_url":"https://arxiv.org/pdf/2506.05108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20277v2","updated":"2025-06-05T14:53:28Z","published":"2025-05-26T17:55:06Z","title":"OmniCharacter: Towards Immersive Role-Playing Agents with Seamless\n  Speech-Language Personality Interaction","summary":"  Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.\n","authors":["Haonan Zhang","Run Luo","Xiong Liu","Yuchuan Wu","Ting-En Lin","Pengpeng Zeng","Qiang Qu","Feiteng Fang","Min Yang","Lianli Gao","Jingkuan Song","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2505.20277v2.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.05096v1","updated":"2025-06-05T14:41:38Z","published":"2025-06-05T14:41:38Z","title":"Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video\n  Diffusion Transformers","summary":"  Video diffusion transformers (vDiTs) have made impressive progress in\ntext-to-video generation, but their high computational demands present major\nchallenges for practical deployment. While existing acceleration methods reduce\nworkload at various granularities, they often rely on heuristics, limiting\ntheir applicability.\n  We introduce ASTRAEA, an automatic framework that searches for near-optimal\nconfigurations for vDiT-based video generation. At its core, ASTRAEA proposes a\nlightweight token selection mechanism and a memory-efficient, GPU-parallel\nsparse attention strategy, enabling linear reductions in execution time with\nminimal impact on generation quality. To determine optimal token reduction for\ndifferent timesteps, we further design a search framework that leverages a\nclassic evolutionary algorithm to automatically determine the distribution of\nthe token budget effectively. Together, ASTRAEA achieves up to 2.4x inference\nspeedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)\nwhile retaining better video quality compared to the state-of-the-art methods\n(<0.5% loss on the VBench score compared to the baseline vDiT models).\n","authors":["Haosong Liu","Yuge Cheng","Zihan Liu","Aiyue Chen","Yiwu Yao","Chen Chen","Jingwen Leng","Yu Feng","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2506.05096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05095v1","updated":"2025-06-05T14:40:49Z","published":"2025-06-05T14:40:49Z","title":"FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial\n  Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety\n  (TrustFAA)","summary":"  With the increasing prevalence and deployment of Emotion AI-powered facial\naffect analysis (FAA) tools, concerns about the trustworthiness of these\nsystems have become more prominent. This first workshop on \"Towards Trustworthy\nFacial Affect Analysis: Advancing Insights of Fairness, Explainability, and\nSafety (TrustFAA)\" aims to bring together researchers who are investigating\ndifferent challenges in relation to trustworthiness-such as interpretability,\nuncertainty, biases, and privacy-across various facial affect analysis tasks,\nincluding macro/ micro-expression recognition, facial action unit detection,\nother corresponding applications such as pain and depression detection, as well\nas human-robot interaction and collaboration. In alignment with FG2025's\nemphasis on ethics, as demonstrated by the inclusion of an Ethical Impact\nStatement requirement for this year's submissions, this workshop supports\nFG2025's efforts by encouraging research, discussion and dialogue on\ntrustworthy FAA.\n","authors":["Jiaee Cheong","Yang Liu","Harold Soh","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2506.05095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05092v1","updated":"2025-06-05T14:37:40Z","published":"2025-06-05T14:37:40Z","title":"Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D\n  Gaussian Splatting for Vision Training","summary":"  Annotated datasets are critical for training neural networks for object\ndetection, yet their manual creation is time- and labour-intensive, subjective\nto human error, and often limited in diversity. This challenge is particularly\npronounced in the domain of robotics, where diverse and dynamic scenarios\nfurther complicate the creation of representative datasets. To address this, we\npropose a novel method for automatically generating annotated synthetic data in\nUnreal Engine. Our approach leverages photorealistic 3D Gaussian splats for\nrapid synthetic data generation. We demonstrate that synthetic datasets can\nachieve performance comparable to that of real-world datasets while\nsignificantly reducing the time required to generate and annotate data.\nAdditionally, combining real-world and synthetic data significantly increases\nobject detection performance by leveraging the quality of real-world images\nwith the easier scalability of synthetic data. To our knowledge, this is the\nfirst application of synthetic data for training object detection algorithms in\nthe highly dynamic and varied environment of robot soccer. Validation\nexperiments reveal that a detector trained on synthetic images performs on par\nwith one trained on manually annotated real-world images when tested on robot\nsoccer match scenarios. Our method offers a scalable and comprehensive\nalternative to traditional dataset creation, eliminating the labour-intensive\nerror-prone manual annotation process. By generating datasets in a simulator\nwhere all elements are intrinsically known, we ensure accurate annotations\nwhile significantly reducing manual effort, which makes it particularly\nvaluable for robotics applications requiring diverse and scalable training\ndata.\n","authors":["Aneesh Deogan","Wout Beks","Peter Teurlings","Koen de Vos","Mark van den Brand","Rene van de Molengraft"],"pdf_url":"https://arxiv.org/pdf/2506.05092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05087v1","updated":"2025-06-05T14:34:04Z","published":"2025-06-05T14:34:04Z","title":"Interpretable Multimodal Framework for Human-Centered Street Assessment:\n  Integrating Visual-Language Models for Perceptual Urban Diagnostics","summary":"  While objective street metrics derived from imagery or GIS have become\nstandard in urban analytics, they remain insufficient to capture subjective\nperceptions essential to inclusive urban design. This study introduces a novel\nMultimodal Street Evaluation Framework (MSEF) that fuses a vision transformer\n(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable\ndual-output assessment of streetscapes. Leveraging over 15,000 annotated\nstreet-view images from Harbin, China, we fine-tune the framework using LoRA\nand P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1\nscore of 0.84 on objective features and 89.3 percent agreement with aggregated\nresident perceptions, validated across stratified socioeconomic geographies.\nBeyond classification accuracy, MSEF captures context-dependent contradictions:\nfor instance, informal commerce boosts perceived vibrancy while simultaneously\nreducing pedestrian comfort. It also identifies nonlinear and semantically\ncontingent patterns -- such as the divergent perceptual effects of\narchitectural transparency across residential and commercial zones -- revealing\nthe limits of universal spatial heuristics. By generating natural-language\nrationales grounded in attention mechanisms, the framework bridges sensory data\nwith socio-affective inference, enabling transparent diagnostics aligned with\nSDG 11. This work offers both methodological innovation in urban perception\nmodeling and practical utility for planning systems seeking to reconcile\ninfrastructural precision with lived experience.\n","authors":["HaoTian Lan"],"pdf_url":"https://arxiv.org/pdf/2506.05087v1.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.05083v1","updated":"2025-06-05T14:30:39Z","published":"2025-06-05T14:30:39Z","title":"SeedEdit 3.0: Fast and High-Quality Generative Image Editing","summary":"  We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0 [22],\nwhich significantly improves over our previous version [27] in both aspects of\nedit instruction following and image content (e.g., ID/IP) preservation on real\nimage inputs. Additional to model upgrading with T2I, in this report, we\npresent several key improvements. First, we develop an enhanced data curation\npipeline with a meta-info paradigm and meta-info embedding strategy that help\nmix images from multiple data sources. This allows us to scale editing data\neffectively, and meta information is helpfult to connect VLM with diffusion\nmodel more closely. Second, we introduce a joint learning pipeline for\ncomputing a diffusion loss and a reward loss. Finally, we evaluate SeedEdit 3.0\non our testing benchmarks, for real image editing, where it achieves a best\ntrade-off between multiple aspects, yielding a high usability rate of 56.1%,\ncompared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).\n","authors":["Peng Wang","Yichun Shi","Xiaochen Lian","Zhonghua Zhai","Xin Xia","Xuefeng Xiao","Weilin Huang","Jianchao Yang"],"pdf_url":"https://arxiv.org/pdf/2506.05083v1.pdf","comment":"Our website: https://seed.bytedance.com/tech/seededit"},{"id":"http://arxiv.org/abs/2506.05080v1","updated":"2025-06-05T14:28:48Z","published":"2025-06-05T14:28:48Z","title":"Parking, Perception, and Retail: Street-Level Determinants of Community\n  Vitality in Harbin","summary":"  The commercial vitality of community-scale streets in Chinese cities is\nshaped by complex interactions between vehicular accessibility, environmental\nquality, and pedestrian perception. This study proposes an interpretable,\nimage-based framework to examine how street-level features -- including parked\nvehicle density, greenery, cleanliness, and street width -- impact retail\nperformance and user satisfaction in Harbin, China. Leveraging street view\nimagery and a multimodal large language model (VisualGLM-6B), we construct a\nCommunity Commercial Vitality Index (CCVI) from Meituan and Dianping data and\nanalyze its relationship with spatial attributes extracted via GPT-4-based\nperception modeling. Our findings reveal that while moderate vehicle presence\nmay enhance commercial access, excessive on-street parking -- especially in\nnarrow streets -- erodes walkability and reduces both satisfaction and\nshop-level pricing. In contrast, streets with higher perceived greenery and\ncleanliness show significantly greater satisfaction scores but only weak\nassociations with pricing. Street width moderates the effects of vehicle\npresence, underscoring the importance of spatial configuration. These results\ndemonstrate the value of integrating AI-assisted perception with urban\nmorphological analysis to capture non-linear and context-sensitive drivers of\ncommercial success. This study advances both theoretical and methodological\nfrontiers by highlighting the conditional role of vehicle activity in\nneighborhood commerce and demonstrating the feasibility of multimodal AI for\nperceptual urban diagnostics. The implications extend to urban design, parking\nmanagement, and scalable planning tools for community revitalization.\n","authors":["HaoTian Lan"],"pdf_url":"https://arxiv.org/pdf/2506.05080v1.pdf","comment":"22 pages,5 figures"},{"id":"http://arxiv.org/abs/2402.17213v2","updated":"2025-06-05T14:04:01Z","published":"2024-02-27T05:10:44Z","title":"VCD: A Dataset for Visual Commonsense Discovery in Images","summary":"  Visual commonsense plays a vital role in understanding and reasoning about\nthe visual world. While commonsense knowledge bases like ConceptNet provide\nstructured collections of general facts, they lack visually grounded\nrepresentations. Scene graph datasets like Visual Genome, though rich in\nobject-level descriptions, primarily focus on directly observable information\nand lack systematic categorization of commonsense knowledge. We present Visual\nCommonsense Dataset (VCD), a large-scale dataset containing over 100,000 images\nand 14 million object-commonsense pairs that bridges this gap. VCD introduces a\nnovel three-level taxonomy for visual commonsense, integrating both Seen\n(directly observable) and Unseen (inferrable) commonsense across Property,\nAction, and Space aspects. Each commonsense is represented as a triple where\nthe head entity is grounded to object bounding boxes in images, enabling\nscene-dependent and object-specific visual commonsense representation. To\ndemonstrate VCD's utility, we develop VCM, a generative model that combines a\nvision-language model with instruction tuning to discover diverse visual\ncommonsense from images. Extensive evaluations demonstrate both the high\nquality of VCD and its value as a resource for advancing visually grounded\ncommonsense understanding and reasoning. Our dataset and code will be released\non https://github.com/NUSTM/VCD.\n","authors":["Xiangqing Shen","Fanfan Wang","Siwei Wu","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2402.17213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05061v1","updated":"2025-06-05T14:03:18Z","published":"2025-06-05T14:03:18Z","title":"A Survey on Vietnamese Document Analysis and Recognition: Challenges and\n  Future Directions","summary":"  Vietnamese document analysis and recognition (DAR) is a crucial field with\napplications in digitization, information retrieval, and automation. Despite\nadvancements in OCR and NLP, Vietnamese text recognition faces unique\nchallenges due to its complex diacritics, tonal variations, and lack of\nlarge-scale annotated datasets. Traditional OCR methods often struggle with\nreal-world document variations, while deep learning approaches have shown\npromise but remain limited by data scarcity and generalization issues.\nRecently, large language models (LLMs) and vision-language models have\ndemonstrated remarkable improvements in text recognition and document\nunderstanding, offering a new direction for Vietnamese DAR. However, challenges\nsuch as domain adaptation, multimodal learning, and computational efficiency\npersist. This survey provide a comprehensive review of existing techniques in\nVietnamese document recognition, highlights key limitations, and explores how\nLLMs can revolutionize the field. We discuss future research directions,\nincluding dataset development, model optimization, and the integration of\nmultimodal approaches for improved document intelligence. By addressing these\ngaps, we aim to foster advancements in Vietnamese DAR and encourage\ncommunity-driven solutions.\n","authors":["Anh Le","Thanh Lam","Dung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2506.05061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05046v1","updated":"2025-06-05T13:54:40Z","published":"2025-06-05T13:54:40Z","title":"FlowDirector: Training-Free Flow Steering for Precise Text-to-Video\n  Editing","summary":"  Text-driven video editing aims to modify video content according to natural\nlanguage instructions. While recent training-free approaches have made progress\nby leveraging pre-trained diffusion models, they typically rely on\ninversion-based techniques that map input videos into the latent space, which\noften leads to temporal inconsistencies and degraded structural fidelity. To\naddress this, we propose FlowDirector, a novel inversion-free video editing\nframework. Our framework models the editing process as a direct evolution in\ndata space, guiding the video via an Ordinary Differential Equation (ODE) to\nsmoothly transition along its inherent spatiotemporal manifold, thereby\npreserving temporal coherence and structural details. To achieve localized and\ncontrollable edits, we introduce an attention-guided masking mechanism that\nmodulates the ODE velocity field, preserving non-target regions both spatially\nand temporally. Furthermore, to address incomplete edits and enhance semantic\nalignment with editing instructions, we present a guidance-enhanced editing\nstrategy inspired by Classifier-Free Guidance, which leverages differential\nsignals between multiple candidate flows to steer the editing trajectory toward\nstronger semantic alignment without compromising structural consistency.\nExtensive experiments across benchmarks demonstrate that FlowDirector achieves\nstate-of-the-art performance in instruction adherence, temporal consistency,\nand background preservation, establishing a new paradigm for efficient and\ncoherent video editing without inversion.\n","authors":["Guangzhao Li","Yanming Yang","Chenxi Song","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05046v1.pdf","comment":"Project Page is https://flowdirector-edit.github.io"},{"id":"http://arxiv.org/abs/2506.05041v1","updated":"2025-06-05T13:45:21Z","published":"2025-06-05T13:45:21Z","title":"DACN: Dual-Attention Convolutional Network for Hyperspectral Image\n  Super-Resolution","summary":"  2D convolutional neural networks (CNNs) have attracted significant attention\nfor hyperspectral image super-resolution tasks. However, a key limitation is\ntheir reliance on local neighborhoods, which leads to a lack of global\ncontextual understanding. Moreover, band correlation and data scarcity continue\nto limit their performance. To mitigate these issues, we introduce DACN, a\ndual-attention convolutional network for hyperspectral image super-resolution.\nSpecifically, the model first employs augmented convolutions, integrating\nmulti-head attention to effectively capture both local and global feature\ndependencies. Next, we infer separate attention maps for the channel and\nspatial dimensions to determine where to focus across different channels and\nspatial positions. Furthermore, a custom optimized loss function is proposed\nthat combines L2 regularization with spatial-spectral gradient loss to ensure\naccurate spectral fidelity. Experimental results on two hyperspectral datasets\ndemonstrate that the combination of multi-head attention and channel attention\noutperforms either attention mechanism used individually.\n","authors":["Usman Muhammad","Jorma Laaksonen"],"pdf_url":"https://arxiv.org/pdf/2506.05041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05032v1","updated":"2025-06-05T13:40:11Z","published":"2025-06-05T13:40:11Z","title":"Identifying and Understanding Cross-Class Features in Adversarial\n  Training","summary":"  Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.\n","authors":["Zeming Wei","Yiwen Guo","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05032v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.05026v1","updated":"2025-06-05T13:37:24Z","published":"2025-06-05T13:37:24Z","title":"Physical Annotation for Automated Optical Inspection: A Concept for\n  In-Situ, Pointer-Based Trainingdata Generation","summary":"  This paper introduces a novel physical annotation system designed to generate\ntraining data for automated optical inspection. The system uses pointer-based\nin-situ interaction to transfer the valuable expertise of trained inspection\npersonnel directly into a machine learning (ML) training pipeline. Unlike\nconventional screen-based annotation methods, our system captures physical\ntrajectories and contours directly on the object, providing a more intuitive\nand efficient way to label data. The core technology uses calibrated, tracked\npointers to accurately record user input and transform these spatial\ninteractions into standardised annotation formats that are compatible with\nopen-source annotation software. Additionally, a simple projector-based\ninterface projects visual guidance onto the object to assist users during the\nannotation process, ensuring greater accuracy and consistency. The proposed\nconcept bridges the gap between human expertise and automated data generation,\nenabling non-IT experts to contribute to the ML training pipeline and\npreventing the loss of valuable training samples. Preliminary evaluation\nresults confirm the feasibility of capturing detailed annotation trajectories\nand demonstrate that integration with CVAT streamlines the workflow for\nsubsequent ML tasks. This paper details the system architecture, calibration\nprocedures and interface design, and discusses its potential contribution to\nfuture ML data generation for automated optical inspection.\n","authors":["Oliver Krumpek","Oliver Heimann","Jörg Krüger"],"pdf_url":"https://arxiv.org/pdf/2506.05026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05011v1","updated":"2025-06-05T13:21:09Z","published":"2025-06-05T13:21:09Z","title":"UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using\n  Gaussian Splatting","summary":"  Despite significant advancements in dynamic neural rendering, existing\nmethods fail to address the unique challenges posed by UAV-captured scenarios,\nparticularly those involving monocular camera setups, top-down perspective, and\nmultiple small, moving humans, which are not adequately represented in existing\ndatasets. In this work, we introduce UAV4D, a framework for enabling\nphotorealistic rendering for dynamic real-world scenes captured by UAVs.\nSpecifically, we address the challenge of reconstructing dynamic scenes with\nmultiple moving pedestrians from monocular video data without the need for\nadditional sensors. We use a combination of a 3D foundation model and a human\nmesh reconstruction model to reconstruct both the scene background and humans.\nWe propose a novel approach to resolve the scene scale ambiguity and place both\nhumans and the scene in world coordinates by identifying human-scene contact\npoints. Additionally, we exploit the SMPL model and background mesh to\ninitialize Gaussian splats, enabling holistic scene rendering. We evaluated our\nmethod on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and\nOkutama-Action, each with distinct characteristics and 10~50 humans. Our\nresults demonstrate the benefits of our approach over existing methods in novel\nview synthesis, achieving a 1.5 dB PSNR improvement and superior visual\nsharpness.\n","authors":["Jaehoon Choi","Dongki Jung","Christopher Maxey","Yonghan Lee","Sungmin Eum","Dinesh Manocha","Heesung Kwon"],"pdf_url":"https://arxiv.org/pdf/2506.05011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05010v1","updated":"2025-06-05T13:20:50Z","published":"2025-06-05T13:20:50Z","title":"ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development","summary":"  We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.\n","authors":["Zhenran Xu","Xue Yang","Yiyu Wang","Qingli Hu","Zijiao Wu","Longyue Wang","Weihua Luo","Kaifu Zhang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05010v1.pdf","comment":"ACL 2025 Demo. Github: https://github.com/AIDC-AI/ComfyUI-Copilot"},{"id":"http://arxiv.org/abs/2408.15011v2","updated":"2025-06-05T13:19:43Z","published":"2024-08-27T12:48:46Z","title":"Pre-training Everywhere: Parameter-Efficient Fine-Tuning for Medical\n  Image Analysis via Target Parameter Pre-training","summary":"  Parameter-efficient fine-tuning (PEFT) techniques have emerged to address\noverfitting and high computational costs associated with fully fine-tuning in\nself-supervised learning. Mainstream PEFT methods add a few trainable\nparameters while keeping the pre-trained backbone parameters fixed. These\nmethods achieve comparative, and often superior, performance to fully\nfine-tuning, demonstrating the powerful representation ability of the\npre-trained backbone. Despite this success, these methods typically ignore the\ninitialization of the new parameters, often relying solely on random\ninitialization. We argue that if pre-training is significantly beneficial, it\nshould be applied to all parameters requiring representational capacity.\nMotivated by this, we propose Target Parameter Pre-training (TPP), a simple yet\neffective fine-tuning framework. TPP pre-trains target parameters, i.e., the\nnew parameters introduced during fine-tuning, in an additional stage before\nPEFT. During this stage, the pre-trained backbone parameters are frozen, and\nonly the new parameters are trainable. A defined pretext task encourages the\nnew parameters to learn specific representations of downstream data.\nSubsequently, when PEFT is employed, the pre-trained new parameters are loaded\nto enhance fine-tuning efficiency. The proposed TPP framework is versatile,\nallowing integration with various pre-trained backbones, pretext tasks, and\nPEFT methods. We evaluated the fine-tuning performance of our method on seven\npublic datasets, covering four modalities and two task types. The results\ndemonstrate that TPP can be easily integrated into existing PEFT methods,\nsignificantly improving performance.\n","authors":["Xingliang Lei","Yiwen Ye","Zhisong Wang","Ziyang Chen","Minglei Shu","Weidong Cai","Yanning Zhang","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2408.15011v2.pdf","comment":"14 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2506.05009v1","updated":"2025-06-05T13:19:27Z","published":"2025-06-05T13:19:27Z","title":"Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian\n  Splatting","summary":"  Training neural networks for tasks such as 3D point cloud semantic\nsegmentation demands extensive datasets, yet obtaining and annotating\nreal-world point clouds is costly and labor-intensive. This work aims to\nintroduce a novel pipeline for generating realistic synthetic data, by\nleveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to\ngenerate 3D assets of multiple different agricultural vehicles instead of using\ngeneric models. These assets are placed in a simulated environment, where the\npoint clouds are generated using a simulated LiDAR. This is a flexible approach\nthat allows changing the LiDAR specifications without incurring additional\ncosts. We evaluated the impact of synthetic data on segmentation models such as\nPointNet++, Point Transformer V3, and OACNN, by training and validating the\nmodels only on synthetic data. Remarkably, the PTv3 model had an mIoU of\n91.35\\%, a noteworthy result given that the model had neither been trained nor\nvalidated on any real data. Further studies even suggested that in certain\nscenarios the models trained only on synthetically generated data performed\nbetter than models trained on real-world data. Finally, experiments\ndemonstrated that the models can generalize across semantic classes, enabling\naccurate predictions on mesh models they were never trained on.\n","authors":["Alfred T. Christiansen","Andreas H. Højrup","Morten K. Stephansen","Md Ibtihaj A. Sakib","Taman S. Poojary","Filip Slezak","Morten S. Laursen","Thomas B. Moeslund","Joakim B. Haurum"],"pdf_url":"https://arxiv.org/pdf/2506.05009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05008v1","updated":"2025-06-05T13:18:48Z","published":"2025-06-05T13:18:48Z","title":"Structure-Aware Radar-Camera Depth Estimation","summary":"  Monocular depth estimation aims to determine the depth of each pixel from an\nRGB image captured by a monocular camera. The development of deep learning has\nsignificantly advanced this field by facilitating the learning of depth\nfeatures from some well-annotated datasets\n\\cite{Geiger_Lenz_Stiller_Urtasun_2013,silberman2012indoor}. Eigen \\textit{et\nal.} \\cite{eigen2014depth} first introduce a multi-scale fusion network for\ndepth regression. Following this, subsequent improvements have come from\nreinterpreting the regression task as a classification problem\n\\cite{bhat2021adabins,Li_Wang_Liu_Jiang_2022}, incorporating additional priors\n\\cite{shao2023nddepth,yang2023gedepth}, and developing more effective objective\nfunction \\cite{xian2020structure,Yin_Liu_Shen_Yan_2019}. Despite these\nadvances, generalizing to unseen domains remains a challenge. Recently, several\nmethods have employed affine-invariant loss to enable multi-dataset joint\ntraining \\cite{MiDaS,ZeroDepth,guizilini2023towards,Dany}. Among them, Depth\nAnything \\cite{Dany} has shown leading performance in zero-shot monocular depth\nestimation. While it struggles to estimate accurate metric depth due to the\nlack of explicit depth cues, it excels at extracting structural information\nfrom unseen images, producing structure-detailed monocular depth.\n","authors":["Fuyi Zhang","Zhu Yu","Chunhao Li","Runmin Zhang","Xiaokai Bai","Zili Zhou","Si-Yuan Cao","Wang Wang","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2506.05008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04999v1","updated":"2025-06-05T13:10:17Z","published":"2025-06-05T13:10:17Z","title":"Beyond Cropped Regions: New Benchmark and Corresponding Baseline for\n  Chinese Scene Text Retrieval in Diverse Layouts","summary":"  Chinese scene text retrieval is a practical task that aims to search for\nimages containing visual instances of a Chinese query text. This task is\nextremely challenging because Chinese text often features complex and diverse\nlayouts in real-world scenes. Current efforts tend to inherit the solution for\nEnglish scene text retrieval, failing to achieve satisfactory performance. In\nthis paper, we establish a Diversified Layout benchmark for Chinese Street View\nText Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval\nperformance across various text layouts, including vertical, cross-line, and\npartial alignments. To address the limitations in existing methods, we propose\nChinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates\nglobal visual information with multi-granularity alignment training. CSTR-CLIP\napplies a two-stage training process to overcome previous limitations, such as\nthe exclusion of visual features outside the text region and reliance on\nsingle-granularity alignment, thereby enabling the model to effectively handle\ndiverse text layouts. Experiments on existing benchmark show that CSTR-CLIP\noutperforms the previous state-of-the-art model by 18.82% accuracy and also\nprovides faster inference speed. Further analysis on DL-CSVTR confirms the\nsuperior performance of CSTR-CLIP in handling various text layouts. The dataset\nand code will be publicly available to facilitate research in Chinese scene\ntext retrieval.\n","authors":["Gengluo Li","Huawen Shen","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.04999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04788v3","updated":"2025-06-05T13:05:31Z","published":"2025-05-07T20:30:08Z","title":"Convex Relaxation for Robust Vanishing Point Estimation in Manhattan\n  World","summary":"  Determining the vanishing points (VPs) in a Manhattan world, as a fundamental\ntask in many 3D vision applications, consists of jointly inferring the line-VP\nassociation and locating each VP. Existing methods are, however, either\nsub-optimal solvers or pursuing global optimality at a significant cost of\ncomputing time. In contrast to prior works, we introduce convex relaxation\ntechniques to solve this task for the first time. Specifically, we employ a\n\"soft\" association scheme, realized via a truncated multi-selection error, that\nallows for joint estimation of VPs' locations and line-VP associations. This\napproach leads to a primal problem that can be reformulated into a\nquadratically constrained quadratic programming (QCQP) problem, which is then\nrelaxed into a convex semidefinite programming (SDP) problem. To solve this SDP\nproblem efficiently, we present a globally optimal outlier-robust iterative\nsolver (called GlobustVP), which independently searches for one VP and its\nassociated lines in each iteration, treating other lines as outliers. After\neach independent update of all VPs, the mutual orthogonality between the three\nVPs in a Manhattan world is reinforced via local refinement. Extensive\nexperiments on both synthetic and real-world data demonstrate that GlobustVP\nachieves a favorable balance between efficiency, robustness, and global\noptimality compared to previous works. The code is publicly available at\nhttps://github.com/WU-CVGL/GlobustVP.\n","authors":["Bangyan Liao","Zhenjun Zhao","Haoang Li","Yi Zhou","Yingping Zeng","Hao Li","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2505.04788v3.pdf","comment":"Accepted to CVPR 2025 as Award Candidate & Oral Presentation. The\n  first two authors contributed equally to this work. Code:\n  https://github.com/WU-CVGL/GlobustVP"},{"id":"http://arxiv.org/abs/2506.04996v1","updated":"2025-06-05T13:05:23Z","published":"2025-06-05T13:05:23Z","title":"PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment","summary":"  Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications.\n","authors":["Edoardo Bianchi","Antonio Liotta"],"pdf_url":"https://arxiv.org/pdf/2506.04996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04990v1","updated":"2025-06-05T13:02:23Z","published":"2025-06-05T13:02:23Z","title":"Multi-scale Image Super Resolution with a Single Auto-Regressive Model","summary":"  In this paper we tackle Image Super Resolution (ISR), using recent advances\nin Visual Auto-Regressive (VAR) modeling. VAR iteratively estimates the\nresidual in latent space between gradually increasing image scales, a process\nreferred to as next-scale prediction. Thus, the strong priors learned during\npre-training align well with the downstream task (ISR). To our knowledge, only\nVARSR has exploited this synergy so far, showing promising results. However,\ndue to the limitations of existing residual quantizers, VARSR works only at a\nfixed resolution, i.e. it fails to map intermediate outputs to the\ncorresponding image scales. Additionally, it relies on a 1B transformer\narchitecture (VAR-d24), and leverages a large-scale private dataset to achieve\nstate-of-the-art results. We address these limitations through two novel\ncomponents: a) a Hierarchical Image Tokenization approach with a multi-scale\nimage tokenizer that progressively represents images at different scales while\nsimultaneously enforcing token overlap across scales, and b) a Direct\nPreference Optimization (DPO) regularization term that, relying solely on the\nLR and HR tokenizations, encourages the transformer to produce the latter over\nthe former. To the best of our knowledge, this is the first time a quantizer is\ntrained to force semantically consistent residuals at different scales, and the\nfirst time that preference-based optimization is used to train a VAR. Using\nthese two components, our model can denoise the LR image and super-resolve at\nhalf and full target upscale factors in a single forward pass. Additionally, we\nachieve \\textit{state-of-the-art results on ISR}, while using a small model\n(300M params vs ~1B params of VARSR), and without using external training data.\n","authors":["Enrique Sanchez","Isma Hadji","Adrian Bulat","Christos Tzelepis","Brais Martinez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2506.04990v1.pdf","comment":"Enrique Sanchez and Isma Hadji equally contributed to this work.\n  Project site https://github.com/saic-fi/ms_sr_var"},{"id":"http://arxiv.org/abs/2506.04983v1","updated":"2025-06-05T12:54:56Z","published":"2025-06-05T12:54:56Z","title":"TextVidBench: A Benchmark for Long Video Scene Text Understanding","summary":"  Despite recent progress on the short-video Text-Visual Question Answering\n(ViteVQA) task - largely driven by benchmarks such as M4-ViteVQA - existing\ndatasets still suffer from limited video duration and narrow evaluation scopes,\nmaking it difficult to adequately assess the growing capabilities of powerful\nmultimodal large language models (MLLMs). To address these limitations, we\nintroduce TextVidBench, the first benchmark specifically designed for\nlong-video text question answering (>3 minutes). TextVidBench makes three key\ncontributions: 1) Cross-domain long-video coverage: Spanning 9 categories\n(e.g., news, sports, gaming), with an average video length of 2306 seconds,\nenabling more realistic evaluation of long-video understanding. 2) A\nthree-stage evaluation framework: \"Text Needle-in-Haystack -> Temporal\nGrounding -> Text Dynamics Captioning\". 3) High-quality fine-grained\nannotations: Containing over 5,000 question-answer pairs with detailed semantic\nlabeling. Furthermore, we propose an efficient paradigm for improving large\nmodels through: (i) introducing the IT-Rope mechanism and temporal prompt\nengineering to enhance temporal perception, (ii) adopting non-uniform\npositional encoding to better handle long video sequences, and (iii) applying\nlightweight fine-tuning on video-text data. Extensive experiments on multiple\npublic datasets as well as TextVidBench demonstrate that our new benchmark\npresents significant challenges to existing models, while our proposed method\noffers valuable insights into improving long-video scene text understanding\ncapabilities.\n","authors":["Yangyang Zhong","Ji Qi","Yuan Yao","Pengxin Luo","Yunfeng Yan","Donglian Qi","Zhiyuan Liu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.04983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v7","updated":"2025-06-05T12:54:00Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after remova1l. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Benlei Cui","Wenhao Sun","Xue-Mei Dong","Jingqun Tang","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12974v7.pdf","comment":"Accepted by AAAI 2025(Oral)"},{"id":"http://arxiv.org/abs/2410.18881v2","updated":"2025-06-05T12:44:07Z","published":"2024-10-24T16:17:18Z","title":"Diff-Instruct++: Training One-step Text-to-image Generator Model to\n  Align with Human Preferences","summary":"  One-step text-to-image generator models offer advantages such as swift\ninference efficiency, flexible architectures, and state-of-the-art generation\nperformance. In this paper, we study the problem of aligning one-step generator\nmodels with human preferences for the first time. Inspired by the success of\nreinforcement learning using human feedback (RLHF), we formulate the alignment\nproblem as maximizing expected human reward functions while adding an Integral\nKullback-Leibler divergence term to prevent the generator from diverging. By\novercoming technical challenges, we introduce Diff-Instruct++ (DI++), the\nfirst, fast-converging and image data-free human preference alignment method\nfor one-step text-to-image generators. We also introduce novel theoretical\ninsights, showing that using CFG for diffusion distillation is secretly doing\nRLHF with DI++. Such an interesting finding brings understanding and potential\ncontributions to future research involving CFG. In the experiment sections, we\nalign both UNet-based and DiT-based one-step generators using DI++, which use\nthe Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion\nprocesses. The resulting DiT-based one-step text-to-image model achieves a\nstrong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO\nvalidation prompt dataset. It also achieves a leading Human preference Score\n(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable\nDiffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical\ncontributions and empirical evidence indicate that DI++ is a strong\nhuman-preference alignment approach for one-step text-to-image models. The\nhomepage of the paper is https://github.com/pkulwj1994/diff_instruct_pp.\n","authors":["Weijian Luo"],"pdf_url":"https://arxiv.org/pdf/2410.18881v2.pdf","comment":"Revision: The paper was accepted by Transactions of Machine Learning\n  Research (TMLR)"},{"id":"http://arxiv.org/abs/2506.04970v1","updated":"2025-06-05T12:43:11Z","published":"2025-06-05T12:43:11Z","title":"Bringing SAM to new heights: Leveraging elevation data for tree crown\n  segmentation from drone imagery","summary":"  Information on trees at the individual level is crucial for monitoring forest\necosystems and planning forest management. Current monitoring methods involve\nground measurements, requiring extensive cost, time and labor. Advances in\ndrone remote sensing and computer vision offer great potential for mapping\nindividual trees from aerial imagery at broad-scale. Large pre-trained vision\nmodels, such as the Segment Anything Model (SAM), represent a particularly\ncompelling choice given limited labeled data. In this work, we compare methods\nleveraging SAM for the task of automatic tree crown instance segmentation in\nhigh resolution drone imagery in three use cases: 1) boreal plantations, 2)\ntemperate forests and 3) tropical forests. We also study the integration of\nelevation data into models, in the form of Digital Surface Model (DSM)\ninformation, which can readily be obtained at no additional cost from RGB drone\nimagery. We present BalSAM, a model leveraging SAM and DSM information, which\nshows potential over other methods, particularly in the context of plantations.\nWe find that methods using SAM out-of-the-box do not outperform a custom Mask\nR-CNN, even with well-designed prompts. However, efficiently tuning SAM\nend-to-end and integrating DSM information are both promising avenues for tree\ncrown instance segmentation models.\n","authors":["Mélisande Teng","Arthur Ouaknine","Etienne Laliberté","Yoshua Bengio","David Rolnick","Hugo Larochelle"],"pdf_url":"https://arxiv.org/pdf/2506.04970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03988v2","updated":"2025-06-05T12:39:28Z","published":"2025-06-04T14:16:00Z","title":"RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated\n  Image Detectors","summary":"  AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustness. Our findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID.\n","authors":["Hicham Eddoubi","Jonas Ricker","Federico Cocchi","Lorenzo Baraldi","Angelo Sotgiu","Maura Pintor","Marcella Cornia","Lorenzo Baraldi","Asja Fischer","Rita Cucchiara","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2506.03988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11224v2","updated":"2025-06-05T12:36:04Z","published":"2024-12-15T15:40:40Z","title":"GenLit: Reformulating Single-Image Relighting as Video Generation","summary":"  Manipulating the illumination of a 3D scene within a single image represents\na fundamental challenge in computer vision and graphics. This problem has\ntraditionally been addressed using inverse rendering techniques, which involve\nexplicit 3D asset reconstruction and costly ray-tracing simulations. Meanwhile,\nrecent advancements in visual foundation models suggest that a new paradigm\ncould soon be possible -- one that replaces explicit physical models with\nnetworks that are trained on large amounts of image and video data. In this\npaper, we exploit the physical world understanding of a video diffusion model,\nparticularly Stable Video Diffusion, to relight a single image. We introduce\nGenLit, a framework that distills the ability of a graphics engine to perform\nlight manipulation into a video-generation model, enabling users to directly\ninsert and manipulate a point light in the 3D world within a given image, and\ngenerate results directly as a video sequence. We find that a model fine-tuned\non only a small synthetic dataset generalizes to real-world scenes, enabling\nsingle-image relighting with plausible and convincing shadows. Our results\nhighlight the ability of video foundation models to capture rich information\nabout lighting, material, and, shape and our findings indicate that such\nmodels, with minimal training, can be used to perform relighting without\nexplicit asset reconstruction or complex ray tracing.\n","authors":["Shrisha Bharadwaj","Haiwen Feng","Giorgio Becherini","Victoria Abrevaya","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2412.11224v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14068v3","updated":"2025-06-05T12:31:11Z","published":"2025-05-20T08:16:37Z","title":"Place Recognition Meet Multiple Modalitie: A Comprehensive Review,\n  Current Challenges and Future Directions","summary":"  Place recognition is a cornerstone of vehicle navigation and mapping, which\nis pivotal in enabling systems to determine whether a location has been\npreviously visited. This capability is critical for tasks such as loop closure\nin Simultaneous Localization and Mapping (SLAM) and long-term navigation under\nvarying environmental conditions. In this survey, we comprehensively review\nrecent advancements in place recognition, emphasizing three representative\nmethodological paradigms: Convolutional Neural Network (CNN)-based approaches,\nTransformer-based frameworks, and cross-modal strategies. We begin by\nelucidating the significance of place recognition within the broader context of\nautonomous systems. Subsequently, we trace the evolution of CNN-based methods,\nhighlighting their contributions to robust visual descriptor learning and\nscalability in large-scale environments. We then examine the emerging class of\nTransformer-based models, which leverage self-attention mechanisms to capture\nglobal dependencies and offer improved generalization across diverse scenes.\nFurthermore, we discuss cross-modal approaches that integrate heterogeneous\ndata sources such as Lidar, vision, and text description, thereby enhancing\nresilience to viewpoint, illumination, and seasonal variations. We also\nsummarize standard datasets and evaluation metrics widely adopted in the\nliterature. Finally, we identify current research challenges and outline\nprospective directions, including domain adaptation, real-time performance, and\nlifelong learning, to inspire future advancements in this domain. The unified\nframework of leading-edge place recognition methods, i.e., code library, and\nthe results of their experimental evaluations are available at\nhttps://github.com/CV4RA/SOTA-Place-Recognitioner.\n","authors":["Zhenyu Li","Tianyi Shang","Pengjie Xu","Zhaojun Deng"],"pdf_url":"https://arxiv.org/pdf/2505.14068v3.pdf","comment":"67 pages"},{"id":"http://arxiv.org/abs/2506.04956v1","updated":"2025-06-05T12:31:02Z","published":"2025-06-05T12:31:02Z","title":"FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video\n  Generation","summary":"  Synthesizing high-quality dynamic medical videos remains a significant\nchallenge due to the need for modeling both spatial consistency and temporal\ndynamics. Existing Transformer-based approaches face critical limitations,\nincluding insufficient channel interactions, high computational complexity from\nself-attention, and coarse denoising guidance from timestep embeddings when\nhandling varying noise levels. In this work, we propose FEAT, a\nfull-dimensional efficient attention Transformer, which addresses these issues\nthrough three key innovations: (1) a unified paradigm with sequential\nspatial-temporal-channel attention mechanisms to capture global dependencies\nacross all dimensions, (2) a linear-complexity design for attention mechanisms\nin each dimension, utilizing weighted key-value attention and global channel\nattention, and (3) a residual value guidance module that provides fine-grained\npixel-level guidance to adapt to different noise levels. We evaluate FEAT on\nstandard benchmarks and downstream tasks, demonstrating that FEAT-S, with only\n23\\% of the parameters of the state-of-the-art model Endora, achieves\ncomparable or even superior performance. Furthermore, FEAT-L surpasses all\ncomparison methods across multiple datasets, showcasing both superior\neffectiveness and scalability. Code is available at\nhttps://github.com/Yaziwel/FEAT.\n","authors":["Huihan Wang","Zhiwen Yang","Hui Zhang","Dan Zhao","Bingzheng Wei","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2506.04956v1.pdf","comment":"This paper has been early accepted by MICCAI 2025"},{"id":"http://arxiv.org/abs/2506.04953v1","updated":"2025-06-05T12:27:10Z","published":"2025-06-05T12:27:10Z","title":"APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual\n  Information Retrieval","summary":"  Current video-based multimodal large language models struggle with hour-level\nvideo understanding due to computational constraints and inefficient\ninformation extraction from extensive temporal sequences. We propose APVR\n(Adaptive Pivot Visual information Retrieval), a training-free framework that\naddresses the memory wall limitation through hierarchical visual information\nretrieval. APVR operates via two complementary components: Pivot Frame\nRetrieval employs semantic expansion and multi-modal confidence scoring to\nidentify semantically relevant video frames, while Pivot Token Retrieval\nperforms query-aware attention-driven token selection within the pivot frames.\nThis dual granularity approach enables processing of hour-long videos while\nmaintaining semantic fidelity. Experimental validation on LongVideoBench and\nVideoMME demonstrates significant performance improvements, establishing\nstate-of-the-art results for not only training-free but also training-based\napproaches while providing plug-and-play integration capability with existing\nMLLM architectures.\n","authors":["Hong Gao","Yiming Bao","Xuezhan Tu","Bin Zhong","Minling Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04951v1","updated":"2025-06-05T12:24:38Z","published":"2025-06-05T12:24:38Z","title":"Robustness as Architecture: Designing IQA Models to Withstand\n  Adversarial Perturbations","summary":"  Image Quality Assessment (IQA) models are increasingly relied upon to\nevaluate image quality in real-world systems -- from compression and\nenhancement to generation and streaming. Yet their adoption brings a\nfundamental risk: these models are inherently unstable. Adversarial\nmanipulations can easily fool them, inflating scores and undermining trust.\nTraditionally, such vulnerabilities are addressed through data-driven defenses\n-- adversarial retraining, regularization, or input purification. But what if\nthis is the wrong lens? What if robustness in perceptual models is not\nsomething to learn but something to design? In this work, we propose a\nprovocative idea: robustness as an architectural prior. Rather than training\nmodels to resist perturbations, we reshape their internal structure to suppress\nsensitivity from the ground up. We achieve this by enforcing orthogonal\ninformation flow, constraining the network to norm-preserving operations -- and\nfurther stabilizing the system through pruning and fine-tuning. The result is a\nrobust IQA architecture that withstands adversarial attacks without requiring\nadversarial training or significant changes to the original model. This\napproach suggests a shift in perspective: from optimizing robustness through\ndata to engineering it through design.\n","authors":["Igor Meleshin","Anna Chistyakova","Anastasia Antsiferova","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2506.04951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04950v1","updated":"2025-06-05T12:22:03Z","published":"2025-06-05T12:22:03Z","title":"Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal\n  Pattern Mining","summary":"  Artificial intelligence has recently shown promise in automated embryo\nselection for In-Vitro Fertilization (IVF). However, current approaches either\naddress partial embryo evaluation lacking holistic quality assessment or target\nclinical outcomes inevitably confounded by extra-embryonic factors, both\nlimiting clinical utility. To bridge this gap, we propose a new task called\nVideo-Based Embryo Grading - the first paradigm that directly utilizes\nfull-length time-lapse monitoring (TLM) videos to predict embryologists'\noverall quality assessments. To support this task, we curate a real-world\nclinical dataset comprising over 2,500 TLM videos, each annotated with a\ngrading label indicating the overall quality of embryos. Grounded in clinical\ndecision-making principles, we propose a Complementary Spatial-Temporal Pattern\nMining (CoSTeM) framework that conceptually replicates embryologists'\nevaluation process. The CoSTeM comprises two branches: (1) a morphological\nbranch using a Mixture of Cross-Attentive Experts layer and a Temporal\nSelection Block to select discriminative local structural features, and (2) a\nmorphokinetic branch employing a Temporal Transformer to model global\ndevelopmental trajectories, synergistically integrating static and dynamic\ndeterminants for grading embryos. Extensive experimental results demonstrate\nthe superiority of our design. This work provides a valuable methodological\nframework for AI-assisted embryo selection. The dataset and source code will be\npublicly available upon acceptance.\n","authors":["Yong Sun","Yipeng Wang","Junyu Shi","Zhiyuan Zhang","Yanmei Xiao","Lei Zhu","Manxi Jiang","Qiang Nie"],"pdf_url":"https://arxiv.org/pdf/2506.04950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07323v2","updated":"2025-06-05T12:17:03Z","published":"2025-03-10T13:39:09Z","title":"Navigating Motion Agents in Dynamic and Cluttered Environments through\n  LLM Reasoning","summary":"  This paper advances motion agents empowered by large language models (LLMs)\ntoward autonomous navigation in dynamic and cluttered environments,\nsignificantly surpassing first and recent seminal but limited studies on LLM's\nspatial reasoning, where movements are restricted in four directions in simple,\nstatic environments in the presence of only single agents much less multiple\nagents. Specifically, we investigate LLMs as spatial reasoners to overcome\nthese limitations by uniformly encoding environments (e.g., real indoor\nfloorplans), agents which can be dynamic obstacles and their paths as discrete\ntokens akin to language tokens. Our training-free framework supports\nmulti-agent coordination, closed-loop replanning, and dynamic obstacle\navoidance without retraining or fine-tuning. We show that LLMs can generalize\nacross agents, tasks, and environments using only text-based interactions,\nopening new possibilities for semantically grounded, interactive navigation in\nboth simulation and embodied systems.\n","authors":["Yubo Zhao","Qi Wu","Yifan Wang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2503.07323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20898v3","updated":"2025-06-05T12:11:49Z","published":"2024-10-28T10:26:19Z","title":"David and Goliath: Small One-step Model Beats Large Diffusion with Score\n  Post-training","summary":"  We propose Diff-Instruct* (DI*), a data-efficient post-training approach for\none-step text-to-image generative models to improve its human preferences\nwithout requiring image data. Our method frames alignment as online\nreinforcement learning from human feedback (RLHF), which optimizes the one-step\nmodel to maximize human reward functions while being regularized to be kept\nclose to a reference diffusion process. Unlike traditional RLHF approaches,\nwhich rely on the Kullback-Leibler divergence as the regularization, we\nintroduce a novel general score-based divergence regularization that\nsubstantially improves performance as well as post-training stability. Although\nthe general score-based RLHF objective is intractable to optimize, we derive a\nstrictly equivalent tractable loss function in theory that can efficiently\ncompute its \\emph{gradient} for optimizations. We introduce\n\\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a\nresolution of $1024\\times 1024$, post-trained from DMD2 w.r.t SDXL. \\textbf{Our\n2.6B \\emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in\nImageReward, PickScore, and CLIP score on the Parti prompts benchmark while\nusing only 1.88\\% of the inference time. This result clearly shows that with\nproper post-training, the small one-step model is capable of beating huge\nmulti-step diffusion models. Our model is open-sourced at this link:\nhttps://github.com/pkulwj1994/diff_instruct_star. We hope our findings can\ncontribute to human-centric machine learning techniques.\n","authors":["Weijian Luo","Colin Zhang","Debing Zhang","Zhengyang Geng"],"pdf_url":"https://arxiv.org/pdf/2410.20898v3.pdf","comment":"Revision: paper accepted by the ICML2025 main conference"},{"id":"http://arxiv.org/abs/2506.04931v1","updated":"2025-06-05T12:05:43Z","published":"2025-06-05T12:05:43Z","title":"CzechLynx: A Dataset for Individual Identification and Pose Estimation\n  of the Eurasian Lynx","summary":"  We introduce CzechLynx, the first large-scale, open-access dataset for\nindividual identification, 2D pose estimation, and instance segmentation of the\nEurasian lynx (Lynx lynx). CzechLynx includes more than 30k camera trap images\nannotated with segmentation masks, identity labels, and 20-point skeletons and\ncovers 219 unique individuals across 15 years of systematic monitoring in two\ngeographically distinct regions: Southwest Bohemia and the Western Carpathians.\nTo increase the data variability, we create a complementary synthetic set with\nmore than 100k photorealistic images generated via a Unity-based pipeline and\ndiffusion-driven text-to-texture modeling, covering diverse environments,\nposes, and coat-pattern variations. To allow testing generalization across\nspatial and temporal domains, we define three tailored evaluation\nprotocols/splits: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware\nclosed-set. This dataset is targeted to be instrumental in benchmarking\nstate-of-the-art models and the development of novel methods for not just\nindividual animal re-identification.\n","authors":["Lukas Picek","Elisa Belotti","Michal Bojda","Ludek Bufka","Vojtech Cermak","Martin Dula","Rostislav Dvorak","Luboslav Hrdy","Miroslav Jirik","Vaclav Kocourek","Josefa Krausova","Jirı Labuda","Jakub Straka","Ludek Toman","Vlado Trulık","Martin Vana","Miroslav Kutal"],"pdf_url":"https://arxiv.org/pdf/2506.04931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04925v1","updated":"2025-06-05T11:59:33Z","published":"2025-06-05T11:59:33Z","title":"Light and 3D: a methodological exploration of digitisation techniques\n  adapted to a selection of objects from the Mus{é}e d'Arch{é}ologie\n  Nationale","summary":"  The need to digitize heritage objects is now widely accepted. This article\npresents the very fashionable context of the creation of ''digital twins''. It\nillustrates the diversity of photographic 3D digitization methods, but this is\nnot its only objective. Using a selection of objects from the collections of\nthe mus{\\'e}e d'Arch{\\'e}ologie nationale, it shows that no single method is\nsuitable for all cases. Rather, the method to be recommended for a given object\nshould be the result of a concerted choice between those involved in heritage\nand those involved in the digital domain, as each new object may require the\nadaptation of existing tools. It would therefore be pointless to attempt an\nabsolute classification of 3D digitization methods. On the contrary, we need to\nfind the digital tool best suited to each object, taking into account not only\nits characteristics, but also the future use of its digital twin.\n","authors":["Antoine Laurent","Jean Mélou","Catherine Schwab","Rolande Simon-Millot","Sophie Féret","Thomas Sagory","Carole Fritz","Jean-Denis Durou"],"pdf_url":"https://arxiv.org/pdf/2506.04925v1.pdf","comment":"in French language"},{"id":"http://arxiv.org/abs/2411.16331v3","updated":"2025-06-05T11:49:59Z","published":"2024-11-25T12:24:52Z","title":"Sonic: Shifting Focus to Global Audio Perception in Portrait Animation","summary":"  The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity.\n","authors":["Xiaozhong Ji","Xiaobin Hu","Zhihong Xu","Junwei Zhu","Chuming Lin","Qingdong He","Jiangning Zhang","Donghao Luo","Yi Chen","Qin Lin","Qinglin Lu","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16331v3.pdf","comment":"refer to our main-page \\url{https://jixiaozhong.github.io/Sonic/}"},{"id":"http://arxiv.org/abs/2506.04908v1","updated":"2025-06-05T11:41:09Z","published":"2025-06-05T11:41:09Z","title":"Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and\n  Expert Knowledge Transfer","summary":"  In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for\nstereo dataset generation, offering an efficient alternative to Neural Radiance\nFields (NeRF)-based methods. To obtain useful geometry estimates, we explore\nutilizing the reconstructed geometry from the explicit 3D representations as\nwell as depth estimates from the FoundationStereo model in an expert knowledge\ntransfer setup. We find that when fine-tuning stereo models on 3DGS-generated\ndatasets, we demonstrate competitive performance in zero-shot generalization\nbenchmarks. When using the reconstructed geometry directly, we observe that it\nis often noisy and contains artifacts, which propagate noise to the trained\nmodel. In contrast, we find that the disparity estimates from FoundationStereo\nare cleaner and consequently result in a better performance on the zero-shot\ngeneralization benchmarks. Our method highlights the potential for low-cost,\nhigh-fidelity dataset creation and fast fine-tuning for deep stereo models.\nMoreover, we also reveal that while the latest Gaussian Splatting based methods\nhave achieved superior performance on established benchmarks, their robustness\nfalls short in challenging in-the-wild settings warranting further exploration.\n","authors":["Filip Slezak","Magnus K. Gjerde","Joakim B. Haurum","Ivan Nikolov","Morten S. Laursen","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2506.04908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04897v1","updated":"2025-06-05T11:28:02Z","published":"2025-06-05T11:28:02Z","title":"From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual\n  Grounding in 3D Scenes","summary":"  3D visual grounding has made notable progress in localizing objects within\ncomplex 3D scenes. However, grounding referring expressions beyond objects in\n3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a\nholistic 3D visual grounding benchmark consisting of 2,632 referring\nexpression-3D bounding box pairs spanning four different grounding levels:\nhuman-activity areas, unoccupied space beyond objects, objects in the scene,\nand fine-grained object parts. We assess a range of state-of-the-art 3D visual\ngrounding methods alongside large language models (LLMs) and multimodal LLMs\n(MLLMs) on Anywhere3D-Bench. Experimental results reveal that space-level and\npart-level visual grounding pose the greatest challenges: space-level tasks\nrequire a more comprehensive spatial reasoning ability, for example, modeling\ndistances and spatial relations within 3D space, while part-level tasks demand\nfine-grained perception of object composition. Even the best performance model,\nOpenAI o4-mini, achieves only 23.57% accuracy on space-level tasks and 33.94%\non part-level tasks, significantly lower than its performance on area-level and\nobject-level tasks. These findings underscore a critical gap in current models'\ncapacity to understand and reason about 3D scene beyond object-level semantics.\n","authors":["Tianxu Wang","Zhuofan Zhang","Ziyu Zhu","Yue Fan","Jing Xiong","Pengxiang Li","Xiaojian Ma","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2506.04897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04892v1","updated":"2025-06-05T11:19:26Z","published":"2025-06-05T11:19:26Z","title":"Learning to Plan via Supervised Contrastive Learning and Strategic\n  Interpolation: A Chess Case Study","summary":"  Modern chess engines achieve superhuman performance through deep tree search\nand regressive evaluation, while human players rely on intuition to select\ncandidate moves followed by a shallow search to validate them. To model this\nintuition-driven planning process, we train a transformer encoder using\nsupervised contrastive learning to embed board states into a latent space\nstructured by positional evaluation. In this space, distance reflects\nevaluative similarity, and visualized trajectories display interpretable\ntransitions between game states. We demonstrate that move selection can occur\nentirely within this embedding space by advancing toward favorable regions,\nwithout relying on deep search. Despite using only a 6-ply beam search, our\nmodel achieves an estimated Elo rating of 2593. Performance improves with both\nmodel size and embedding dimensionality, suggesting that latent planning may\noffer a viable alternative to traditional search. Although we focus on chess,\nthe proposed embedding-based planning method can be generalized to other\nperfect-information games where state evaluations are learnable. All source\ncode is available at https://github.com/andrewhamara/SOLIS.\n","authors":["Andrew Hamara","Greg Hamerly","Pablo Rivas","Andrew C. Freeman"],"pdf_url":"https://arxiv.org/pdf/2506.04892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03440v2","updated":"2025-06-05T11:08:44Z","published":"2025-06-03T22:51:44Z","title":"Geometric Visual Fusion Graph Neural Networks for Multi-Person\n  Human-Object Interaction Recognition in Videos","summary":"  Human-Object Interaction (HOI) recognition in videos requires understanding\nboth visual patterns and geometric relationships as they evolve over time.\nVisual and geometric features offer complementary strengths. Visual features\ncapture appearance context, while geometric features provide structural\npatterns. Effectively fusing these multimodal features without compromising\ntheir unique characteristics remains challenging. We observe that establishing\nrobust, entity-specific representations before modeling interactions helps\npreserve the strengths of each modality. Therefore, we hypothesize that a\nbottom-up approach is crucial for effective multimodal fusion. Following this\ninsight, we propose the Geometric Visual Fusion Graph Neural Network\n(GeoVis-GNN), which uses dual-attention feature fusion combined with\ninterdependent entity graph learning. It progressively builds from\nentity-specific representations toward high-level interaction understanding. To\nadvance HOI recognition to real-world scenarios, we introduce the Concurrent\nPartial Interaction Dataset (MPHOI-120). It captures dynamic multi-person\ninteractions involving concurrent actions and partial engagement. This dataset\nhelps address challenges like complex human-object dynamics and mutual\nocclusions. Extensive experiments demonstrate the effectiveness of our method\nacross various HOI scenarios. These scenarios include two-person interactions,\nsingle-person activities, bimanual manipulations, and complex concurrent\npartial interactions. Our method achieves state-of-the-art performance.\n","authors":["Tanqiu Qiao","Ruochen Li","Frederick W. B. Li","Yoshiki Kubotani","Shigeo Morishima","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2506.03440v2.pdf","comment":"Accepted by Expert Systems with Applications (ESWA)"},{"id":"http://arxiv.org/abs/2506.04879v1","updated":"2025-06-05T10:51:58Z","published":"2025-06-05T10:51:58Z","title":"Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking","summary":"  Diffusion models have achieved remarkable progress in both image generation\nand editing. However, recent studies have revealed their vulnerability to\nbackdoor attacks, in which specific patterns embedded in the input can\nmanipulate the model's behavior. Most existing research in this area has\nproposed attack frameworks focused on the image generation pipeline, leaving\nbackdoor attacks in image editing relatively unexplored. Among the few studies\ntargeting image editing, most utilize visible triggers, which are impractical\nbecause they introduce noticeable alterations to the input image before\nediting. In this paper, we propose a novel attack framework that embeds\ninvisible triggers into the image editing process via poisoned training data.\nWe leverage off-the-shelf deep watermarking models to encode imperceptible\nwatermarks as backdoor triggers. Our goal is to make the model produce the\npredefined backdoor target when it receives watermarked inputs, while editing\nclean images normally according to the given prompt. With extensive experiments\nacross different watermarking models, the proposed method achieves promising\nattack success rates. In addition, the analysis results of the watermark\ncharacteristics in term of backdoor attack further support the effectiveness of\nour approach. The code is available\nat:https://github.com/aiiu-lab/BackdoorImageEditing\n","authors":["Yu-Feng Chen","Tzuhsuan Huang","Pin-Yen Chiu","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2506.04879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04869v1","updated":"2025-06-05T10:45:27Z","published":"2025-06-05T10:45:27Z","title":"Geological Field Restoration through the Lens of Image Inpainting","summary":"  We present a new viewpoint on a reconstructing multidimensional geological\nfields from sparse observations. Drawing inspiration from deterministic image\ninpainting techniques, we model a partially observed spatial field as a\nmultidimensional tensor and recover missing values by enforcing a global\nlow-rank structure. Our approach combines ideas from tensor completion and\ngeostatistics, providing a robust optimization framework. Experiments on\nsynthetic geological fields demonstrate that used tensor completion method\nsignificant improvements in reconstruction accuracy over ordinary kriging for\nvarious percent of observed data.\n","authors":["Vladislav Trifonov","Ivan Oseledets","Ekaterina Muravleva"],"pdf_url":"https://arxiv.org/pdf/2506.04869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03643v2","updated":"2025-06-05T10:20:34Z","published":"2025-06-04T07:40:33Z","title":"Images are Worth Variable Length of Representations","summary":"  Most existing vision encoders map images into a fixed-length sequence of\ntokens, overlooking the fact that different images contain varying amounts of\ninformation. For example, a visually complex image (e.g., a cluttered room)\ninherently carries more information and thus deserves more tokens than a simple\nimage (e.g., a blank wall). To address this inefficiency, we propose DOVE, a\ndynamic vision encoder that produces a variable number of visual tokens (i.e.,\ncontinuous representation vectors) to reconstruct each image. Our results show\nthat DOVE significantly reduces the average number of tokens while maintaining\nhigh reconstruction quality. In several linear probing and downstream\nmultimodal tasks, it outperforms existing autoencoder-based tokenization\nmethods when using far fewer tokens, capturing more expressive semantic\nfeatures compared to fixed-length encoding. We further extend DOVE with\nquery-conditioned tokenization. By guiding the model to focus on query-relevant\nregions, it achieves more efficient and targeted semantic extraction. Our code\nand checkpoints are available at https://dove-encoder.github.io/dove-encoder.\n","authors":["Lingjun Mao","Rodolfo Corona","Xin Liang","Wenhao Yan","Zineng Tang"],"pdf_url":"https://arxiv.org/pdf/2506.03643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04842v1","updated":"2025-06-05T10:08:24Z","published":"2025-06-05T10:08:24Z","title":"MineInsight: A Multi-sensor Dataset for Humanitarian Demining Robotics\n  in Off-Road Environments","summary":"  The use of robotics in humanitarian demining increasingly involves computer\nvision techniques to improve landmine detection capabilities. However, in the\nabsence of diverse and realistic datasets, the reliable validation of\nalgorithms remains a challenge for the research community. In this paper, we\nintroduce MineInsight, a publicly available multi-sensor, multi-spectral\ndataset designed for off-road landmine detection. The dataset features 35\ndifferent targets (15 landmines and 20 commonly found objects) distributed\nalong three distinct tracks, providing a diverse and realistic testing\nenvironment. MineInsight is, to the best of our knowledge, the first dataset to\nintegrate dual-view sensor scans from both an Unmanned Ground Vehicle and its\nrobotic arm, offering multiple viewpoints to mitigate occlusions and improve\nspatial awareness. It features two LiDARs, as well as images captured at\ndiverse spectral ranges, including visible (RGB, monochrome), visible\nshort-wave infrared (VIS-SWIR), and long-wave infrared (LWIR). Additionally,\nthe dataset comes with an estimation of the location of the targets, offering a\nbenchmark for evaluating detection algorithms. We recorded approximately one\nhour of data in both daylight and nighttime conditions, resulting in around\n38,000 RGB frames, 53,000 VIS-SWIR frames, and 108,000 LWIR frames. MineInsight\nserves as a benchmark for developing and evaluating landmine detection\nalgorithms. Our dataset is available at\nhttps://github.com/mariomlz99/MineInsight.\n","authors":["Mario Malizia","Charles Hamesse","Ken Hasselmann","Geert De Cubber","Nikolaos Tsiogkas","Eric Demeester","Rob Haelterman"],"pdf_url":"https://arxiv.org/pdf/2506.04842v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2311.16462v3","updated":"2025-06-05T10:08:18Z","published":"2023-11-28T03:45:29Z","title":"Viewport Prediction for Volumetric Video Streaming by Exploring Video\n  Saliency and Trajectory Information","summary":"  Volumetric video, also known as hologram video, is a novel medium that\nportrays natural content in Virtual Reality (VR), Augmented Reality (AR), and\nMixed Reality (MR). It is expected to be the next-gen video technology and a\nprevalent use case for 5G and beyond wireless communication. Considering that\neach user typically only watches a section of the volumetric video, known as\nthe viewport, it is essential to have precise viewport prediction for optimal\nperformance. However, research on this topic is still in its infancy. In the\nend, this paper presents and proposes a novel approach, named Saliency and\nTrajectory Viewport Prediction (STVP), which aims to improve the precision of\nviewport prediction in volumetric video streaming. The STVP extensively\nutilizes video saliency information and viewport trajectory. To our knowledge,\nthis is the first comprehensive study of viewport prediction in volumetric\nvideo streaming. In particular, we introduce a novel sampling method, Uniform\nRandom Sampling (URS), to reduce computational complexity while still\npreserving video features in an efficient manner. Then we present a saliency\ndetection technique that incorporates both spatial and temporal information for\ndetecting static, dynamic geometric, and color salient regions. Finally, we\nintelligently fuse saliency and trajectory information to achieve more accurate\nviewport prediction. We conduct extensive simulations to evaluate the\neffectiveness of our proposed viewport prediction methods using\nstate-of-the-art volumetric video sequences. The experimental results show the\nsuperiority of the proposed method over existing schemes. The dataset and\nsource code will be publicly accessible after acceptance.\n","authors":["Jie Li","Zhixin Li","Zhi Liu","Pengyuan Zhou","Richang Hong","Qiyue Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2311.16462v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02785v2","updated":"2025-06-05T10:06:50Z","published":"2025-01-06T06:01:01Z","title":"Hybrid deep convolution model for lung cancer detection with transfer\n  learning","summary":"  Advances in healthcare research have significantly enhanced our understanding\nof disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung\ncancer remains one of the leading causes of cancer-related mortality worldwide\ndue to challenges in early and accurate diagnosis. While current lung cancer\ndetection models show promise, there is considerable potential for further\nimproving the accuracy for timely intervention. To address this challenge, we\nintroduce a hybrid deep convolution model leveraging transfer learning, named\nthe Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the\nprecision of lung cancer detection by refining sensitivity and specificity.\nThis model has surpassed existing deep learning approaches through experimental\nvalidation, achieving an accuracy of 98% and a sensitivity of 97%. By\noverlaying sensitivity maps onto lung Computed Tomography (CT) scans, it\nenables the visualization of regions most indicative of malignant or benign\nclassifications. This innovative method demonstrates exceptional performance in\ndistinguishing lung cancer with minimal false positives, thereby enhancing the\naccuracy of medical diagnoses.\n","authors":["Sugandha Saxena","S. N. Prasad","Ashwin M Polnaya","Shweta Agarwala"],"pdf_url":"https://arxiv.org/pdf/2501.02785v2.pdf","comment":"Authors realized mistake in the model. Also some data was\n  misinterpreted"},{"id":"http://arxiv.org/abs/2505.20507v2","updated":"2025-06-05T10:02:51Z","published":"2025-05-26T20:16:38Z","title":"Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging\n  Benchmark Dataset","summary":"  The global challenge of sustainable recycling demands automated, fast, and\naccurate, state-of-the-art (SOTA) material detection systems that act as a\nbedrock for a circular economy. Democratizing access to these cutting-edge\nsolutions that enable real-time waste analysis is essential for scaling up\nrecycling efforts and fostering the Green Deal. In response, we introduce\n\\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to\naccelerate the recovery of critical raw materials through accurate electrolyzer\nmaterials classification. The dataset comprises 55 co-registered\nhigh-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning\nthe 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and\n424,169 labeled ones. This enables non-invasive spectral analysis of shredded\nelectrolyzer samples, supporting quantitative and qualitative material\nclassification and spectral properties investigation. We evaluate a suite of\nbaseline machine learning (ML) methods alongside SOTA transformer-based deep\nlearning (DL) architectures, including Vision Transformer, SpectralFormer, and\nthe Multimodal Fusion Transformer, to investigate architectural bottlenecks for\nfurther efficiency optimisation when deploying transformers in material\nidentification. We implement zero-shot detection techniques and majority voting\nacross pixel-level predictions to establish object-level classification\nrobustness. In adherence to the FAIR data principles, the electrolyzers-HSI\ndataset and accompanying codebase are openly available at\nhttps://github.com/hifexplo/Electrolyzers-HSI and\nhttps://rodare.hzdr.de/record/3668, supporting reproducible research and\nfacilitating the broader adoption of smart and sustainable e-waste recycling\nsolutions.\n","authors":["Elias Arbash","Ahmed Jamal Afifi","Ymane Belahsen","Margret Fuchs","Pedram Ghamisi","Paul Scheunders","Richard Gloaguen"],"pdf_url":"https://arxiv.org/pdf/2505.20507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04837v1","updated":"2025-06-05T09:57:43Z","published":"2025-06-05T09:57:43Z","title":"OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model","summary":"  Although perception systems have made remarkable advancements in recent\nyears, particularly in 2D reasoning segmentation, these systems still rely on\nexplicit human instruction or pre-defined categories to identify target objects\nbefore executing visual recognition tasks. Such systems have matured\nsignificantly, demonstrating the ability to reason and comprehend implicit user\nintentions in two-dimensional contexts, producing accurate segmentation masks\nbased on complex and implicit query text. However, a comparable framework and\nstructure for 3D reasoning segmentation remain absent. This paper introduces\nOpenMaskDINO3D, a LLM designed for comprehensive 3D understanding and\nsegmentation. OpenMaskDINO3D processes point cloud data and text prompts to\nproduce instance segmentation masks, excelling in many 3D tasks. By introducing\na SEG token and object identifier, we achieve high-precision 3D segmentation\nmask generation, enabling the model to directly produce accurate point cloud\nsegmentation results from natural language instructions. Experimental results\non large-scale ScanNet datasets validate the effectiveness of our\nOpenMaskDINO3D across various tasks.\n","authors":["Kunshen Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04837v1.pdf","comment":"Project Page: https://github.com/Zhangkuns/OpenMaskDINO3D"},{"id":"http://arxiv.org/abs/2506.04830v1","updated":"2025-06-05T09:53:44Z","published":"2025-06-05T09:53:44Z","title":"DualX-VSR: Dual Axial Spatial$\\times$Temporal Transformer for Real-World\n  Video Super-Resolution without Motion Compensation","summary":"  Transformer-based models like ViViT and TimeSformer have advanced video\nunderstanding by effectively modeling spatiotemporal dependencies. Recent video\ngeneration models, such as Sora and Vidu, further highlight the power of\ntransformers in long-range feature extraction and holistic spatiotemporal\nmodeling. However, directly applying these models to real-world video\nsuper-resolution (VSR) is challenging, as VSR demands pixel-level precision,\nwhich can be compromised by tokenization and sequential attention mechanisms.\nWhile recent transformer-based VSR models attempt to address these issues using\nsmaller patches and local attention, they still face limitations such as\nrestricted receptive fields and dependence on optical flow-based alignment,\nwhich can introduce inaccuracies in real-world settings. To overcome these\nissues, we propose Dual Axial Spatial$\\times$Temporal Transformer for\nReal-World Video Super-Resolution (DualX-VSR), which introduces a novel dual\naxial spatial$\\times$temporal attention mechanism that integrates spatial and\ntemporal information along orthogonal directions. DualX-VSR eliminates the need\nfor motion compensation, offering a simplified structure that provides a\ncohesive representation of spatiotemporal information. As a result, DualX-VSR\nachieves high fidelity and superior performance in real-world VSR task.\n","authors":["Shuo Cao","Yihao Liu","Xiaohui Li. Yuanting Gao. Yu Zhou","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2506.04830v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.24848v2","updated":"2025-06-05T09:53:39Z","published":"2025-05-30T17:46:13Z","title":"Reading Recognition in the Wild","summary":"  To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism.\n","authors":["Charig Yang","Samiul Alam","Shakhrul Iman Siam","Michael J. Proulx","Lambert Mathias","Kiran Somasundaram","Luis Pesqueira","James Fort","Sheroze Sheriffdeen","Omkar Parkhi","Carl Ren","Mi Zhang","Yuning Chai","Richard Newcombe","Hyo Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2505.24848v2.pdf","comment":"Project Page:\n  https://www.projectaria.com/datasets/reading-in-the-wild/"},{"id":"http://arxiv.org/abs/2505.19536v2","updated":"2025-06-05T09:50:13Z","published":"2025-05-26T05:54:48Z","title":"FlowCut: Rethinking Redundancy via Information Flow for Efficient\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut\n","authors":["Jintao Tong","Wenwei Jin","Pengda Qin","Anqi Li","Yixiong Zou","Yuhong Li","Yuhua Li","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2505.19536v2.pdf","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2506.04823v1","updated":"2025-06-05T09:41:12Z","published":"2025-06-05T09:41:12Z","title":"Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light\n  Detectors","summary":"  Realistic adversarial attacks on various camera-based perception tasks of\nautonomous vehicles have been successfully demonstrated so far. However, only a\nfew works considered attacks on traffic light detectors. This work shows how\nCNNs for traffic light detection can be attacked with printed patches. We\npropose a threat model, where each instance of a traffic light is attacked with\na patch placed under it, and describe a training strategy. We demonstrate\nsuccessful adversarial patch attacks in universal settings. Our experiments\nshow realistic targeted red-to-green label-flipping attacks and attacks on\npictogram classification. Finally, we perform a real-world evaluation with\nprinted patches and demonstrate attacks in the lab settings with a mobile\ntraffic light for construction sites and in a test area with stationary traffic\nlights. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.\n","authors":["Svetlana Pavlitska","Jamie Robb","Nikolai Polley","Melih Yazgan","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2506.04823v1.pdf","comment":"Accepted for publication at IV 2025"},{"id":"http://arxiv.org/abs/2506.04817v1","updated":"2025-06-05T09:38:42Z","published":"2025-06-05T09:38:42Z","title":"Spike-TBR: a Noise Resilient Neuromorphic Event Representation","summary":"  Event cameras offer significant advantages over traditional frame-based\nsensors, including higher temporal resolution, lower latency and dynamic range.\nHowever, efficiently converting event streams into formats compatible with\nstandard computer vision pipelines remains a challenging problem, particularly\nin the presence of noise. In this paper, we propose Spike-TBR, a novel\nevent-based encoding strategy based on Temporal Binary Representation (TBR),\naddressing its vulnerability to noise by integrating spiking neurons. Spike-TBR\ncombines the frame-based advantages of TBR with the noise-filtering\ncapabilities of spiking neural networks, creating a more robust representation\nof event streams. We evaluate four variants of Spike-TBR, each using different\nspiking neurons, across multiple datasets, demonstrating superior performance\nin noise-affected scenarios while improving the results on clean data. Our\nmethod bridges the gap between spike-based and frame-based processing, offering\na simple noise-resilient solution for event-driven vision applications.\n","authors":["Gabriele Magrini. Federico Becattini","Luca Cultrera","Lorenzo Berlincioni","Pietro Pala","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2506.04817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04807v1","updated":"2025-06-05T09:33:06Z","published":"2025-06-05T09:33:06Z","title":"MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories","summary":"  Foundational to the Chinese language and culture, Chinese characters\nencompass extraordinarily extensive and ever-expanding categories, with the\nlatest Chinese GB18030-2022 standard containing 87,887 categories. The accurate\nrecognition of this vast number of characters, termed mega-category\nrecognition, presents a formidable yet crucial challenge for cultural heritage\npreservation and digital applications. Despite significant advances in Optical\nCharacter Recognition (OCR), mega-category recognition remains unexplored due\nto the absence of comprehensive datasets, with the largest existing dataset\ncontaining merely 16,151 categories. To bridge this critical gap, we introduce\nMegaHan97K, a mega-category, large-scale dataset covering an unprecedented\n97,455 categories of Chinese characters. Our work offers three major\ncontributions: (1) MegaHan97K is the first dataset to fully support the latest\nGB18030-2022 standard, providing at least six times more categories than\nexisting datasets; (2) It effectively addresses the long-tail distribution\nproblem by providing balanced samples across all categories through its three\ndistinct subsets: handwritten, historical and synthetic subsets; (3)\nComprehensive benchmarking experiments reveal new challenges in mega-category\nscenarios, including increased storage demands, morphologically similar\ncharacter recognition, and zero-shot learning difficulties, while also\nunlocking substantial opportunities for future research. To the best of our\nknowledge, the MetaHan97K is likely the dataset with the largest classes not\nonly in the field of OCR but may also in the broader domain of pattern\nrecognition. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MegaHan97K.\n","authors":["Yuyi Zhang","Yongxin Shi","Peirong Zhang","Yixin Zhao","Zhenhua Yang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2506.04807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04803v1","updated":"2025-06-05T09:30:27Z","published":"2025-06-05T09:30:27Z","title":"SupeRANSAC: One RANSAC to Rule Them All","summary":"  Robust estimation is a cornerstone in computer vision, particularly for tasks\nlike Structure-from-Motion and Simultaneous Localization and Mapping. RANSAC\nand its variants are the gold standard for estimating geometric models (e.g.,\nhomographies, relative/absolute poses) from outlier-contaminated data. Despite\nRANSAC's apparent simplicity, achieving consistently high performance across\ndifferent problems is challenging. While recent research often focuses on\nimproving specific RANSAC components (e.g., sampling, scoring), overall\nperformance is frequently more influenced by the \"bells and whistles\" (i.e.,\nthe implementation details and problem-specific optimizations) within a given\nlibrary. Popular frameworks like OpenCV and PoseLib demonstrate varying\nperformance, excelling in some tasks but lagging in others. We introduce\nSupeRANSAC, a novel unified RANSAC pipeline, and provide a detailed analysis of\nthe techniques that make RANSAC effective for specific vision tasks, including\nhomography, fundamental/essential matrix, and absolute/rigid pose estimation.\nSupeRANSAC is designed for consistent accuracy across these tasks, improving\nupon the best existing methods by, for example, 6 AUC points on average for\nfundamental matrix estimation. We demonstrate significant performance\nimprovements over the state-of-the-art on multiple problems and datasets. Code:\nhttps://github.com/danini/superansac\n","authors":["Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2506.04803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04790v1","updated":"2025-06-05T09:17:30Z","published":"2025-06-05T09:17:30Z","title":"LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff\n  Table","summary":"  Approximate nearest neighbor search (ANNS) is an essential building block for\napplications like RAG but can sometimes yield results that are overly similar\nto each other. In certain scenarios, search results should be similar to the\nquery and yet diverse. We propose LotusFilter, a post-processing module to\ndiversify ANNS results. We precompute a cutoff table summarizing vectors that\nare close to each other. During the filtering, LotusFilter greedily looks up\nthe table to delete redundant vectors from the candidates. We demonstrated that\nthe LotusFilter operates fast (0.02 [ms/query]) in settings resembling\nreal-world RAG applications, utilizing features such as OpenAI embeddings. Our\ncode is publicly available at https://github.com/matsui528/lotf.\n","authors":["Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2506.04790v1.pdf","comment":"CVPR 2025. GitHub: https://github.com/matsui528/lotf"},{"id":"http://arxiv.org/abs/2506.04789v1","updated":"2025-06-05T09:14:42Z","published":"2025-06-05T09:14:42Z","title":"Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations","summary":"  Learning effective multi-modal 3D representations of objects is essential for\nnumerous applications, such as augmented reality and robotics. Existing methods\noften rely on task-specific embeddings that are tailored either for semantic\nunderstanding or geometric reconstruction. As a result, these embeddings\ntypically cannot be decoded into explicit geometry and simultaneously reused\nacross tasks. In this paper, we propose Object-X, a versatile multi-modal\nobject representation framework capable of encoding rich object embeddings\n(e.g. images, point cloud, text) and decoding them back into detailed geometric\nand visual reconstructions. Object-X operates by geometrically grounding the\ncaptured modalities in a 3D voxel grid and learning an unstructured embedding\nfusing the information from the voxels with the object attributes. The learned\nembedding enables 3D Gaussian Splatting-based object reconstruction, while also\nsupporting a range of downstream tasks, including scene alignment, single-image\n3D object reconstruction, and localization. Evaluations on two challenging\nreal-world datasets demonstrate that Object-X produces high-fidelity novel-view\nsynthesis comparable to standard 3D Gaussian Splatting, while significantly\nimproving geometric accuracy. Moreover, Object-X achieves competitive\nperformance with specialized methods in scene alignment and localization.\nCritically, our object-centric descriptors require 3-4 orders of magnitude less\nstorage compared to traditional image- or point cloud-based approaches,\nestablishing Object-X as a scalable and highly practical solution for\nmulti-modal 3D scene representation.\n","authors":["Gaia Di Lorenzo","Federico Tombari","Marc Pollefeys","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2506.04789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04781v1","updated":"2025-06-05T09:10:31Z","published":"2025-06-05T09:10:31Z","title":"Deep learning image burst stacking to reconstruct high-resolution\n  ground-based solar observations","summary":"  Large aperture ground based solar telescopes allow the solar atmosphere to be\nresolved in unprecedented detail. However, observations are limited by Earths\nturbulent atmosphere, requiring post image corrections. Current reconstruction\nmethods using short exposure bursts face challenges with strong turbulence and\nhigh computational costs. We introduce a deep learning approach that\nreconstructs 100 short exposure images into one high quality image in real\ntime. Using unpaired image to image translation, our model is trained on\ndegraded bursts with speckle reconstructions as references, improving\nrobustness and generalization. Our method shows an improved robustness in terms\nof perceptual quality, especially when speckle reconstructions show artifacts.\nAn evaluation with a varying number of images per burst demonstrates that our\nmethod makes efficient use of the combined image information and achieves the\nbest reconstructions when provided with the full image burst.\n","authors":["Christoph Schirninger","Robert Jarolim","Astrid M. Veronig","Christoph Kuckein"],"pdf_url":"https://arxiv.org/pdf/2506.04781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02882v2","updated":"2025-06-05T09:05:13Z","published":"2025-06-03T13:47:59Z","title":"GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation","summary":"  Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset.\n","authors":["Sohyun Lee","Yeho Gwon","Lukas Hoyer","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2506.02882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03951v2","updated":"2025-06-05T08:59:39Z","published":"2025-06-04T13:40:41Z","title":"Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective","summary":"  The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters. Code:\nhttps://github.com/byyx666/Dual-Arch.\n","authors":["Aojun Lu","Hangjie Yuan","Tao Feng","Yanan Sun"],"pdf_url":"https://arxiv.org/pdf/2506.03951v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2407.19889v2","updated":"2025-06-05T08:49:26Z","published":"2024-07-29T11:11:17Z","title":"Self-Supervised Learning for Text Recognition: A Critical Survey","summary":"  Text Recognition (TR) refers to the research area that focuses on retrieving\ntextual information from images, a topic that has seen significant advancements\nin the last decade due to the use of Deep Neural Networks (DNN). However, these\nsolutions often necessitate vast amounts of manually labeled or synthetic data.\nAddressing this challenge, Self-Supervised Learning (SSL) has gained attention\nby utilizing large datasets of unlabeled data to train DNN, thereby generating\nmeaningful and robust representations. Although SSL was initially overlooked in\nTR because of its unique characteristics, recent years have witnessed a surge\nin the development of SSL methods specifically for this field. This rapid\ndevelopment, however, has led to many methods being explored independently,\nwithout taking previous efforts in methodology or comparison into account,\nthereby hindering progress in the field of research. This paper, therefore,\nseeks to consolidate the use of SSL in the field of TR, offering a critical and\ncomprehensive overview of the current state of the art. We will review and\nanalyze the existing methods, compare their results, and highlight\ninconsistencies in the current literature. This thorough analysis aims to\nprovide general insights into the field, propose standardizations, identify new\nresearch directions, and foster its proper development.\n","authors":["Carlos Penarrubia","Jose J. Valero-Mas","Jorge Calvo-Zaragoza"],"pdf_url":"https://arxiv.org/pdf/2407.19889v2.pdf","comment":"Published at International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2506.04764v1","updated":"2025-06-05T08:47:15Z","published":"2025-06-05T08:47:15Z","title":"HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular\n  Visual Place Recognition","summary":"  When applying Visual Place Recognition (VPR) to real-world mobile robots and\nsimilar applications, perspective-to-equirectangular (P2E) formulation\nnaturally emerges as a suitable approach to accommodate diverse query images\ncaptured from various viewpoints. In this paper, we introduce HypeVPR, a novel\nhierarchical embedding framework in hyperbolic space, designed to address the\nunique challenges of P2E VPR. The key idea behind HypeVPR is that visual\nenvironments captured by panoramic views exhibit inherent hierarchical\nstructures. To leverage this property, we employ hyperbolic space to represent\nhierarchical feature relationships and preserve distance properties within the\nfeature space. To achieve this, we propose a hierarchical feature aggregation\nmechanism that organizes local-to-global feature representations within\nhyperbolic space. Additionally, HypeVPR adopts an efficient coarse-to-fine\nsearch strategy, optimally balancing speed and accuracy to ensure robust\nmatching, even between descriptors from different image types. This approach\nenables HypeVPR to outperform state-of-the-art methods while significantly\nreducing retrieval time, achieving up to 5x faster retrieval across diverse\nbenchmark datasets. The code and models will be released at\nhttps://github.com/suhan-woo/HypeVPR.git.\n","authors":["Suhan Woo","Seongwon Lee","Jinwoo Jang","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2506.04764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04758v1","updated":"2025-06-05T08:43:24Z","published":"2025-06-05T08:43:24Z","title":"Toward Better SSIM Loss for Unsupervised Monocular Depth Estimation","summary":"  Unsupervised monocular depth learning generally relies on the photometric\nrelation among temporally adjacent images. Most of previous works use both mean\nabsolute error (MAE) and structure similarity index measure (SSIM) with\nconventional form as training loss. However, they ignore the effect of\ndifferent components in the SSIM function and the corresponding hyperparameters\non the training. To address these issues, this work proposes a new form of\nSSIM. Compared with original SSIM function, the proposed new form uses addition\nrather than multiplication to combine the luminance, contrast, and structural\nsimilarity related components in SSIM. The loss function constructed with this\nscheme helps result in smoother gradients and achieve higher performance on\nunsupervised depth estimation. We conduct extensive experiments to determine\nthe relatively optimal combination of parameters for our new SSIM. Based on the\npopular MonoDepth approach, the optimized SSIM loss function can remarkably\noutperform the baseline on the KITTI-2015 outdoor dataset.\n","authors":["Yijun Cao","Fuya Luo","Yongjie Li"],"pdf_url":"https://arxiv.org/pdf/2506.04758v1.pdf","comment":"12 pages,4 figures"},{"id":"http://arxiv.org/abs/2506.04756v1","updated":"2025-06-05T08:41:23Z","published":"2025-06-05T08:41:23Z","title":"Ontology-based knowledge representation for bone disease diagnosis: a\n  foundation for safe and sustainable medical artificial intelligence systems","summary":"  Medical artificial intelligence (AI) systems frequently lack systematic\ndomain expertise integration, potentially compromising diagnostic reliability.\nThis study presents an ontology-based framework for bone disease diagnosis,\ndeveloped in collaboration with Ho Chi Minh City Hospital for Traumatology and\nOrthopedics. The framework introduces three theoretical contributions: (1) a\nhierarchical neural network architecture guided by bone disease ontology for\nsegmentation-classification tasks, incorporating Visual Language Models (VLMs)\nthrough prompts, (2) an ontology-enhanced Visual Question Answering (VQA)\nsystem for clinical reasoning, and (3) a multimodal deep learning model that\nintegrates imaging, clinical, and laboratory data through ontological\nrelationships. The methodology maintains clinical interpretability through\nsystematic knowledge digitization, standardized medical terminology mapping,\nand modular architecture design. The framework demonstrates potential for\nextension beyond bone diseases through its standardized structure and reusable\ncomponents. While theoretical foundations are established, experimental\nvalidation remains pending due to current dataset and computational resource\nlimitations. Future work will focus on expanding the clinical dataset and\nconducting comprehensive system validation.\n","authors":["Loan Dao","Ngoc Quoc Ly"],"pdf_url":"https://arxiv.org/pdf/2506.04756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04755v1","updated":"2025-06-05T08:40:24Z","published":"2025-06-05T08:40:24Z","title":"Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning","summary":"  While multi-modal large language models (MLLMs) have made significant\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\nbelieved that extensive training data is necessary for improving multi-modal\nreasoning ability, inevitably leading to data redundancy and substantial\ncomputational costs. However, can smaller high-value datasets match or\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\nchallenge this assumption through a key observation: meaningful multi-modal\nreasoning is triggered by only a sparse subset of training samples, termed\ncognitive samples, whereas the majority contribute marginally. Building on this\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\nPotential (RAP), which identifies cognitive samples by estimating each sample's\npotential to stimulate genuine multi-modal reasoning by two complementary\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\noutcome model principle, eliminates samples that overly rely on language priors\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\nConfidence Estimator (ACE), which exploits token-level self-attention to\ndiscard samples dominated by irrelevant but over-emphasized tokens in\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\nReplacement Module (DRM) to substitute trivial instances with cognitively\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\nExperiments on six datasets show that our RAP method consistently achieves\nsuperior performance using only 9.3% of the training data, while reducing\ncomputational costs by over 43%. Our code is available at\nhttps://github.com/Leo-ssl/RAP.\n","authors":["Shenshen Li","Kaiyuan Deng","Lei Wang","Hao Yang","Chong Peng","Peng Yan","Fumin Shen","Heng Tao Shen","Xing Xu"],"pdf_url":"https://arxiv.org/pdf/2506.04755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04753v1","updated":"2025-06-05T08:39:17Z","published":"2025-06-05T08:39:17Z","title":"Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater\n  Image Enhancement","summary":"  We present a novel dual-stream architecture that achieves state-of-the-art\nunderwater image enhancement by explicitly integrating the Jaffe-McGlamery\nphysical model with capsule clustering-based feature representation learning.\nOur method simultaneously estimates transmission maps and spatially-varying\nbackground light through a dedicated physics estimator while extracting\nentity-level features via capsule clustering in a parallel stream. This\nphysics-guided approach enables parameter-free enhancement that respects\nunderwater formation constraints while preserving semantic structures and\nfine-grained details. Our approach also features a novel optimization objective\nensuring both physical adherence and perceptual quality across multiple spatial\nfrequencies. To validate our approach, we conducted extensive experiments\nacross six challenging benchmarks. Results demonstrate consistent improvements\nof $+0.5$dB PSNR over the best existing methods while requiring only one-third\nof their computational complexity (FLOPs), or alternatively, more than $+1$dB\nPSNR improvement when compared to methods with similar computational budgets.\nCode and data \\textit{will} be available at https://github.com/iN1k1/.\n","authors":["Niki Martinel","Rita Pucci"],"pdf_url":"https://arxiv.org/pdf/2506.04753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19054v3","updated":"2025-06-05T08:25:25Z","published":"2025-01-31T11:28:16Z","title":"Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models","summary":"  Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.\n","authors":["Ruiyu Wang","Yu Yuan","Shizhao Sun","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2501.19054v3.pdf","comment":"ICML 2025 camera ready"},{"id":"http://arxiv.org/abs/2506.04743v1","updated":"2025-06-05T08:22:24Z","published":"2025-06-05T08:22:24Z","title":"SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in\n  VLMs","summary":"  Vision-Language Models (VLMs) have achieved remarkable performance in image\ncaptioning, but recent studies show they are vulnerable to backdoor attacks.\nAttackers can inject imperceptible perturbations-such as local pixel triggers\nor global semantic phrases-into the training data, causing the model to\ngenerate malicious, attacker-controlled captions for specific inputs. These\nattacks are hard to detect and defend due to their stealthiness and cross-modal\nnature. By analyzing attack samples, we identify two key vulnerabilities: (1)\nabnormal attention concentration on specific image regions, and (2) semantic\ndrift and incoherence in generated captions. To counter this, we propose\nSemantic Reward Defense (SRD), a reinforcement learning framework that\nmitigates backdoor behavior without prior knowledge of triggers. SRD uses a\nDeep Q-Network to learn policies for applying discrete perturbations (e.g.,\nocclusion, color masking) to sensitive image regions, aiming to disrupt the\nactivation of malicious pathways. We design a semantic fidelity score as the\nreward signal, which jointly evaluates semantic consistency and linguistic\nfluency of the output, guiding the agent toward generating robust yet faithful\ncaptions. Experiments across mainstream VLMs and datasets show SRD reduces\nattack success rates to 5.6%, while preserving caption quality on clean inputs\nwith less than 10% performance drop. SRD offers a trigger-agnostic,\ninterpretable defense paradigm against stealthy backdoor threats in multimodal\ngenerative models.\n","authors":["Shuhan Xu","Siyuan Liang","Hongling Zheng","Yong Luo","Aishan Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.04743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01483v2","updated":"2025-06-05T08:21:51Z","published":"2025-04-02T08:37:32Z","title":"GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design\n  and Generic Garment Modeling","summary":"  Realistic digital garment modeling remains a labor-intensive task due to the\nintricate process of translating 2D sewing patterns into high-fidelity,\nsimulation-ready 3D garments. We introduce GarmageNet, a unified generative\nframework that automates the creation of 2D sewing patterns, the construction\nof sewing relationships, and the synthesis of 3D garment initializations\ncompatible with physics-based simulation. Central to our approach is Garmage, a\nnovel garment representation that encodes each panel as a structured geometry\nimage, effectively bridging the semantic and geometric gap between 2D\nstructural patterns and 3D garment shapes. GarmageNet employs a latent\ndiffusion transformer to synthesize panel-wise geometry images and integrates\nGarmageJigsaw, a neural module for predicting point-to-point sewing connections\nalong panel contours. To support training and evaluation, we build GarmageSet,\na large-scale dataset comprising over 10,000 professionally designed garments\nwith detailed structural and style annotations. Our method demonstrates\nversatility and efficacy across multiple application scenarios, including\nscalable garment generation from multi-modal design concepts (text prompts,\nsketches, photographs), automatic modeling from raw flat sewing patterns,\npattern recovery from unstructured point clouds, and progressive garment\nediting using conventional instructions-laying the foundation for fully\nautomated, production-ready pipelines in digital fashion. Project page:\nhttps://style3d.github.io/garmagenet.\n","authors":["Siran Li","Ruiyang Liu","Chen Liu","Zhendong Wang","Gaofeng He","Yong-Lu Li","Xiaogang Jin","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2504.01483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04737v1","updated":"2025-06-05T08:16:15Z","published":"2025-06-05T08:16:15Z","title":"Bridging Annotation Gaps: Transferring Labels to Align Object Detection\n  Datasets","summary":"  Combining multiple object detection datasets offers a path to improved\ngeneralisation but is hindered by inconsistencies in class semantics and\nbounding box annotations. Some methods to address this assume shared label\ntaxonomies and address only spatial inconsistencies; others require manual\nrelabelling, or produce a unified label space, which may be unsuitable when a\nfixed target label space is required. We propose Label-Aligned Transfer (LAT),\na label transfer framework that systematically projects annotations from\ndiverse source datasets into the label space of a target dataset. LAT begins by\ntraining dataset-specific detectors to generate pseudo-labels, which are then\ncombined with ground-truth annotations via a Privileged Proposal Generator\n(PPG) that replaces the region proposal network in two-stage detectors. To\nfurther refine region features, a Semantic Feature Fusion (SFF) module injects\nclass-aware context and features from overlapping proposals using a\nconfidence-weighted attention mechanism. This pipeline preserves\ndataset-specific annotation granularity while enabling many-to-one label space\ntransfer across heterogeneous datasets, resulting in a semantically and\nspatially aligned representation suitable for training a downstream detector.\nLAT thus jointly addresses both class-level misalignments and bounding box\ninconsistencies without relying on shared label spaces or manual annotations.\nAcross multiple benchmarks, LAT demonstrates consistent improvements in\ntarget-domain detection performance, achieving gains of up to +4.8AP over\nsemi-supervised baselines.\n","authors":["Mikhail Kennerley","Angelica Alives-Reviro","Carola-Bibiane Schönlieb","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2506.04737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03956v2","updated":"2025-06-05T08:01:57Z","published":"2025-06-04T13:46:33Z","title":"Adapt before Continual Learning","summary":"  Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL. Code is available at\nhttps://github.com/byyx666/ACL_code.\n","authors":["Aojun Lu","Tao Feng","Hangjie Yuan","Chunhui Ding","Yanan Sun"],"pdf_url":"https://arxiv.org/pdf/2506.03956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04717v1","updated":"2025-06-05T07:42:31Z","published":"2025-06-05T07:42:31Z","title":"Using In-Context Learning for Automatic Defect Labelling of Display\n  Manufacturing Data","summary":"  This paper presents an AI-assisted auto-labeling system for display panel\ndefect detection that leverages in-context learning capabilities. We adopt and\nenhance the SegGPT architecture with several domain-specific training\ntechniques and introduce a scribble-based annotation mechanism to streamline\nthe labeling process. Our two-stage training approach, validated on industrial\ndisplay panel datasets, demonstrates significant improvements over the baseline\nmodel, achieving an average IoU increase of 0.22 and a 14% improvement in\nrecall across multiple product types, while maintaining approximately 60%\nauto-labeling coverage. Experimental results show that models trained on our\nauto-labeled data match the performance of those trained on human-labeled data,\noffering a practical solution for reducing manual annotation efforts in\nindustrial inspection systems.\n","authors":["Babar Hussain","Qiang Liu","Gang Chen","Bihai She","Dahai Yu"],"pdf_url":"https://arxiv.org/pdf/2506.04717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04716v1","updated":"2025-06-05T07:41:19Z","published":"2025-06-05T07:41:19Z","title":"Learning dissection trajectories from expert surgical videos via\n  imitation learning with equivariant diffusion","summary":"  Endoscopic Submucosal Dissection (ESD) is a well-established technique for\nremoving epithelial lesions. Predicting dissection trajectories in ESD videos\noffers significant potential for enhancing surgical skill training and\nsimplifying the learning process, yet this area remains underexplored. While\nimitation learning has shown promise in acquiring skills from expert\ndemonstrations, challenges persist in handling uncertain future movements,\nlearning geometric symmetries, and generalizing to diverse surgical scenarios.\nTo address these, we introduce a novel approach: Implicit Diffusion Policy with\nEquivariant Representations for Imitation Learning (iDPOE). Our method models\nexpert behavior through a joint state action distribution, capturing the\nstochastic nature of dissection trajectories and enabling robust visual\nrepresentation learning across various endoscopic views. By incorporating a\ndiffusion model into policy learning, iDPOE ensures efficient training and\nsampling, leading to more accurate predictions and better generalization.\nAdditionally, we enhance the model's ability to generalize to geometric\nsymmetries by embedding equivariance into the learning process. To address\nstate mismatches, we develop a forward-process guided action inference strategy\nfor conditional sampling. Using an ESD video dataset of nearly 2000 clips,\nexperimental results show that our approach surpasses state-of-the-art methods,\nboth explicit and implicit, in trajectory prediction. To the best of our\nknowledge, this is the first application of imitation learning to surgical\nskill development for dissection trajectory prediction.\n","authors":["Hongyu Wang","Yonghao Long","Yueyao Chen","Hon-Chi Yip","Markus Scheppach","Philip Wai-Yan Chiu","Yeung Yam","Helen Mei-Ling Meng","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2506.04716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04715v1","updated":"2025-06-05T07:40:12Z","published":"2025-06-05T07:40:12Z","title":"Towards Holistic Visual Quality Assessment of AI-Generated Videos: A\n  LLM-Based Multi-Dimensional Evaluation Model","summary":"  The development of AI-Generated Video (AIGV) technology has been remarkable\nin recent years, significantly transforming the paradigm of video content\nproduction. However, AIGVs still suffer from noticeable visual quality defects,\nsuch as noise, blurriness, frame jitter and low dynamic degree, which severely\nimpact the user's viewing experience. Therefore, an effective automatic visual\nquality assessment is of great importance for AIGV content regulation and\ngenerative model improvement. In this work, we decompose the visual quality of\nAIGVs into three dimensions: technical quality, motion quality, and video\nsemantics. For each dimension, we design corresponding encoder to achieve\neffective feature representation. Moreover, considering the outstanding\nperformance of large language models (LLMs) in various vision and language\ntasks, we introduce a LLM as the quality regression module. To better enable\nthe LLM to establish reasoning associations between multi-dimensional features\nand visual quality, we propose a specially designed multi-modal prompt\nengineering framework. Additionally, we incorporate LoRA fine-tuning technology\nduring the training phase, allowing the LLM to better adapt to specific tasks.\nOur proposed method achieved \\textbf{second place} in the NTIRE 2025 Quality\nAssessment of AI-Generated Content Challenge: Track 2 AI Generated video,\ndemonstrating its effectiveness. Codes can be obtained at\nhttps://github.com/QiZelu/AIGVEval.\n","authors":["Zelu Qi","Ping Shi","Chaoyang Zhang","Shuqi Wang","Fei Zhao","Da Pan","Zefeng Ying"],"pdf_url":"https://arxiv.org/pdf/2506.04715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04713v1","updated":"2025-06-05T07:37:15Z","published":"2025-06-05T07:37:15Z","title":"Robust Few-Shot Vision-Language Model Adaptation","summary":"  Pretrained VLMs achieve strong performance on downstream tasks when adapted\nwith just a few labeled examples. As the adapted models inevitably encounter\nout-of-distribution (OOD) test data that deviates from the in-distribution (ID)\ntask-specific training data, enhancing OOD generalization in few-shot\nadaptation is critically important. We study robust few-shot VLM adaptation,\naiming to increase both ID and OOD accuracy. By comparing different adaptation\nmethods (e.g., prompt tuning, linear probing, contrastive finetuning, and full\nfinetuning), we uncover three key findings: (1) finetuning with proper\nhyperparameters significantly outperforms the popular VLM adaptation methods\nprompt tuning and linear probing; (2) visual encoder-only finetuning achieves\nbetter efficiency and accuracy than contrastively finetuning both visual and\ntextual encoders; (3) finetuning the top layers of the visual encoder provides\nthe best balance between ID and OOD accuracy. Building on these findings, we\npropose partial finetuning of the visual encoder empowered with two simple\naugmentation techniques: (1) retrieval augmentation which retrieves\ntask-relevant data from the VLM's pretraining dataset to enhance adaptation,\nand (2) adversarial perturbation which promotes robustness during finetuning.\nResults show that the former/latter boosts OOD/ID accuracy while slightly\nsacrificing the ID/OOD accuracy. Yet, perhaps understandably, naively combining\nthe two does not maintain their best OOD/ID accuracy. We address this dilemma\nwith the developed SRAPF, Stage-wise Retrieval Augmentation-based Adversarial\nPartial Finetuning. SRAPF consists of two stages: (1) partial finetuning the\nvisual encoder using both ID and retrieved data, and (2) adversarial partial\nfinetuning with few-shot ID data. Extensive experiments demonstrate that SRAPF\nachieves the state-of-the-art ID and OOD accuracy on the ImageNet OOD\nbenchmarks.\n","authors":["Hanxin Wang","Tian Liu","Shu Kong"],"pdf_url":"https://arxiv.org/pdf/2506.04713v1.pdf","comment":"Project website: https://hannawang09.github.io/projects/srapf/"},{"id":"http://arxiv.org/abs/2506.04706v1","updated":"2025-06-05T07:30:58Z","published":"2025-06-05T07:30:58Z","title":"Line of Sight: On Linear Representations in VLLMs","summary":"  Language models can be equipped with multimodal capabilities by fine-tuning\non embeddings of visual inputs. But how do such multimodal models represent\nimages in their hidden activations? We explore representations of image\nconcepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set\nof ImageNet classes represented via linearly decodable features in the residual\nstream. We show that the features are causal by performing targeted edits on\nthe model output. In order to increase the diversity of the studied linear\nfeatures, we train multimodal Sparse Autoencoders (SAEs), creating a highly\ninterpretable dictionary of text and image features. We find that although\nmodel representations across modalities are quite disjoint, they become\nincreasingly shared in deeper layers.\n","authors":["Achyuta Rajaram","Sarah Schwettmann","Jacob Andreas","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2506.04706v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.04704v1","updated":"2025-06-05T07:26:34Z","published":"2025-06-05T07:26:34Z","title":"HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta\n  Token for Vision-Language Model","summary":"  Despite emerging efforts to enhance the safety of Vision-Language Models\n(VLMs), current approaches face two main shortcomings. 1) Existing\nsafety-tuning datasets and benchmarks only partially consider how image-text\ninteractions can yield harmful content, often overlooking contextually unsafe\noutcomes from seemingly benign pairs. This narrow coverage leaves VLMs\nvulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely\nprimarily on data-centric tuning, with limited architectural innovations to\nintrinsically strengthen safety. We address these gaps by introducing a\nholistic safety dataset and benchmark, HoliSafe, that spans all five\nsafe/unsafe image-text combinations, providing a more robust basis for both\ntraining and evaluation. We further propose SafeLLaVA, a novel VLM augmented\nwith a learnable safety meta token and a dedicated safety head. The meta token\nencodes harmful visual cues during training, intrinsically guiding the language\nmodel toward safer responses, while the safety head offers interpretable\nharmfulness classification aligned with refusal rationales. Experiments show\nthat SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety\nperformance across multiple VLM benchmarks. Additionally, the HoliSafe\nbenchmark itself reveals critical vulnerabilities in existing models. We hope\nthat HoliSafe and SafeLLaVA will spur further research into robust and\ninterpretable VLM safety, expanding future avenues for multimodal alignment.\n","authors":["Youngwan Lee","Kangsan Kim","Kwanyong Park","Ilcahe Jung","Soojin Jang","Seanie Lee","Yong-Ju Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.04704v1.pdf","comment":"Project page: https://youngwanlee.github.io/holisafe"},{"id":"http://arxiv.org/abs/2502.09560v3","updated":"2025-06-05T07:22:50Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 24 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9\\% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code and dataset are available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v3.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.04688v1","updated":"2025-06-05T07:11:36Z","published":"2025-06-05T07:11:36Z","title":"MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models","summary":"  This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.\n","authors":["Gio Paik","Geewook Kim","Jinbae Im"],"pdf_url":"https://arxiv.org/pdf/2506.04688v1.pdf","comment":"ACL Findings 2025"},{"id":"http://arxiv.org/abs/2503.18033v2","updated":"2025-06-05T07:10:09Z","published":"2025-03-23T11:26:48Z","title":"OmnimatteZero: Fast Training-free Omnimatte with Pre-trained Video\n  Diffusion Models","summary":"  In Omnimatte, one aims to decompose a given video into semantically\nmeaningful layers, including the background and individual objects along with\ntheir associated effects, such as shadows and reflections. Existing methods\noften require extensive training or costly self-supervised optimization. In\nthis paper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. These are accomplished by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. To overcome this, we introduce\ntemporal and spatial attention guidance modules that steer the diffusion\nprocess for accurate object removal and temporally consistent background\nreconstruction. We further show that self-attention maps capture information\nabout the object and its footprints and use them to inpaint the object's\neffects, leaving a clean background. Additionally, through simple latent\narithmetic, object layers can be isolated and recombined seamlessly with new\nvideo layers to produce new videos. Evaluations show that OmnimatteZero not\nonly achieves superior performance in terms of background reconstruction but\nalso sets a new record for the fastest Omnimatte approach, achieving real-time\nperformance with minimal frame runtime.\n","authors":["Dvir Samuel","Matan Levy","Nir Darshan","Gal Chechik","Rami Ben-Ari"],"pdf_url":"https://arxiv.org/pdf/2503.18033v2.pdf","comment":"Project Page: https://dvirsamuel.github.io/omnimattezero.github.io/"},{"id":"http://arxiv.org/abs/2506.04682v1","updated":"2025-06-05T07:03:50Z","published":"2025-06-05T07:03:50Z","title":"MARS: Radio Map Super-resolution and Reconstruction Method under Sparse\n  Channel Measurements","summary":"  Radio maps reflect the spatial distribution of signal strength and are\nessential for applications like smart cities, IoT, and wireless network\nplanning. However, reconstructing accurate radio maps from sparse measurements\nremains challenging. Traditional interpolation and inpainting methods lack\nenvironmental awareness, while many deep learning approaches depend on detailed\nscene data, limiting generalization. To address this, we propose MARS, a\nMulti-scale Aware Radiomap Super-resolution method that combines CNNs and\nTransformers with multi-scale feature fusion and residual connections. MARS\nfocuses on both global and local feature extraction, enhancing feature\nrepresentation across different receptive fields and improving reconstruction\naccuracy. Experiments across different scenes and antenna locations show that\nMARS outperforms baseline models in both MSE and SSIM, while maintaining low\ncomputational cost, demonstrating strong practical potential.\n","authors":["Chuyun Deng","Na Liu","Wei Xie","Lianming Xu","Li Wang"],"pdf_url":"https://arxiv.org/pdf/2506.04682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04676v1","updated":"2025-06-05T06:52:26Z","published":"2025-06-05T06:52:26Z","title":"Gen-n-Val: Agentic Image Data Generation and Validation","summary":"  Recently, Large Language Models (LLMs) and Vision Large Language Models\n(VLLMs) have demonstrated impressive performance as agents across various tasks\nwhile data scarcity and label noise remain significant challenges in computer\nvision tasks, such as object detection and instance segmentation. A common\nsolution for resolving these issues is to generate synthetic data. However,\ncurrent synthetic data generation methods struggle with issues, such as\nmultiple objects per mask, inaccurate segmentation, and incorrect category\nlabels, limiting their effectiveness. To address these issues, we introduce\nGen-n-Val, a novel agentic data generation framework that leverages Layer\nDiffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks\nand diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt\nagent, an LLM, optimizes prompts for LD to generate high-quality foreground\ninstance images and segmentation masks. These optimized prompts ensure the\ngeneration of single-object synthetic data with precise instance masks and\nclean backgrounds. (2) The data validation agent, a VLLM, which filters out\nlow-quality synthetic instance images. The system prompts for both agents are\nrefined through TextGrad. Additionally, we use image harmonization to combine\nmultiple instances within scenes. Compared to state-of-the-art synthetic data\napproaches like MosaicFusion, our approach reduces invalid synthetic data from\n50% to 7% and improves performance by 1% mAP on rare classes in COCO instance\nsegmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant\nimprovements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object\ndetection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance\nof YOLOv9 and YOLO11 families in instance segmentation and object detection.\n","authors":["Jing-En Huang","I-Sheng Fang","Tzuhsuan Huang","Chih-Yu Wang","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2506.04676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01320v2","updated":"2025-06-05T06:50:56Z","published":"2025-06-02T05:02:33Z","title":"Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models","summary":"  We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments. Project Webpage:\nhttps://psi-sampler.github.io/\n","authors":["Taehoon Yoon","Yunhong Min","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2506.01320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04673v1","updated":"2025-06-05T06:39:43Z","published":"2025-06-05T06:39:43Z","title":"Interpretable Few-Shot Image Classification via Prototypical\n  Concept-Guided Mixture of LoRA Experts","summary":"  Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to\nenable their visual recognition processes more interpretable, but they often\nstruggle in data-scarce settings where insufficient training samples lead to\nsuboptimal performance.To address this limitation, we propose a Few-Shot\nPrototypical Concept Classification (FSPCC) framework that systematically\nmitigates two key challenges under low-data regimes: parametric imbalance and\nrepresentation misalignment. Specifically, our approach leverages a Mixture of\nLoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced\nallocation of trainable parameters between the backbone and the PCL\nmodule.Meanwhile, cross-module concept guidance enforces tight alignment\nbetween the backbone's feature representations and the prototypical concept\nactivation patterns.In addition, we incorporate a multi-level feature\npreservation strategy that fuses spatial and semantic cues across various\nlayers, thereby enriching the learned representations and mitigating the\nchallenges posed by limited data availability.Finally, to enhance\ninterpretability and minimize concept overlap, we introduce a geometry-aware\nconcept discrimination loss that enforces orthogonality among concepts,\nencouraging more disentangled and transparent decision boundaries.Experimental\nresults on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS,\nStanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach\nconsistently outperforms existing SEMs by a notable margin, with 4.2%-8.7%\nrelative gains in 5-way 5-shot classification.These findings highlight the\nefficacy of coupling concept learning with few-shot adaptation to achieve both\nhigher accuracy and clearer model interpretability, paving the way for more\ntransparent visual recognition systems.\n","authors":["Zhong Ji","Rongshuai Wei","Jingren Liu","Yanwei Pang","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2506.04673v1.pdf","comment":"13 pages,5 figures"},{"id":"http://arxiv.org/abs/2506.04668v1","updated":"2025-06-05T06:30:11Z","published":"2025-06-05T06:30:11Z","title":"Feature-Based Lie Group Transformer for Real-World Applications","summary":"  The main goal of representation learning is to acquire meaningful\nrepresentations from real-world sensory inputs without supervision.\nRepresentation learning explains some aspects of human development. Various\nneural network (NN) models have been proposed that acquire empirically good\nrepresentations. However, the formulation of a good representation has not been\nestablished. We recently proposed a method for categorizing changes between a\npair of sensory inputs. A unique feature of this approach is that\ntransformations between two sensory inputs are learned to satisfy algebraic\nstructural constraints. Conventional representation learning often assumes that\ndisentangled independent feature axes is a good representation; however, we\nfound that such a representation cannot account for conditional independence.\nTo overcome this problem, we proposed a new method using group decomposition in\nGalois algebra theory. Although this method is promising for defining a more\ngeneral representation, it assumes pixel-to-pixel translation without feature\nextraction, and can only process low-resolution images with no background,\nwhich prevents real-world application. In this study, we provide a simple\nmethod to apply our group decomposition theory to a more realistic scenario by\ncombining feature extraction and object segmentation. We replace pixel\ntranslation with feature translation and formulate object segmentation as\ngrouping features under the same transformation. We validated the proposed\nmethod on a practical dataset containing both real-world object and background.\nWe believe that our model will lead to a better understanding of human\ndevelopment of object recognition in the real world.\n","authors":["Takayuki Komatsu","Yoshiyuki Ohmura","Kayato Nishitsunoi","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2506.04668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04664v1","updated":"2025-06-05T06:18:48Z","published":"2025-06-05T06:18:48Z","title":"A Fast Unsupervised Scheme for Polygonal Approximation","summary":"  This paper proposes a fast and unsupervised scheme for a polygonal\napproximation of a closed digital curve. It is demonstrated that the\napproximation scheme is faster than state-of-the-art approximation and is\ncompetitive with the same in Rosin's measure and in its aesthetic aspect. The\nscheme comprises of three phases: initial segmentation, iterative vertex\ninsertion, and iterative merging, followed by vertex adjustment. The initial\nsegmentation is used to detect sharp turnings - the vertices that seemingly\nhave high curvature. It is likely that some of important vertices with low\ncurvature might have been missed out at the first phase and so iterative vertex\ninsertion is used to add vertices in a region where the curvature changes\nslowly but steadily. The initial phase may pick up some undesirable vertices\nand so merging is used to eliminate the redundant vertices. Finally, vertex\nadjustment is used to facilitate enhancement in the aesthetic look of the\napproximation. The quality of the approximations is measured using Rosin's\nmeasure. The robustness of the proposed scheme with respect to geometric\ntransformation is observed.\n","authors":["Bimal Kumar Ray"],"pdf_url":"https://arxiv.org/pdf/2506.04664v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.05334v1","updated":"2025-06-05T17:59:26Z","published":"2025-06-05T17:59:26Z","title":"Search Arena: Analyzing Search-Augmented LLMs","summary":"  Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.\n","authors":["Mihran Miroyan","Tsung-Han Wu","Logan King","Tianle Li","Jiayi Pan","Xinyan Hu","Wei-Lin Chiang","Anastasios N. Angelopoulos","Trevor Darrell","Narges Norouzi","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2506.05334v1.pdf","comment":"Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k"},{"id":"http://arxiv.org/abs/2406.05085v3","updated":"2025-06-05T15:57:36Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.\n","authors":["Maciej Besta","Ales Kubicek","Robert Gerstenberger","Marcin Chrapek","Roman Niggli","Patrik Okanovic","Yi Zhu","Patrick Iff","Michal Podstawski","Lucas Weitzendorf","Mingyuan Chi","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.05085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05182v1","updated":"2025-06-05T15:52:44Z","published":"2025-06-05T15:52:44Z","title":"On the Comprehensibility of Multi-structured Financial Documents using\n  LLMs and Pre-processing Tools","summary":"  The proliferation of complex structured data in hybrid sources, such as PDF\ndocuments and web pages, presents unique challenges for current Large Language\nModels (LLMs) and Multi-modal Large Language Models (MLLMs) in providing\naccurate answers. Despite the recent advancements of MLLMs, they still often\nfalter when interpreting intricately structured information, such as nested\ntables and multi-dimensional plots, leading to hallucinations and erroneous\noutputs. This paper explores the capabilities of LLMs and MLLMs in\nunderstanding and answering questions from complex data structures found in PDF\ndocuments by leveraging industrial and open-source tools as part of a\npre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,\nachieves an accuracy of 56% on multi-structured documents when fed documents\ndirectly, and that integrating pre-processing tools raises the accuracy of LLMs\nto 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is\npublicly available at https://github.com/OGCDS/FinancialQA.\n","authors":["Shivani Upadhyay","Messiah Ataey","Shariyar Murtuza","Yifan Nie","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2506.05182v1.pdf","comment":"15 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2506.05167v1","updated":"2025-06-05T15:43:49Z","published":"2025-06-05T15:43:49Z","title":"ECoRAG: Evidentiality-guided Compression for Long Context RAG","summary":"  Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\n\\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing\nretrieved documents based on evidentiality, ensuring whether answer generation\nis supported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.\n","authors":["Yeonseok Jeong","Jinsu Kim","Dohyeon Lee","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.05167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05154v1","updated":"2025-06-05T15:34:15Z","published":"2025-06-05T15:34:15Z","title":"Knowledgeable-r1: Policy Optimization for Knowledge Exploration in\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) is a mainstream method for improving\nperformance on knowledge-intensive tasks. However,current RAG systems often\nplace too much emphasis on retrieved contexts. This can lead to reliance on\ninaccurate sources and overlook the model's inherent knowledge, especially when\ndealing with misleading or excessive information. To resolve this imbalance, we\npropose Knowledgeable-r1 that using joint sampling and define multi policy\ndistributions in knowledge capability exploration to stimulate large language\nmodels'self-integrated utilization of parametric and contextual knowledge.\nExperiments show that Knowledgeable-r1 significantly enhances robustness and\nreasoning accuracy in both parameters and contextual conflict tasks and general\nRAG tasks, especially outperforming baselines by 17.07% in counterfactual\nscenarios and demonstrating consistent gains across RAG tasks. Our code are\navailable at https://github.com/lcy80366872/ knowledgeable-r1.\n","authors":["Chenyu Lin","Yilin Wen","Du Su","Fei Sun","Muhan Chen","Chenfu Bao","Zhonghou Lv"],"pdf_url":"https://arxiv.org/pdf/2506.05154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05069v1","updated":"2025-06-05T14:16:44Z","published":"2025-06-05T14:16:44Z","title":"Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance\n  LLM Recommendation","summary":"  Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D.\n","authors":["Keyu Zhao","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2506.05069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05044v1","updated":"2025-06-05T13:52:57Z","published":"2025-06-05T13:52:57Z","title":"Rethinking Contrastive Learning in Session-based Recommendation","summary":"  Session-based recommendation aims to predict intents of anonymous users based\non limited behaviors. With the ability in alleviating data sparsity,\ncontrastive learning is prevailing in the task. However, we spot that existing\ncontrastive learning based methods still suffer from three obstacles: (1) they\noverlook item-level sparsity and primarily focus on session-level sparsity; (2)\nthey typically augment sessions using item IDs like crop, mask and reorder,\nfailing to ensure the semantic consistency of augmented views; (3) they treat\nall positive-negative signals equally, without considering their varying\nutility. To this end, we propose a novel multi-modal adaptive contrastive\nlearning framework called MACL for session-based recommendation. In MACL, a\nmulti-modal augmentation is devised to generate semantically consistent views\nat both item and session levels by leveraging item multi-modal features.\nBesides, we present an adaptive contrastive loss that distinguishes varying\ncontributions of positive-negative signals to improve self-supervised learning.\nExtensive experiments on three real-world datasets demonstrate the superiority\nof MACL over state-of-the-art methods.\n","authors":["Xiaokun Zhang","Bo Xu","Fenglong Ma","Zhizheng Wang","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2506.05044v1.pdf","comment":"This work has been accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2506.04997v1","updated":"2025-06-05T13:06:01Z","published":"2025-06-05T13:06:01Z","title":"Towards Storage-Efficient Visual Document Retrieval: An Empirical Study\n  on Reducing Patch-Level Embeddings","summary":"  Despite the strong performance of ColPali/ColQwen2 in Visualized Document\nRetrieval (VDR), it encodes each page into multiple patch-level embeddings and\nleads to excessive memory usage. This empirical study investigates methods to\nreduce patch embeddings per page at minimum performance degradation. We\nevaluate two token-reduction strategies: token pruning and token merging.\nRegarding token pruning, we surprisingly observe that a simple random strategy\noutperforms other sophisticated pruning methods, though still far from\nsatisfactory. Further analysis reveals that pruning is inherently unsuitable\nfor VDR as it requires removing certain page embeddings without query-specific\ninformation. Turning to token merging (more suitable for VDR), we search for\nthe optimal combinations of merging strategy across three dimensions and\ndevelop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance\nwith only 11.8% of original memory usage, and preserves 94.6% effectiveness at\n2.8% memory footprint. We expect our empirical findings and resulting\nLight-ColPali/ColQwen2 offer valuable insights and establish a competitive\nbaseline for future research towards efficient VDR.\n","authors":["Yubo Ma","Jinsong Li","Yuhang Zang","Xiaobao Wu","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Haodong Duan","Jiaqi Wang","Yixin Cao","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2506.04997v1.pdf","comment":"Accepted by ACL 2025 findings"},{"id":"http://arxiv.org/abs/2506.04907v1","updated":"2025-06-05T11:41:05Z","published":"2025-06-05T11:41:05Z","title":"Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning\n  Blind Spots","summary":"  Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.\n","authors":["Alex Pan","Mary-Anne Williams"],"pdf_url":"https://arxiv.org/pdf/2506.04907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04790v1","updated":"2025-06-05T09:17:30Z","published":"2025-06-05T09:17:30Z","title":"LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff\n  Table","summary":"  Approximate nearest neighbor search (ANNS) is an essential building block for\napplications like RAG but can sometimes yield results that are overly similar\nto each other. In certain scenarios, search results should be similar to the\nquery and yet diverse. We propose LotusFilter, a post-processing module to\ndiversify ANNS results. We precompute a cutoff table summarizing vectors that\nare close to each other. During the filtering, LotusFilter greedily looks up\nthe table to delete redundant vectors from the candidates. We demonstrated that\nthe LotusFilter operates fast (0.02 [ms/query]) in settings resembling\nreal-world RAG applications, utilizing features such as OpenAI embeddings. Our\ncode is publicly available at https://github.com/matsui528/lotf.\n","authors":["Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2506.04790v1.pdf","comment":"CVPR 2025. GitHub: https://github.com/matsui528/lotf"},{"id":"http://arxiv.org/abs/2506.04762v1","updated":"2025-06-05T08:45:48Z","published":"2025-06-05T08:45:48Z","title":"GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner\n  for Query Expansion in Information Retrieval","summary":"  Large language models (LLMs)-based query expansion for information retrieval\naugments queries with generated hypothetical documents with LLMs. However, its\nperformance relies heavily on the scale of the language models (LMs),\nnecessitating larger, more advanced LLMs. This approach is costly,\ncomputationally intensive, and often has limited accessibility. To address\nthese limitations, we introduce GOLFer - Smaller LMs-Generated Documents\nHallucination Filter & Combiner - a novel method leveraging smaller open-source\nLMs for query expansion. GOLFer comprises two modules: a hallucination filter\nand a documents combiner. The former detects and removes non-factual and\ninconsistent sentences in generated documents, a common issue with smaller LMs,\nwhile the latter combines the filtered content with the query using a weight\nvector to balance their influence. We evaluate GOLFer alongside dominant\nLLM-based query expansion methods on three web search and ten low-resource\ndatasets. Experimental results demonstrate that GOLFer consistently outperforms\nother methods using smaller LMs, and maintains competitive performance against\nmethods using large-size LLMs, demonstrating its effectiveness.\n","authors":["Lingyuan Liu","Mengxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04760v1","updated":"2025-06-05T08:44:34Z","published":"2025-06-05T08:44:34Z","title":"Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using\n  Large Language Model-based Query Expansion","summary":"  Large Language Models (LLMs) have shown potential in generating hypothetical\ndocuments for query expansion, thereby enhancing information retrieval\nperformance. However, the efficacy of this method is highly dependent on the\nquality of the generated documents, which often requires complex prompt\nstrategies and the integration of advanced dense retrieval techniques. This can\nbe both costly and computationally intensive. To mitigate these limitations, we\nexplore the use of zero-shot LLM-based query expansion to improve sparse\nretrieval, particularly for learned sparse retrievers. We introduce a novel\nfusion ranking framework, Exp4Fuse, which enhances the performance of sparse\nretrievers through an indirect application of zero-shot LLM-based query\nexpansion. Exp4Fuse operates by simultaneously considering two retrieval\nroutes-one based on the original query and the other on the LLM-augmented\nquery. It then generates two ranked lists using a sparse retriever and fuses\nthem using a modified reciprocal rank fusion method. We conduct extensive\nevaluations of Exp4Fuse against leading LLM-based query expansion methods and\nadvanced retrieval techniques on three MS MARCO-related datasets and seven\nlow-resource datasets. Experimental results reveal that Exp4Fuse not only\nsurpasses existing LLM-based query expansion methods in enhancing sparse\nretrievers but also, when combined with advanced sparse retrievers, achieves\nSOTA results on several benchmarks. This highlights the superior performance\nand effectiveness of Exp4Fuse in improving query expansion for sparse\nretrieval.\n","authors":["Lingyuan Liu","Mengxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18470v4","updated":"2025-06-05T05:50:15Z","published":"2025-02-04T01:30:06Z","title":"Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World\n  Spatial Reasoning Questions","summary":"  Spatial reasoning remains a challenge for Large Language Models (LLMs), which\nstruggle with spatial data retrieval and reasoning. We propose Spatial\nRetrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to\nspatial tasks by integrating sparse spatial retrieval (spatial databases) and\ndense semantic retrieval (LLM-based similarity). A multi-objective ranking\nstrategy balances spatial constraints and semantic relevance, while an\nLLM-guided generator ensures coherent responses. Experiments on a real-world\ntourism dataset show that Spatial-RAG significantly improves spatial question\nanswering, bridging the gap between LLMs and spatial intelligence.\n","authors":["Dazhou Yu","Riyang Bao","Ruiyu Ning","Jinghong Peng","Gengchen Mai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18470v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04551v1","updated":"2025-06-05T01:57:36Z","published":"2025-06-05T01:57:36Z","title":"PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for\n  Recommender System Evaluation","summary":"  Traditional offline evaluation methods for recommender systems struggle to\ncapture the complexity of modern platforms due to sparse behavioural signals,\nnoisy data, and limited modelling of user personality traits. While simulation\nframeworks can generate synthetic data to address these gaps, existing methods\nfail to replicate behavioural diversity, limiting their effectiveness. To\novercome these challenges, we propose the Personality-driven User Behaviour\nSimulator (PUB), an LLM-based simulation framework that integrates the Big Five\npersonality traits to model personalised user behaviour. PUB dynamically infers\nuser personality from behavioural logs (e.g., ratings, reviews) and item\nmetadata, then generates synthetic interactions that preserve statistical\nfidelity to real-world data. Experiments on the Amazon review datasets show\nthat logs generated by PUB closely align with real user behaviour and reveal\nmeaningful associations between personality traits and recommendation outcomes.\nThese results highlight the potential of the personality-driven simulator to\nadvance recommender system evaluation, offering scalable, controllable,\nhigh-fidelity alternatives to resource-intensive real-world experiments.\n","authors":["Chenglong Ma","Ziqi Xu","Yongli Ren","Danula Hettiachchi","Jeffrey Chan"],"pdf_url":"https://arxiv.org/pdf/2506.04551v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2506.05345v1","updated":"2025-06-05T17:59:55Z","published":"2025-06-05T17:59:55Z","title":"Inference-Time Hyper-Scaling with KV Cache Compression","summary":"  Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.\n","authors":["Adrian Łańcucki","Konrad Staniszewski","Piotr Nawrot","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2506.05345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05346v1","updated":"2025-06-05T17:59:55Z","published":"2025-06-05T17:59:55Z","title":"Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets","summary":"  Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.\n","authors":["Lei Hsiung","Tianyu Pang","Yung-Chen Tang","Linyue Song","Tsung-Yi Ho","Pin-Yu Chen","Yaoqing Yang"],"pdf_url":"https://arxiv.org/pdf/2506.05346v1.pdf","comment":"Project Page: https://hsiung.cc/llm-similarity-risk/"},{"id":"http://arxiv.org/abs/2506.05340v1","updated":"2025-06-05T17:59:40Z","published":"2025-06-05T17:59:40Z","title":"Exploring Diffusion Transformer Designs via Grafting","summary":"  Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu\n","authors":["Keshigeyan Chandrasegaran","Michael Poli","Daniel Y. Fu","Dongjun Kim","Lea M. Hadzic","Manling Li","Agrim Gupta","Stefano Massaroli","Azalia Mirhoseini","Juan Carlos Niebles","Stefano Ermon","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2506.05340v1.pdf","comment":"22 pages; Project website: https://grafting.stanford.edu"},{"id":"http://arxiv.org/abs/2506.05334v1","updated":"2025-06-05T17:59:26Z","published":"2025-06-05T17:59:26Z","title":"Search Arena: Analyzing Search-Augmented LLMs","summary":"  Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.\n","authors":["Mihran Miroyan","Tsung-Han Wu","Logan King","Tianle Li","Jiayi Pan","Xinyan Hu","Wei-Lin Chiang","Anastasios N. Angelopoulos","Trevor Darrell","Narges Norouzi","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2506.05334v1.pdf","comment":"Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k"},{"id":"http://arxiv.org/abs/2506.05333v1","updated":"2025-06-05T17:59:24Z","published":"2025-06-05T17:59:24Z","title":"Kinetics: Rethinking Test-Time Scaling Laws","summary":"  We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.\n","authors":["Ranajoy Sadhukhan","Zhuoming Chen","Haizhong Zheng","Yang Zhou","Emma Strubell","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16400v3","updated":"2025-06-05T17:59:12Z","published":"2025-05-22T08:50:47Z","title":"AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning","summary":"  Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.\n","authors":["Yang Chen","Zhuolin Yang","Zihan Liu","Chankyu Lee","Peng Xu","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2505.16400v3.pdf","comment":"Add pass@1024 evaluation results for LiveCodeBench v6. We release the\n  models at:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485"},{"id":"http://arxiv.org/abs/2412.03782v3","updated":"2025-06-05T17:58:57Z","published":"2024-12-05T00:05:11Z","title":"The broader spectrum of in-context learning","summary":"  The ability of language models to learn a task from a few examples in context\nhas generated substantial interest. Here, we provide a perspective that\nsituates this type of supervised few-shot learning within a much broader\nspectrum of meta-learned in-context learning. Indeed, we suggest that any\ndistribution of sequences in which context non-trivially decreases loss on\nsubsequent predictions can be interpreted as eliciting a kind of in-context\nlearning. We suggest that this perspective helps to unify the broad set of\nin-context abilities that language models exhibit -- such as adapting to tasks\nfrom instructions or role play, or extrapolating time series. This perspective\nalso sheds light on potential roots of in-context learning in lower-level\nprocessing of linguistic dependencies (e.g. coreference or parallel\nstructures). Finally, taking this perspective highlights the importance of\ngeneralization, which we suggest can be studied along several dimensions: not\nonly the ability to learn something novel, but also flexibility in learning\nfrom different presentations, and in applying what is learned. We discuss\nbroader connections to past literature in meta-learning and goal-conditioned\nagents, and other perspectives on learning and adaptation. We close by\nsuggesting that research on in-context learning should consider this broader\nspectrum of in-context capabilities and types of generalization.\n","authors":["Andrew Kyle Lampinen","Stephanie C. Y. Chan","Aaditya K. Singh","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2412.03782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05329v1","updated":"2025-06-05T17:58:43Z","published":"2025-06-05T17:58:43Z","title":"Admissibility of Completely Randomized Trials: A Large-Deviation\n  Approach","summary":"  When an experimenter has the option of running an adaptive trial, is it\nadmissible to ignore this option and run a non-adaptive trial instead? We\nprovide a negative answer to this question in the best-arm identification\nproblem, where the experimenter aims to allocate measurement efforts\njudiciously to confidently deploy the most effective treatment arm. We find\nthat, whenever there are at least three treatment arms, there exist simple\nadaptive designs that universally and strictly dominate non-adaptive completely\nrandomized trials. This dominance is characterized by a notion called\nefficiency exponent, which quantifies a design's statistical efficiency when\nthe experimental sample is large. Our analysis focuses on the class of batched\narm elimination designs, which progressively eliminate underperforming arms at\npre-specified batch intervals. We characterize simple sufficient conditions\nunder which these designs universally and strictly dominate completely\nrandomized trials. These results resolve the second open problem posed in Qin\n[2022].\n","authors":["Guido Imbens","Chao Qin","Stefan Wager"],"pdf_url":"https://arxiv.org/pdf/2506.05329v1.pdf","comment":"A one-page abstract of this work will appear at the 26th ACM\n  Conference on Economics and Computation (EC'25)"},{"id":"http://arxiv.org/abs/2506.05325v1","updated":"2025-06-05T17:58:09Z","published":"2025-06-05T17:58:09Z","title":"Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via\n  Latent Alignment","summary":"  Quasiparticle interference (QPI) imaging is a powerful tool for probing\nelectronic structures in quantum materials, but extracting the single-scatterer\nQPI pattern (i.e., the kernel) from a multi-scatterer image remains a\nfundamentally ill-posed inverse problem. In this work, we propose the first\nAI-based framework for QPI kernel extraction. We introduce a two-step learning\nstrategy that decouples kernel representation learning from\nobservation-to-kernel inference. In the first step, we train a variational\nautoencoder to learn a compact latent space of scattering kernels. In the\nsecond step, we align the latent representation of QPI observations with those\nof the pre-learned kernels using a dedicated encoder. This design enables the\nmodel to infer kernels robustly even under complex, entangled scattering\nconditions. We construct a diverse and physically realistic QPI dataset\ncomprising 100 unique kernels and evaluate our method against a direct one-step\nbaseline. Experimental results demonstrate that our approach achieves\nsignificantly higher extraction accuracy, and improved generalization to unseen\nkernels.\n","authors":["Yingshuai Ji","Haomin Zhuang","Matthew Toole","James McKenzie","Xiaolong Liu","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05321v1","updated":"2025-06-05T17:57:11Z","published":"2025-06-05T17:57:11Z","title":"LSM-2: Learning from Incomplete Wearable Sensor Data","summary":"  Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications.\n","authors":["Maxwell A. Xu","Girish Narayanswamy","Kumar Ayush","Dimitris Spathis","Shun Liao","Shyam A. Tailor","Ahmed Metwally","A. Ali Heydari","Yuwei Zhang","Jake Garrison","Samy Abdel-Ghaffar","Xuhai Xu","Ken Gu","Jacob Sunshine","Ming-Zher Poh","Yun Liu","Tim Althoff","Shrikanth Narayanan","Pushmeet Kohli","Mark Malhotra","Shwetak Patel","Yuzhe Yang","James M. Rehg","Xin Liu","Daniel McDuff"],"pdf_url":"https://arxiv.org/pdf/2506.05321v1.pdf","comment":"Xu and Narayanswamy are co-first authors. McDuff and Liu are co-last\n  authors"},{"id":"http://arxiv.org/abs/2506.05320v1","updated":"2025-06-05T17:57:08Z","published":"2025-06-05T17:57:08Z","title":"Generalizable, real-time neural decoding with hybrid state-space models","summary":"  Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications.\n","authors":["Avery Hee-Woon Ryoo","Nanda H. Krishna","Ximeng Mao","Mehdi Azabou","Eva L. Dyer","Matthew G. Perich","Guillaume Lajoie"],"pdf_url":"https://arxiv.org/pdf/2506.05320v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2506.05316v1","updated":"2025-06-05T17:55:43Z","published":"2025-06-05T17:55:43Z","title":"Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay","summary":"  Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.\n","authors":["Yifan Sun","Jingyan Shen","Yibin Wang","Tianyu Chen","Zhendong Wang","Mingyuan Zhou","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05314v1","updated":"2025-06-05T17:55:23Z","published":"2025-06-05T17:55:23Z","title":"Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models","summary":"  Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.\n","authors":["Taha Entesari","Arman Hatami","Rinat Khaziev","Anil Ramakrishna","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2506.05314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05310v1","updated":"2025-06-05T17:53:57Z","published":"2025-06-05T17:53:57Z","title":"Learning normalized image densities via dual score matching","summary":"  Learning probability models from data is at the heart of many machine\nlearning endeavors, but is notoriously difficult due to the curse of\ndimensionality. We introduce a new framework for learning \\emph{normalized}\nenergy (log probability) models that is inspired from diffusion generative\nmodels, which rely on networks optimized to estimate the score. We modify a\nscore network architecture to compute an energy while preserving its inductive\nbiases. The gradient of this energy network with respect to its input image is\nthe score of the learned density, which can be optimized using a denoising\nobjective. Importantly, the gradient with respect to the noise level provides\nan additional score that can be optimized with a novel secondary objective,\nensuring consistent and normalized energies across noise levels. We train an\nenergy network with this \\emph{dual} score matching objective on the ImageNet64\ndataset, and obtain a cross-entropy (negative log likelihood) value comparable\nto the state of the art. We further validate our approach by showing that our\nenergy model \\emph{strongly generalizes}: estimated log probabilities are\nnearly independent of the specific images in the training set. Finally, we\ndemonstrate that both image probability and dimensionality of local\nneighborhoods vary significantly with image content, in contrast with\ntraditional assumptions such as concentration of measure or support on a\nlow-dimensional manifold.\n","authors":["Florentin Guth","Zahra Kadkhodaie","Eero P Simoncelli"],"pdf_url":"https://arxiv.org/pdf/2506.05310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05305v1","updated":"2025-06-05T17:52:30Z","published":"2025-06-05T17:52:30Z","title":"ProRefine: Inference-time Prompt Refinement with Textual Feedback","summary":"  Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.\n","authors":["Deepak Pandita","Tharindu Cyril Weerasooriya","Ankit Parag Shah","Christopher M. Homan","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2506.05305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05300v1","updated":"2025-06-05T17:50:32Z","published":"2025-06-05T17:50:32Z","title":"Power Law Guided Dynamic Sifting for Efficient Attention","summary":"  Efficient inference on GPUs using large language models remains challenging\ndue to memory bandwidth limitations, particularly during data transfers between\nHigh Bandwidth Memory (HBM) and SRAM in attention computations. Approximate\nattention methods address this issue by reducing computational and memory\noverhead but often rely on expensive top-$k$ operations, which perform poorly\non GPUs. We propose SiftAttention, a novel approximate attention method that\nreplaces the top-$k$ step with a computationally efficient element-wise\nfiltering operation based on a threshold value. Our intuition for doing this is\nbased on our empirical observation that the $\\tau$-th quantile of attention\nscores follows a predictable power-law over sequential generation steps.\nExploiting this insight, our approach dynamically estimates a threshold value\nper prompt at each generation step. Only attention scores above this threshold\nand their corresponding value vectors are loaded/used to compute the attention\noutput, reducing data movement between HBM and SRAM. Our evaluation\ndemonstrates that SiftAttention preserves model quality better than existing\napproximate attention methods while reducing memory bandwidth usage when\nloading value vectors.\n","authors":["Nirav Koley","Prajwal Singhania","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2506.05300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05296v1","updated":"2025-06-05T17:48:39Z","published":"2025-06-05T17:48:39Z","title":"Control Tax: The Price of Keeping AI in Check","summary":"  The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.\n","authors":["Mikhail Terekhov","Zhen Ning David Liu","Caglar Gulcehre","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2506.05296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05295v1","updated":"2025-06-05T17:48:19Z","published":"2025-06-05T17:48:19Z","title":"Sample Complexity and Representation Ability of Test-time Scaling\n  Paradigms","summary":"  Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.\n","authors":["Baihe Huang","Shanda Li","Tianhao Wu","Yiming Yang","Ameet Talwalkar","Kannan Ramchandran","Michael I. Jordan","Jiantao Jiao"],"pdf_url":"https://arxiv.org/pdf/2506.05295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05294v1","updated":"2025-06-05T17:47:40Z","published":"2025-06-05T17:47:40Z","title":"A Smooth Sea Never Made a Skilled $\\texttt{SAILOR}$: Robust Imitation\n  via Learning to Search","summary":"  The fundamental limitation of the behavioral cloning (BC) approach to\nimitation learning is that it only teaches an agent what the expert did at\nstates the expert visited. This means that when a BC agent makes a mistake\nwhich takes them out of the support of the demonstrations, they often don't\nknow how to recover from it. In this sense, BC is akin to giving the agent the\nfish -- giving them dense supervision across a narrow set of states -- rather\nthan teaching them to fish: to be able to reason independently about achieving\nthe expert's outcome even when faced with unseen situations at test-time. In\nresponse, we explore learning to search (L2S) from expert demonstrations, i.e.\nlearning the components required to, at test time, plan to match expert\noutcomes, even after making a mistake. These include (1) a world model and (2)\na reward model. We carefully ablate the set of algorithmic and design decisions\nrequired to combine these and other components for stable and\nsample/interaction-efficient learning of recovery behavior without additional\nhuman corrections. Across a dozen visual manipulation tasks from three\nbenchmarks, our approach $\\texttt{SAILOR}$ consistently out-performs\nstate-of-the-art Diffusion Policies trained via BC on the same data.\nFurthermore, scaling up the amount of demonstrations used for BC by\n5-10$\\times$ still leaves a performance gap. We find that $\\texttt{SAILOR}$ can\nidentify nuanced failures and is robust to reward hacking. Our code is\navailable at https://github.com/arnavkj1995/SAILOR .\n","authors":["Arnav Kumar Jain","Vibhakar Mohta","Subin Kim","Atiksh Bhardwaj","Juntao Ren","Yunhai Feng","Sanjiban Choudhury","Gokul Swamy"],"pdf_url":"https://arxiv.org/pdf/2506.05294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05292v1","updated":"2025-06-05T17:46:07Z","published":"2025-06-05T17:46:07Z","title":"Learning Beyond Experience: Generalizing to Unseen State Space with\n  Reservoir Computing","summary":"  Machine learning techniques offer an effective approach to modeling dynamical\nsystems solely from observed data. However, without explicit structural priors\n-- built-in assumptions about the underlying dynamics -- these techniques\ntypically struggle to generalize to aspects of the dynamics that are poorly\nrepresented in the training data. Here, we demonstrate that reservoir computing\n-- a simple, efficient, and versatile machine learning framework often used for\ndata-driven modeling of dynamical systems -- can generalize to unexplored\nregions of state space without explicit structural priors. First, we describe a\nmultiple-trajectory training scheme for reservoir computers that supports\ntraining across a collection of disjoint time series, enabling effective use of\navailable training data. Then, applying this training scheme to multistable\ndynamical systems, we show that RCs trained on trajectories from a single basin\nof attraction can achieve out-of-domain generalization by capturing system\nbehavior in entirely unobserved basins.\n","authors":["Declan A. Norton","Yuanzhao Zhang","Michelle Girvan"],"pdf_url":"https://arxiv.org/pdf/2506.05292v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.05286v1","updated":"2025-06-05T17:43:27Z","published":"2025-06-05T17:43:27Z","title":"Stable Vision Concept Transformers for Medical Diagnosis","summary":"  Transparency is a paramount concern in the medical field, prompting\nresearchers to delve into the realm of explainable AI (XAI). Among these XAI\nmethods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent\nspace to human-understandable high-level concepts by generating a conceptual\nlayer for extracting conceptual features, which has drawn much attention\nrecently. However, existing methods rely solely on concept features to\ndetermine the model's predictions, which overlook the intrinsic feature\nembeddings within medical images. To address this utility gap between the\noriginal models and concept-based models, we propose Vision Concept Transformer\n(VCT). Furthermore, despite their benefits, CBMs have been found to negatively\nimpact model performance and fail to provide stable explanations when faced\nwith input perturbations, which limits their application in the medical field.\nTo address this faithfulness issue, this paper further proposes the Stable\nVision Concept Transformer (SVCT) based on VCT, which leverages the vision\ntransformer (ViT) as its backbone and incorporates a conceptual layer. SVCT\nemploys conceptual features to enhance decision-making capabilities by fusing\nthem with image features and ensures model faithfulness through the integration\nof Denoised Diffusion Smoothing. Comprehensive experiments on four medical\ndatasets demonstrate that our VCT and SVCT maintain accuracy while remaining\ninterpretable compared to baselines. Furthermore, even when subjected to\nperturbations, our SVCT model consistently provides faithful explanations, thus\nmeeting the needs of the medical field.\n","authors":["Lijie Hu","Songning Lai","Yuan Hua","Shu Yang","Jingfeng Zhang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05286v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.06129 by other authors"},{"id":"http://arxiv.org/abs/2506.05281v1","updated":"2025-06-05T17:35:46Z","published":"2025-06-05T17:35:46Z","title":"Fast-DataShapley: Neural Modeling for Training Data Valuation","summary":"  The value and copyright of training data are crucial in the artificial\nintelligence industry. Service platforms should protect data providers'\nlegitimate rights and fairly reward them for their contributions. Shapley\nvalue, a potent tool for evaluating contributions, outperforms other methods in\ntheory, but its computational overhead escalates exponentially with the number\nof data providers. Recent works based on Shapley values attempt to mitigate\ncomputation complexity by approximation algorithms. However, they need to\nretrain for each test sample, leading to intolerable costs. We propose\nFast-DataShapley, a one-pass training method that leverages the weighted least\nsquares characterization of the Shapley value to train a reusable explainer\nmodel with real-time reasoning speed. Given new test samples, no retraining is\nrequired to calculate the Shapley values of the training data. Additionally, we\npropose three methods with theoretical guarantees to reduce training overhead\nfrom two aspects: the approximate calculation of the utility function and the\ngroup calculation of the training data. We analyze time complexity to show the\nefficiency of our methods. The experimental evaluations on various image\ndatasets demonstrate superior performance and efficiency compared to baselines.\nSpecifically, the performance is improved to more than 2.5 times, and the\nexplainer's training speed can be increased by two orders of magnitude.\n","authors":["Haifeng Sun","Yu Xiong","Runze Wu","Xinyu Cai","Changjie Fan","Lan Zhang","Xiang-Yang Li"],"pdf_url":"https://arxiv.org/pdf/2506.05281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05276v1","updated":"2025-06-05T17:32:00Z","published":"2025-06-05T17:32:00Z","title":"How to Unlock Time Series Editing? Diffusion-Driven Approach with\n  Multi-Grained Control","summary":"  Recent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing\n(TSE) - making precise modifications while preserving temporal coherence -\nconsider both point-level constraints and segment-level controls that current\nmethods struggle to provide. We introduce the CocktailEdit framework to enable\nsimultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted anchor control for\npoint-wise constraints and a classifier-based control for managing statistical\nproperties such as sums and averages over segments. Our methods achieve precise\nlocal control during the denoising inference stage while maintaining temporal\ncoherence and integrating seamlessly, with any conditionally trained\ndiffusion-based time series models. Extensive experiments across diverse\ndatasets and models demonstrate its effectiveness. Our work bridges the gap\nbetween pure generative modeling and real-world time series editing needs,\noffering a flexible solution for human-in-the-loop time series generation and\nediting. The code and demo are provided for validation.\n","authors":["Hao Yu","Chu Xin Cheng","Runlong Yu","Yuyang Ye","Shiwei Tong","Zhaofeng Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2506.05276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05271v1","updated":"2025-06-05T17:30:18Z","published":"2025-06-05T17:30:18Z","title":"Tight analyses of first-order methods with error feedback","summary":"  Communication between agents often constitutes a major computational\nbottleneck in distributed learning. One of the most common mitigation\nstrategies is to compress the information exchanged, thereby reducing\ncommunication overhead. To counteract the degradation in convergence associated\nwith compressed communication, error feedback schemes -- most notably\n$\\mathrm{EF}$ and $\\mathrm{EF}^{21}$ -- were introduced. In this work, we\nprovide a tight analysis of both of these methods. Specifically, we find the\nLyapunov function that yields the best possible convergence rate for each\nmethod -- with matching lower bounds. This principled approach yields sharp\nperformance guarantees and enables a rigorous, apples-to-apples comparison\nbetween $\\mathrm{EF}$, $\\mathrm{EF}^{21}$, and compressed gradient descent. Our\nanalysis is carried out in a simplified yet representative setting, which\nallows for clean theoretical insights and fair comparison of the underlying\nmechanisms.\n","authors":["Daniel Berg Thomsen","Adrien Taylor","Aymeric Dieuleveut"],"pdf_url":"https://arxiv.org/pdf/2506.05271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18959v4","updated":"2025-06-05T17:25:49Z","published":"2024-10-24T17:56:08Z","title":"Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information","summary":"  Forecasting is a critical task in decision-making across numerous domains.\nWhile historical numerical data provide a start, they fail to convey the\ncomplete context for reliable and accurate predictions. Human forecasters\nfrequently rely on additional information, such as background knowledge and\nconstraints, which can efficiently be communicated through natural language.\nHowever, in spite of recent progress with LLM-based forecasters, their ability\nto effectively integrate this textual information remains an open question. To\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\nbenchmark that pairs numerical data with diverse types of carefully crafted\ntextual context, requiring models to integrate both modalities; crucially,\nevery task in CiK requires understanding textual context to be solved\nsuccessfully. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. This\nbenchmark aims to advance multimodal forecasting by promoting models that are\nboth accurate and accessible to decision-makers with varied technical\nexpertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.\n","authors":["Andrew Robert Williams","Arjun Ashok","Étienne Marcotte","Valentina Zantedeschi","Jithendaraa Subramanian","Roland Riachi","James Requeima","Alexandre Lacoste","Irina Rish","Nicolas Chapados","Alexandre Drouin"],"pdf_url":"https://arxiv.org/pdf/2410.18959v4.pdf","comment":"ICML 2025. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2506.05259v1","updated":"2025-06-05T17:20:39Z","published":"2025-06-05T17:20:39Z","title":"Learning long range dependencies through time reversal symmetry breaking","summary":"  Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,\nas RNNs could natively be embodied into dynamical systems. This calls for\ndedicated learning algorithms obeying to core physical principles, with\nefficient techniques to simulate these systems and guide their design. We\npropose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably\ncomputes loss gradients as finite differences of physical trajectories of\nnon-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three\n\"forward passes\" irrespective of model size, without explicit Jacobian\ncomputation, nor incurring any variance in the gradient estimation. Motivated\nby the physical realization of our algorithm, we first introduce RHEL in\ncontinuous time and demonstrate its formal equivalence with the continuous\nadjoint state method. To facilitate the simulation of Hamiltonian systems\ntrained by RHEL, we propose a discrete-time version of RHEL which is equivalent\nto Backpropagation Through Time (BPTT) when applied to a class of recurrent\nmodules which we call Hamiltonian Recurrent Units (HRUs). This setting allows\nus to demonstrate the scalability of RHEL by generalizing these results to\nhierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to\ntrain HSSMs with linear and nonlinear dynamics on a variety of time-series\ntasks ranging from mid-range to long-range classification and regression with\nsequence length reaching $\\sim 50k$. We show that RHEL consistently matches the\nperformance of BPTT across all models and tasks. This work opens new doors for\nthe design of scalable, energy-efficient physical systems endowed with\nself-learning capabilities for sequence modelling.\n","authors":["Guillaume Pourcel","Maxence Ernoult"],"pdf_url":"https://arxiv.org/pdf/2506.05259v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2405.01114v4","updated":"2025-06-05T17:17:51Z","published":"2024-05-02T09:22:54Z","title":"Continual Learning from Simulated Interactions via Multitask Prospective\n  Rehearsal for Bionic Limb Behavior Modeling","summary":"  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. While\nmotorized bionic limbs show promise, their effectiveness depends on replicating\nthe dynamic coordination of human movement across diverse environments. In this\npaper, we introduce a model for human behavior in the context of bionic\nprosthesis control. Our approach leverages human locomotion demonstrations to\nlearn the synergistic coupling of the lower limbs, enabling the prediction of\nthe kinematic behavior of a missing limb during tasks such as walking, climbing\ninclines, and stairs. We propose a multitasking, continually adaptive model\nthat anticipates and refines movements over time. At the core of our method is\na technique called multitask prospective rehearsal, that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. Our evolving architecture\nmerges lightweight, task-specific modules on a shared backbone, ensuring both\nspecificity and scalability. We validate our model through experiments on\nreal-world human gait datasets, including transtibial amputees, across a wide\nrange of locomotion tasks. Results demonstrate that our approach consistently\noutperforms baseline models, particularly in scenarios with distributional\nshifts, adversarial perturbations, and noise.\n","authors":["Sharmita Dey","Benjamin Paassen","Sarath Ravindran Nair","Sabri Boughorbel","Arndt F. Schilling"],"pdf_url":"https://arxiv.org/pdf/2405.01114v4.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR) 2025"},{"id":"http://arxiv.org/abs/2506.05256v1","updated":"2025-06-05T17:17:05Z","published":"2025-06-05T17:17:05Z","title":"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties\n  Reinforcement Learning","summary":"  Large reasoning models (LRMs) achieve higher performance on challenging\nreasoning tasks by generating more tokens at inference time, but this verbosity\noften wastes computation on easy problems. Existing solutions, including\nsupervised finetuning on shorter traces, user-controlled budgets, or RL with\nuniform penalties, either require data curation, manual configuration, or treat\nall problems alike regardless of difficulty. We introduce Adaptive Length\nPenalty (ALP), a reinforcement learning objective tailoring generation length\nto per-prompt solve rate. During training, ALP monitors each prompt's online\nsolve rate through multiple rollouts and adds a differentiable penalty whose\nmagnitude scales inversely with that rate, so confident (easy) prompts incur a\nhigh cost for extra tokens while hard prompts remain unhindered. Posttraining\nDeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly\ndropping performance. Relative to fixed-budget and uniform penalty baselines,\nALP redistributes its reduced budget more intelligently by cutting compute on\neasy prompts and reallocating saved tokens to difficult ones, delivering higher\naccuracy on the hardest problems with higher cost.\n","authors":["Violet Xiang","Chase Blagden","Rafael Rafailov","Nathan Lile","Sang Truong","Chelsea Finn","Nick Haber"],"pdf_url":"https://arxiv.org/pdf/2506.05256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05252v1","updated":"2025-06-05T17:13:59Z","published":"2025-06-05T17:13:59Z","title":"Conservative classifiers do consistently well with improving agents:\n  characterizing statistical and online learning","summary":"  Machine learning is now ubiquitous in societal decision-making, for example\nin evaluating job candidates or loan applications, and it is increasingly\nimportant to take into account how classified agents will react to the learning\nalgorithms. The majority of recent literature on strategic classification has\nfocused on reducing and countering deceptive behaviors by the classified\nagents, but recent work of Attias et al. identifies surprising properties of\nlearnability when the agents genuinely improve in order to attain the desirable\nclassification, such as smaller generalization error than standard\nPAC-learning. In this paper we characterize so-called learnability with\nimprovements across multiple new axes. We introduce an asymmetric variant of\nminimally consistent concept classes and use it to provide an exact\ncharacterization of proper learning with improvements in the realizable\nsetting. While prior work studies learnability only under general, arbitrary\nagent improvement regions, we give positive results for more natural Euclidean\nball improvement sets. In particular, we characterize improper learning under a\nmild generative assumption on the data distribution. We further show how to\nlearn in more challenging settings, achieving lower generalization error under\nwell-studied bounded noise models and obtaining mistake bounds in realizable\nand agnostic online learning. We resolve open questions posed by Attias et al.\nfor both proper and improper learning.\n","authors":["Dravyansh Sharma","Alec Sun"],"pdf_url":"https://arxiv.org/pdf/2506.05252v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2506.05249v1","updated":"2025-06-05T17:10:22Z","published":"2025-06-05T17:10:22Z","title":"On the Convergence of Gradient Descent on Learning Transformers with\n  Residual Connections","summary":"  Transformer models have emerged as fundamental tools across various\nscientific and engineering disciplines, owing to their outstanding performance\nin diverse applications. Despite this empirical success, the theoretical\nfoundations of Transformers remain relatively underdeveloped, particularly in\nunderstanding their training dynamics. Existing research predominantly examines\nisolated components--such as self-attention mechanisms and feedforward\nnetworks--without thoroughly investigating the interdependencies between these\ncomponents, especially when residual connections are present. In this paper, we\naim to bridge this gap by analyzing the convergence behavior of a structurally\ncomplete yet single-layer Transformer, comprising self-attention, a feedforward\nnetwork, and residual connections. We demonstrate that, under appropriate\ninitialization, gradient descent exhibits a linear convergence rate, where the\nconvergence speed is determined by the minimum and maximum singular values of\nthe output matrix from the attention layer. Moreover, our analysis reveals that\nresidual connections serve to ameliorate the ill-conditioning of this output\nmatrix, an issue stemming from the low-rank structure imposed by the softmax\noperation, thereby promoting enhanced optimization stability. We also extend\nour theoretical findings to a multi-layer Transformer architecture, confirming\nthe linear convergence rate of gradient descent under suitable initialization.\nEmpirical results corroborate our theoretical insights, illustrating the\nbeneficial role of residual connections in promoting convergence stability.\n","authors":["Zhen Qin","Jinxin Zhou","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.05249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04075v2","updated":"2025-06-05T17:09:08Z","published":"2025-05-07T02:26:17Z","title":"Rethinking LLM Advancement: Compute-Dependent and Independent Paths to\n  Progress","summary":"  Regulatory efforts to govern large language model (LLM) development have\npredominantly focused on restricting access to high-performance computational\nresources. This study evaluates the efficacy of such measures by examining\nwhether LLM capabilities can advance through algorithmic innovation in\ncompute-constrained environments. We propose a novel framework distinguishing\ncompute-dependent innovations--which yield disproportionate benefits at high\ncompute--from compute-independent innovations, which improve efficiency across\ncompute scales. The impact is quantified using Compute-Equivalent Gain (CEG).\nExperimental validation with nanoGPT models confirms that compute-independent\nadvancements yield significant performance gains (e.g., with combined CEG up to\n$3.5\\times$) across the tested scales. In contrast, compute-dependent\nadvancements were detrimental to performance at smaller experimental scales,\nbut showed improved CEG (on par with the baseline) as model size increased, a\ntrend consistent with their definition of yielding primary benefits at higher\ncompute. Crucially, these findings indicate that restrictions on computational\nhardware, while potentially slowing LLM progress, are insufficient to prevent\nall capability gains driven by algorithmic advancements. We argue that\neffective AI oversight must therefore incorporate mechanisms for understanding,\nanticipating, and potentially guiding algorithmic research, moving beyond a\nsingular focus on hardware. The proposed framework also serves as an analytical\ntool for forecasting AI progress.\n","authors":["Jack Sanderson","Teddy Foley","Spencer Guo","Anqi Qu","Henry Josephson"],"pdf_url":"https://arxiv.org/pdf/2505.04075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19158v3","updated":"2025-06-05T17:06:46Z","published":"2025-01-31T14:21:02Z","title":"A theoretical framework for overfitting in energy-based modeling","summary":"  We investigate the impact of limited data on training pairwise energy-based\nmodels for inverse problems aimed at identifying interaction networks.\nUtilizing the Gaussian model as testbed, we dissect training trajectories\nacross the eigenbasis of the coupling matrix, exploiting the independent\nevolution of eigenmodes and revealing that the learning timescales are tied to\nthe spectral decomposition of the empirical covariance matrix. We see that\noptimal points for early stopping arise from the interplay between these\ntimescales and the initial conditions of training. Moreover, we show that\nfinite data corrections can be accurately modeled through asymptotic random\nmatrix theory calculations and provide the counterpart of generalized\ncross-validation in the energy based model context. Our analytical framework\nextends to binary-variable maximum-entropy pairwise models with minimal\nvariations. These findings offer strategies to control overfitting in\ndiscrete-variable models through empirical shrinkage corrections, improving the\nmanagement of overfitting in energy-based generative models. Finally, we\npropose a generalization to arbitrary energy-based models by deriving the\nneural tangent kernel dynamics of the score function under the score-matching\nalgorithm.\n","authors":["Giovanni Catania","Aurélien Decelle","Cyril Furtlehner","Beatriz Seoane"],"pdf_url":"https://arxiv.org/pdf/2501.19158v3.pdf","comment":"29 pages, 20 figures (including appendix). Accepted at Proceedings of\n  the 42nd International Conference on Machine Learning, Vancouver, Canada.\n  PMLR 267, 2025"},{"id":"http://arxiv.org/abs/2506.05245v1","updated":"2025-06-05T17:03:42Z","published":"2025-06-05T17:03:42Z","title":"Robust Moment Identification for Nonlinear PDEs via a Neural ODE\n  Approach","summary":"  We propose a data-driven framework for learning reduced-order moment dynamics\nfrom PDE-governed systems using Neural ODEs. In contrast to derivative-based\nmethods like SINDy, which necessitate densely sampled data and are sensitive to\nnoise, our approach based on Neural ODEs directly models moment trajectories,\nenabling robust learning from sparse and potentially irregular time series.\nUsing as an application platform the nonlinear Schr\\\"{o}dinger equation, the\nframework accurately recovers governing moment dynamics when closure is\navailable, even with limited and irregular observations. For systems without\nanalytical closure, we introduce a data-driven coordinate transformation\nstrategy based on Stiefel manifold optimization, enabling the discovery of\nlow-dimensional representations in which the moment dynamics become closed,\nfacilitating interpretable and reliable modeling. We also explore cases where a\nclosure model is not known, such as a Fisher-KPP reaction-diffusion system.\nHere we demonstrate that Neural ODEs can still effectively approximate the\nunclosed moment dynamics and achieve superior extrapolation accuracy compared\nto physical-expert-derived ODE models. This advantage remains robust even under\nsparse and irregular sampling, highlighting the method's robustness in\ndata-limited settings. Our results highlight the Neural ODE framework as a\npowerful and flexible tool for learning interpretable, low-dimensional moment\ndynamics in complex PDE-governed systems.\n","authors":["Shaoxuan Chen","Su Yang","Panayotis G. Kevrekidis","Wei Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.05245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05240v1","updated":"2025-06-05T16:59:53Z","published":"2025-06-05T16:59:53Z","title":"Aligning Latent Spaces with Flow Priors","summary":"  This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.\n","authors":["Yizhuo Li","Yuying Ge","Yixiao Ge","Ying Shan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2506.05240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05239v1","updated":"2025-06-05T16:57:58Z","published":"2025-06-05T16:57:58Z","title":"Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit","summary":"  Sparse autoencoders (SAEs) have recently become central tools for\ninterpretability, leveraging dictionary learning principles to extract sparse,\ninterpretable features from neural representations whose underlying structure\nis typically unknown. This paper evaluates SAEs in a controlled setting using\nMNIST, which reveals that current shallow architectures implicitly rely on a\nquasi-orthogonality assumption that limits the ability to extract correlated\nfeatures. To move beyond this, we introduce a multi-iteration SAE by unrolling\nMatching Pursuit (MP-SAE), enabling the residual-guided extraction of\ncorrelated features that arise in hierarchical settings such as handwritten\ndigit generation while guaranteeing monotonic improvement of the reconstruction\nas more atoms are selected.\n","authors":["Valérie Costa","Thomas Fel","Ekdeep Singh Lubana","Bahareh Tolooshams","Demba Ba"],"pdf_url":"https://arxiv.org/pdf/2506.05239v1.pdf","comment":"Complementary work to arXiv:2506.03093"},{"id":"http://arxiv.org/abs/2505.24835v2","updated":"2025-06-05T16:57:09Z","published":"2025-05-30T17:36:45Z","title":"Timing is Important: Risk-aware Fund Allocation based on Time-Series\n  Forecasting","summary":"  Fund allocation has been an increasingly important problem in the financial\ndomain. In reality, we aim to allocate the funds to buy certain assets within a\ncertain future period. Naive solutions such as prediction-only or\nPredict-then-Optimize approaches suffer from goal mismatch. Additionally, the\nintroduction of the SOTA time series forecasting model inevitably introduces\nadditional uncertainty in the predicted result. To solve both problems\nmentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate\n(RTS-PnO) framework, which holds no prior assumption on the forecasting models.\nSuch a framework contains three features: (i) end-to-end training with\nobjective alignment measurement, (ii) adaptive forecasting uncertainty\ncalibration, and (iii) agnostic towards forecasting models. The evaluation of\nRTS-PnO is conducted over both online and offline experiments. For offline\nexperiments, eight datasets from three categories of financial applications are\nused: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other\ncompetitive baselines. The online experiment is conducted on the Cross-Border\nPayment business at FiT, Tencent, and an 8.4\\% decrease in regret is witnessed\nwhen compared with the product-line approach. The code for the offline\nexperiment is available at https://github.com/fuyuanlyu/RTS-PnO.\n","authors":["Fuyuan Lyu","Linfeng Du","Yunpeng Weng","Qiufang Ying","Zhiyan Xu","Wen Zou","Haolun Wu","Xiuqiang He","Xing Tang"],"pdf_url":"https://arxiv.org/pdf/2505.24835v2.pdf","comment":"Accepted by KDD 2025 ADS Track"},{"id":"http://arxiv.org/abs/2502.00921v2","updated":"2025-06-05T16:55:06Z","published":"2025-02-02T21:19:53Z","title":"Blink of an eye: a simple theory for feature localization in generative\n  models","summary":"  Large language models can exhibit unexpected behavior in the blink of an eye.\nIn a recent computer use demo, a language model switched from coding to\nGoogling pictures of Yellowstone, and these sudden shifts in behavior have also\nbeen observed in reasoning patterns and jailbreaks. This phenomenon is not\nunique to autoregressive models: in diffusion models, key features of the final\noutput are decided in narrow ``critical windows'' of the generation process. In\nthis work we develop a simple, unifying theory to explain this phenomenon using\nthe formalism of stochastic localization samplers. We show that it emerges\ngenerically as the generation process localizes to a sub-population of the\ndistribution it models.\n  While critical windows have been studied at length in diffusion models,\nexisting theory heavily relies on strong distributional assumptions and the\nparticulars of Gaussian diffusion. In contrast to existing work our theory (1)\napplies to autoregressive and diffusion models; (2) makes no distributional\nassumptions; (3) quantitatively improves previous bounds even when specialized\nto diffusions; and (4) requires basic tools and no stochastic calculus or\nstatistical-physics-based machinery. We also identify an intriguing connection\nto the all-or-nothing phenomenon from statistical inference. Finally, we\nvalidate our predictions empirically for LLMs and find that critical windows\noften coincide with failures in problem solving for various math and reasoning\nbenchmarks.\n","authors":["Marvin Li","Aayush Karan","Sitan Chen"],"pdf_url":"https://arxiv.org/pdf/2502.00921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19136v2","updated":"2025-06-05T16:54:25Z","published":"2025-03-24T20:47:51Z","title":"Stochastic Poisson Surface Reconstruction with One Solve using Geometric\n  Gaussian Processes","summary":"  Poisson Surface Reconstruction is a widely-used algorithm for reconstructing\na surface from an oriented point cloud. To facilitate applications where only\npartial surface information is available, or scanning is performed\nsequentially, a recent line of work proposes to incorporate uncertainty into\nthe reconstructed surface via Gaussian process models. The resulting algorithms\nfirst perform Gaussian process interpolation, then solve a set of volumetric\npartial differential equations globally in space, resulting in a\ncomputationally expensive two-stage procedure. In this work, we apply\nrecently-developed techniques from geometric Gaussian processes to combine\ninterpolation and surface reconstruction into a single stage, requiring only\none linear solve per sample. The resulting reconstructed surface samples can be\nqueried locally in space, without the use of problem-dependent volumetric\nmeshes or grids. These capabilities enable one to (a) perform probabilistic\ncollision detection locally around the region of interest, (b) perform ray\ncasting without evaluating points not on the ray's trajectory, and (c) perform\nnext-view planning on a per-ray basis. They also do not requiring one to\napproximate kernel matrix inverses with diagonal matrices as part of\nintermediate computations, unlike prior methods. Results show that our approach\nprovides a cleaner, more-principled, and more-flexible stochastic surface\nreconstruction pipeline.\n","authors":["Sidhanth Holalkere","David S. Bindel","Silvia Sellán","Alexander Terenin"],"pdf_url":"https://arxiv.org/pdf/2503.19136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05233v1","updated":"2025-06-05T16:50:23Z","published":"2025-06-05T16:50:23Z","title":"MesaNet: Sequence Modeling by Locally Optimal Test-Time Training","summary":"  Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.\n","authors":["Johannes von Oswald","Nino Scherrer","Seijin Kobayashi","Luca Versari","Songlin Yang","Maximilian Schlegel","Kaitlin Maile","Yanick Schimpf","Oliver Sieberling","Alexander Meulemans","Rif A. Saurous","Guillaume Lajoie","Charlotte Frenkel","Razvan Pascanu","Blaise Agüera y Arcas","João Sacramento"],"pdf_url":"https://arxiv.org/pdf/2506.05233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06257v4","updated":"2025-06-05T16:49:18Z","published":"2021-10-12T18:12:57Z","title":"Causal Discovery from Conditionally Stationary Time Series","summary":"  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.\n","authors":["Carles Balsells-Rodas","Xavier Sumba","Tanmayee Narendra","Ruibo Tu","Gabriele Schweikert","Hedvig Kjellstrom","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2110.06257v4.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.05231v1","updated":"2025-06-05T16:46:04Z","published":"2025-06-05T16:46:04Z","title":"Progressive Tempering Sampler with Diffusion","summary":"  Recent research has focused on designing neural samplers that amortize the\nprocess of sampling from unnormalized densities. However, despite significant\nadvancements, they still fall short of the state-of-the-art MCMC approach,\nParallel Tempering (PT), when it comes to the efficiency of target evaluations.\nOn the other hand, unlike a well-trained neural sampler, PT yields only\ndependent samples and needs to be rerun -- at considerable computational cost\n-- whenever new samples are required. To address these weaknesses, we propose\nthe Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion\nmodels sequentially across temperatures, leveraging the advantages of PT to\nimprove the training of neural samplers. We also introduce a novel method to\ncombine high-temperature diffusion models to generate approximate\nlower-temperature samples, which are minimally refined using MCMC and used to\ntrain the next diffusion model. PTSD enables efficient reuse of sample\ninformation across temperature levels while generating well-mixed, uncorrelated\nsamples. Our method significantly improves target evaluation efficiency,\noutperforming diffusion-based neural samplers.\n","authors":["Severi Rissanen","RuiKang OuYang","Jiajun He","Wenlin Chen","Markus Heinonen","Arno Solin","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2506.05231v1.pdf","comment":"Accepted for publication at ICML 2025"},{"id":"http://arxiv.org/abs/2506.05229v1","updated":"2025-06-05T16:43:48Z","published":"2025-06-05T16:43:48Z","title":"Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts","summary":"  Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.\n","authors":["Danil Sivtsov","Ivan Rodkin","Gleb Kuzmin","Yuri Kuratov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2506.05229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07301v2","updated":"2025-06-05T16:34:24Z","published":"2025-01-13T13:10:16Z","title":"The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning","summary":"  Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.\n","authors":["Zhenru Zhang","Chujie Zheng","Yangzhen Wu","Beichen Zhang","Runji Lin","Bowen Yu","Dayiheng Liu","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05216v1","updated":"2025-06-05T16:30:53Z","published":"2025-06-05T16:30:53Z","title":"A Unified Framework for Provably Efficient Algorithms to Estimate\n  Shapley Values","summary":"  Shapley values have emerged as a critical tool for explaining which features\nimpact the decisions made by machine learning models. However, computing exact\nShapley values is difficult, generally requiring an exponential (in the feature\ndimension) number of model evaluations. To address this, many model-agnostic\nrandomized estimators have been developed, the most influential and widely used\nbeing the KernelSHAP method (Lundberg & Lee, 2017). While related estimators\nsuch as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco &\nWitter, 2025) are known to satisfy theoretical guarantees, bounds for\nKernelSHAP have remained elusive. We describe a broad and unified framework\nthat encompasses KernelSHAP and related estimators constructed using both with\nand without replacement sampling strategies. We then prove strong\nnon-asymptotic theoretical guarantees that apply to all estimators from our\nframework. This provides, to the best of our knowledge, the first theoretical\nguarantees for KernelSHAP and sheds further light on tradeoffs between existing\nestimators. Through comprehensive benchmarking on small and medium dimensional\ndatasets for Decision-Tree models, we validate our approach against exact\nShapley values, consistently achieving low mean squared error with modest\nsample sizes. Furthermore, we make specific implementation improvements to\nenable scalability of our methods to high-dimensional datasets. Our methods,\ntested on datasets such MNIST and CIFAR10, provide consistently better results\ncompared to the KernelSHAP library.\n","authors":["Tyler Chen","Akshay Seshadri","Mattia J. Villani","Pradeep Niroula","Shouvanik Chakrabarti","Archan Ray","Pranav Deshpande","Romina Yalovetzky","Marco Pistoia","Niraj Kumar"],"pdf_url":"https://arxiv.org/pdf/2506.05216v1.pdf","comment":"44 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2506.05215v1","updated":"2025-06-05T16:30:05Z","published":"2025-06-05T16:30:05Z","title":"Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm","summary":"  We propose a new decentralized robust kernel-based learning algorithm within\nthe framework of reproducing kernel Hilbert space (RKHS) by utilizing a\nnetworked system that can be represented as a connected graph. The robust loss\nfunction $\\mathcal{L}_\\sigma$ induced by a windowing function $W$ and a\nrobustness scaling parameter $\\sigma>0$, can encompass a broad spectrum of\nrobust losses. Consequently, the proposed algorithm effectively provides a\nunified decentralized learning framework for robust regression, which\nfundamentally differs from the existing distributed robust kernel learning\nschemes, all of which are divide-and-conquer based. We rigorously establish the\nlearning theory and offer a comprehensive convergence analysis for the\nalgorithm. We show each local robust estimator generated from the decentralized\nalgorithm can be utilized to approximate the regression function. Based on\nkernel-based integral operator techniques, we derive general high confidence\nconvergence bounds for each local approximating sequence in terms of the mean\nsquare distance, RKHS norm, and generalization error, respectively. Moreover,\nwe provide rigorous selection rules for local sample size and show that, under\nproperly selected step size and scaling parameter $\\sigma$, the decentralized\nrobust algorithm can achieve optimal learning rates (up to logarithmic factors)\nin both norms. The parameter $\\sigma$ is shown to be essential for enhancing\nrobustness while also ensuring favorable convergence behavior. The intrinsic\nconnection among decentralization, sample selection, robustness of the\nalgorithm, and its convergence is clearly reflected.\n","authors":["Zhan Yu"],"pdf_url":"https://arxiv.org/pdf/2506.05215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05214v1","updated":"2025-06-05T16:28:12Z","published":"2025-06-05T16:28:12Z","title":"Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph\n  Contrastive Learning","summary":"  Graph Neural Networks (GNNs) often suffer from degree bias in node\nclassification tasks, where prediction performance varies across nodes with\ndifferent degrees. Several approaches, which adopt Graph Contrastive Learning\n(GCL), have been proposed to mitigate this bias. However, the limited number of\npositive pairs and the equal weighting of all positives and negatives in GCL\nstill lead to low-degree nodes acquiring insufficient and noisy information.\nThis paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to\nmitigate degree bias. It adds more positive pairs by leveraging node labels and\nadaptively weights positive and negative pairs based on their learning\nhardness. In addition, we develop an experimental framework named SHARP to\nextend HAR to a broader range of scenarios. Both our theoretical analysis and\nexperiments validate the effectiveness of SHARP. The experimental results\nacross four datasets show that SHARP achieves better performance against\nbaselines at both global and degree levels.\n","authors":["Jingyu Hu","Hongbo Bo","Jun Hong","Xiaowei Liu","Weiru Liu"],"pdf_url":"https://arxiv.org/pdf/2506.05214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09496v2","updated":"2025-06-05T16:23:46Z","published":"2025-05-14T15:44:10Z","title":"Reinforcement Learning for Individual Optimal Policy from Heterogeneous\n  Data","summary":"  Offline reinforcement learning (RL) aims to find optimal policies in dynamic\nenvironments in order to maximize the expected total rewards by leveraging\npre-collected data. Learning from heterogeneous data is one of the fundamental\nchallenges in offline RL. Traditional methods focus on learning an optimal\npolicy for all individuals with pre-collected data from a single episode or\nhomogeneous batch episodes, and thus, may result in a suboptimal policy for a\nheterogeneous population. In this paper, we propose an individualized offline\npolicy optimization framework for heterogeneous time-stationary Markov decision\nprocesses (MDPs). The proposed heterogeneous model with individual latent\nvariables enables us to efficiently estimate the individual Q-functions, and\nour Penalized Pessimistic Personalized Policy Learning (P4L) algorithm\nguarantees a fast rate on the average regret under a weak partial coverage\nassumption on behavior policies. In addition, our simulation studies and a real\ndata application demonstrate the superior numerical performance of the proposed\nmethod compared with existing methods.\n","authors":["Rui Miao","Babak Shahbaba","Annie Qu"],"pdf_url":"https://arxiv.org/pdf/2505.09496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08328v2","updated":"2025-06-05T16:21:33Z","published":"2024-08-12T14:22:14Z","title":"Unleashing The Power of Pre-Trained Language Models for Irregularly\n  Sampled Time Series","summary":"  Pre-trained Language Models (PLMs), such as ChatGPT, have significantly\nadvanced the field of natural language processing. This progress has inspired a\nseries of innovative studies that explore the adaptation of PLMs to time series\nanalysis, intending to create a unified foundation model that addresses various\ntime series analytical tasks. However, these efforts predominantly focus on\nRegularly Sampled Time Series (RSTS), neglecting the unique challenges posed by\nIrregularly Sampled Time Series (ISTS), which are characterized by uneven\nsampling intervals and prevalent missing data. To bridge this gap, this work\ntakes the first step in exploring the potential of PLMs for ISTS analysis. We\nbegin by investigating the effect of various methods for representing ISTS,\naiming to maximize the efficacy of PLMs in the analysis. Furthermore, we\npropose a unified PLM-based framework, named ISTS-PLM, to address diverse ISTS\nanalytical tasks. It integrates novel time-aware and variable-aware PLMs\ntailored to tackle the intractable intra- and inter-time series modeling in\nISTS. Finally, extensive experiments on a comprehensive benchmark demonstrate\nthat the ISTS-PLM, utilizing a structured and effective series-based\nrepresentation for ISTS, consistently achieves state-of-the-art performance\nacross various analytical tasks, such as classification, interpolation,\nextrapolation, few-shot and zero-shot learning scenarios, spanning scientific\ndomains like healthcare, biomechanics, and climate science.\n","authors":["Weijia Zhang","Chenlong Yin","Hao Liu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2408.08328v2.pdf","comment":"Accepted by KDD'25"},{"id":"http://arxiv.org/abs/2506.05209v1","updated":"2025-06-05T16:21:30Z","published":"2025-06-05T16:21:30Z","title":"The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text","summary":"  Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.\n","authors":["Nikhil Kandpal","Brian Lester","Colin Raffel","Sebastian Majstorovic","Stella Biderman","Baber Abbasi","Luca Soldaini","Enrico Shippole","A. Feder Cooper","Aviya Skowron","John Kirchenbauer","Shayne Longpre","Lintang Sutawika","Alon Albalak","Zhenlin Xu","Guilherme Penedo","Loubna Ben Allal","Elie Bakouch","John David Pressman","Honglu Fan","Dashiell Stander","Guangyu Song","Aaron Gokaslan","Tom Goldstein","Brian R. Bartoldson","Bhavya Kailkhura","Tyler Murray"],"pdf_url":"https://arxiv.org/pdf/2506.05209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01482v2","updated":"2025-06-05T16:15:38Z","published":"2024-10-02T12:34:04Z","title":"One Wave To Explain Them All: A Unifying Perspective On Feature\n  Attribution","summary":"  Feature attribution methods aim to improve the transparency of deep neural\nnetworks by identifying the input features that influence a model's decision.\nPixel-based heatmaps have become the standard for attributing features to\nhigh-dimensional inputs, such as images, audio representations, and volumes.\nWhile intuitive and convenient, these pixel-based attributions fail to capture\nthe underlying structure of the data. Moreover, the choice of domain for\ncomputing attributions has often been overlooked. This work demonstrates that\nthe wavelet domain allows for informative and meaningful attributions. It\nhandles any input dimension and offers a unified approach to feature\nattribution. Our method, the Wavelet Attribution Method (WAM), leverages the\nspatial and scale-localized properties of wavelet coefficients to provide\nexplanations that capture both the where and what of a model's decision-making\nprocess. We show that WAM quantitatively matches or outperforms existing\ngradient-based methods across multiple modalities, including audio, images, and\nvolumes. Additionally, we discuss how WAM bridges attribution with broader\naspects of model robustness and transparency. Project page:\nhttps://gabrielkasmi.github.io/wam/\n","authors":["Gabriel Kasmi","Amandine Brunetto","Thomas Fel","Jayneel Parekh"],"pdf_url":"https://arxiv.org/pdf/2410.01482v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.05203v1","updated":"2025-06-05T16:14:57Z","published":"2025-06-05T16:14:57Z","title":"Trustworthiness Preservation by Copies of Machine Learning Systems","summary":"  A common practice of ML systems development concerns the training of the same\nmodel under different data sets, and the use of the same (training and test)\nsets for different learning models. The first case is a desirable practice for\nidentifying high quality and unbiased training conditions. The latter case\ncoincides with the search for optimal models under a common dataset for\ntraining. These differently obtained systems have been considered akin to\ncopies. In the quest for responsible AI, a legitimate but hardly investigated\nquestion is how to verify that trustworthiness is preserved by copies. In this\npaper we introduce a calculus to model and verify probabilistic complex queries\nover data and define four distinct notions: Justifiably, Equally, Weakly and\nAlmost Trustworthy which can be checked analysing the (partial) behaviour of\nthe copy with respect to its original. We provide a study of the relations\nbetween these notions of trustworthiness, and how they compose with each other\nand under logical operations. The aim is to offer a computational tool to check\nthe trustworthiness of possibly complex systems copied from an original whose\nbehavour is known.\n","authors":["Leonardo Ceragioli","Giuseppe Primiero"],"pdf_url":"https://arxiv.org/pdf/2506.05203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05202v1","updated":"2025-06-05T16:14:35Z","published":"2025-06-05T16:14:35Z","title":"Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants","summary":"  This paper investigates causal effect identification in latent variable\nLinear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,\naddressing two prominent setups that are challenging in the presence of latent\nconfounding: (1) a single proxy variable that may causally influence the\ntreatment and (2) underspecified instrumental variable cases where fewer\ninstruments exist than treatments. We prove that causal effects are\nidentifiable with a single proxy or instrument and provide corresponding\nestimation methods. Experimental results demonstrate the accuracy and\nrobustness of our approaches compared to existing methods, advancing the\ntheoretical and practical understanding of causal inference in linear systems\nwith latent confounders.\n","authors":["Daniele Tramontano","Yaroslav Kivva","Saber Salehkaleybar Mathias Drton","Negar Kiyavash"],"pdf_url":"https://arxiv.org/pdf/2506.05202v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.05200v1","updated":"2025-06-05T16:12:51Z","published":"2025-06-05T16:12:51Z","title":"Transformers Meet In-Context Learning: A Universal Approximation Theory","summary":"  Modern large language models are capable of in-context learning, the ability\nto perform new tasks at inference time using only a handful of input-output\nexamples in the prompt, without any fine-tuning or parameter updates. We\ndevelop a universal approximation theory to better understand how transformers\nenable in-context learning. For any class of functions (each representing a\ndistinct task), we demonstrate how to construct a transformer that, without any\nfurther weight updates, can perform reliable prediction given only a few\nin-context examples. In contrast to much of the recent literature that frames\ntransformers as algorithm approximators -- i.e., constructing transformers to\nemulate the iterations of optimization algorithms as a means to approximate\nsolutions of learning problems -- our work adopts a fundamentally different\napproach rooted in universal function approximation. This alternative approach\noffers approximation guarantees that are not constrained by the effectiveness\nof the optimization algorithms being approximated, thereby extending far beyond\nconvex problems and linear function classes. Our construction sheds light on\nhow transformers can simultaneously learn general-purpose representations and\nadapt dynamically to in-context examples.\n","authors":["Gen Li","Yuchen Jiao","Yu Huang","Yuting Wei","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05198v1","updated":"2025-06-05T16:10:47Z","published":"2025-06-05T16:10:47Z","title":"Quantifying Cross-Modality Memorization in Vision-Language Models","summary":"  Understanding what and how neural networks memorize during training is\ncrucial, both from the perspective of unintentional memorization of potentially\nsensitive information and from the standpoint of effective knowledge\nacquisition for real-world, knowledge-intensive tasks. While previous studies\nprimarily investigate memorization within a single modality, such as text\nmemorization in large language models or image memorization in diffusion\nmodels, unified multimodal models are becoming increasingly prevalent in\npractical applications. In this work, we focus on the unique characteristics of\ncross-modality memorization and conduct a systematic study centered on\nvision-language models. To facilitate controlled experiments, we first\nintroduce a synthetic persona dataset comprising diverse synthetic person\nimages and textual descriptions. We quantify factual knowledge memorization and\ncross-modal transferability by training models on a single modality and\nevaluating their performance in the other. Our results reveal that facts\nlearned in one modality transfer to the other, but a significant gap exists\nbetween recalling information in the source and target modalities. Furthermore,\nwe observe that this gap exists across various scenarios, including more\ncapable models, machine unlearning, and the multi-hop case. At the end, we\npropose a baseline method to mitigate this challenge. We hope our study can\ninspire future research on developing more robust multimodal learning\ntechniques to enhance cross-modal transferability.\n","authors":["Yuxin Wen","Yangsibo Huang","Tom Goldstein","Ravi Kumar","Badih Ghazi","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16139v2","updated":"2025-06-05T16:09:29Z","published":"2023-12-26T17:57:46Z","title":"Abnormal component analysis","summary":"  At the crossway of machine learning and data analysis, anomaly detection aims\nat identifying observations that exhibit abnormal behaviour. Be it measurement\nerrors, disease development, severe weather, production quality default(s)\n(items) or failed equipment, financial frauds or crisis events, their on-time\nidentification and isolation constitute an important task in almost any area of\nindustry and science. While a substantial body of literature is devoted to\ndetection of anomalies, little attention is payed to their explanation. This is\nthe case mostly due to intrinsically non-supervised nature of the task and\nnon-robustness of the exploratory methods like principal component analysis\n(PCA).\n  We introduce a new statistical tool dedicated for exploratory analysis of\nabnormal observations using data depth as a score. Abnormal component analysis\n(shortly ACA) is a method that searches a low-dimensional data representation\nthat best visualises and explains anomalies. This low-dimensional\nrepresentation not only allows to distinguish groups of anomalies better than\nthe methods of the state of the art, but as well provides a -- linear in\nvariables and thus easily interpretable -- explanation for anomalies. In a\ncomparative simulation and real-data study, ACA also proves advantageous for\nanomaly analysis with respect to methods present in the literature.\n","authors":["Romain Valla","Pavlo Mozharovskyi","Florence d'Alché-Buc"],"pdf_url":"https://arxiv.org/pdf/2312.16139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05196v1","updated":"2025-06-05T16:07:31Z","published":"2025-06-05T16:07:31Z","title":"Locality Preserving Markovian Transition for Instance Retrieval","summary":"  Diffusion-based re-ranking methods are effective in modeling the data\nmanifolds through similarity propagation in affinity graphs. However, positive\nsignals tend to diminish over several steps away from the source, reducing\ndiscriminative power beyond local regions. To address this issue, we introduce\nthe Locality Preserving Markovian Transition (LPMT) framework, which employs a\nlong-term thermodynamic transition process with multiple states for accurate\nmanifold distance measurement. The proposed LPMT first integrates diffusion\nprocesses across separate graphs using Bidirectional Collaborative Diffusion\n(BCD) to establish strong similarity relationships. Afterwards, Locality State\nEmbedding (LSE) encodes each instance into a distribution for enhanced local\nconsistency. These distributions are interconnected via the Thermodynamic\nMarkovian Transition (TMT) process, enabling efficient global retrieval while\nmaintaining local effectiveness. Experimental results across diverse tasks\nconfirm the effectiveness of LPMT for instance retrieval.\n","authors":["Jifei Luo","Wenzheng Wu","Hantao Yao","Lu Yu","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2506.05196v1.pdf","comment":"This paper has been accepted by ICML2025"},{"id":"http://arxiv.org/abs/2209.10166v4","updated":"2025-06-05T16:02:14Z","published":"2022-09-21T07:57:07Z","title":"Chaotic Hedging with Iterated Integrals and Neural Networks","summary":"  In this paper, we derive an $L^p$-chaos expansion based on iterated\nStratonovich integrals with respect to a given exponentially integrable\ncontinuous semimartingale. By omitting the orthogonality of the expansion, we\nshow that every $p$-integrable functional, $p \\in [1,\\infty)$, can be\napproximated by a finite sum of iterated Stratonovich integrals. Using\n(possibly random) neural networks as integrands, we therefere obtain universal\napproximation results for $p$-integrable financial derivatives in the\n$L^p$-sense. Moreover, we can approximately solve the $L^p$-hedging problem\n(coinciding for $p = 2$ with the quadratic hedging problem), where the\napproximating hedging strategy can be computed in closed form within short\nruntime.\n","authors":["Ariel Neufeld","Philipp Schmocker"],"pdf_url":"https://arxiv.org/pdf/2209.10166v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05188v1","updated":"2025-06-05T16:02:07Z","published":"2025-06-05T16:02:07Z","title":"Counterfactual reasoning: an analysis of in-context emergence","summary":"  Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .\n","authors":["Moritz Miller","Bernhard Schölkopf","Siyuan Guo"],"pdf_url":"https://arxiv.org/pdf/2506.05188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05183v1","updated":"2025-06-05T15:56:38Z","published":"2025-06-05T15:56:38Z","title":"TreeRPO: Tree Relative Policy Optimization","summary":"  Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.\n","authors":["Zhicheng Yang","Zhijiang Guo","Yinya Huang","Xiaodan Liang","Yiwei Wang","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2506.05183v1.pdf","comment":"13pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.05178v1","updated":"2025-06-05T15:51:47Z","published":"2025-06-05T15:51:47Z","title":"Associative Memory and Generative Diffusion in the Zero-noise Limit","summary":"  Connections between generative diffusion and continuous-state associative\nmemory models are studied. Morse-Smale dynamical systems are emphasized as\nuniversal approximators of gradient-based associative memory models and\ndiffusion models as white-noise perturbed systems thereof. Universal properties\nof associative memory that follow from this description are described and used\nto characterize a generic transition from generation to memory as noise levels\ndiminish. Structural stability inherited by Morse-Smale flows is shown to imply\na notion of stability for diffusions at vanishing noise levels. Applied to one-\nand two-parameter families of gradients, this indicates stability at all but\nisolated points of associative memory learning landscapes and the learning and\ngeneration landscapes of diffusion models with gradient drift in the zero-noise\nlimit, at which small sets of generic bifurcations characterize qualitative\ntransitions between stable systems. Examples illustrating the characterization\nof these landscapes by sequences of these bifurcations are given, along with\nstructural stability criterion for classic and modern Hopfield networks\n(equivalently, the attention mechanism).\n","authors":["Joshua Hess","Quaid Morris"],"pdf_url":"https://arxiv.org/pdf/2506.05178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06854v2","updated":"2025-06-05T15:48:54Z","published":"2025-02-07T17:23:48Z","title":"Can Large Language Models Understand Intermediate Representations in\n  Compilers?","summary":"  Intermediate Representations (IRs) play a critical role in compiler design\nand program analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. In this paper, we present an explorative empirical study\nevaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,\nDeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,\nwe assess model performance across four core tasks: control flow graph\nreconstruction, decompilation, code summarization, and execution reasoning.\nWhile LLMs exhibit competence in parsing IR syntax and identifying high-level\nstructures, they consistently struggle with instruction-level reasoning,\nespecially in control flow reasoning, loop handling, and dynamic execution.\nCommon failure modes include misinterpreting branching instructions, omitting\ncritical operations, and relying on heuristic reasoning rather than precise\ninstruction-level logic. Our findings highlight the need for IR-specific\nenhancements in LLM design. We recommend fine-tuning on structured IR datasets\nand integrating control-flow-sensitive architectures to improve model\neffectiveness. All experimental data and source code are publicly available at\n","authors":["Hailong Jiang","Jianfeng Zhu","Yao Wan","Bo Fang","Hongyu Zhang","Ruoming Jin","Qiang Guan"],"pdf_url":"https://arxiv.org/pdf/2502.06854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13577v3","updated":"2025-06-05T15:32:11Z","published":"2024-10-17T14:12:35Z","title":"Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes\n  and Sample Compression Hypernetworks","summary":"  Both PAC-Bayesian and Sample Compress learning frameworks are instrumental\nfor deriving tight (non-vacuous) generalization bounds for neural networks. We\nleverage these results in a meta-learning scheme, relying on a hypernetwork\nthat outputs the parameters of a downstream predictor from a dataset input. The\noriginality of our approach lies in the investigated hypernetwork architectures\nthat encode the dataset before decoding the parameters: (1) a PAC-Bayesian\nencoder that expresses a posterior distribution over a latent space, (2) a\nSample Compress encoder that selects a small sample of the dataset input along\nwith a message from a discrete set, and (3) a hybrid between both approaches\nmotivated by a new Sample Compress theorem handling continuous messages. The\nlatter theorem exploits the pivotal information transiting at the\nencoder-decoder junction in order to compute generalization guarantees for each\ndownstream predictor obtained by our meta-learning scheme.\n","authors":["Benjamin Leblanc","Mathieu Bazinet","Nathaniel D'Amours","Alexandre Drouin","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2410.13577v3.pdf","comment":"Accepted at ICML 2025 (also at the NeurIPS 2024 Workshop on\n  Compression in Machine Learning)"},{"id":"http://arxiv.org/abs/2504.17493v2","updated":"2025-06-05T15:31:17Z","published":"2025-04-24T12:34:43Z","title":"Goal-Oriented Time-Series Forecasting: Foundation Framework Design","summary":"  Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts within a region\nof interest. We tested our method on standard datasets, including a new\nwireless communication dataset, and found that not only it improves prediction\naccuracy but also enhances the performance of end application employing the\nforecasting model. This research provides a basis for creating forecasting\nsystems that better connect prediction and decision-making in various practical\napplications.\n","authors":["Luca-Andrei Fechete","Mohamed Sana","Fadhel Ayed","Nicola Piovesan","Wenjie Li","Antonio De Domenico","Tareq Si Salem"],"pdf_url":"https://arxiv.org/pdf/2504.17493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10033v2","updated":"2025-06-05T15:31:07Z","published":"2025-05-15T07:29:16Z","title":"Evaluating Robustness of Deep Reinforcement Learning for Autonomous\n  Surface Vehicle Control in Field Tests","summary":"  Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers.\n","authors":["Luis F. W. Batista","Stéphanie Aravecchia","Seth Hutchinson","Cédric Pradalier"],"pdf_url":"https://arxiv.org/pdf/2505.10033v2.pdf","comment":"Presented at the 2025 IEEE ICRA Workshop on Field Robotics"},{"id":"http://arxiv.org/abs/2408.11721v2","updated":"2025-06-05T15:25:55Z","published":"2024-08-21T15:51:46Z","title":"Detection-Driven Object Count Optimization for Text-to-Image Diffusion\n  Models","summary":"  Accurately controlling object count in text-to-image generation remains a key\nchallenge. Supervised methods often fail, as training data rarely covers all\ncount variations. Methods that manipulate the denoising process to add or\nremove objects can help; however, they still require labeled data, limit\nrobustness and image quality, and rely on a slow, iterative process.\n  Pre-trained differentiable counting models that rely on soft object density\nsummation exist and could steer generation, but employing them presents three\nmain challenges: (i) they are pre-trained on clean images, making them less\neffective during denoising steps that operate on noisy inputs; (ii) they are\nnot robust to viewpoint changes; and (iii) optimization is computationally\nexpensive, requiring repeated model evaluations per image.\n  We propose a new framework that uses pre-trained object counting techniques\nand object detectors to guide generation. First, we optimize a counting token\nusing an outer-loop loss computed on fully generated images. Second, we\nintroduce a detection-driven scaling term that corrects errors caused by\nviewpoint and proportion shifts, among other factors, without requiring\nbackpropagation through the detection model. Third, we show that the optimized\nparameters can be reused for new prompts, removing the need for repeated\noptimization. Our method provides efficiency through token reuse, flexibility\nvia compatibility with various detectors, and accuracy with improved counting\nacross diverse object categories.\n","authors":["Oz Zafar","Yuval Cohen","Lior Wolf","Idan Schwartz"],"pdf_url":"https://arxiv.org/pdf/2408.11721v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2505.19619v2","updated":"2025-06-05T15:24:20Z","published":"2025-05-26T07:34:11Z","title":"SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows","summary":"  Deep generative models have recently garnered significant attention across\nvarious fields, from physics to chemistry, where sampling from unnormalized\nBoltzmann-like distributions represents a fundamental challenge. In particular,\nautoregressive models and normalizing flows have become prominent due to their\nappealing ability to yield closed-form probability densities. Moreover, it is\nwell-established that incorporating prior knowledge - such as symmetries - into\ndeep neural networks can substantially improve training performances. In this\ncontext, recent advances have focused on developing symmetry-equivariant\ngenerative models, achieving remarkable results. Building upon these\nfoundations, this paper introduces Symmetry-Enforcing Stochastic Modulation\n(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the\nincorporation of inductive biases (e.g., symmetries) into normalizing flows\nthrough a novel technique called stochastic modulation. This approach enhances\nthe flexibility of the generative model, allowing to effectively learn a\nvariety of exact and broken symmetries. Our numerical experiments benchmark\nSESaMo in different scenarios, including an 8-Gaussian mixture model and\nphysically relevant field theories, such as the $\\phi^4$ theory and the Hubbard\nmodel.\n","authors":["Janik Kreit","Dominic Schuh","Kim A. Nicoli","Lena Funcke"],"pdf_url":"https://arxiv.org/pdf/2505.19619v2.pdf","comment":"27 pages, 14 figures"},{"id":"http://arxiv.org/abs/2506.05138v1","updated":"2025-06-05T15:22:04Z","published":"2025-06-05T15:22:04Z","title":"Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT\n  Systems","summary":"  Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning.\n","authors":["Pavle Vasiljevic","Milica Matic","Miroslav Popovic"],"pdf_url":"https://arxiv.org/pdf/2506.05138v1.pdf","comment":"6 pages, 4 algorithms, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2506.05128v1","updated":"2025-06-05T15:16:14Z","published":"2025-06-05T15:16:14Z","title":"DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning","summary":"  Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.\n","authors":["Tanmay Parekh","Kartik Mehta","Ninareh Mehrabi","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2506.05128v1.pdf","comment":"Submitted at ACL ARR May 2025"},{"id":"http://arxiv.org/abs/2506.05126v1","updated":"2025-06-05T15:13:57Z","published":"2025-06-05T15:13:57Z","title":"Membership Inference Attacks on Sequence Models","summary":"  Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.\n","authors":["Lorenzo Rossi","Michael Aerni","Jie Zhang","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2506.05126v1.pdf","comment":"Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)"},{"id":"http://arxiv.org/abs/2502.09755v2","updated":"2025-06-05T15:08:36Z","published":"2025-02-13T20:25:40Z","title":"Jailbreak Attack Initializations as Extractors of Compliance Directions","summary":"  Safety-aligned LLMs respond to prompts with either compliance or refusal,\neach corresponding to distinct directions in the model's activation space.\nRecent works show that initializing attacks via self-transfer from other\nprompts significantly enhances their performance. However, the underlying\nmechanisms of these initializations remain unclear, and attacks utilize\narbitrary or hand-picked initializations. This work presents that each\ngradient-based jailbreak attack and subsequent initialization gradually\nconverge to a single compliance direction that suppresses refusal, thereby\nenabling an efficient transition from refusal to compliance. Based on this\ninsight, we propose CRI, an initialization framework that aims to project\nunseen prompts further along compliance directions. We demonstrate our approach\non multiple attacks, models, and datasets, achieving an increased attack\nsuccess rate (ASR) and reduced computational overhead, highlighting the\nfragility of safety-aligned LLMs. A reference implementation is available at:\nhttps://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation.\n","authors":["Amit Levi","Rom Himelstein","Yaniv Nemcovsky","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2502.09755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05120v1","updated":"2025-06-05T15:08:01Z","published":"2025-06-05T15:08:01Z","title":"Nonlinear Causal Discovery for Grouped Data","summary":"  Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.\n","authors":["Konstantin Göbler","Tobias Windisch","Mathias Drton"],"pdf_url":"https://arxiv.org/pdf/2506.05120v1.pdf","comment":"9 pages, 5 figures, to be published at UAI'25"},{"id":"http://arxiv.org/abs/2505.09833v2","updated":"2025-06-05T15:00:47Z","published":"2025-05-14T22:23:30Z","title":"Learning Rock Pushability on Rough Planetary Terrain","summary":"  In the context of mobile navigation in unstructured environments, the\npredominant approach entails the avoidance of obstacles. The prevailing path\nplanning algorithms are contingent upon deviating from the intended path for an\nindefinite duration and returning to the closest point on the route after the\nobstacle is left behind spatially. However, avoiding an obstacle on a path that\nwill be used repeatedly by multiple agents can hinder long-term efficiency and\nlead to a lasting reliance on an active path planning system. In this study, we\npropose an alternative approach to mobile navigation in unstructured\nenvironments by leveraging the manipulation capabilities of a robotic\nmanipulator mounted on top of a mobile robot. Our proposed framework integrates\nexteroceptive and proprioceptive feedback to assess the push affordance of\nobstacles, facilitating their repositioning rather than avoidance. While our\npreliminary visual estimation takes into account the characteristics of both\nthe obstacle and the surface it relies on, the push affordance estimation\nmodule exploits the force feedback obtained by interacting with the obstacle\nvia a robotic manipulator as the guidance signal. The objective of our\nnavigation approach is to enhance the efficiency of routes utilized by multiple\nagents over extended periods by reducing the overall time spent by a fleet in\nenvironments where autonomous infrastructure development is imperative, such as\nlunar or Martian surfaces.\n","authors":["Tuba Girgin","Emre Girgin","Cagri Kilic"],"pdf_url":"https://arxiv.org/pdf/2505.09833v2.pdf","comment":"Paper presented at the Workshop on Field Robotics, ICRA 2025,\n  Atlanta, GA, United States"},{"id":"http://arxiv.org/abs/2506.05104v1","updated":"2025-06-05T14:46:04Z","published":"2025-06-05T14:46:04Z","title":"Survey on the Evaluation of Generative Models in Music","summary":"  Research on generative systems in music has seen considerable attention and\ngrowth in recent years. A variety of attempts have been made to systematically\nevaluate such systems. We provide an interdisciplinary review of the common\nevaluation targets, methodologies, and metrics for the evaluation of both\nsystem output and model usability, covering subjective and objective\napproaches, qualitative and quantitative approaches, as well as empirical and\ncomputational methods. We discuss the advantages and challenges of such\napproaches from a musicological, an engineering, and an HCI perspective.\n","authors":["Alexander Lerch","Claire Arthur","Nick Bryan-Kinns","Corey Ford","Qianyi Sun","Ashvala Vinay"],"pdf_url":"https://arxiv.org/pdf/2506.05104v1.pdf","comment":"Submitted to ACM CSUR, 26-Jun-2024"},{"id":"http://arxiv.org/abs/2502.18334v3","updated":"2025-06-05T14:44:56Z","published":"2025-02-25T16:26:25Z","title":"Structural Alignment Improves Graph Test-Time Adaptation","summary":"  Graph-based learning excels at capturing interaction patterns in diverse\ndomains like recommendation, fraud detection, and particle physics. However,\nits performance often degrades under distribution shifts, especially those\naltering network connectivity. Current methods to address these shifts\ntypically require retraining with the source dataset, which is often infeasible\ndue to computational or privacy limitations. We introduce Test-Time Structural\nAlignment (TSA), a novel algorithm for Graph Test-Time Adaptation (GTTA) that\naligns graph structures during inference without accessing the source data.\nGrounded in a theoretical understanding of graph data distribution shifts, TSA\nemploys three synergistic strategies: uncertainty-aware neighborhood weighting\nto accommodate neighbor label distribution shifts, adaptive balancing of\nself-node and aggregated neighborhood representations based on their\nsignal-to-noise ratio, and decision boundary refinement to correct residual\nlabel and feature shifts. Extensive experiments on synthetic and real-world\ndatasets demonstrate TSA's consistent outperformance of both non-graph TTA\nmethods and state-of-the-art GTTA baselines.\n","authors":["Hans Hao-Hsun Hsu","Shikun Liu","Han Zhao","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2502.18334v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05101v1","updated":"2025-06-05T14:44:15Z","published":"2025-06-05T14:44:15Z","title":"Privacy Amplification Through Synthetic Data: Insights from Linear\n  Regression","summary":"  Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.\n","authors":["Clément Pierquin","Aurélien Bellet","Marc Tommasi","Matthieu Boussard"],"pdf_url":"https://arxiv.org/pdf/2506.05101v1.pdf","comment":"26 pages, ICML 2025"},{"id":"http://arxiv.org/abs/2406.03198v2","updated":"2025-06-05T14:35:42Z","published":"2024-05-28T04:36:15Z","title":"The Impossibility of Fair LLMs","summary":"  The rise of general-purpose artificial intelligence (AI) systems,\nparticularly large language models (LLMs), has raised pressing moral questions\nabout how to reduce bias and ensure fairness at scale. Researchers have\ndocumented a sort of \"bias\" in the significant correlations between\ndemographics (e.g., race, gender) in LLM prompts and responses, but it remains\nunclear how LLM fairness could be evaluated with more rigorous definitions,\nsuch as group fairness or fair representations. We analyze a variety of\ntechnical fairness frameworks and find inherent challenges in each that make\nthe development of a fair LLM intractable. We show that each framework either\ndoes not logically extend to the general-purpose AI context or is infeasible in\npractice, primarily due to the large amounts of unstructured training data and\nthe many potential combinations of human populations, use cases, and sensitive\nattributes. These inherent challenges would persist for general-purpose AI,\nincluding LLMs, even if empirical challenges, such as limited participatory\ninput and limited measurement methods, were overcome. Nonetheless, fairness\nwill remain an important type of model evaluation, and there are still\npromising research directions, particularly the development of standards for\nthe responsibility of LLM developers, context-specific evaluations, and methods\nof iterative, participatory, and AI-assisted evaluation that could scale\nfairness across the diverse contexts of modern human-AI interaction.\n","authors":["Jacy Anthis","Kristian Lum","Michael Ekstrand","Avi Feller","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2406.03198v2.pdf","comment":"Published in ACL 2025"},{"id":"http://arxiv.org/abs/2506.05088v1","updated":"2025-06-05T14:34:37Z","published":"2025-06-05T14:34:37Z","title":"Semi-Implicit Variational Inference via Kernelized Path Gradient Descent","summary":"  Semi-implicit variational inference (SIVI) is a powerful framework for\napproximating complex posterior distributions, but training with the\nKullback-Leibler (KL) divergence can be challenging due to high variance and\nbias in high-dimensional settings. While current state-of-the-art semi-implicit\nvariational inference methods, particularly Kernel Semi-Implicit Variational\nInference (KSIVI), have been shown to work in high dimensions, training remains\nmoderately expensive. In this work, we propose a kernelized KL divergence\nestimator that stabilizes training through nonparametric smoothing. To further\nreduce the bias, we introduce an importance sampling correction. We provide a\ntheoretical connection to the amortized version of the Stein variational\ngradient descent, which estimates the score gradient via Stein's identity,\nshowing that both methods minimize the same objective, but our semi-implicit\napproach achieves lower gradient variance. In addition, our method's bias in\nfunction space is benign, leading to more stable and efficient optimization.\nEmpirical results demonstrate that our method outperforms or matches\nstate-of-the-art SIVI methods in both performance and training efficiency.\n","authors":["Tobias Pielok","Bernd Bischl","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2506.05088v1.pdf","comment":"Preliminary version"},{"id":"http://arxiv.org/abs/2411.01679v2","updated":"2025-06-05T14:26:44Z","published":"2024-11-03T20:41:38Z","title":"Autoformulation of Mathematical Optimization Models Using LLMs","summary":"  Mathematical optimization is fundamental to decision-making across diverse\ndomains, from operations research to healthcare. Yet, translating real-world\nproblems into optimization models remains a difficult task, often demanding\nspecialized expertise. This paper approaches the problem of\n$\\textit{autoformulation}$: the automated creation of solver-ready optimization\nmodels from natural language problem descriptions. We identify three core\nchallenges of autoformulation: $\\textit{(1)}$ the vast, problem-dependent\nhypothesis space, $\\textit{(2)}$ efficient and diverse exploration of this\nspace under uncertainty, and $\\textit{(3)}$ evaluation of formulation\ncorrectness against problem description. To address these challenges, we\npresent a novel method leveraging $\\textit{Large Language Models}$ (LLMs) with\n$\\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of\noptimization modeling to generate and systematically explore possible\nformulations. To enhance search efficiency, we introduce symbolic pruning to\neliminate trivially equivalent search paths (branches), and employ LLM-based\nevaluation of partial formulations to guide search. Empirical analysis on\nlinear and mixed-integer programming benchmarks demonstrates our method's\neffectiveness, with significant performance gains from both LLM-based value\nestimation and symbolic pruning techniques.\n","authors":["Nicolás Astorga","Tennison Liu","Yuanzhang Xiao","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2411.01679v2.pdf","comment":"*Astorga and Liu contributed equally. Published as a conference paper\n  at ICML 2025"},{"id":"http://arxiv.org/abs/2506.05074v1","updated":"2025-06-05T14:20:36Z","published":"2025-06-05T14:20:36Z","title":"EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware\n  Classifiers","summary":"  A lack of accessible data has historically restricted malware analysis\nresearch, and practitioners have relied heavily on datasets provided by\nindustry sources to advance. Existing public datasets are limited by narrow\nscope - most include files targeting a single platform, have labels supporting\njust one type of malware classification task, and make no effort to capture the\nevasive files that make malware detection difficult in practice. We present\nEMBER2024, a new dataset that enables holistic evaluation of malware\nclassifiers. Created in collaboration with the authors of EMBER2017 and\nEMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,\nand labels for more than 3.2 million files from six file formats. Our dataset\nsupports the training and evaluation of machine learning models on seven\nmalware classification tasks, including malware detection, malware family\nclassification, and malware behavior identification. EMBER2024 is the first to\ninclude a collection of malicious files that initially went undetected by a set\nof antivirus products, creating a \"challenge\" set to assess classifier\nperformance against evasive malware. This work also introduces EMBER feature\nversion 3, with added support for several new feature types. We are releasing\nthe EMBER2024 dataset to promote reproducibility and empower researchers in the\npursuit of new malware research topics.\n","authors":["Robert J. Joyce","Gideon Miller","Phil Roth","Richard Zak","Elliott Zaresky-Williams","Hyrum Anderson","Edward Raff","James Holt"],"pdf_url":"https://arxiv.org/pdf/2506.05074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12171v2","updated":"2025-06-05T14:16:46Z","published":"2025-02-13T10:33:58Z","title":"GoRA: Gradient-driven Adaptive Low Rank Adaptation","summary":"  Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings.\n","authors":["Haonan He","Peng Ye","Yuchen Ren","Yuan Yuan","Luyang Zhou","Shucun Ju","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05065v1","updated":"2025-06-05T14:11:36Z","published":"2025-06-05T14:11:36Z","title":"UnHiPPO: Uncertainty-aware Initialization for State Space Models","summary":"  State space models are emerging as a dominant model class for sequence\nproblems with many relying on the HiPPO framework to initialize their dynamics.\nHowever, HiPPO fundamentally assumes data to be noise-free; an assumption often\nviolated in practice. We extend the HiPPO theory with measurement noise and\nderive an uncertainty-aware initialization for state space model dynamics. In\nour analysis, we interpret HiPPO as a linear stochastic control problem where\nthe data enters as a noise-free control signal. We then reformulate the problem\nso that the data become noisy outputs of a latent system and arrive at an\nalternative dynamics initialization that infers the posterior of this latent\nsystem from the data without increasing runtime. Our experiments show that our\ninitialization improves the resistance of state-space models to noise both at\ntraining and inference time. Find our implementation at\nhttps://cs.cit.tum.de/daml/unhippo.\n","authors":["Marten Lienen","Abdullah Saydemir","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2506.05065v1.pdf","comment":"Published at ICML 2025"},{"id":"http://arxiv.org/abs/2506.05059v1","updated":"2025-06-05T14:02:55Z","published":"2025-06-05T14:02:55Z","title":"NIMO: a Nonlinear Interpretable MOdel","summary":"  Neural networks (NNs) have achieved tremendous success over the past decade,\nyet they are still extremely difficult to interpret. In contrast, linear models\nare less expressive but offer inherent interpretability. Linear coefficients\nare interpretable as the marginal effect of a feature on the prediction,\nassuming all other features are kept fixed. To combine the benefits of both\napproaches, we introduce NIMO (Nonlinear Interpretable MOdel). The key idea is\nto define a model where the NN is designed to learn nonlinear corrections to\nthe linear model predictions, while also maintaining the original\ninterpretability of the linear coefficients. Relevantly, we develop an\noptimization algorithm based on profile likelihood that elegantly allows for\noptimizing over the NN parameters while updating the linear coefficients\nanalytically. By relying on adaptive ridge regression we can easily incorporate\nsparsity constraints as well. We show empirically that we can recover the\nunderlying linear coefficients while significantly improving the predictive\naccuracy. Compared to other hybrid interpretable approaches, our model is the\nonly one that actually maintains the same interpretability of linear\ncoefficients as in linear models. We also achieve higher performance on various\nregression and classification settings.\n","authors":["Shijian Xu","Marcello Massimo Negri","Volker Roth"],"pdf_url":"https://arxiv.org/pdf/2506.05059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09430v2","updated":"2025-06-05T14:01:16Z","published":"2025-05-14T14:34:40Z","title":"Mini Diffuser: Fast Multi-task Diffusion Policy Training Using Two-level\n  Mini-batches","summary":"  We present a method that reduces, by an order of magnitude, the time and\nmemory needed to train multi-task vision-language robotic diffusion policies.\nThis improvement arises from a previously underexplored distinction between\naction diffusion and the image diffusion techniques that inspired it: In image\ngeneration, the target is high-dimensional. By contrast, in action generation,\nthe dimensionality of the target is comparatively small, and only the image\ncondition is high-dimensional. Our approach, \\emph{Mini Diffuser}, exploits\nthis asymmetry by introducing \\emph{two-level minibatching}, which pairs\nmultiple noised action samples with each vision-language condition, instead of\nthe conventional one-to-one sampling strategy. To support this batching scheme,\nwe introduce architectural adaptations to the diffusion transformer that\nprevent information leakage across samples while maintaining full conditioning\naccess. In RLBench simulations, Mini-Diffuser achieves 95\\% of the performance\nof state-of-the-art multi-task diffusion policies, while using only 5\\% of the\ntraining time and 7\\% of the memory. Real-world experiments further validate\nthat Mini-Diffuser preserves the key strengths of diffusion-based policies,\nincluding the ability to model multimodal action distributions and produce\nbehavior conditioned on diverse perceptual inputs. Code available at\nmini-diffuse-actor.github.io\n","authors":["Yutong Hu","Pinhao Song","Kehan Wen","Renaud Detry"],"pdf_url":"https://arxiv.org/pdf/2505.09430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19354v2","updated":"2025-06-05T13:58:39Z","published":"2025-04-27T20:43:33Z","title":"Neurosymbolic Association Rule Mining from Tabular Data","summary":"  Association Rule Mining (ARM) is the task of mining patterns among data\nfeatures in the form of logical rules, with applications across a myriad of\ndomains. However, high-dimensional datasets often result in an excessive number\nof rules, increasing execution time and negatively impacting downstream task\nperformance. Managing this rule explosion remains a central challenge in ARM\nresearch. To address this, we introduce Aerial+, a novel neurosymbolic ARM\nmethod. Aerial+ leverages an under-complete autoencoder to create a neural\nrepresentation of the data, capturing associations between features. It\nextracts rules from this neural representation by exploiting the model's\nreconstruction mechanism. Extensive evaluations on five datasets against seven\nbaselines demonstrate that Aerial+ achieves state-of-the-art results by\nlearning more concise, high-quality rule sets with full data coverage. When\nintegrated into rule-based interpretable machine learning models, Aerial+\nsignificantly reduces execution time while maintaining or improving accuracy.\n","authors":["Erkan Karabulut","Paul Groth","Victoria Degeler"],"pdf_url":"https://arxiv.org/pdf/2504.19354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19182v3","updated":"2025-06-05T13:57:28Z","published":"2025-01-31T14:46:11Z","title":"A Compressive-Expressive Communication Framework for Compositional\n  Representations","summary":"  Compositional generalization--the ability to interpret novel combinations of\nfamiliar elements--is a hallmark of human cognition and language. Despite\nrecent advances, deep neural networks still struggle to acquire this property\nreliably. In this work, we introduce CELEBI (Compressive-Expressive Language\nEmergence through a discrete Bottleneck and Iterated learning), a novel\nself-supervised framework for inducing compositionality in learned\nrepresentations from pre-trained models, through a reconstruction-based\ncommunication game between a sender and a receiver. Building on theories of\nlanguage emergence, we integrate three mechanisms that jointly promote\ncompressibility, expressivity, and efficiency in the emergent language. First,\ninteractive decoding incentivizes intermediate reasoning by requiring the\nreceiver to produce partial reconstructions after each symbol. Second, a\nreconstruction-based imitation phase, inspired by iterated learning, trains\nsuccessive generations of agents to imitate reconstructions rather than\nmessages, enforcing a tighter communication bottleneck. Third, pairwise\ndistance maximization regularizes message diversity by encouraging high\ndistances between messages, with formal links to entropy maximization. Our\nmethod significantly improves both the efficiency and compositionality of the\nlearned messages on the Shapes3D and MPI3D datasets, surpassing prior discrete\ncommunication frameworks in both reconstruction accuracy and topographic\nsimilarity. This work provides new theoretical and empirical evidence for the\nemergence of structured, generalizable communication protocols from\nsimplicity-based inductive biases.\n","authors":["Rafael Elberg","Felipe del Rio","Mircea Petrache","Denis Parra"],"pdf_url":"https://arxiv.org/pdf/2501.19182v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05047v1","updated":"2025-06-05T13:56:18Z","published":"2025-06-05T13:56:18Z","title":"Reliably detecting model failures in deployment without labels","summary":"  The distribution of data changes over time; models operating operating in\ndynamic environments need retraining. But knowing when to retrain, without\naccess to labels, is an open challenge since some, but not all shifts degrade\nmodel performance. This paper formalizes and addresses the problem of\npost-deployment deterioration (PDD) monitoring. We propose D3M, a practical and\nefficient monitoring algorithm based on the disagreement of predictive models,\nachieving low false positive rates under non-deteriorating shifts and provides\nsample complexity bounds for high true positive rates under deteriorating\nshifts. Empirical results on both standard benchmark and a real-world\nlarge-scale internal medicine dataset demonstrate the effectiveness of the\nframework and highlight its viability as an alert mechanism for high-stakes\nmachine learning pipelines.\n","authors":["Viet Nguyen Changjian Shui","Vijay Giri","Siddarth Arya","Amol Verma","Fahad Razak","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2506.05047v1.pdf","comment":"36 pages, 6 figures, 7 tables, submitted to NeurIPS 2025, includes\n  theoretical analysis and extensive empirical evaluation across benchmark and\n  clinical datasets. Code available at https://github.com/teivng/d3m. Viet\n  Nguyen and Changjian Shui contributed equally"},{"id":"http://arxiv.org/abs/2505.13438v2","updated":"2025-06-05T13:55:06Z","published":"2025-05-19T17:58:44Z","title":"Optimizing Anytime Reasoning via Budget Relative Policy Optimization","summary":"  Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.\n","authors":["Penghui Qi","Zichen Liu","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2505.13438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12911v4","updated":"2025-06-05T13:47:27Z","published":"2025-01-22T14:37:44Z","title":"A Selective Homomorphic Encryption Approach for Faster\n  Privacy-Preserving Federated Learning","summary":"  Federated learning (FL) has come forward as a critical approach for\nprivacy-preserving machine learning in healthcare, allowing collaborative model\ntraining across decentralized medical datasets without exchanging clients'\ndata. However, current security implementations for these systems face a\nfundamental trade-off: rigorous cryptographic protections like fully\nhomomorphic encryption (FHE) impose prohibitive computational overhead, while\nlightweight alternatives risk vulnerable data leakage through model updates. To\naddress this issue, we present FAS (Fast and Secure Federated Learning), a\nnovel approach that strategically combines selective homomorphic encryption,\ndifferential privacy, and bitwise scrambling to achieve robust security without\ncompromising practical usability. Our approach eliminates the need for model\npretraining phases while dynamically protecting high-risk model parameters\nthrough layered encryption and obfuscation. We implemented FAS using the Flower\nframework and evaluated it on a cluster of eleven physical machines. Our\napproach was up to 90\\% faster than applying FHE on the model weights. In\naddition, we eliminated the computational overhead that is required by\ncompetitors such as FedML-HE and MaskCrypt. Our approach was up to 1.5$\\times$\nfaster than the competitors while achieving comparable security results.\n  Experimental evaluations on medical imaging datasets confirm that FAS\nmaintains similar security results to conventional FHE against gradient\ninversion attacks while preserving diagnostic model accuracy. These results\nposition FAS as a practical solution for latency-sensitive healthcare\napplications where both privacy preservation and computational efficiency are\nrequirements.\n","authors":["Abdulkadir Korkmaz","Praveen Rao"],"pdf_url":"https://arxiv.org/pdf/2501.12911v4.pdf","comment":"18 pages, 18 figures"},{"id":"http://arxiv.org/abs/2506.05039v1","updated":"2025-06-05T13:43:24Z","published":"2025-06-05T13:43:24Z","title":"iN2V: Bringing Transductive Node Embeddings to Inductive Graphs","summary":"  Shallow node embeddings like node2vec (N2V) can be used for nodes without\nfeatures or to supplement existing features with structure-based information.\nEmbedding methods like N2V are limited in their application on new nodes, which\nrestricts them to the transductive setting where the entire graph, including\nthe test nodes, is available during training. We propose inductive node2vec\n(iN2V), which combines a post-hoc procedure to compute embeddings for nodes\nunseen during training and modifications to the original N2V training procedure\nto prepare the embeddings for this post-hoc procedure. We conduct experiments\non several benchmark datasets and demonstrate that iN2V is an effective\napproach to bringing transductive embeddings to an inductive setting. Using\niN2V embeddings improves node classification by 1 point on average, with up to\n6 points of improvement depending on the dataset and the number of unseen\nnodes. Our iN2V is a plug-in approach to create new or enrich existing\nembeddings. It can also be combined with other embedding methods, making it a\nversatile approach for inductive node representation learning. Code to\nreproduce the results is available at https://github.com/Foisunt/iN2V .\n","authors":["Nicolas Lell","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2506.05039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11260v2","updated":"2025-06-05T13:43:08Z","published":"2025-02-16T20:28:42Z","title":"Scalable Multi-Agent Offline Reinforcement Learning and the Role of\n  Information","summary":"  Offline Reinforcement Learning (RL) focuses on learning policies solely from\na batch of previously collected data. offering the potential to leverage such\ndatasets effectively without the need for costly or risky active exploration.\nWhile recent advances in Offline Multi-Agent RL (MARL) have shown promise, most\nexisting methods either rely on large datasets jointly collected by all agents\nor agent-specific datasets collected independently. The former approach ensures\nstrong performance but raises scalability concerns, while the latter emphasizes\nscalability at the expense of performance guarantees. In this work, we propose\na novel scalable routine for both dataset collection and offline learning.\nAgents first collect diverse datasets coherently with a pre-specified\ninformation-sharing network and subsequently learn coherent localized policies\nwithout requiring either full observability or falling back to complete\ndecentralization. We theoretically demonstrate that this structured approach\nallows a multi-agent extension of the seminal Fitted Q-Iteration (FQI)\nalgorithm to globally converge, in high probability, to near-optimal policies.\nThe convergence is subject to error terms that depend on the informativeness of\nthe shared information. Furthermore, we show how this approach allows to bound\nthe inherent error of the supervised-learning phase of FQI with the mutual\ninformation between shared and unshared information. Our algorithm, SCAlable\nMulti-agent FQI (SCAM-FQI), is then evaluated on a distributed decision-making\nproblem. The empirical results align with our theoretical findings, supporting\nthe effectiveness of SCAM-FQI in achieving a balance between scalability and\npolicy performance.\n","authors":["Riccardo Zamboni","Enrico Brunetti","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2502.11260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05035v1","updated":"2025-06-05T13:40:40Z","published":"2025-06-05T13:40:40Z","title":"TIMING: Temporality-Aware Integrated Gradients for Time Series\n  Explanation","summary":"  Recent explainable artificial intelligence (XAI) methods for time series\nprimarily estimate point-wise attribution magnitudes, while overlooking the\ndirectional impact on predictions, leading to suboptimal identification of\nsignificant points. Our analysis shows that conventional Integrated Gradients\n(IG) effectively capture critical points with both positive and negative\nimpacts on predictions. However, current evaluation metrics fail to assess this\ncapability, as they inadvertently cancel out opposing feature contributions. To\naddress this limitation, we propose novel evaluation metrics-Cumulative\nPrediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to\nsystematically assess whether attribution methods accurately identify\nsignificant positive and negative points in time series XAI. Under these\nmetrics, conventional IG outperforms recent counterparts. However, directly\napplying IG to time series data may lead to suboptimal outcomes, as generated\npaths ignore temporal relationships and introduce out-of-distribution samples.\nTo overcome these challenges, we introduce TIMING, which enhances IG by\nincorporating temporal awareness while maintaining its theoretical properties.\nExtensive experiments on synthetic and real-world time series benchmarks\ndemonstrate that TIMING outperforms existing time series XAI baselines. Our\ncode is available at https://github.com/drumpt/TIMING.\n","authors":["Hyeongwon Jang","Changhun Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2506.05035v1.pdf","comment":"ICML 2025 Spotlight Presentation; Code is available at\n  https://github.com/drumpt/TIMING"},{"id":"http://arxiv.org/abs/2506.05032v1","updated":"2025-06-05T13:40:11Z","published":"2025-06-05T13:40:11Z","title":"Identifying and Understanding Cross-Class Features in Adversarial\n  Training","summary":"  Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.\n","authors":["Zeming Wei","Yiwen Guo","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05032v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.05030v1","updated":"2025-06-05T13:39:37Z","published":"2025-06-05T13:39:37Z","title":"Artificial Intelligence Should Genuinely Support Clinical Reasoning and\n  Decision Making To Bridge the Translational Gap","summary":"  Artificial intelligence promises to revolutionise medicine, yet its impact\nremains limited because of the pervasive translational gap. We posit that the\nprevailing technology-centric approaches underpin this challenge, rendering\nsuch systems fundamentally incompatible with clinical practice, specifically\ndiagnostic reasoning and decision making. Instead, we propose a novel\nsociotechnical conceptualisation of data-driven support tools designed to\ncomplement doctors' cognitive and epistemic activities. Crucially, it\nprioritises real-world impact over superhuman performance on inconsequential\nbenchmarks.\n","authors":["Kacper Sokol","James Fackler","Julia E Vogt"],"pdf_url":"https://arxiv.org/pdf/2506.05030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16950v2","updated":"2025-06-05T13:38:34Z","published":"2025-05-22T17:33:49Z","title":"Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning","summary":"  Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.\n","authors":["Adnan Oomerjee","Zafeirios Fountas","Zhongwei Yu","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05027v1","updated":"2025-06-05T13:37:33Z","published":"2025-06-05T13:37:33Z","title":"Tuning the Right Foundation Models is What you Need for Partial Label\n  Learning","summary":"  Partial label learning (PLL) seeks to train generalizable classifiers from\ndatasets with inexact supervision, a common challenge in real-world\napplications. Existing studies have developed numerous approaches to\nprogressively refine and recover ground-truth labels by training convolutional\nneural networks. However, limited attention has been given to foundation models\nthat offer transferrable representations. In this work, we empirically conduct\ncomprehensive evaluations of 11 foundation models across 13 PLL approaches on 8\nbenchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an\nefficient fine-tuning framework for foundation models in PLL. Our findings\nreveal that current PLL approaches tend to 1) achieve significant performance\ngains when using foundation models, 2) exhibit remarkably similar performance\nto each other, 3) maintain stable performance across varying ambiguity levels,\nwhile 4) are susceptible to foundation model selection and adaptation\nstrategies. Additionally, we demonstrate the efficacy of text-embedding\nclassifier initialization and effective candidate label filtering using\nzero-shot CLIP. Our experimental results and analysis underscore the\nlimitations of current PLL approaches and provide valuable insights for\ndeveloping more generalizable PLL models. The source code can be found at\nhttps://github.com/SEU-hk/PartialCLIP.\n","authors":["Kuang He","Wei Tang","Tong Wei","Min-Ling Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05027v1.pdf","comment":"The code can be found at \\url{https://github.com/SEU-hk/PartialCLIP}"},{"id":"http://arxiv.org/abs/2311.16872v3","updated":"2025-06-05T13:35:36Z","published":"2023-11-28T15:24:02Z","title":"A unified weighting framework for evaluating nearest neighbour\n  classification","summary":"  We present the first comprehensive and large-scale evaluation of classical\n(NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification. We\nstandardise existing proposals for nearest neighbour weighting with kernel\nfunctions, applied to the distance values and/or ranks of the nearest\nneighbours of a test instance. In particular, we show that the theoretically\noptimal Samworth weights converge to a kernel. Kernel functions are closely\nrelated to fuzzy negation operators, and we propose a new kernel based on Yager\nnegation. We also consider various distance and scaling measures, which we show\ncan be related to each other. Through a systematic series of experiments on 85\nreal-life classification datasets, we find that NN, FNN and FRNN all perform\nbest with Boscovich distance, and that NN and FRNN perform best with a\ncombination of Samworth rank- and distance-weights and scaling by the mean\nabsolute deviation around the median ($r_1$), the standard deviation ($r_2$) or\nthe semi-interquartile range ($r_{\\infty}^*$), while FNN performs best with\nonly Samworth distance-weights and $r_1$- or $r_2$-scaling. However, NN\nachieves comparable performance with Yager-$\\frac{1}{2}$ distance-weights,\nwhich are simpler to implement than a combination of Samworth distance- and\nrank-weights. Finally, FRNN generally outperforms NN, which in turn performs\nsystematically better than FNN.\n","authors":["Oliver Urs Lenz","Henri Bollaert","Chris Cornelis"],"pdf_url":"https://arxiv.org/pdf/2311.16872v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09117v3","updated":"2025-06-05T13:34:42Z","published":"2025-03-12T07:08:54Z","title":"GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs","summary":"  Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. It motivates\nthis paper to explore enhanced unlearning schemes that can mitigate this\ntrade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an\nimproved framework that regulates the directions of gradient updates during the\nunlearning procedure such that their side impacts on other, unrelated responses\ncan be minimized. GRU is easy and general to implement, demonstrating practical\neffectiveness across a variety of well-established unlearning benchmarks.\n","authors":["Yue Wang","Qizhou Wang","Feng Liu","Wei Huang","Yali Du","Xiaojiang Du","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2503.09117v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01405v7","updated":"2025-06-05T13:30:23Z","published":"2024-10-02T10:31:17Z","title":"On Expressive Power of Looped Transformers: Theoretical Analysis and\n  Enhancement via Timestep Encoding","summary":"  Looped Transformers provide advantages in parameter efficiency, computational\ncapabilities, and generalization for reasoning tasks. However, their expressive\npower regarding function approximation remains underexplored. In this paper, we\nestablish the approximation rate of Looped Transformers by defining the modulus\nof continuity for sequence-to-sequence functions. This reveals a limitation\nspecific to the looped architecture. That is, the analysis prompts the\nincorporation of scaling parameters for each loop, conditioned on timestep\nencoding. Experiments validate the theoretical results, showing that increasing\nthe number of loops enhances performance, with further gains achieved through\nthe timestep encoding.\n","authors":["Kevin Xu","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2410.01405v7.pdf","comment":"Camera-ready version for ICML 2025"},{"id":"http://arxiv.org/abs/2410.10390v2","updated":"2025-06-05T13:25:39Z","published":"2024-10-14T11:24:41Z","title":"Stein Variational Evolution Strategies","summary":"  Stein Variational Gradient Descent (SVGD) is a highly efficient method to\nsample from an unnormalized probability distribution. However, the SVGD update\nrelies on gradients of the log-density, which may not always be available.\nExisting gradient-free versions of SVGD make use of simple Monte Carlo\napproximations or gradients from surrogate distributions, both with\nlimitations. To improve gradient-free Stein variational inference, we combine\nSVGD steps with evolution strategy (ES) updates. Our results demonstrate that\nthe resulting algorithm generates high-quality samples from unnormalized target\ndensities without requiring gradient information. Compared to prior\ngradient-free SVGD methods, we find that the integration of the ES update in\nSVGD significantly improves the performance on multiple challenging benchmark\nproblems.\n","authors":["Cornelius V. Braun","Robert T. Lange","Marc Toussaint"],"pdf_url":"https://arxiv.org/pdf/2410.10390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05017v1","updated":"2025-06-05T13:25:28Z","published":"2025-06-05T13:25:28Z","title":"Controlling Summarization Length Through EOS Token Weighting","summary":"  Controlling the length of generated text can be crucial in various\ntext-generation tasks, including summarization. Existing methods often require\ncomplex model alterations, limiting compatibility with pre-trained models. We\naddress these limitations by developing a simple approach for controlling the\nlength of automatic text summaries by increasing the importance of correctly\npredicting the EOS token in the cross-entropy loss computation. The proposed\nmethodology is agnostic to architecture and decoding algorithms and orthogonal\nto other inference-time techniques to control the generation length. We tested\nit with encoder-decoder and modern GPT-style LLMs, and show that this method\ncan control generation length, often without affecting the quality of the\nsummary.\n","authors":["Zeno Belligoli","Emmanouil Stergiadis","Eran Fainman","Ilya Gusev"],"pdf_url":"https://arxiv.org/pdf/2506.05017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03789v2","updated":"2025-06-05T13:22:50Z","published":"2025-05-01T04:49:53Z","title":"A new architecture of high-order deep neural networks that learn\n  martingales","summary":"  A new deep-learning neural network architecture based on high-order weak\napproximation algorithms for stochastic differential equations (SDEs) is\nproposed. The architecture enables the efficient learning of martingales by\ndeep learning models. The behaviour of deep neural networks based on this\narchitecture, when applied to the problem of pricing financial derivatives, is\nalso examined. The core of this new architecture lies in the high-order weak\napproximation algorithms of the explicit Runge--Kutta type, wherein the\napproximation is realised solely through iterative compositions and linear\ncombinations of vector fields of the target SDEs.\n","authors":["Syoiti Ninomiya","Yuming Ma"],"pdf_url":"https://arxiv.org/pdf/2505.03789v2.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.05016v1","updated":"2025-06-05T13:22:47Z","published":"2025-06-05T13:22:47Z","title":"Multi-Point Proximity Encoding For Vector-Mode Geospatial Machine\n  Learning","summary":"  Vector-mode geospatial data -- points, lines, and polygons -- must be encoded\ninto an appropriate form in order to be used with traditional machine learning\nand artificial intelligence models. Encoding methods attempt to represent a\ngiven shape as a vector that captures its essential geometric properties. This\npaper presents an encoding method based on scaled distances from a shape to a\nset of reference points within a region of interest. The method, MultiPoint\nProximity (MPP) encoding, can be applied to any type of shape, enabling the\nparameterization of machine learning models with encoded representations of\nvector-mode geospatial features. We show that MPP encoding possesses the\ndesirable properties of shape-centricity and continuity, can be used to\ndifferentiate spatial objects based on their geometric features, and can\ncapture pairwise spatial relationships with high precision. In all cases, MPP\nencoding is shown to perform better than an alternative method based on\nrasterization.\n","authors":["John Collins"],"pdf_url":"https://arxiv.org/pdf/2506.05016v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.05014v1","updated":"2025-06-05T13:22:29Z","published":"2025-06-05T13:22:29Z","title":"Towards Reasonable Concept Bottleneck Models","summary":"  In this paper, we propose $\\textbf{C}$oncept $\\textbf{REA}$soning\n$\\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)\nthat: (i) explicitly encodes concept-concept (${\\texttt{C-C}}$) and\nconcept-task (${\\texttt{C$\\rightarrow$Y}}$) relationships to enforce a desired\nmodel reasoning; and (ii) use a regularized side-channel to achieve competitive\ntask performance, while keeping high concept importance. Specifically, CREAM\narchitecturally embeds (bi)directed concept-concept, and concept to task\nrelationships specified by a human expert, while severing undesired information\nflows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates\na black-box side-channel that is regularized to encourage task predictions to\nbe grounded in the relevant concepts, thereby utilizing the side-channel only\nwhen necessary to enhance performance. Our experiments show that: (i) CREAM\nmainly relies on concepts while achieving task performance on par with\nblack-box models; and (ii) the embedded ${\\texttt{C-C}}$ and\n${\\texttt{C$\\rightarrow$Y}}$ relationships ease model interventions and\nmitigate concept leakage.\n","authors":["Nektarios Kalampalikis","Kavya Gupta","Georgi Vitanov","Isabel Valera"],"pdf_url":"https://arxiv.org/pdf/2506.05014v1.pdf","comment":"26 pages, 17 figures"},{"id":"http://arxiv.org/abs/2502.13063v2","updated":"2025-06-05T13:20:09Z","published":"2025-02-18T17:08:45Z","title":"Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity","summary":"  A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.\n","authors":["Yuri Kuratov","Mikhail Arkhipov","Aydar Bulatov","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2502.13063v2.pdf","comment":"ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2504.12880v2","updated":"2025-06-05T13:20:02Z","published":"2025-04-17T12:13:25Z","title":"Can Masked Autoencoders Also Listen to Birds?","summary":"  Masked Autoencoders (MAEs) have shown competitive results in audio\nclassification by learning rich semantic representations through an efficient\nself-supervised reconstruction task. However, general-purpose models fail to\ngeneralize well when applied directly to fine-grained audio domains.\nSpecifically, bird-sound classification requires distinguishing subtle\ninter-species differences and managing high intra-species acoustic variability,\nthereby revealing the performance limitations of general-domain Audio-MAE\nmodels. This work demonstrates that bridging this domain gap requires more than\ndomain-specific pretraining data; adapting the entire training pipeline is\ncrucial. We systematically revisit and adapt the pretraining recipe,\nfine-tuning methods, and frozen feature utilization to bird sounds using\nBirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our\nresulting Bird-MAE achieves new state-of-the-art results in BirdSet's\nmulti-label classification benchmark. Additionally, we introduce the\nparameter-efficient prototypical probing, enhancing the utility of frozen MAE\nrepresentations and closely approaching fine-tuning performance in low-resource\nsettings. Bird-MAE's prototypical probes outperform linear probing by up to\n37%$_\\text{p}$ in MAP and narrow the gap to fine-tuning to approximately\n3.3%$_\\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also\ndemonstrates robust few-shot capabilities with prototypical probing in our\nnewly established few-shot benchmark on BirdSet, highlighting the potential of\ntailored self-supervised learning pipelines for fine-grained audio domains.\n","authors":["Lukas Rauch","René Heinrich","Ilyass Moummad","Alexis Joly","Bernhard Sick","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2504.12880v2.pdf","comment":"under review @TMLR"},{"id":"http://arxiv.org/abs/2506.05007v1","updated":"2025-06-05T13:17:50Z","published":"2025-06-05T13:17:50Z","title":"QiMeng: Fully Automated Hardware and Software Design for Processor Chip","summary":"  Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.\n","authors":["Rui Zhang","Yuanbo Wen","Shuyao Cheng","Di Huang","Shaohui Peng","Jiaming Guo","Pengwei Jin","Jiacheng Zhao","Tianrui Ma","Yaoyu Zhu","Yifan Hao","Yongwei Zhao","Shengwen Liang","Ying Wang","Xing Hu","Zidong Du","Huimin Cui","Ling Li","Qi Guo","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2506.05007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23158v2","updated":"2025-06-05T13:14:42Z","published":"2024-10-30T16:11:40Z","title":"Monotonic anomaly detection","summary":"  Semi-supervised anomaly detection is based on the principle that potential\nanomalies are those records that look different from normal training data.\nHowever, in some cases we are specifically interested in anomalies that\ncorrespond to high attribute values (or low, but not both). We present two\nasymmetrical distance measures that take this monotonicity into account: ramp\ndistance and signed distance. Through experiments on synthetic and real-life\ndatasets, we show that ramp distance increases anomaly detection performance\nover the traditional absolute distance. While signed distance also performs\nwell on synthetic data, it performs substantially poorer on real-life datasets.\nWe argue that this is a consequence of the fact that when using signed\ndistance, low values of certain attributes automatically compensate for high\nvalues of other attributes, such that anomaly detection is reduced to counting\nthe total attribute value sum, which is too simplistic in practice.\n","authors":["Oliver Urs Lenz","Matthijs van Leeuwen"],"pdf_url":"https://arxiv.org/pdf/2410.23158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05005v1","updated":"2025-06-05T13:13:48Z","published":"2025-06-05T13:13:48Z","title":"Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General\n  Games","summary":"  Recent work [Soleymani et al., 2025] introduced a variant of Optimistic\nMultiplicative Weights Updates (OMWU) that adaptively controls the learning\npace in a dynamic, non-monotone manner, achieving new state-of-the-art regret\nminimization guarantees in general games. In this work, we demonstrate that\nno-regret learning acceleration through adaptive pacing of the learners is not\nan isolated phenomenon. We introduce \\emph{Cautious Optimism}, a framework for\nsubstantially faster regularized learning in general games. Cautious Optimism\ntakes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs\nan accelerated no-regret learning algorithm by pacing the underlying FTRL with\nminimal computational overhead. Importantly, we retain uncoupledness (learners\ndo not need to know other players' utilities). Cautious Optimistic FTRL\nachieves near-optimal $O_T(\\log T)$ regret in diverse self-play\n(mixing-and-matching regularizers) while preserving the optimal $O(\\sqrt{T})$\nregret in adversarial scenarios. In contrast to prior works (e.g. Syrgkanis et\nal. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic\nstep-sizes, showcasing a novel route for fast learning in general games.\n","authors":["Ashkan Soleymani","Georgios Piliouras","Gabriele Farina"],"pdf_url":"https://arxiv.org/pdf/2506.05005v1.pdf","comment":"Extended abstract appeared at Twenty-Sixth ACM Conference on\n  Economics and Computation (EC), 2025"},{"id":"http://arxiv.org/abs/2506.04985v1","updated":"2025-06-05T12:56:12Z","published":"2025-06-05T12:56:12Z","title":"FPTQuant: Function-Preserving Transforms for LLM Quantization","summary":"  Large language models (LLMs) require substantial compute, and thus energy, at\ninference time. While quantizing weights and activations is effective at\nimproving efficiency, naive quantization of LLMs can significantly degrade\nperformance due to large magnitude outliers. This paper describes FPTQuant,\nwhich introduces four novel, lightweight, and expressive function-preserving\ntransforms (FPTs) to facilitate quantization of transformers: (1) a mergeable\npre-RoPE transform for queries and keys, (2) a mergeable transform for values,\n(3) a mergeable scaling transform within the MLP block, and (4) a cheap,\ndynamic scaling transform. By leveraging the equivariances and independencies\ninherent to canonical transformer operation, we designed these FPTs to maintain\nthe model's function while shaping the intermediate activation distributions to\nbe more quantization friendly. FPTQuant requires no custom kernels and adds\nvirtually no overhead during inference. The FPTs are trained both locally to\nreduce outliers, and end-to-end such that the outputs of the quantized and\nfull-precision models match. FPTQuant enables static INT4 quantization with\nminimal overhead and shows SOTA speed-up of up to 3.9 times over FP.\nEmpirically, FPTQuant has an excellent accuracy-speed trade-off -- it is\nperforming on par or exceeding most prior work and only shows slightly lower\naccuracy compared to a method that is up to 29% slower.\n","authors":["Boris van Breugel","Yelysei Bondarenko","Paul Whatmough","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2506.04985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04980v1","updated":"2025-06-05T12:50:54Z","published":"2025-06-05T12:50:54Z","title":"Agentic AI for Intent-Based Industrial Automation","summary":"  The recent development of Agentic AI systems, empowered by autonomous large\nlanguage models (LLMs) agents with planning and tool-usage capabilities,\nenables new possibilities for the evolution of industrial automation and\nreduces the complexity introduced by Industry 4.0. This work proposes a\nconceptual framework that integrates Agentic AI with the intent-based paradigm,\noriginally developed in network research, to simplify human-machine interaction\n(HMI) and better align automation systems with the human-centric, sustainable,\nand resilient principles of Industry 5.0. Based on the intent-based processing,\nthe framework allows human operators to express high-level business or\noperational goals in natural language, which are decomposed into actionable\ncomponents. These intents are broken into expectations, conditions, targets,\ncontext, and information that guide sub-agents equipped with specialized tools\nto execute domain-specific tasks. A proof of concept was implemented using the\nCMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the\nfeasibility of intent decomposition, agent orchestration, and autonomous\ndecision-making in predictive maintenance scenarios. The results confirm the\npotential of this approach to reduce technical barriers and enable scalable,\nintent-driven automation, despite data quality and explainability concerns.\n","authors":["Marcos Lima Romero","Ricardo Suyama"],"pdf_url":"https://arxiv.org/pdf/2506.04980v1.pdf","comment":"Preprint - Submitted to 16th IEEE/IAS International Conference on\n  Industry Applications - INDUSCON 2025"},{"id":"http://arxiv.org/abs/2306.04952v2","updated":"2025-06-05T12:46:15Z","published":"2023-06-08T05:56:05Z","title":"Entropy-based Training Methods for Scalable Neural Implicit Sampler","summary":"  Efficiently sampling from un-normalized target distributions is a fundamental\nproblem in scientific computing and machine learning. Traditional approaches\nsuch as Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased\nsamples from such distributions but suffer from computational inefficiency,\nparticularly when dealing with high-dimensional targets, as they require\nnumerous iterations to generate a batch of samples. In this paper, we introduce\nan efficient and scalable neural implicit sampler that overcomes these\nlimitations. The implicit sampler can generate large batches of samples with\nlow computational costs by leveraging a neural transformation that directly\nmaps easily sampled latent vectors to target samples without the need for\niterative procedures. To train the neural implicit samplers, we introduce two\nnovel methods: the KL training method and the Fisher training method. The\nformer method minimizes the Kullback-Leibler divergence, while the latter\nminimizes the Fisher divergence between the sampler and the target\ndistributions. By employing the two training methods, we effectively optimize\nthe neural implicit samplers to learn and generate from the desired target\ndistribution. To demonstrate the effectiveness, efficiency, and scalability of\nour proposed samplers, we evaluate them on three sampling benchmarks with\ndifferent scales.\n","authors":["Weijian Luo","Boya Zhang","Zhihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04952v2.pdf","comment":"Revision: The paper was accepted by NeurIPS2023"},{"id":"http://arxiv.org/abs/2410.18881v2","updated":"2025-06-05T12:44:07Z","published":"2024-10-24T16:17:18Z","title":"Diff-Instruct++: Training One-step Text-to-image Generator Model to\n  Align with Human Preferences","summary":"  One-step text-to-image generator models offer advantages such as swift\ninference efficiency, flexible architectures, and state-of-the-art generation\nperformance. In this paper, we study the problem of aligning one-step generator\nmodels with human preferences for the first time. Inspired by the success of\nreinforcement learning using human feedback (RLHF), we formulate the alignment\nproblem as maximizing expected human reward functions while adding an Integral\nKullback-Leibler divergence term to prevent the generator from diverging. By\novercoming technical challenges, we introduce Diff-Instruct++ (DI++), the\nfirst, fast-converging and image data-free human preference alignment method\nfor one-step text-to-image generators. We also introduce novel theoretical\ninsights, showing that using CFG for diffusion distillation is secretly doing\nRLHF with DI++. Such an interesting finding brings understanding and potential\ncontributions to future research involving CFG. In the experiment sections, we\nalign both UNet-based and DiT-based one-step generators using DI++, which use\nthe Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion\nprocesses. The resulting DiT-based one-step text-to-image model achieves a\nstrong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO\nvalidation prompt dataset. It also achieves a leading Human preference Score\n(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable\nDiffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical\ncontributions and empirical evidence indicate that DI++ is a strong\nhuman-preference alignment approach for one-step text-to-image models. The\nhomepage of the paper is https://github.com/pkulwj1994/diff_instruct_pp.\n","authors":["Weijian Luo"],"pdf_url":"https://arxiv.org/pdf/2410.18881v2.pdf","comment":"Revision: The paper was accepted by Transactions of Machine Learning\n  Research (TMLR)"},{"id":"http://arxiv.org/abs/2506.03988v2","updated":"2025-06-05T12:39:28Z","published":"2025-06-04T14:16:00Z","title":"RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated\n  Image Detectors","summary":"  AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustness. Our findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID.\n","authors":["Hicham Eddoubi","Jonas Ricker","Federico Cocchi","Lorenzo Baraldi","Angelo Sotgiu","Maura Pintor","Marcella Cornia","Lorenzo Baraldi","Asja Fischer","Rita Cucchiara","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2506.03988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07286v2","updated":"2025-06-05T12:37:46Z","published":"2025-05-12T07:18:09Z","title":"Piloting Structure-Based Drug Design via Modality-Specific Optimal\n  Schedule","summary":"  Structure-Based Drug Design (SBDD) is crucial for identifying bioactive\nmolecules. Recent deep generative models are faced with challenges in geometric\nstructure modeling. A major bottleneck lies in the twisted probability path of\nmulti-modalities -- continuous 3D positions and discrete 2D topologies -- which\njointly determine molecular geometries. By establishing the fact that noise\nschedules decide the Variational Lower Bound (VLB) for the twisted probability\npath, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored\narea, which optimizes VLB as a path integral for SBDD. Our model effectively\nenhances molecular geometries and interaction modeling, achieving\nstate-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10%\nimprovement upon strong baselines, while maintaining high affinities and robust\nintramolecular validity evaluated on held-out test set. Code is available at\nhttps://github.com/AlgoMole/MolCRAFT.\n","authors":["Keyue Qiu","Yuxuan Song","Zhehuan Fan","Peidong Liu","Zhe Zhang","Mingyue Zheng","Hao Zhou","Wei-Ying Ma"],"pdf_url":"https://arxiv.org/pdf/2505.07286v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2405.17951v3","updated":"2025-06-05T12:34:18Z","published":"2024-05-28T08:28:18Z","title":"Efficient Time Series Processing for Transformers and State-Space Models\n  through Token Merging","summary":"  Despite recent advances in subquadratic attention mechanisms or state-space\nmodels, processing long token sequences still imposes significant computational\nrequirements. Token merging has emerged as a solution to increase computational\nefficiency in computer vision architectures. In this work, we perform the first\ninvestigations of token merging in time series analysis on both transformers\nand state-space models. We further introduce local merging, a domain-specific\ntoken merging algorithm that selectively combines tokens within a local\nneighborhood, achieving two major benefits: a) Local merging can adjust its\ncomputational complexity from quadratic to linear based on the neighborhood\nsize to effectively scale to long sequences; b) Local merging is the first\ncausal merging scheme enabling token merging in transformer decoders. Further,\nwe identify spectral properties of the input data that reliably predict the\npotential benefits of local merging without requiring evaluation on downstream\ntasks. Our comprehensive empirical evaluation demonstrates that local merging\noffers substantial efficiency gains with minimal impact on accuracy, achieving\nup to 5400% acceleration on the recently proposed Chronos foundation model.\n","authors":["Leon Götz","Marcel Kollovieh","Stephan Günnemann","Leo Schwinn"],"pdf_url":"https://arxiv.org/pdf/2405.17951v3.pdf","comment":"21 pages in total, 20 figures"},{"id":"http://arxiv.org/abs/2506.04945v1","updated":"2025-06-05T12:20:50Z","published":"2025-06-05T12:20:50Z","title":"Learning Joint Interventional Effects from Single-Variable Interventions\n  in Additive Models","summary":"  Estimating causal effects of joint interventions on multiple variables is\ncrucial in many domains, but obtaining data from such simultaneous\ninterventions can be challenging. Our study explores how to learn joint\ninterventional effects using only observational data and single-variable\ninterventions. We present an identifiability result for this problem, showing\nthat for a class of nonlinear additive outcome mechanisms, joint effects can be\ninferred without access to joint interventional data. We propose a practical\nestimator that decomposes the causal effect into confounded and unconfounded\ncontributions for each intervention variable. Experiments on synthetic data\ndemonstrate that our method achieves performance comparable to models trained\ndirectly on joint interventional data, outperforming a purely observational\nestimator.\n","authors":["Armin Kekić","Sergio Hernan Garrido Mejia","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2506.04945v1.pdf","comment":"To be published at the International Conference on Machine Learning\n  (ICML) 2025"},{"id":"http://arxiv.org/abs/2405.19256v2","updated":"2025-06-05T12:19:11Z","published":"2024-05-29T16:41:42Z","title":"Weak Generative Sampler to Efficiently Sample Invariant Distribution of\n  Stochastic Differential Equation","summary":"  Sampling invariant distributions from an It\\^o diffusion process presents a\nsignificant challenge in stochastic simulation. Traditional numerical solvers\nfor stochastic differential equations require both a fine step size and a\nlengthy simulation period, resulting in biased and correlated samples. The\ncurrent deep learning-based method solves the stationary Fokker--Planck\nequation to determine the invariant probability density function in the form of\ndeep neural networks, but they generally do not directly address the problem of\nsampling from the computed density function. In this work, we introduce a\nframework that employs a weak generative sampler (WGS) to directly generate\nindependent and identically distributed (iid) samples induced by a\ntransformation map derived from the stationary Fokker--Planck equation. Our\nproposed loss function is based on the weak form of the Fokker--Planck\nequation, integrating normalizing flows to characterize the invariant\ndistribution and facilitate sample generation from a base distribution. Our\nrandomized test function circumvents the need for min-max optimization in the\ntraditional weak formulation. Our method necessitates neither the\ncomputationally intensive calculation of the Jacobian determinant nor the\ninvertibility of the transformation map. A crucial component of our framework\nis the adaptively chosen family of test functions in the form of Gaussian\nkernel functions with centers related to the generated data samples.\nExperimental results on several benchmark examples demonstrate the\neffectiveness and scalability of our method, which offers both low\ncomputational costs and excellent capability in exploring multiple metastable\nstates.\n","authors":["Zhiqiang Cai","Yu Cao","Yuanfei Huang","Xiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.19256v2.pdf","comment":"33 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.20898v3","updated":"2025-06-05T12:11:49Z","published":"2024-10-28T10:26:19Z","title":"David and Goliath: Small One-step Model Beats Large Diffusion with Score\n  Post-training","summary":"  We propose Diff-Instruct* (DI*), a data-efficient post-training approach for\none-step text-to-image generative models to improve its human preferences\nwithout requiring image data. Our method frames alignment as online\nreinforcement learning from human feedback (RLHF), which optimizes the one-step\nmodel to maximize human reward functions while being regularized to be kept\nclose to a reference diffusion process. Unlike traditional RLHF approaches,\nwhich rely on the Kullback-Leibler divergence as the regularization, we\nintroduce a novel general score-based divergence regularization that\nsubstantially improves performance as well as post-training stability. Although\nthe general score-based RLHF objective is intractable to optimize, we derive a\nstrictly equivalent tractable loss function in theory that can efficiently\ncompute its \\emph{gradient} for optimizations. We introduce\n\\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a\nresolution of $1024\\times 1024$, post-trained from DMD2 w.r.t SDXL. \\textbf{Our\n2.6B \\emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in\nImageReward, PickScore, and CLIP score on the Parti prompts benchmark while\nusing only 1.88\\% of the inference time. This result clearly shows that with\nproper post-training, the small one-step model is capable of beating huge\nmulti-step diffusion models. Our model is open-sourced at this link:\nhttps://github.com/pkulwj1994/diff_instruct_star. We hope our findings can\ncontribute to human-centric machine learning techniques.\n","authors":["Weijian Luo","Colin Zhang","Debing Zhang","Zhengyang Geng"],"pdf_url":"https://arxiv.org/pdf/2410.20898v3.pdf","comment":"Revision: paper accepted by the ICML2025 main conference"},{"id":"http://arxiv.org/abs/2408.14915v3","updated":"2025-06-05T12:08:50Z","published":"2024-08-27T09:44:01Z","title":"Can Transformers Do Enumerative Geometry?","summary":"  How can Transformers model and learn enumerative geometry? What is a robust\nprocedure for using Transformers in abductive knowledge discovery within a\nmathematician-machine collaboration? In this work, we introduce a\nTransformer-based approach to computational enumerative geometry, specifically\ntargeting the computation of $\\psi$-class intersection numbers on the moduli\nspace of curves. By reformulating the problem as a continuous optimization\ntask, we compute intersection numbers across a wide value range from $10^{-45}$\nto $10^{45}$. To capture the recursive nature inherent in these intersection\nnumbers, we propose the Dynamic Range Activator (DRA), a new activation\nfunction that enhances the Transformer's ability to model recursive patterns\nand handle severe heteroscedasticity. Given precision requirements for\ncomputing the intersections, we quantify the uncertainty of the predictions\nusing Conformal Prediction with a dynamic sliding window adaptive to the\npartitions of equivalent number of marked points. To the best of our knowledge,\nthere has been no prior work on modeling recursive functions with such a\nhigh-variance and factorial growth. Beyond simply computing intersection\nnumbers, we explore the enumerative \"world-model\" of Transformers. Our\ninterpretability analysis reveals that the network is implicitly modeling the\nVirasoro constraints in a purely data-driven manner. Moreover, through\nabductive hypothesis testing, probing, and causal inference, we uncover\nevidence of an emergent internal representation of the the large-genus\nasymptotic of $\\psi$-class intersection numbers. These findings suggest that\nthe network internalizes the parameters of the asymptotic closed-form and the\npolynomiality phenomenon of intersection numbers in a non-linear manner. This\nopens up new possibilities in inferring asymptotic closed-form expressions\ndirectly from limited amount of data.\n","authors":["Baran Hashemi","Roderic G. Corominas","Alessandro Giacchetto"],"pdf_url":"https://arxiv.org/pdf/2408.14915v3.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2506.04924v1","updated":"2025-06-05T11:59:20Z","published":"2025-06-05T11:59:20Z","title":"Predicting ICU In-Hospital Mortality Using Adaptive Transformer Layer\n  Fusion","summary":"  Early identification of high-risk ICU patients is crucial for directing\nlimited medical resources. We introduce ALFIA (Adaptive Layer Fusion with\nIntelligent Attention), a modular, attention-based architecture that jointly\ntrains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting\nmechanism to fuse multi-layer semantic features from a BERT backbone. Trained\non our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses\nstate-of-the-art tabular classifiers in AUPRC while preserving a balanced\nprecision-recall profile. The embeddings produced by ALFIA's fusion module,\ncapturing both fine-grained clinical cues and high-level concepts, enable\nseamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro\nnetworks as ALFIA-nn, yielding additional performance gains. Our experiments\nconfirm ALFIA's superior early-warning performance, by operating directly on\nroutine clinical text, it furnishes clinicians with a convenient yet robust\ntool for risk stratification and timely intervention in critical-care settings.\n","authors":["Han Wang","Ruoyun He","Guoguang Lao","Ting Liu","Hejiao Luo","Changqi Qin","Hongying Luo","Junmin Huang","Zihan Wei","Lu Chen","Yongzhi Xu","Ziqian Bi","Junhao Song","Tianyang Wang","Chia Xin Liang","Xinyuan Song","Huafeng Liu","Junfeng Hao","Chunjie Tian"],"pdf_url":"https://arxiv.org/pdf/2506.04924v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.04916v1","updated":"2025-06-05T11:52:21Z","published":"2025-06-05T11:52:21Z","title":"Energentic Intelligence: From Self-Sustaining Systems to Enduring\n  Artificial Life","summary":"  This paper introduces Energentic Intelligence, a class of autonomous systems\ndefined not by task performance, but by their capacity to sustain themselves\nthrough internal energy regulation. Departing from conventional reward-driven\nparadigms, these agents treat survival-maintaining functional operation under\nfluctuating energetic and thermal conditions-as the central objective. We\nformalize this principle through an energy-based utility function and a\nviability-constrained survival horizon, and propose a modular architecture that\nintegrates energy harvesting, thermal regulation, and adaptive computation into\na closed-loop control system. A simulated environment demonstrates the\nemergence of stable, resource-aware behavior without external supervision.\nTogether, these contributions provide a theoretical and architectural\nfoundation for deploying autonomous agents in resource-volatile settings where\npersistence must be self-regulated and infrastructure cannot be assumed.\n","authors":["Atahan Karagoz"],"pdf_url":"https://arxiv.org/pdf/2506.04916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07608v2","updated":"2025-06-05T11:49:09Z","published":"2025-05-12T14:30:11Z","title":"MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining","summary":"  We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.\n","authors":["LLM-Core Xiaomi"," :","Bingquan Xia","Bowen Shen"," Cici","Dawei Zhu","Di Zhang","Gang Wang","Hailin Zhang","Huaqiu Liu","Jiebao Xiao","Jinhao Dong","Liang Zhao","Peidian Li","Peng Wang","Shihua Yu","Shimao Chen","Weikun Wang","Wenhan Ma","Xiangwei Deng","Yi Huang","Yifan Song","Zihan Jiang","Bowen Ye","Can Cai","Chenhong He","Dong Zhang","Duo Zhang","Guoan Wang","Hao Tian","Haochen Zhao","Heng Qu","Hongshen Xu","Jun Shi","Kainan Bao","Kai Fang","Kang Zhou","Kangyang Zhou","Lei Li","Menghang Zhu","Nuo Chen","Qiantong Wang","Shaohui Liu","Shicheng Li","Shuhao Gu","Shuhuai Ren","Shuo Liu","Sirui Deng","Weiji Zhuang","Weiwei Lv","Wenyu Yang","Xin Zhang","Xing Yong","Xing Zhang","Xingchen Song","Xinzhe Xu","Xu Wang","Yihan Yan","Yu Tu","Yuanyuan Tian","Yudong Wang","Yue Yu","Zhenru Lin","Zhichao Song","Zihao Yue"],"pdf_url":"https://arxiv.org/pdf/2505.07608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04913v1","updated":"2025-06-05T11:47:10Z","published":"2025-06-05T11:47:10Z","title":"Dissecting Long Reasoning Models: An Empirical Study","summary":"  Despite recent progress in training long-context reasoning models via\nreinforcement learning (RL), several open questions and counterintuitive\nbehaviors remain. This work focuses on three key aspects: (1) We systematically\nanalyze the roles of positive and negative samples in RL, revealing that\npositive samples mainly facilitate data fitting, whereas negative samples\nsignificantly enhance generalization and robustness. Interestingly, training\nsolely on negative samples can rival standard RL training performance. (2) We\nidentify substantial data inefficiency in group relative policy optimization,\nwhere over half of the samples yield zero advantage. To address this, we\nexplore two straightforward strategies, including relative length rewards and\noffline sample injection, to better leverage these data and enhance reasoning\nefficiency and capability. (3) We investigate unstable performance across\nvarious reasoning models and benchmarks, attributing instability to uncertain\nproblems with ambiguous outcomes, and demonstrate that multiple evaluation runs\nmitigate this issue.\n","authors":["Yongyu Mu","Jiali Zeng","Bei Li","Xinyan Guan","Fandong Meng","Jie Zhou","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.04913v1.pdf","comment":"Working in process"},{"id":"http://arxiv.org/abs/2506.04909v1","updated":"2025-06-05T11:44:19Z","published":"2025-06-05T11:44:19Z","title":"When Thinking LLMs Lie: Unveiling the Strategic Deception in\n  Representations of Reasoning Models","summary":"  The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.\n","authors":["Kai Wang","Yihao Zhang","Meng Sun"],"pdf_url":"https://arxiv.org/pdf/2506.04909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24473v2","updated":"2025-06-05T11:42:24Z","published":"2025-05-30T11:20:44Z","title":"Train One Sparse Autoencoder Across Multiple Sparsity Budgets to\n  Preserve Interpretability and Accuracy","summary":"  Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting\nneural networks by decomposing hidden representations into disentangled,\ninterpretable features via sparsity constraints. However, conventional SAEs are\nconstrained by the fixed sparsity level chosen during training; meeting\ndifferent sparsity requirements therefore demands separate models and increases\nthe computational footprint during both training and evaluation. We introduce a\nnovel training objective, \\emph{HierarchicalTopK}, which trains a single SAE to\noptimise reconstructions across multiple sparsity levels simultaneously.\nExperiments with Gemma-2 2B demonstrate that our approach achieves\nPareto-optimal trade-offs between sparsity and explained variance,\noutperforming traditional SAEs trained at individual sparsity levels. Further\nanalysis shows that HierarchicalTopK preserves high interpretability scores\neven at higher sparsity. The proposed objective thus closes an important gap\nbetween flexibility and interpretability in SAE design.\n","authors":["Nikita Balagansky","Yaroslav Aksenov","Daniil Laptev","Vadim Kurochkin","Gleb Gerasimov","Nikita Koryagin","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2505.24473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04907v1","updated":"2025-06-05T11:41:05Z","published":"2025-06-05T11:41:05Z","title":"Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning\n  Blind Spots","summary":"  Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.\n","authors":["Alex Pan","Mary-Anne Williams"],"pdf_url":"https://arxiv.org/pdf/2506.04907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06765v2","updated":"2025-06-05T11:34:12Z","published":"2025-02-10T18:44:30Z","title":"Are all models wrong? Fundamental limits in distribution-free empirical\n  model falsification","summary":"  In statistics and machine learning, when we train a fitted model on available\ndata, we typically want to ensure that we are searching within a model class\nthat contains at least one accurate model -- that is, we would like to ensure\nan upper bound on the model class risk (the lowest possible risk that can be\nattained by any model in the class). However, it is also of interest to\nestablish lower bounds on the model class risk, for instance so that we can\ndetermine whether our fitted model is at least approximately optimal within the\nclass, or, so that we can decide whether the model class is unsuitable for the\nparticular task at hand. Particularly in the setting of interpolation learning\nwhere machine learning models are trained to reach zero error on the training\ndata, we might ask if, at the very least, a positive lower bound on the model\nclass risk is possible -- or are we unable to detect that \"all models are\nwrong\"? In this work, we answer these questions in a distribution-free setting\nby establishing a model-agnostic, fundamental hardness result for the problem\nof constructing a lower bound on the best test error achievable over a model\nclass, and examine its implications on specific model classes such as\ntree-based methods and linear regression.\n","authors":["Manuel M. Müller","Yuetian Luo","Rina Foygel Barber"],"pdf_url":"https://arxiv.org/pdf/2502.06765v2.pdf","comment":"39 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.11802v5","updated":"2025-06-05T11:28:35Z","published":"2024-10-15T17:23:49Z","title":"TSFM-Bench: A Comprehensive and Unified Benchmarking of Foundation\n  Models for Time Series Forecasting","summary":"  Time Series Forecasting (TSF) is key functionality in numerous fields, such\nas financial investment, weather services, and energy management. Although\nincreasingly capable TSF methods occur, many of them require domain-specific\ndata collection and model training and do not generalize well when applied in\nother domains. Time Series Foundation Models (TSFMs) that are pre-trained on\nmassive heterogeneous time series data aim to overcome these limitations. The\nprospects for generalizability have spurred the development of a new generation\nof TSFMs. This study proposes a benchmark, TSFM-Bench, to facilitate\ncomprehensive and unified evaluation of TSFMs. TSFM-Bench covers a wide range\nof TSFMs, including those based on large language models and those pre-trained\non time series data. TSFM-Bench supports multiple forecasting scenarios,\nincluding zero-shot, few-shot, and full-shot, enabling assessment across the\nfull range of adaptation strategies. TSFM-Bench also provides a standardized\nexperimental protocols for critical evaluation processes such as dataset\nsplitting, loading, normalization, and few-shot sampling, facilitating\nconsistency and fairness. We report on an extensive evaluation of TSFMs across\na diverse range of datasets spanning multiple domains and exhibiting varied\nstatistical characteristics. Specifically, we identify pros and cons and\ninherent limitations of existing TSFMs, and we propose potential directions for\nnew model designs.\n","authors":["Zhe Li","Xiangfei Qiu","Peng Chen","Yihang Wang","Hanyin Cheng","Yang Shu","Jilin Hu","Chenjuan Guo","Aoying Zhou","Christian S. Jensen","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.11802v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20863v2","updated":"2025-06-05T11:27:22Z","published":"2025-05-27T08:14:58Z","title":"Leveraging Diffusion Models for Parameterized Quantum Circuit Generation","summary":"  Quantum computing holds immense potential, yet its practical success depends\non multiple factors, including advances in quantum circuit design. In this\npaper, we introduce a generative approach based on denoising diffusion models\n(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent\ndiffusion model pipeline of F\\\"urrutter et al. [1], our model effectively\nconditions the synthesis process, enabling the simultaneous generation of\ncircuit architectures and their continuous gate parameters. We demonstrate our\napproach in synthesizing PQCs optimized for generating high-fidelity\nGreenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum\nmachine learning (QML) classification tasks. Our results indicate a strong\ngeneralization across varying gate sets and scaling qubit counts, highlighting\nthe versatility and computational efficiency of diffusion-based methods. This\nwork illustrates the potential of generative models as a powerful tool for\naccelerating and optimizing the design of PQCs, supporting the development of\nmore practical and scalable quantum applications.\n","authors":["Daniel Barta","Darya Martyniuk","Johannes Jung","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2505.20863v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2506.04891v1","updated":"2025-06-05T11:19:05Z","published":"2025-06-05T11:19:05Z","title":"TQml Simulator: Optimized Simulation of Quantum Machine Learning","summary":"  Hardware-efficient circuits employed in Quantum Machine Learning are\ntypically composed of alternating layers of uniformly applied gates. High-speed\nnumerical simulators for such circuits are crucial for advancing research in\nthis field. In this work, we numerically benchmark universal and gate-specific\ntechniques for simulating the action of layers of gates on quantum state\nvectors, aiming to accelerate the overall simulation of Quantum Machine\nLearning algorithms. Our analysis shows that the optimal simulation method for\na given layer of gates depends on the number of qubits involved, and that a\ntailored combination of techniques can yield substantial performance gains in\nthe forward and backward passes for a given circuit. Building on these\ninsights, we developed a numerical simulator, named TQml Simulator, that\nemploys the most efficient simulation method for each layer in a given circuit.\nWe evaluated TQml Simulator on circuits constructed from standard gate sets,\nsuch as rotations and CNOTs, as well as on native gates from IonQ and IBM\nquantum processing units. In most cases, our simulator outperforms equivalent\nPennylane's default.qubit simulator by approximately 2- to 100-fold, depending\non the circuit, the number of qubits, the batch size of the input data, and the\nhardware used.\n","authors":["Viacheslav Kuzmin","Basil Kyriacou","Mateusz Papierz","Mo Kordzanganeh","Alexey Melnikov"],"pdf_url":"https://arxiv.org/pdf/2506.04891v1.pdf","comment":"25 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.02476v2","updated":"2025-06-05T11:15:22Z","published":"2024-07-02T17:53:56Z","title":"Scalable Multi-Output Gaussian Processes with Stochastic Variational\n  Inference","summary":"  The Multi-Output Gaussian Process is is a popular tool for modelling data\nfrom multiple sources. A typical choice to build a covariance function for a\nMOGP is the Linear Model of Coregionalization (LMC) which parametrically models\nthe covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises\nthis idea by modelling the covariance between outputs using a kernel applied to\nlatent variables, one per output, leading to a flexible MOGP model that allows\nefficient generalization to new outputs with few data points. Computational\ncomplexity in LV-MOGP grows linearly with the number of outputs, which makes it\nunsuitable for problems with a large number of outputs. In this paper, we\npropose a stochastic variational inference approach for the LV-MOGP that allows\nmini-batches for both inputs and outputs, making computational complexity per\ntraining iteration independent of the number of outputs.\n","authors":["Xiaoyu Jiang","Sokratia Georgaka","Magnus Rattray","Mauricio A. Álvarez"],"pdf_url":"https://arxiv.org/pdf/2407.02476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05946v2","updated":"2025-06-05T11:13:42Z","published":"2025-05-09T10:43:37Z","title":"Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency\n  and Domain Knowledge","summary":"  In this technical report, we empirically investigate the relationship between\nlinguistic fluency and domain knowledge in the context of continual learning\nwith large language models (LLMs). Specifically, we enhance the linguistic\nfluency of the Gemma2 LLM for the Lithuanian language by autoregressively\npretraining its full parameter set on the first 10\\% of the Lithuanian language\ncomponent of the CulturaX dataset. To prevent catastrophic forgetting of the\nmodel's existing domain knowledge, we apply Elastic Weight Consolidation (EWC),\nleveraging Fisher information estimated using data from the Massive Multitask\nLanguage Understanding (MMLU) benchmark. In the post-training evaluations, we\nassess linguistic fluency through perplexity and evaluate domain knowledge\nusing accuracy on a suite of language understanding benchmarks, including\nARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both\nEnglish and Lithuanian. The empirical results demonstrate that EWC not only\nmitigates catastrophic forgetting by preserving the model's performance in\nterms of both linguistic fluency and domain knowledge but also improves or\nmaintains these capabilities for the newly added Lithuanian language. These\nfindings highlight the potential for more efficient adaptation of\ngeneral-purpose LLMs to under-represented languages without requiring access to\nthe original training data. The accompanying codebase is openly accessible at\nhttps://github.com/Neurotechnology/LLM_EWC.\n","authors":["Vytenis Šliogeris","Povilas Daniušis","Artūras Nakvosas"],"pdf_url":"https://arxiv.org/pdf/2505.05946v2.pdf","comment":"9 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.04886v1","updated":"2025-06-05T11:08:12Z","published":"2025-06-05T11:08:12Z","title":"Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms\n  Angle-Based Methods for Assessment of Hip Dysplasia","summary":"  Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,\nearly diagnosis of dysplasia is important to provide opportunities for surgical\ninterventions aimed at reducing the risk of hip OA. We have developed a\npipeline for semi-automated classification of dysplasia using volumetric CT\nscans of patients' hips and a minimal set of clinically annotated landmarks,\ncombining the framework of the Gaussian Process Latent Variable Model with\ndiffeomorphism to create a statistical shape model, which we termed the\nGaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT\nscans, 100 for model training and 92 for testing. The GPDSSM effectively\ndistinguishes dysplastic samples from controls while also highlighting regions\nof the underlying surface that show dysplastic variations. As well as improving\nclassification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),\nthe GPDSSM can save time for clinicians by removing the need to manually\nmeasure angles and interpreting 2D scans for possible markers of dysplasia.\n","authors":["Allen Paul","George Grammatopoulos","Adwaye Rambojun","Neill D. F. Campbell","Harinderjit S. Gill","Tony Shardlow"],"pdf_url":"https://arxiv.org/pdf/2506.04886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12730v2","updated":"2025-06-05T11:07:16Z","published":"2025-03-17T01:47:50Z","title":"TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic\n  Interpretability Research","summary":"  Mechanistic interpretability research faces a gap between analyzing simple\ncircuits in toy tasks and discovering features in large models. To bridge this\ngap, we propose text-to-SQL generation as an ideal task to study, as it\ncombines the formal structure of toy tasks with real-world complexity. We\nintroduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL\noperations, and train models ranging from 33M to 1B parameters to establish a\ncomprehensive testbed for interpretability. We apply multiple complementary\ninterpretability techniques, including Edge Attribution Patching and Sparse\nAutoencoders, to identify minimal circuits and components supporting SQL\ngeneration. We compare circuits for different SQL subskills, evaluating their\nminimality, reliability, and identifiability. Finally, we conduct a layerwise\nlogit lens analysis to reveal how models compose SQL queries across layers:\nfrom intent recognition to schema resolution to structured generation. Our work\nprovides a robust framework for probing and comparing interpretability methods\nin a structured, progressively complex setting.\n","authors":["Abir Harrasse","Philip Quirke","Clement Neo","Dhruv Nathawani","Amir Abdullah"],"pdf_url":"https://arxiv.org/pdf/2503.12730v2.pdf","comment":"9 pages, 19 figures, 7 tables, 18 trained models"},{"id":"http://arxiv.org/abs/2410.00023v2","updated":"2025-06-05T10:58:03Z","published":"2024-09-16T05:07:47Z","title":"Self-Tuning Spectral Clustering for Speaker Diarization","summary":"  Spectral clustering has proven effective in grouping speech representations\nfor speaker diarization tasks, although post-processing the affinity matrix\nremains difficult due to the need for careful tuning before constructing the\nLaplacian. In this study, we present a novel pruning algorithm to create a\nsparse affinity matrix called spectral clustering on p-neighborhood retained\naffinity matrix (SC-pNA). Our method improves on node-specific fixed neighbor\nselection by allowing a variable number of neighbors, eliminating the need for\nexternal tuning data as the pruning parameters are derived directly from the\naffinity matrix. SC-pNA does so by identifying two clusters in every row of the\ninitial affinity matrix, and retains only the top p % similarity scores from\nthe cluster containing larger similarities. Spectral clustering is performed\nsubsequently, with the number of clusters determined as the maximum eigengap.\nExperimental results on the challenging DIHARD-III dataset highlight the\nsuperiority of SC-pNA, which is also computationally more efficient than\nexisting auto-tuning approaches. Our implementations are available at\nhttps://github.com/nikhilraghav29/SC-pNA.\n","authors":["Nikhil Raghav","Avisek Gupta","Md Sahidullah","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2410.00023v2.pdf","comment":"This is the camera-ready version accepted for publication in the\n  ICASSP 2025 proceedings"},{"id":"http://arxiv.org/abs/2405.05334v2","updated":"2025-06-05T10:55:57Z","published":"2024-05-08T18:09:16Z","title":"Multiplicative Dynamic Mode Decomposition","summary":"  Koopman operators are infinite-dimensional operators that linearize nonlinear\ndynamical systems, facilitating the study of their spectral properties and\nenabling the prediction of the time evolution of observable quantities. Recent\nmethods have aimed to approximate Koopman operators while preserving key\nstructures. However, approximating Koopman operators typically requires a\ndictionary of observables to capture the system's behavior in a\nfinite-dimensional subspace. The selection of these functions is often\nheuristic, may result in the loss of spectral information, and can severely\ncomplicate structure preservation. This paper introduces Multiplicative Dynamic\nMode Decomposition (MultDMD), which enforces the multiplicative structure\ninherent in the Koopman operator within its finite-dimensional approximation.\nLeveraging this multiplicative property, we guide the selection of observables\nand define a constrained optimization problem for the matrix approximation,\nwhich can be efficiently solved. MultDMD presents a structured approach to\nfinite-dimensional approximations and can more accurately reflect the spectral\nproperties of the Koopman operator. We elaborate on the theoretical framework\nof MultDMD, detailing its formulation, optimization strategy, and convergence\nproperties. The efficacy of MultDMD is demonstrated through several examples,\nincluding the nonlinear pendulum, the Lorenz system, and fluid dynamics data,\nwhere we demonstrate its remarkable robustness to noise.\n","authors":["Nicolas Boullé","Matthew J. Colbrook"],"pdf_url":"https://arxiv.org/pdf/2405.05334v2.pdf","comment":"24 pages, 13 figures. To appear in SIAM Journal on Applied Dynamical\n  Systems"},{"id":"http://arxiv.org/abs/2401.15098v3","updated":"2025-06-05T10:52:51Z","published":"2024-01-25T03:06:51Z","title":"Multi-granularity Knowledge Transfer for Continual Reinforcement\n  Learning","summary":"  Continual reinforcement learning (CRL) empowers RL agents with the ability to\nlearn a sequence of tasks, accumulating knowledge learned in the past and using\nthe knowledge for problemsolving or future task learning. However, existing\nmethods often focus on transferring fine-grained knowledge across similar\ntasks, which neglects the multi-granularity structure of human cognitive\ncontrol, resulting in insufficient knowledge transfer across diverse tasks. To\nenhance coarse-grained knowledge transfer, we propose a novel framework called\nMT-Core (as shorthand for Multi-granularity knowledge Transfer for Continual\nreinforcement learning). MT-Core has a key characteristic of multi-granularity\npolicy learning: 1) a coarsegrained policy formulation for utilizing the\npowerful reasoning ability of the large language model (LLM) to set goals, and\n2) a fine-grained policy learning through RL which is oriented by the goals. We\nalso construct a new policy library (knowledge base) to store policies that can\nbe retrieved for multi-granularity knowledge transfer. Experimental results\ndemonstrate the superiority of the proposed MT-Core in handling diverse CRL\ntasks versus popular baselines.\n","authors":["Chaofan Pan","Lingfei Ren","Yihui Feng","Linbo Xiong","Wei Wei","Yonghao Li","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.15098v3.pdf","comment":"the 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2506.04878v1","updated":"2025-06-05T10:51:18Z","published":"2025-06-05T10:51:18Z","title":"kTULA: A Langevin sampling algorithm with improved KL bounds under\n  super-linear log-gradients","summary":"  Motivated by applications in deep learning, where the global Lipschitz\ncontinuity condition is often not satisfied, we examine the problem of sampling\nfrom distributions with super-linearly growing log-gradients. We propose a\nnovel tamed Langevin dynamics-based algorithm, called kTULA, to solve the\naforementioned sampling problem, and provide a theoretical guarantee for its\nperformance. More precisely, we establish a non-asymptotic convergence bound in\nKullback-Leibler (KL) divergence with the best-known rate of convergence equal\nto $2-\\overline{\\epsilon}$, $\\overline{\\epsilon}>0$, which significantly\nimproves relevant results in existing literature. This enables us to obtain an\nimproved non-asymptotic error bound in Wasserstein-2 distance, which can be\nused to further derive a non-asymptotic guarantee for kTULA to solve the\nassociated optimization problems. To illustrate the applicability of kTULA, we\napply the proposed algorithm to the problem of sampling from a high-dimensional\ndouble-well potential distribution and to an optimization problem involving a\nneural network. We show that our main results can be used to provide\ntheoretical guarantees for the performance of kTULA.\n","authors":["Iosif Lytras","Sotirios Sabanis","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04877v1","updated":"2025-06-05T10:50:42Z","published":"2025-06-05T10:50:42Z","title":"There Was Never a Bottleneck in Concept Bottleneck Models","summary":"  Deep learning representations are often difficult to interpret, which can\nhinder their deployment in sensitive applications. Concept Bottleneck Models\n(CBMs) have emerged as a promising approach to mitigate this issue by learning\nrepresentations that support target task performance while ensuring that each\ncomponent predicts a concrete concept from a predefined set. In this work, we\nargue that CBMs do not impose a true bottleneck: the fact that a component can\npredict a concept does not guarantee that it encodes only information about\nthat concept. This shortcoming raises concerns regarding interpretability and\nthe validity of intervention procedures. To overcome this limitation, we\npropose Minimal Concept Bottleneck Models (MCBMs), which incorporate an\nInformation Bottleneck (IB) objective to constrain each representation\ncomponent to retain only the information relevant to its corresponding concept.\nThis IB is implemented via a variational regularization term added to the\ntraining loss. As a result, MCBMs support concept-level interventions with\ntheoretical guarantees, remain consistent with Bayesian principles, and offer\ngreater flexibility in key design choices.\n","authors":["Antonio Almudévar","José Miguel Hernández-Lobato","Alfonso Ortega"],"pdf_url":"https://arxiv.org/pdf/2506.04877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04870v1","updated":"2025-06-05T10:47:14Z","published":"2025-06-05T10:47:14Z","title":"Aligning Multimodal Representations through an Information Bottleneck","summary":"  Contrastive losses have been extensively used as a tool for multimodal\nrepresentation learning. However, it has been empirically observed that their\nuse is not effective to learn an aligned representation space. In this paper,\nwe argue that this phenomenon is caused by the presence of modality-specific\ninformation in the representation space. Although some of the most widely used\ncontrastive losses maximize the mutual information between representations of\nboth modalities, they are not designed to remove the modality-specific\ninformation. We give a theoretical description of this problem through the lens\nof the Information Bottleneck Principle. We also empirically analyze how\ndifferent hyperparameters affect the emergence of this phenomenon in a\ncontrolled experimental setup. Finally, we propose a regularization term in the\nloss function that is derived by means of a variational approximation and aims\nto increase the representational alignment. We analyze in a set of controlled\nexperiments and real-world applications the advantages of including this\nregularization term.\n","authors":["Antonio Almudévar","José Miguel Hernández-Lobato","Sameer Khurana","Ricard Marxer","Alfonso Ortega"],"pdf_url":"https://arxiv.org/pdf/2506.04870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04867v1","updated":"2025-06-05T10:38:28Z","published":"2025-06-05T10:38:28Z","title":"LLMs for sensory-motor control: Combining in-context and iterative\n  learning","summary":"  We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. Initially, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. In most cases, it\nsuccessfully identifies optimal or high-performing solutions by integrating\nsymbolic knowledge derived through reasoning with sub-symbolic sensory-motor\ndata gathered as the agent interacts with its environment.\n","authors":["Jônata Tyska Carvalho","Stefano Nolfi"],"pdf_url":"https://arxiv.org/pdf/2506.04867v1.pdf","comment":"24 pages (13 pages are from appendix), 6 figures, code for\n  experiments replication and supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/"},{"id":"http://arxiv.org/abs/2501.09659v2","updated":"2025-06-05T10:28:30Z","published":"2025-01-16T16:54:40Z","title":"Fokker-Planck to Callan-Symanzik: evolution of weight matrices under\n  training","summary":"  The dynamical evolution of a neural network during training has been an\nincredibly fascinating subject of study. First principal derivation of generic\nevolution of variables in statistical physics systems has proved useful when\nused to describe training dynamics conceptually, which in practice means\nnumerically solving equations such as Fokker-Planck equation. Simulating entire\nnetworks inevitably runs into the curse of dimensionality. In this paper, we\nutilize Fokker-Planck to simulate the probability density evolution of\nindividual weight matrices in the bottleneck layers of a simple\n2-bottleneck-layered auto-encoder and compare the theoretical evolutions\nagainst the empirical ones by examining the output data distributions. We also\nderive physically relevant partial differential equations such as\nCallan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation\nwe have.\n","authors":["Wei Bu","Uri Kol","Ziming Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09659v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.04859v1","updated":"2025-06-05T10:26:06Z","published":"2025-06-05T10:26:06Z","title":"Sparse Autoencoders, Again?","summary":"  Is there really much more to say about sparse autoencoders (SAEs)?\nAutoencoders in general, and SAEs in particular, represent deep architectures\nthat are capable of modeling low-dimensional latent structure in data. Such\nstructure could reflect, among other things, correlation patterns in large\nlanguage model activations, or complex natural image manifolds. And yet despite\nthe wide-ranging applicability, there have been relatively few changes to SAEs\nbeyond the original recipe from decades ago, namely, standard deep\nencoder/decoder layers trained with a classical/deterministic sparse\nregularizer applied within the latent space. One possible exception is the\nvariational autoencoder (VAE), which adopts a stochastic encoder module capable\nof producing sparse representations when applied to manifold data. In this work\nwe formalize underappreciated weaknesses with both canonical SAEs, as well as\nanalogous VAEs applied to similar tasks, and propose a hybrid alternative model\nthat circumvents these prior limitations. In terms of theoretical support, we\nprove that global minima of our proposed model recover certain forms of\nstructured data spread across a union of manifolds. Meanwhile, empirical\nevaluations on synthetic and real-world datasets substantiate the efficacy of\nour approach in accurately estimating underlying manifold dimensions and\nproducing sparser latent representations without compromising reconstruction\nerror. In general, we are able to exceed the performance of equivalent-capacity\nSAEs and VAEs, as well as recent diffusion models where applicable, within\ndomains such as images and language model activation patterns.\n","authors":["Yin Lu","Tong He","Xuening Zhu","David Wipf"],"pdf_url":"https://arxiv.org/pdf/2506.04859v1.pdf","comment":"Accepted to the International Conference on Machine Learning (ICML)\n  2025"},{"id":"http://arxiv.org/abs/2506.04852v1","updated":"2025-06-05T10:22:54Z","published":"2025-06-05T10:22:54Z","title":"Improving AI-generated music with user-guided training","summary":"  AI music generation has advanced rapidly, with models like diffusion and\nautoregressive algorithms enabling high-fidelity outputs. These tools can alter\nstyles, mix instruments, or isolate them. Since sound can be visualized as\nspectrograms, image-generation algorithms can be applied to generate novel\nmusic. However, these algorithms are typically trained on fixed datasets, which\nmakes it challenging for them to interpret and respond to user input\naccurately. This is especially problematic because music is highly subjective\nand requires a level of personalization that image generation does not provide.\nIn this work, we propose a human-computation approach to gradually improve the\nperformance of these algorithms based on user interactions. The\nhuman-computation element involves aggregating and selecting user ratings to\nuse as the loss function for fine-tuning the model. We employ a genetic\nalgorithm that incorporates user feedback to enhance the baseline performance\nof a model initially trained on a fixed dataset. The effectiveness of this\napproach is measured by the average increase in user ratings with each\niteration. In the pilot test, the first iteration showed an average rating\nincrease of 0.2 compared to the baseline. The second iteration further improved\nupon this, achieving an additional increase of 0.39 over the first iteration.\n","authors":["Vishwa Mohan Singh","Sai Anirudh Aryasomayajula","Ahan Chatterjee","Beste Aydemir","Rifat Mehreen Amin"],"pdf_url":"https://arxiv.org/pdf/2506.04852v1.pdf","comment":"Select for presentation in HHAI 2025"},{"id":"http://arxiv.org/abs/2406.18370v2","updated":"2025-06-05T10:09:29Z","published":"2024-06-26T14:13:50Z","title":"Learning pure quantum states (almost) without regret","summary":"  We initiate the study of sample-optimal quantum state tomography with minimal\ndisturbance to the samples. Can we efficiently learn a precise description of a\nquantum state through sequential measurements of samples while at the same time\nmaking sure that the post-measurement state of the samples is only minimally\nperturbed? Defining regret as the cumulative disturbance of all samples, the\nchallenge is to find a balance between the most informative sequence of\nmeasurements on the one hand and measurements incurring minimal regret on the\nother. Here we answer this question for qubit states by exhibiting a protocol\nthat for pure states achieves maximal precision while incurring a regret that\ngrows only polylogarithmically with the number of samples, a scaling that we\nshow to be optimal.\n","authors":["Josep Lumbreras","Mikhail Terekhov","Marco Tomamichel"],"pdf_url":"https://arxiv.org/pdf/2406.18370v2.pdf","comment":"28 pages, 2 figures"},{"id":"http://arxiv.org/abs/2501.02785v2","updated":"2025-06-05T10:06:50Z","published":"2025-01-06T06:01:01Z","title":"Hybrid deep convolution model for lung cancer detection with transfer\n  learning","summary":"  Advances in healthcare research have significantly enhanced our understanding\nof disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung\ncancer remains one of the leading causes of cancer-related mortality worldwide\ndue to challenges in early and accurate diagnosis. While current lung cancer\ndetection models show promise, there is considerable potential for further\nimproving the accuracy for timely intervention. To address this challenge, we\nintroduce a hybrid deep convolution model leveraging transfer learning, named\nthe Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the\nprecision of lung cancer detection by refining sensitivity and specificity.\nThis model has surpassed existing deep learning approaches through experimental\nvalidation, achieving an accuracy of 98% and a sensitivity of 97%. By\noverlaying sensitivity maps onto lung Computed Tomography (CT) scans, it\nenables the visualization of regions most indicative of malignant or benign\nclassifications. This innovative method demonstrates exceptional performance in\ndistinguishing lung cancer with minimal false positives, thereby enhancing the\naccuracy of medical diagnoses.\n","authors":["Sugandha Saxena","S. N. Prasad","Ashwin M Polnaya","Shweta Agarwala"],"pdf_url":"https://arxiv.org/pdf/2501.02785v2.pdf","comment":"Authors realized mistake in the model. Also some data was\n  misinterpreted"},{"id":"http://arxiv.org/abs/2506.04831v1","updated":"2025-06-05T09:54:01Z","published":"2025-06-05T09:54:01Z","title":"From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health\n  Trajectories with LLMs","summary":"  Healthcare systems face significant challenges in managing and interpreting\nvast, heterogeneous patient data for personalized care. Existing approaches\noften focus on narrow use cases with a limited feature space, overlooking the\ncomplex, longitudinal interactions needed for a holistic understanding of\npatient health. In this work, we propose a novel approach to patient pathway\nmodeling by transforming diverse electronic health record (EHR) data into a\nstructured representation and designing a holistic pathway prediction model,\nEHR2Path, optimized to predict future health trajectories. Further, we\nintroduce a novel summary mechanism that embeds long-term temporal context into\ntopic-specific summary tokens, improving performance over text-only models,\nwhile being much more token-efficient. EHR2Path demonstrates strong performance\nin both next time-step prediction and longitudinal simulation, outperforming\ncompetitive baselines. It enables detailed simulations of patient trajectories,\ninherently targeting diverse evaluation tasks, such as forecasting vital signs,\nlab test results, or length-of-stay, opening a path towards predictive and\npersonalized healthcare.\n","authors":["Chantal Pellegrini","Ege Özsoy","David Bani-Harouni","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2506.04831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24848v2","updated":"2025-06-05T09:53:39Z","published":"2025-05-30T17:46:13Z","title":"Reading Recognition in the Wild","summary":"  To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism.\n","authors":["Charig Yang","Samiul Alam","Shakhrul Iman Siam","Michael J. Proulx","Lambert Mathias","Kiran Somasundaram","Luis Pesqueira","James Fort","Sheroze Sheriffdeen","Omkar Parkhi","Carl Ren","Mi Zhang","Yuning Chai","Richard Newcombe","Hyo Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2505.24848v2.pdf","comment":"Project Page:\n  https://www.projectaria.com/datasets/reading-in-the-wild/"},{"id":"http://arxiv.org/abs/2505.17786v2","updated":"2025-06-05T09:50:34Z","published":"2025-05-23T11:59:35Z","title":"Supervised Graph Contrastive Learning for Gene Regulatory Network","summary":"  Graph representation learning is effective for obtaining a meaningful latent\nspace utilizing the structure of graph data and is widely applied, including\nbiological networks. In particular, Graph Contrastive Learning (GCL) has\nemerged as a powerful self-supervised method that relies on applying\nperturbations to graphs for data augmentation. However, when applying existing\nGCL methods to biological networks such as Gene Regulatory Networks (GRNs),\nthey overlooked meaningful biologically relevant perturbations, e.g., gene\nknockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive\nLearning), a novel GCL method for GRNs that directly incorporates biological\nperturbations derived from gene knockdown experiments as the supervision.\nSupGCL mathematically extends existing GCL methods that utilize non-biological\nperturbations to probabilistic models that introduce actual biological gene\nperturbation utilizing gene knockdown data. Using the GRN representation\nobtained by our proposed method, our aim is to improve the performance of\nbiological downstream tasks such as patient hazard prediction and disease\nsubtype classification (graph-level task), and gene function classification\n(node-level task). We applied SupGCL on real GRN datasets derived from patients\nwith multiple types of cancer, and in all experiments SupGCL achieves better\nperformance than state-of-the-art baselines.\n","authors":["Sho Oshima","Yuji Okamoto","Taisei Tosaki","Ryosuke Kojima","Yasushi Okuno"],"pdf_url":"https://arxiv.org/pdf/2505.17786v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2506.04823v1","updated":"2025-06-05T09:41:12Z","published":"2025-06-05T09:41:12Z","title":"Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light\n  Detectors","summary":"  Realistic adversarial attacks on various camera-based perception tasks of\nautonomous vehicles have been successfully demonstrated so far. However, only a\nfew works considered attacks on traffic light detectors. This work shows how\nCNNs for traffic light detection can be attacked with printed patches. We\npropose a threat model, where each instance of a traffic light is attacked with\na patch placed under it, and describe a training strategy. We demonstrate\nsuccessful adversarial patch attacks in universal settings. Our experiments\nshow realistic targeted red-to-green label-flipping attacks and attacks on\npictogram classification. Finally, we perform a real-world evaluation with\nprinted patches and demonstrate attacks in the lab settings with a mobile\ntraffic light for construction sites and in a test area with stationary traffic\nlights. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.\n","authors":["Svetlana Pavlitska","Jamie Robb","Nikolai Polley","Melih Yazgan","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2506.04823v1.pdf","comment":"Accepted for publication at IV 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.22266v2","updated":"2025-06-05T13:27:06Z","published":"2025-05-28T11:54:09Z","title":"FGAS: Fixed Decoder Network-Based Audio Steganography with Adversarial\n  Perturbation Generation","summary":"  The rapid development of Artificial Intelligence Generated Content (AIGC) has\nmade high-fidelity generated audio widely available across the Internet,\nproviding diverse cover signals for covert communication. Driven by advances in\ndeep learning, current audio steganography schemes are mainly based on\nencoding-decoding network architectures. While these methods greatly improve\nthe security of audio steganography, they typically require complex training\nand large pre-trained models. To address the aforementioned issues, this paper\npioneers a Fixed Decoder Network-Based Audio Steganography with Adversarial\nPerturbation Generation (FGAS). Adversarial perturbations carrying secret\nmessage are embedded into the cover audio to generate stego audio. The receiver\nonly needs to share the structure and weights of the fixed decoder network to\naccurately extract the secret message from the stego audio, this eliminates the\nreliance on large pre-trained models. In FGAS, we propose an audio Adversarial\nPerturbation Generation (APG) strategy and design a lightweight fixed decoder.\nThe fixed decoder guarantees reliable extraction of the hidden message, while\nthe adversarial perturbations are optimized to keep the stego audio\nperceptually and statistically close to the cover audio, thereby improving\nresistance to steganalysis. The experimental results show that FGAS\nsignificantly improves the quality of stego audio, achieving an average PSNR\ngain of over 10 dB compared to SOTA methods. Moreover, FGAS exhibits superior\nanti-steganalysis performance under different relative payloads; under\nhigh-capacity embedding, it achieves a classification error rate about 2%\nhigher, indicating stronger anti-steganalysis performance compared to current\nSOTA methods.\n","authors":["Jialin Yan","Yu Cheng","Zhaoxia Yin","Xinpeng Zhang","Shilin Wang","Tanfeng Sun","Xinghao Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.22266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20898v3","updated":"2025-06-05T12:11:49Z","published":"2024-10-28T10:26:19Z","title":"David and Goliath: Small One-step Model Beats Large Diffusion with Score\n  Post-training","summary":"  We propose Diff-Instruct* (DI*), a data-efficient post-training approach for\none-step text-to-image generative models to improve its human preferences\nwithout requiring image data. Our method frames alignment as online\nreinforcement learning from human feedback (RLHF), which optimizes the one-step\nmodel to maximize human reward functions while being regularized to be kept\nclose to a reference diffusion process. Unlike traditional RLHF approaches,\nwhich rely on the Kullback-Leibler divergence as the regularization, we\nintroduce a novel general score-based divergence regularization that\nsubstantially improves performance as well as post-training stability. Although\nthe general score-based RLHF objective is intractable to optimize, we derive a\nstrictly equivalent tractable loss function in theory that can efficiently\ncompute its \\emph{gradient} for optimizations. We introduce\n\\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a\nresolution of $1024\\times 1024$, post-trained from DMD2 w.r.t SDXL. \\textbf{Our\n2.6B \\emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in\nImageReward, PickScore, and CLIP score on the Parti prompts benchmark while\nusing only 1.88\\% of the inference time. This result clearly shows that with\nproper post-training, the small one-step model is capable of beating huge\nmulti-step diffusion models. Our model is open-sourced at this link:\nhttps://github.com/pkulwj1994/diff_instruct_star. We hope our findings can\ncontribute to human-centric machine learning techniques.\n","authors":["Weijian Luo","Colin Zhang","Debing Zhang","Zhengyang Geng"],"pdf_url":"https://arxiv.org/pdf/2410.20898v3.pdf","comment":"Revision: paper accepted by the ICML2025 main conference"},{"id":"http://arxiv.org/abs/2411.16331v3","updated":"2025-06-05T11:49:59Z","published":"2024-11-25T12:24:52Z","title":"Sonic: Shifting Focus to Global Audio Perception in Portrait Animation","summary":"  The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity.\n","authors":["Xiaozhong Ji","Xiaobin Hu","Zhihong Xu","Junwei Zhu","Chuming Lin","Qingdong He","Jiangning Zhang","Donghao Luo","Yi Chen","Qin Lin","Qinglin Lu","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16331v3.pdf","comment":"refer to our main-page \\url{https://jixiaozhong.github.io/Sonic/}"},{"id":"http://arxiv.org/abs/2506.04858v1","updated":"2025-06-05T10:25:46Z","published":"2025-06-05T10:25:46Z","title":"Beyond the Desktop: XR-Driven Segmentation with Meta Quest 3 and MX Ink","summary":"  Medical imaging segmentation is essential in clinical settings for diagnosing\ndiseases, planning surgeries, and other procedures. However, manual annotation\nis a cumbersome and effortful task. To mitigate these aspects, this study\nimplements and evaluates the usability and clinical applicability of an\nextended reality (XR)-based segmentation tool for anatomical CT scans, using\nthe Meta Quest 3 headset and Logitech MX Ink stylus. We develop an immersive\ninterface enabling real-time interaction with 2D and 3D medical imaging data in\na customizable workspace designed to mitigate workflow fragmentation and\ncognitive demands inherent to conventional manual segmentation tools. The\nplatform combines stylus-driven annotation, mirroring traditional pen-on-paper\nworkflows, with instant 3D volumetric rendering. A user study with a public\ncraniofacial CT dataset demonstrated the tool's foundational viability,\nachieving a System Usability Scale (SUS) score of 66, within the expected range\nfor medical applications. Participants highlighted the system's intuitive\ncontrols (scoring 4.1/5 for self-descriptiveness on ISONORM metrics) and\nspatial interaction design, with qualitative feedback highlighting strengths in\nhybrid 2D/3D navigation and realistic stylus ergonomics. While users identified\nopportunities to enhance task-specific precision and error management, the\nplatform's core workflow enabled dynamic slice adjustment, reducing cognitive\nload compared to desktop tools. Results position the XR-stylus paradigm as a\npromising foundation for immersive segmentation tools, with iterative\nrefinements targeting haptic feedback calibration and workflow personalization\nto advance adoption in preoperative planning.\n","authors":["Lisle Faray de Paiva","Gijs Luijten","Ana Sofia Ferreira Santos","Moon Kim","Behrus Puladi","Jens Kleesiek","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2506.04858v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.16462v3","updated":"2025-06-05T10:08:18Z","published":"2023-11-28T03:45:29Z","title":"Viewport Prediction for Volumetric Video Streaming by Exploring Video\n  Saliency and Trajectory Information","summary":"  Volumetric video, also known as hologram video, is a novel medium that\nportrays natural content in Virtual Reality (VR), Augmented Reality (AR), and\nMixed Reality (MR). It is expected to be the next-gen video technology and a\nprevalent use case for 5G and beyond wireless communication. Considering that\neach user typically only watches a section of the volumetric video, known as\nthe viewport, it is essential to have precise viewport prediction for optimal\nperformance. However, research on this topic is still in its infancy. In the\nend, this paper presents and proposes a novel approach, named Saliency and\nTrajectory Viewport Prediction (STVP), which aims to improve the precision of\nviewport prediction in volumetric video streaming. The STVP extensively\nutilizes video saliency information and viewport trajectory. To our knowledge,\nthis is the first comprehensive study of viewport prediction in volumetric\nvideo streaming. In particular, we introduce a novel sampling method, Uniform\nRandom Sampling (URS), to reduce computational complexity while still\npreserving video features in an efficient manner. Then we present a saliency\ndetection technique that incorporates both spatial and temporal information for\ndetecting static, dynamic geometric, and color salient regions. Finally, we\nintelligently fuse saliency and trajectory information to achieve more accurate\nviewport prediction. We conduct extensive simulations to evaluate the\neffectiveness of our proposed viewport prediction methods using\nstate-of-the-art volumetric video sequences. The experimental results show the\nsuperiority of the proposed method over existing schemes. The dataset and\nsource code will be publicly accessible after acceptance.\n","authors":["Jie Li","Zhixin Li","Zhi Liu","Pengyuan Zhou","Richang Hong","Qiyue Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2311.16462v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04755v1","updated":"2025-06-05T08:40:24Z","published":"2025-06-05T08:40:24Z","title":"Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning","summary":"  While multi-modal large language models (MLLMs) have made significant\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\nbelieved that extensive training data is necessary for improving multi-modal\nreasoning ability, inevitably leading to data redundancy and substantial\ncomputational costs. However, can smaller high-value datasets match or\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\nchallenge this assumption through a key observation: meaningful multi-modal\nreasoning is triggered by only a sparse subset of training samples, termed\ncognitive samples, whereas the majority contribute marginally. Building on this\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\nPotential (RAP), which identifies cognitive samples by estimating each sample's\npotential to stimulate genuine multi-modal reasoning by two complementary\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\noutcome model principle, eliminates samples that overly rely on language priors\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\nConfidence Estimator (ACE), which exploits token-level self-attention to\ndiscard samples dominated by irrelevant but over-emphasized tokens in\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\nReplacement Module (DRM) to substitute trivial instances with cognitively\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\nExperiments on six datasets show that our RAP method consistently achieves\nsuperior performance using only 9.3% of the training data, while reducing\ncomputational costs by over 43%. Our code is available at\nhttps://github.com/Leo-ssl/RAP.\n","authors":["Shenshen Li","Kaiyuan Deng","Lei Wang","Hao Yang","Chong Peng","Peng Yan","Fumin Shen","Heng Tao Shen","Xing Xu"],"pdf_url":"https://arxiv.org/pdf/2506.04755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11315v2","updated":"2025-06-05T05:58:37Z","published":"2025-03-14T11:31:30Z","title":"MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens","summary":"  Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early AV-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.72% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%.\n","authors":["Jeong Hun Yeo","Hyeongseop Rha","Se Jin Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2503.11315v2.pdf","comment":"Accepted at Findings of ACL 2025. The code and models are available\n  https://github.com/JeongHun0716/MMS-LLaMA"},{"id":"http://arxiv.org/abs/2505.18614v2","updated":"2025-06-05T04:48:21Z","published":"2025-05-24T09:28:09Z","title":"MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation","summary":"  Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.\n","authors":["Woohyun Cho","Youngmin Kim","Sunghyun Lee","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2505.18614v2.pdf","comment":"28 pages, 8 figures, our codes and datasets are available at\n  https://github.com/k1064190/MAVL"},{"id":"http://arxiv.org/abs/2502.17709v2","updated":"2025-06-05T02:50:14Z","published":"2025-02-24T23:05:31Z","title":"Contrastive Visual Data Augmentation","summary":"  Large multimodal models (LMMs) often struggle to recognize novel concepts, as\nthey rely on pre-trained knowledge and have limited ability to capture subtle\nvisual details. Domain-specific knowledge gaps in training also make them prone\nto confusing visually similar, commonly misrepresented, or low-resource\nconcepts. To help LMMs better align nuanced visual features with language,\nimproving their ability to recognize and reason about novel or rare concepts,\nwe propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA\nextracts key contrastive textual and visual features of target concepts against\nthe known concepts they are misrecognized as, and then uses multimodal\ngenerative models to produce targeted synthetic data. Automatic filtering of\nextracted features and augmented images is implemented to guarantee their\nquality, as verified by human annotators. We show the effectiveness and\nefficiency of CoDA on low-resource concept and diverse scene recognition\ndatasets including INaturalist and SUN. We additionally collect NovelSpecies, a\nbenchmark dataset consisting of newly discovered animal species that are\nguaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these\nthree datasets show CoDA significantly improves SOTA visual data augmentation\nstrategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains\nin accuracy.\n","authors":["Yu Zhou","Bingxuan Li","Mohan Tang","Xiaomeng Jin","Te-Lin Wu","Kuan-Hao Huang","Heng Ji","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2502.17709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04555v1","updated":"2025-06-05T02:01:59Z","published":"2025-06-05T02:01:59Z","title":"Enhancing Frequency for Single Image Super-Resolution with Learnable\n  Separable Kernels","summary":"  Existing approaches often enhance the performance of single-image\nsuper-resolution (SISR) methods by incorporating auxiliary structures, such as\nspecialized loss functions, to indirectly boost the quality of low-resolution\nimages. In this paper, we propose a plug-and-play module called Learnable\nSeparable Kernels (LSKs), which are formally rank-one matrices designed to\ndirectly enhance image frequency components. We begin by explaining why LSKs\nare particularly suitable for SISR tasks from a frequency perspective. Baseline\nmethods incorporating LSKs demonstrate a significant reduction of over 60\\% in\nboth the number of parameters and computational requirements. This reduction is\nachieved through the decomposition of LSKs into orthogonal and mergeable\none-dimensional kernels. Additionally, we perform an interpretable analysis of\nthe feature maps generated by LSKs. Visualization results reveal the capability\nof LSKs to enhance image frequency components effectively. Extensive\nexperiments show that incorporating LSKs not only reduces the number of\nparameters and computational load but also improves overall model performance.\nMoreover, these experiments demonstrate that models utilizing LSKs exhibit\nsuperior performance, particularly as the upscaling factor increases.\n","authors":["Heng Tian"],"pdf_url":"https://arxiv.org/pdf/2506.04555v1.pdf","comment":null}]},"2025-06-04T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.03100v2","updated":"2025-06-04T22:06:43Z","published":"2025-06-03T17:31:53Z","title":"Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified\n  Theory and Risk Bounds","summary":"  Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.\n","authors":["Yang Guo","Yutian Tao","Yifei Ming","Robert D. Nowak","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2506.03100v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2506.04422v1","updated":"2025-06-04T20:13:42Z","published":"2025-06-04T20:13:42Z","title":"I'm Sorry Dave, I'm Afraid I Can't Return That: On YouTube Search API\n  Use in Research","summary":"  YouTube is among the most widely-used platforms worldwide, and has seen a lot\nof recent academic attention. Despite its popularity and the number of studies\nconducted on it, much less is understood about the way in which YouTube's Data\nAPI, and especially the Search endpoint, operates. In this paper, we analyze\nthe API's behavior by running identical queries across a period of 12 weeks.\nOur findings suggest that the search endpoint returns highly inconsistent\nresults between queries in ways that are not officially documented.\nSpecifically, the API seems to randomize returned videos based on the relative\npopularity of the respective topic during the query period, making it nearly\nimpossible to obtain representative historical video samples, especially during\nnon-peak topical periods. Our results also suggest that the API may prioritize\nshorter, more popular videos, although the role of channel popularity is not as\nclear. We conclude with suggested strategies for researchers using the API for\ndata collection, as well as future research directions on expanding the API's\nuse-cases.\n","authors":["Alexandros Efstratiou"],"pdf_url":"https://arxiv.org/pdf/2506.04422v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.18247v2","updated":"2025-06-04T20:03:54Z","published":"2025-05-23T17:18:45Z","title":"MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized\n  Domain Question-Answering","summary":"  Retrieval-Augmented Generation (RAG) struggles with domain-specific\nenterprise datasets, often isolated behind firewalls and rich in complex,\nspecialized terminology unseen by LLMs during pre-training. Semantic\nvariability across domains like medicine, networking, or law hampers RAG's\ncontext precision, while fine-tuning solutions are costly, slow, and lack\ngeneralization as new data emerges. Achieving zero-shot precision with\nretrievers without fine-tuning still remains a key challenge. We introduce\n'MetaGen Blended RAG', a novel enterprise search approach that enhances\nsemantic retrievers through a metadata generation pipeline and hybrid query\nindexes using dense and sparse vectors. By leveraging key concepts, topics, and\nacronyms, our method creates metadata-enriched semantic indexes and boosted\nhybrid queries, delivering robust, scalable performance without fine-tuning. On\nthe biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks\nand even rivaling fine-tuned models on that dataset, while also excelling on\ndatasets like SQuAD and NQ. This approach redefines enterprise search using a\nnew approach to building semantic retrievers with unmatched generalization\nacross specialized domains.\n","authors":["Kunal Sawarkar","Shivam R. Solanki","Abhilasha Mangal"],"pdf_url":"https://arxiv.org/pdf/2505.18247v2.pdf","comment":"Preprint. Paper Submitted for NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems"},{"id":"http://arxiv.org/abs/2506.04140v1","updated":"2025-06-04T16:31:44Z","published":"2025-06-04T16:31:44Z","title":"Quantifying Query Fairness Under Unawareness","summary":"  Traditional ranking algorithms are designed to retrieve the most relevant\nitems for a user's query, but they often inherit biases from data that can\nunfairly disadvantage vulnerable groups. Fairness in information access systems\n(IAS) is typically assessed by comparing the distribution of groups in a\nranking to a target distribution, such as the overall group distribution in the\ndataset. These fairness metrics depend on knowing the true group labels for\neach item. However, when groups are defined by demographic or sensitive\nattributes, these labels are often unknown, leading to a setting known as\n\"fairness under unawareness\". To address this, group membership can be inferred\nusing machine-learned classifiers, and group prevalence is estimated by\ncounting the predicted labels. Unfortunately, such an estimation is known to be\nunreliable under dataset shift, compromising the accuracy of fairness\nevaluations. In this paper, we introduce a robust fairness estimator based on\nquantification that effectively handles multiple sensitive attributes beyond\nbinary classifications. Our method outperforms existing baselines across\nvarious sensitive attributes and, to the best of our knowledge, is the first to\nestablish a reliable protocol for measuring fairness under unawareness across\nmultiple queries and groups.\n","authors":["Thomas Jaenich","Alejandro Moreo","Alessandro Fabris","Graham McDonald","Andrea Esuli","Iadh Ounis","Fabrizio Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2506.04140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18223v2","updated":"2025-06-04T15:54:37Z","published":"2025-03-23T21:51:58Z","title":"MammAlps: A multi-view video behavior monitoring dataset of wild mammals\n  in the Swiss Alps","summary":"  Monitoring wildlife is essential for ecology and ethology, especially in\nlight of the increasing human impact on ecosystems. Camera traps have emerged\nas habitat-centric sensors enabling the study of wildlife populations at scale\nwith minimal disturbance. However, the lack of annotated video datasets limits\nthe development of powerful video understanding models needed to process the\nvast amount of fieldwork data collected. To advance research in wild animal\nbehavior monitoring we present MammAlps, a multimodal and multi-view dataset of\nwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.\nMammAlps contains over 14 hours of video with audio, 2D segmentation maps and\n8.5 hours of individual tracks densely labeled for species and behavior. Based\non 6135 single animal clips, we propose the first hierarchical and multimodal\nanimal behavior recognition benchmark using audio, video and reference scene\nsegmentation maps as inputs. Furthermore, we also propose a second\necology-oriented benchmark aiming at identifying activities, species, number of\nindividuals and meteorological conditions from 397 multi-view and long-term\necological events, including false positive triggers. We advocate that both\ntasks are complementary and contribute to bridging the gap between machine\nlearning and ecology. Code and data are available at:\nhttps://github.com/eceo-epfl/MammAlps\n","authors":["Valentin Gabeff","Haozhe Qi","Brendan Flaherty","Gencer Sumbül","Alexander Mathis","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2503.18223v2.pdf","comment":"CVPR 2025; Benchmark and code at:\n  https://github.com/eceo-epfl/MammAlps. After submission of v1, we noticed\n  that a few audio files were not correctly aligned with the corresponding\n  video. We fixed the issue, which had little to no impact on performance. We\n  also now report results for three runs"},{"id":"http://arxiv.org/abs/2506.04083v1","updated":"2025-06-04T15:44:50Z","published":"2025-06-04T15:44:50Z","title":"A Generative Adaptive Replay Continual Learning Model for Temporal\n  Knowledge Graph Reasoning","summary":"  Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning\n(TKGR) methods focus on significantly reducing computational cost and\nmitigating catastrophic forgetting caused by fine-tuning models with new data.\nHowever, existing CL-based TKGR methods still face two key limitations: (1)\nThey usually one-sidedly reorganize individual historical facts, while\noverlooking the historical context essential for accurately understanding the\nhistorical semantics of these facts; (2) They preserve historical knowledge by\nsimply replaying historical facts, while ignoring the potential conflicts\nbetween historical and emerging facts. In this paper, we propose a Deep\nGenerative Adaptive Replay (DGAR) method, which can generate and adaptively\nreplay historical entity distribution representations from the whole historical\ncontext. To address the first challenge, historical context prompts as sampling\nunits are built to preserve the whole historical context information. To\novercome the second challenge, a pre-trained diffusion model is adopted to\ngenerate the historical distribution. During the generation process, the common\nfeatures between the historical and current distributions are enhanced under\nthe guidance of the TKGR model. In addition, a layer-by-layer adaptive replay\nmechanism is designed to effectively integrate historical and current\ndistributions. Experimental results demonstrate that DGAR significantly\noutperforms baselines in reasoning and mitigating forgetting.\n","authors":["Zhiyu Zhang","Wei Chen","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2506.04083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09492v3","updated":"2025-06-04T14:47:48Z","published":"2025-03-12T15:52:51Z","title":"Learning Cascade Ranking as One Network","summary":"  Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances have introduced interaction-aware training paradigms,\nbut still struggle to 1) align training objectives with the goal of the entire\ncascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn\neffective collaboration patterns for different stages. To address these\nchallenges, we propose LCRON, which introduces a novel surrogate loss function\nderived from the lower bound probability that ground truth items are selected\nby cascade ranking, ensuring alignment with the overall objective of the\nsystem. According to the properties of the derived bound, we further design an\nauxiliary loss for each stage to drive the reduction of this bound, leading to\na more robust and effective top-k selection. LCRON enables end-to-end training\nof the entire cascade ranking system as a unified network. Experimental results\ndemonstrate that LCRON achieves significant improvement over existing methods\non public benchmarks and industrial applications, addressing key limitations in\ncascade ranking training and significantly enhancing system performance.\n","authors":["Yunli Wang","Zhen Zhang","Zhiqiang Wang","Zixuan Yang","Yu Li","Jian Yang","Shiyang Wen","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2503.09492v3.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.04015v1","updated":"2025-06-04T14:46:18Z","published":"2025-06-04T14:46:18Z","title":"GORACS: Group-level Optimal Transport-guided Coreset Selection for\n  LLM-based Recommender Systems","summary":"  Although large language models (LLMs) have shown great potential in\nrecommender systems, the prohibitive computational costs for fine-tuning LLMs\non entire datasets hinder their successful deployment in real-world scenarios.\nTo develop affordable and effective LLM-based recommender systems, we focus on\nthe task of coreset selection which identifies a small subset of fine-tuning\ndata to optimize the test loss, thereby facilitating efficient LLMs'\nfine-tuning. Although there exist some intuitive solutions of subset selection,\nincluding distribution-based and importance-based approaches, they often lead\nto suboptimal performance due to the misalignment with downstream fine-tuning\nobjectives or weak generalization ability caused by individual-level sample\nselection. To overcome these challenges, we propose GORACS, which is a novel\nGroup-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based\nrecommender systems. GORACS is designed based on two key principles for coreset\nselection: 1) selecting the subsets that minimize the test loss to align with\nfine-tuning objectives, and 2) enhancing model generalization through\ngroup-level data selection. Corresponding to these two principles, GORACS has\ntwo key components: 1) a Proxy Optimization Objective (POO) leveraging optimal\ntransport and gradient information to bound the intractable test loss, thus\nreducing computational costs by avoiding repeated LLM retraining, and 2) a\ntwo-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient\ngroup-level selection. Our extensive experiments across diverse recommendation\ndatasets and tasks validate that GORACS significantly reduces fine-tuning costs\nof LLMs while achieving superior performance over the state-of-the-art\nbaselines and full data training. The source code of GORACS are available at\nhttps://github.com/Mithas-114/GORACS.\n","authors":["Tiehua Mei","Hengrui Chen","Peng Yu","Jiaqing Liang","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2506.04015v1.pdf","comment":"Accepted by KDD 2025"},{"id":"http://arxiv.org/abs/2502.16540v2","updated":"2025-06-04T13:17:52Z","published":"2025-02-23T11:19:44Z","title":"D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model\n  Generation Using Large Language Models","summary":"  In electronic design, engineers often manually search through extensive\ndocuments to retrieve component parameters required for constructing SPICE\nmodels, a process that is both labor-intensive and time-consuming. To address\nthis challenge, we present an automated framework called D2S-FLOW that\nleverages large language models (LLMs) to extract electrical parameters from\ndatasheets and generate SPICE models with high precision and efficiency,\nsignificantly reducing the need for manual intervention. Unlike traditional RAG\nsystems, D2S-FLOW employs a workflow to enhance precision in handling\nunstructured documents and inconsistent naming conventions through three\ninnovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical\nDocument-Enhanced Retrieval (HDER), and Heterogeneous Named Entity\nNormalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER\nutilizes document structure for precise parameter localization, and HNEN\nstandardizes terminology via semantic inference. Experimental results\ndemonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1\nscore of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the\nstrongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it\nreduces API token consumption by 38% and minimizes the irrelevant information\nratio to 4%, showcasing substantial improvements in resource efficiency. This\nresearch provides an effective automated solution for circuit design.\n","authors":["Hong Cai Chen","Yi Pin Xu","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.16540v2.pdf","comment":"14 pages, 18 figures"},{"id":"http://arxiv.org/abs/2506.03895v1","updated":"2025-06-04T12:43:17Z","published":"2025-06-04T12:43:17Z","title":"Graph-Embedding Empowered Entity Retrieval","summary":"  In this research, we investigate methods for entity retrieval using graph\nembeddings. While various methods have been proposed over the years, most\nutilize a single graph embedding and entity linking approach. This hinders our\nunderstanding of how different graph embedding and entity linking methods\nimpact entity retrieval. To address this gap, we investigate the effects of\nthree different categories of graph embedding techniques and five different\nentity linking methods. We perform a reranking of entities using the distance\nbetween the embeddings of annotated entities and the entities we wish to\nrerank. We conclude that the selection of both graph embeddings and entity\nlinkers significantly impacts the effectiveness of entity retrieval. For graph\nembeddings, methods that incorporate both graph structure and textual\ndescriptions of entities are the most effective. For entity linking, both\nprecision and recall concerning concepts are important for optimal retrieval\nperformance. Additionally, it is essential for the graph to encompass as many\nentities as possible.\n","authors":["Emma J. Gerritse","Faegheh Hasibi","Arjen P. de Vries"],"pdf_url":"https://arxiv.org/pdf/2506.03895v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2005.02843"},{"id":"http://arxiv.org/abs/2505.16865v2","updated":"2025-06-04T11:31:59Z","published":"2025-05-22T16:22:54Z","title":"LARES: Latent Reasoning for Sequential Recommendation","summary":"  Sequential recommender systems have become increasingly important in\nreal-world applications that model user behavior sequences to predict their\npreferences. However, existing sequential recommendation methods predominantly\nrely on non-reasoning paradigms, which may limit the model's computational\ncapacity and result in suboptimal recommendation performance. To address these\nlimitations, we present LARES, a novel and scalable LAtent REasoning framework\nfor Sequential recommendation that enhances model's representation capabilities\nthrough increasing the computation density of parameters by depth-recurrent\nlatent reasoning. Our proposed approach employs a recurrent architecture that\nallows flexible expansion of reasoning depth without increasing parameter\ncomplexity, thereby effectively capturing dynamic and intricate user interest\npatterns. A key difference of LARES lies in refining all input tokens at each\nimplicit reasoning step to improve the computation utilization. To fully unlock\nthe model's reasoning potential, we design a two-phase training strategy: (1)\nSelf-supervised pre-training (SPT) with dual alignment objectives; (2)\nReinforcement post-training (RPT). During the first phase, we introduce\ntrajectory-level alignment and step-level alignment objectives, which enable\nthe model to learn recommendation-oriented latent reasoning patterns without\nrequiring supplementary annotated data. The subsequent phase utilizes\nreinforcement learning (RL) to harness the model's exploratory ability, further\nrefining its reasoning capabilities. Comprehensive experiments on real-world\nbenchmarks demonstrate our framework's superior performance. Notably, LARES\nexhibits seamless compatibility with existing advanced models, further\nimproving their recommendation performance. Our code is available at\nhttps://anonymous.4open.science/r/LARES-E458/.\n","authors":["Enze Liu","Bowen Zheng","Xiaolei Wang","Wayne Xin Zhao","Jinpeng Wang","Sheng Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.16865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03827v1","updated":"2025-06-04T10:57:18Z","published":"2025-06-04T10:57:18Z","title":"Multi-objective Aligned Bidword Generation Model for E-commerce Search\n  Advertising","summary":"  Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.\n","authors":["Zhenhui Liu","Chunyuan Yuan","Ming Pang","Zheng Fang","Li Yuan","Xue Jiang","Changping Peng","Zhangang Lin","Zheng Luo","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2506.03827v1.pdf","comment":"Accepted by SIGIR2025"},{"id":"http://arxiv.org/abs/2506.03822v1","updated":"2025-06-04T10:52:55Z","published":"2025-06-04T10:52:55Z","title":"CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents","summary":"  Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.\n","authors":["Fabian Karl","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2506.03822v1.pdf","comment":"Accepted at SCOLIA 2025"},{"id":"http://arxiv.org/abs/2506.03807v1","updated":"2025-06-04T10:27:22Z","published":"2025-06-04T10:27:22Z","title":"Understanding Mental Models of Generative Conversational Search and The\n  Effect of Interface Transparency","summary":"  The experience and adoption of conversational search is tied to the accuracy\nand completeness of users' mental models -- their internal frameworks for\nunderstanding and predicting system behaviour. Thus, understanding these models\ncan reveal areas for design interventions. Transparency is one such\nintervention which can improve system interpretability and enable mental model\nalignment. While past research has explored mental models of search engines,\nthose of generative conversational search remain underexplored, even while the\npopularity of these systems soars. To address this, we conducted a study with\n16 participants, who performed 4 search tasks using 4 conversational interfaces\nof varying transparency levels. Our analysis revealed that most user mental\nmodels were too abstract to support users in explaining individual search\ninstances. These results suggest that 1) mental models may pose a barrier to\nappropriate trust in conversational search, and 2) hybrid web-conversational\nsearch is a promising novel direction for future search interface design.\n","authors":["Chadha Degachi","Samuel Kernan Freire","Evangelos Niforatos","Gerd Kortuem"],"pdf_url":"https://arxiv.org/pdf/2506.03807v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2504.14175v2","updated":"2025-06-04T09:32:19Z","published":"2025-04-19T04:32:38Z","title":"Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query\n  Expansion","summary":"  Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyze whether the generated documents contain information\nentailed by ground-truth evidence and assess their impact on performance. Our\nfindings indicate that, on average, performance improvements consistently\noccurred for claims whose generated documents included sentences entailed by\ngold evidence. This suggests that knowledge leakage may be present in\nfact-verification benchmarks, potentially inflating the perceived performance\nof LLM-based query expansion methods.\n","authors":["Yejun Yoon","Jaeyoon Jung","Seunghyun Yoon","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2504.14175v2.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2409.05546v3","updated":"2025-06-04T09:22:33Z","published":"2024-09-09T12:11:53Z","title":"Generative Recommender with End-to-End Learnable Item Tokenization","summary":"  Generative recommendation systems have gained increasing attention as an\ninnovative approach that directly generates item identifiers for recommendation\ntasks. Despite their potential, a major challenge is the effective construction\nof item identifiers that align well with recommender systems. Current\napproaches often treat item tokenization and generative recommendation training\nas separate processes, which can lead to suboptimal performance. To overcome\nthis issue, we introduce ETEGRec, a novel End-To-End Generative Recommender\nthat unifies item tokenization and generative recommendation into a cohesive\nframework. Built on a dual encoder-decoder architecture, ETEGRec consists of an\nitem tokenizer and a generative recommender. To enable synergistic interaction\nbetween these components, we propose a recommendation-oriented alignment\nstrategy, which includes two key optimization objectives: sequence-item\nalignment and preference-semantic alignment. These objectives tightly couple\nthe learning processes of the item tokenizer and the generative recommender,\nfostering mutual enhancement. Additionally, we develop an alternating\noptimization technique to ensure stable and efficient end-to-end training of\nthe entire framework. Extensive experiments demonstrate the superior\nperformance of our approach compared to traditional sequential recommendation\nmodels and existing generative recommendation baselines. Our code is available\nat https://github.com/RUCAIBox/ETEGRec.\n","authors":["Enze Liu","Bowen Zheng","Cheng Ling","Lantao Hu","Han Li","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.05546v3.pdf","comment":"Accepted by SIGIR 2025 Research Track"},{"id":"http://arxiv.org/abs/2503.06474v2","updated":"2025-06-04T08:32:02Z","published":"2025-03-09T06:20:24Z","title":"ROGRAG: A Robustly Optimized GraphRAG Framework","summary":"  Large language models (LLMs) commonly struggle with specialized or emerging\ntopics which are rarely seen in the training corpus. Graph-based\nretrieval-augmented generation (GraphRAG) addresses this by structuring domain\nknowledge as a graph for dynamic retrieval. However, existing pipelines involve\ncomplex engineering workflows, making it difficult to isolate the impact of\nindividual components. It is also challenging to evaluate the retrieval\neffectiveness due to the overlap between the pretraining and evaluation\ndatasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG\nframework. Specifically, we propose a multi-stage retrieval mechanism that\nintegrates dual-level with logic form retrieval methods to improve retrieval\nrobustness without increasing computational cost. To further refine the system,\nwe incorporate various result verification methods and adopt an incremental\ndatabase construction approach. Through extensive ablation experiments, we\nrigorously assess the effectiveness of each component. Our implementation\nincludes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct\ninitially underperformed. ROGRAG significantly improves the score from 60.0% to\n75.0% and outperforms mainstream methods. Experiments on domain-specific\ndatasets reveal that dual-level retrieval enhances fuzzy matching, while logic\nform retrieval improves structured reasoning, highlighting the importance of\nmulti-stage retrieval.ROGRAG is released as an open-source resource and\nsupports installation with pip.\n","authors":["Zhefan Wang","Huanjun Kong","Jie Ying","Wanli Ouyang","Nanqing Dong"],"pdf_url":"https://arxiv.org/pdf/2503.06474v2.pdf","comment":"ACL2025 demo track, 10 pages"},{"id":"http://arxiv.org/abs/2506.03699v1","updated":"2025-06-04T08:31:33Z","published":"2025-06-04T08:31:33Z","title":"Scaling Transformers for Discriminative Recommendation via Generative\n  Pretraining","summary":"  Discriminative recommendation tasks, such as CTR (click-through rate) and CVR\n(conversion rate) prediction, play critical roles in the ranking stage of\nlarge-scale industrial recommender systems. However, training a discriminative\nmodel encounters a significant overfitting issue induced by data sparsity.\nMoreover, this overfitting issue worsens with larger models, causing them to\nunderperform smaller ones. To address the overfitting issue and enhance model\nscalability, we propose a framework named GPSD (\\textbf{G}enerative\n\\textbf{P}retraining for \\textbf{S}calable \\textbf{D}iscriminative\nRecommendation), drawing inspiration from generative training, which exhibits\nno evident signs of overfitting. GPSD leverages the parameters learned from a\npretrained generative model to initialize a discriminative model, and\nsubsequently applies a sparse parameter freezing strategy. Extensive\nexperiments conducted on both industrial-scale and publicly available datasets\ndemonstrate the superior performance of GPSD. Moreover, it delivers remarkable\nimprovements in online A/B tests. GPSD offers two primary advantages: 1) it\nsubstantially narrows the generalization gap in model training, resulting in\nbetter test performance; and 2) it leverages the scalability of Transformers,\ndelivering consistent performance gains as models are scaled up. Specifically,\nwe observe consistent performance improvements as the model dense parameters\nscale from 13K to 0.3B, closely adhering to power laws. These findings pave the\nway for unifying the architectures of recommendation models and language\nmodels, enabling the direct application of techniques well-established in large\nlanguage models to recommendation models.\n","authors":["Chunqi Wang","Bingchao Wu","Zheng Chen","Lei Shen","Bing Wang","Xiaoyi Zeng"],"pdf_url":"https://arxiv.org/pdf/2506.03699v1.pdf","comment":"KDD'25"},{"id":"http://arxiv.org/abs/2404.17589v5","updated":"2025-06-04T08:19:53Z","published":"2024-04-19T08:43:03Z","title":"An Offline Reinforcement Learning Algorithm Customized for Multi-Task\n  Fusion in Large-Scale Recommender Systems","summary":"  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for\ncombining multiple scores outputted by Multi-Task Learning (MTL) into a final\nscore to maximize user satisfaction, which determines the ultimate\nrecommendation results. Recently, to optimize long-term user satisfaction\nwithin a recommendation session, Reinforcement Learning (RL) is used for MTF in\nthe industry. However, the offline RL algorithms used for MTF so far have the\nfollowing severe problems: 1) to avoid out-of-distribution (OOD) problem, their\nconstraints are overly strict, which seriously damage their performance; 2)\nthey are unaware of the exploration policy used for producing training data and\nnever interact with real environment, so only suboptimal policy can be learned;\n3) the traditional exploration policies are inefficient and hurt user\nexperience. To solve the above problems, we propose a novel method named\nIntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF\nintegrates offline RL model with our online exploration policy to relax\noverstrict and complicated constraints, which significantly improves its\nperformance. We also design an extremely efficient exploration policy, which\neliminates low-value exploration space and focuses on exploring potential\nhigh-value state-action pairs. Moreover, we adopt progressive training mode to\nfurther enhance our model's performance with the help of our exploration\npolicy. We conduct extensive offline and online experiments in the short video\nchannel of Tencent News. The results demonstrate that our model outperforms\nother models remarkably. IntegratedRL-MTF has been fully deployed in our RS and\nother large-scale RSs in Tencent, which have achieved significant improvements.\n","authors":["Peng Liu","Cong Xu","Ming Zhao","Jiawei Zhu","Bin Wang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2404.17589v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03734v2","updated":"2025-06-04T07:36:34Z","published":"2023-08-07T17:32:33Z","title":"Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity\n  Resolution","summary":"  The entity resolution problem requires finding pairs across datasets that\nbelong to different owners but refer to the same entity in the real world. To\ntrain and evaluate solutions (either rule-based or machine-learning-based) to\nthe entity resolution problem, generating a ground truth dataset with entity\npairs or clusters is needed. However, such a data annotation process involves\nhumans as domain oracles to review the plaintext data for all candidate record\npairs from different parties, which inevitably infringes the privacy of data\nowners, especially in privacy-sensitive cases like medical records. To the best\nof our knowledge, there is no prior work on privacy-preserving ground truth\ndataset generation, especially in the domain of entity resolution. We propose a\nnovel blind annotation protocol based on homomorphic encryption that allows\ndomain oracles to collaboratively label ground truths without sharing data in\nplaintext with other parties. In addition, we design a domain-specific\neasy-to-use language that hides the sophisticated underlying homomorphic\nencryption layer. Rigorous proof of the privacy guarantee is provided and our\nempirical experiments via an annotation simulator indicate the feasibility of\nour privacy-preserving protocol (f-measure on average achieves more than 90\\%\ncompared with the real ground truths).\n","authors":["Yixiang Yao","Weizhao Jin","Srivatsan Ravi"],"pdf_url":"https://arxiv.org/pdf/2308.03734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02544v2","updated":"2025-06-04T06:31:54Z","published":"2025-06-03T07:32:40Z","title":"CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG","summary":"  Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose Cross-source knowledge \\textbf{Re}conciliation for\nMultimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively\nreconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a\nfour-stage pipeline: it first generates an internal response from parametric\nknowledge, then selects the most relevant multimodal evidence via joint\nsimilarity assessment, generates an external response, and finally integrates\nboth to produce a reliable answer. Additionally, a specialized training\nparadigm enhances knowledge source discrimination, multimodal integration, and\nunified answer generation. Experiments on KB-VQA benchmarks show that\nCoRe-MMRAG achieves substantial improvements over baseline methods, achieving\n5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.\n","authors":["Yang Tian","Fan Liu","Jingyuan Zhang","Victoria W.","Yupeng Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2506.02544v2.pdf","comment":"Accepted to ACL 2025 Main"},{"id":"http://arxiv.org/abs/2506.03487v1","updated":"2025-06-04T02:00:44Z","published":"2025-06-04T02:00:44Z","title":"ProRank: Prompt Warmup via Reinforcement Learning for Small Language\n  Models Reranking","summary":"  Reranking is fundamental to information retrieval and retrieval-augmented\ngeneration, with recent Large Language Models (LLMs) significantly advancing\nreranking quality. While recent advances with LLMs have significantly improved\ndocument reranking quality, current approaches primarily rely on large-scale\nLLMs (>7B parameters) through zero-shot prompting, presenting high\ncomputational costs. Small Language Models (SLMs) offer a promising alternative\nbecause of their efficiency, but our preliminary quantitative analysis reveals\nthey struggle with understanding task prompts without fine-tuning. This limits\ntheir effectiveness for document reranking tasks. To address this issue, we\nintroduce a novel two-stage training approach, ProRank, for SLM-based document\nreranking. First, we propose a prompt warmup stage using reinforcement learning\nGRPO to steer SLMs to understand task prompts and generate more accurate\ncoarse-grained binary relevance scores for document reranking. Then, we\ncontinuously fine-tune the SLMs with a fine-grained score learning stage\nwithout introducing additional layers to further improve the reranking quality.\nComprehensive experimental results demonstrate that the proposed ProRank\nconsistently outperforms both the most advanced open-source and proprietary\nreranking models. Notably, our lightweight ProRank-0.5B model even surpasses\nthe powerful 32B LLM reranking model on the BEIR benchmark, establishing that\nproperly trained SLMs can achieve superior document reranking performance while\nmaintaining computational efficiency.\n","authors":["Xianming Li","Aamir Shakir","Rui Huang","Julius Lipp","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2506.03487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02916v2","updated":"2025-06-04T01:42:22Z","published":"2025-06-03T14:18:19Z","title":"MMM4Rec: A Transfer-Efficient Framework for Multi-modal Sequential\n  Recommendation","summary":"  Sequential Recommendation (SR) systems model user preferences by analyzing\ninteraction histories. Although transferable multi-modal SR architectures\ndemonstrate superior performance compared to traditional ID-based approaches,\ncurrent methods incur substantial fine-tuning costs when adapting to new\ndomains due to complex optimization requirements and negative transfer effects\n- a significant deployment bottleneck that hinders engineers from efficiently\nrepurposing pre-trained models for novel application scenarios with minimal\ntuning overhead. We propose MMM4Rec (Multi-Modal Mamba for Sequential\nRecommendation), a novel multi-modal SR framework that incorporates a dedicated\nalgebraic constraint mechanism for efficient transfer learning. By combining\nState Space Duality (SSD)'s temporal decay properties with a time-aware\nmodeling design, our model dynamically prioritizes key modality information,\novercoming limitations of Transformer-based approaches. The framework\nimplements a constrained two-stage process: (1) sequence-level cross-modal\nalignment via shared projection matrices, followed by (2) temporal fusion using\nour newly designed Cross-SSD module and dual-channel Fourier adaptive\nfiltering. This architecture maintains semantic consistency while suppressing\nnoise propagation.MMM4Rec achieves rapid fine-tuning convergence with simple\ncross-entropy loss, significantly improving multi-modal recommendation accuracy\nwhile maintaining strong transferability. Extensive experiments demonstrate\nMMM4Rec's state-of-the-art performance, achieving the maximum 31.78% NDCG@10\nimprovement over existing models and exhibiting 10 times faster average\nconvergence speed when transferring to large-scale downstream datasets.\n","authors":["Hao Fan","Yanrong Hu","Kai Fang","Qingyang Liu","Hongjiu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.02916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20730v2","updated":"2025-06-04T00:54:43Z","published":"2025-05-27T05:18:57Z","title":"What LLMs Miss in Recommendations: Bridging the Gap with\n  Retrieval-Augmented Collaborative Signals","summary":"  User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders.\n","authors":["Shahrooz Pouryousef","Ali Montazeralghaem"],"pdf_url":"https://arxiv.org/pdf/2505.20730v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.04444v1","updated":"2025-06-04T20:53:43Z","published":"2025-06-04T20:53:43Z","title":"Photoreal Scene Reconstruction from an Egocentric Device","summary":"  In this paper, we investigate the challenges associated with using egocentric\ndevices to photorealistic reconstruct the scene in high dynamic range. Existing\nmethodologies typically assume using frame-rate 6DoF pose estimated from the\ndevice's visual-inertial odometry system, which may neglect crucial details\nnecessary for pixel-accurate reconstruction. This study presents two\nsignificant findings. Firstly, in contrast to mainstream work treating RGB\ncamera as global shutter frame-rate camera, we emphasize the importance of\nemploying visual-inertial bundle adjustment (VIBA) to calibrate the precise\ntimestamps and movement of the rolling shutter RGB sensing camera in a high\nfrequency trajectory format, which ensures an accurate calibration of the\nphysical properties of the rolling-shutter camera. Secondly, we incorporate a\nphysical image formation model based into Gaussian Splatting, which effectively\naddresses the sensor characteristics, including the rolling-shutter effect of\nRGB cameras and the dynamic ranges measured by sensors. Our proposed\nformulation is applicable to the widely-used variants of Gaussian Splats\nrepresentation. We conduct a comprehensive evaluation of our pipeline using the\nopen-source Project Aria device under diverse indoor and outdoor lighting\nconditions, and further validate it on a Meta Quest3 device. Across all\nexperiments, we observe a consistent visual enhancement of +1 dB in PSNR by\nincorporating VIBA, with an additional +1 dB achieved through our proposed\nimage formation model. Our complete implementation, evaluation datasets, and\nrecording profile are available at\nhttp://www.projectaria.com/photoreal-reconstruction/\n","authors":["Zhaoyang Lv","Maurizio Monge","Ka Chen","Yufeng Zhu","Michael Goesele","Jakob Engel","Zhao Dong","Richard Newcombe"],"pdf_url":"https://arxiv.org/pdf/2506.04444v1.pdf","comment":"Paper accepted to SIGGRAPH Conference Paper 2025"},{"id":"http://arxiv.org/abs/2506.04214v1","updated":"2025-06-04T17:57:26Z","published":"2025-06-04T17:57:26Z","title":"Sounding that Object: Interactive Object-Aware Image to Audio Generation","summary":"  Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/\n","authors":["Tingle Li","Baihe Huang","Xiaobin Zhuang","Dongya Jia","Jiawei Chen","Yuping Wang","Zhuo Chen","Gopala Anumanchipalli","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2506.04214v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.04070v1","updated":"2025-06-04T15:34:33Z","published":"2025-06-04T15:34:33Z","title":"LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\n  Impaired via GRPO with LLM-as-Follower Reward","summary":"  Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.\n","authors":["Yi Zhao","Siqi Wang","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2506.04070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03831v1","updated":"2025-06-04T10:58:39Z","published":"2025-06-04T10:58:39Z","title":"Conformer-based Ultrasound-to-Speech Conversion","summary":"  Deep neural networks have shown promising potential for ultrasound-to-speech\nconversion task towards Silent Speech Interfaces. In this work, we applied two\nConformer-based DNN architectures (Base and one with bi-LSTM) for this task.\nSpeaker-specific models were trained on the data of four speakers from the\nUltrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized\nto audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN\nbaseline, objective measurements (MSE and mel cepstral distortion) showed no\nstatistically significant improvement for either model. However, a MUSHRA\nlistening test revealed that Conformer with bi-LSTM provided better perceptual\nquality, while Conformer Base matched the performance of the baseline along\nwith a 3x faster training time due to its simpler architecture. These findings\nsuggest that Conformer-based models, especially the Conformer with bi-LSTM,\noffer a promising alternative to CNNs for ultrasound-to-speech conversion.\n","authors":["Ibrahim Ibrahimov","Zainkó Csaba","Gábor Gosztolya"],"pdf_url":"https://arxiv.org/pdf/2506.03831v1.pdf","comment":"accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.03594v1","updated":"2025-06-04T05:53:16Z","published":"2025-06-04T05:53:16Z","title":"SplArt: Articulation Estimation and Part-Level Reconstruction with 3D\n  Gaussian Splatting","summary":"  Reconstructing articulated objects prevalent in daily environments is crucial\nfor applications in augmented/virtual reality and robotics. However, existing\nmethods face scalability limitations (requiring 3D supervision or costly\nannotations), robustness issues (being susceptible to local optima), and\nrendering shortcomings (lacking speed or photorealism). We introduce SplArt, a\nself-supervised, category-agnostic framework that leverages 3D Gaussian\nSplatting (3DGS) to reconstruct articulated objects and infer kinematics from\ntwo sets of posed RGB images captured at different articulation states,\nenabling real-time photorealistic rendering for novel viewpoints and\narticulations. SplArt augments 3DGS with a differentiable mobility parameter\nper Gaussian, achieving refined part segmentation. A multi-stage optimization\nstrategy is employed to progressively handle reconstruction, part segmentation,\nand articulation estimation, significantly enhancing robustness and accuracy.\nSplArt exploits geometric self-supervision, effectively addressing challenging\nscenarios without requiring 3D annotations or category-specific priors.\nEvaluations on established and newly proposed benchmarks, along with\napplications to real-world scenarios using a handheld RGB camera, demonstrate\nSplArt's state-of-the-art performance and real-world practicality. Code is\npublicly available at https://github.com/ripl/splart.\n","authors":["Shengjie Lin","Jiading Fang","Muhammad Zubair Irshad","Vitor Campagnolo Guizilini","Rares Andrei Ambrus","Greg Shakhnarovich","Matthew R. Walter"],"pdf_url":"https://arxiv.org/pdf/2506.03594v1.pdf","comment":"https://github.com/ripl/splart"},{"id":"http://arxiv.org/abs/2505.14151v3","updated":"2025-06-04T04:30:30Z","published":"2025-05-20T10:01:37Z","title":"ReactDiff: Latent Diffusion for Facial Reaction Generation","summary":"  Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.\n","authors":["Jiaming Li","Sheng Wang","Xin Wang","Yitao Zhu","Honglin Xiong","Zixu Zhuang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14151v3.pdf","comment":"Accepted by Neural Networks"},{"id":"http://arxiv.org/abs/2506.03530v1","updated":"2025-06-04T03:22:44Z","published":"2025-06-04T03:22:44Z","title":"How Far Are We from Predicting Missing Modalities with Foundation\n  Models?","summary":"  Multimodal foundation models have demonstrated impressive capabilities across\ndiverse tasks. However, their potential as plug-and-play solutions for missing\nmodality prediction remains underexplored. To investigate this, we categorize\nexisting approaches into three representative paradigms, encompassing a total\nof 42 model variants, and conduct a comprehensive evaluation in terms of\nprediction accuracy and adaptability to downstream tasks. Our analysis reveals\nthat current foundation models often fall short in two critical aspects: (i)\nfine-grained semantic extraction from the available modalities, and (ii) robust\nvalidation of generated modalities. These limitations lead to suboptimal and,\nat times, misaligned predictions. To address these challenges, we propose an\nagentic framework tailored for missing modality prediction. This framework\ndynamically formulates modality-aware mining strategies based on the input\ncontext, facilitating the extraction of richer and more discriminative semantic\nfeatures. In addition, we introduce a \\textit{self-refinement mechanism}, which\niteratively verifies and enhances the quality of generated modalities through\ninternal feedback. Experimental results show that our method reduces FID for\nmissing image prediction by at least 14% and MER for missing text prediction by\nat least 10% compared to baselines.\n","authors":["Guanzhou Ke","Yi Xie","Xiaoli Wang","Guoqing Chao","Bo Wang","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2506.03530v1.pdf","comment":null}]},"2025-06-03T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.03437v1","updated":"2025-06-03T22:37:37Z","published":"2025-06-03T22:37:37Z","title":"Quake: Adaptive Indexing for Vector Search","summary":"  Vector search, the task of finding the k-nearest neighbors of\nhigh-dimensional vectors, underpins many machine learning applications,\nincluding recommendation systems and information retrieval. However, existing\napproximate nearest neighbor (ANN) methods perform poorly under dynamic, skewed\nworkloads where data distributions evolve. We introduce Quake, an adaptive\nindexing system that maintains low latency and high recall in such\nenvironments. Quake employs a hierarchical partitioning scheme that adjusts to\nupdates and changing access patterns, guided by a cost model that predicts\nquery latency based on partition sizes and access frequencies. Quake also\ndynamically optimizes query execution parameters to meet recall targets using a\nnovel recall estimation model. Furthermore, Quake utilizes optimized query\nprocessing, leveraging NUMA-aware parallelism for improved memory bandwidth\nutilization. To evaluate Quake, we prepare a Wikipedia vector search workload\nand develop a workload generator to create vector search workloads with\nconfigurable access patterns. Our evaluation shows that on dynamic workloads,\nQuake achieves query latency reductions of 1.5-22x and update latency\nreductions of 6-83x compared to state-of-the-art indexes SVS, DiskANN, HNSW,\nand SCANN.\n","authors":["Jason Mohoney","Devesh Sarda","Mengze Tang","Shihabur Rahman Chowdhury","Anil Pacaci","Ihab F. Ilyas","Theodoros Rekatsinas","Shivaram Venkataraman"],"pdf_url":"https://arxiv.org/pdf/2506.03437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03424v1","updated":"2025-06-03T22:10:39Z","published":"2025-06-03T22:10:39Z","title":"DistRAG: Towards Distance-Based Spatial Reasoning in LLMs","summary":"  Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs.\n","authors":["Nicole R Schneider","Nandini Ramachandran","Kent O'Sullivan","Hanan Samet"],"pdf_url":"https://arxiv.org/pdf/2506.03424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03391v1","updated":"2025-06-03T21:00:34Z","published":"2025-06-03T21:00:34Z","title":"Universal Reusability in Recommender Systems: The Case for Dataset- and\n  Task-Independent Frameworks","summary":"  Recommender systems are pivotal in delivering personalized experiences across\nindustries, yet their adoption and scalability remain hindered by the need for\nextensive dataset- and task-specific configurations. Existing systems often\nrequire significant manual intervention, domain expertise, and engineering\neffort to adapt to new datasets or tasks, creating barriers to entry and\nlimiting reusability. In contrast, recent advancements in large language models\n(LLMs) have demonstrated the transformative potential of reusable systems,\nwhere a single model can handle diverse tasks without significant\nreconfiguration. Inspired by this paradigm, we propose the Dataset- and\nTask-Independent Recommender System (DTIRS), a framework aimed at maximizing\nthe reusability of recommender systems while minimizing barriers to entry.\nUnlike LLMs, which achieve task generalization directly, DTIRS focuses on\neliminating the need to rebuild or reconfigure recommendation pipelines for\nevery new dataset or task, even though models may still need retraining on new\ndata. By leveraging the novel Dataset Description Language (DsDL), DTIRS\nenables standardized dataset descriptions and explicit task definitions,\nallowing autonomous feature engineering, model selection, and optimization.\nThis paper introduces the concept of DTIRS and establishes a roadmap for\ntransitioning from Level-1 automation (dataset-agnostic but task-specific\nsystems) to Level-2 automation (fully dataset- and task-independent systems).\nAchieving this paradigm would maximize code reusability and lower barriers to\nadoption. We discuss key challenges, including the trade-offs between\ngeneralization and specialization, computational overhead, and scalability,\nwhile presenting DsDL as a foundational tool for this vision.\n","authors":["Tri Kurniawan Wijaya","Xinyang Shao","Gonzalo Fiz Pontiveros","Edoardo D'Amico"],"pdf_url":"https://arxiv.org/pdf/2506.03391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03369v1","updated":"2025-06-03T20:26:14Z","published":"2025-06-03T20:26:14Z","title":"Impact of Rankings and Personalized Recommendations in Marketplaces","summary":"  Individuals often navigate several options with incomplete knowledge of their\nown preferences. Information provisioning tools such as public rankings and\npersonalized recommendations have become central to helping individuals make\nchoices, yet their value proposition under different marketplace environments\nremains unexplored. This paper studies a stylized model to explore the impact\nof these tools in two marketplace settings: uncapacitated supply, where items\ncan be selected by any number of agents, and capacitated supply, where each\nitem is constrained to be matched to a single agent. We model the agents\nutility as a weighted combination of a common term which depends only on the\nitem, reflecting the item's population level quality, and an idiosyncratic\nterm, which depends on the agent item pair capturing individual specific\ntastes. Public rankings reveal the common term, while personalized\nrecommendations reveal both terms. In the supply unconstrained settings, both\npublic rankings and personalized recommendations improve welfare, with their\nrelative value determined by the degree of preference heterogeneity. Public\nrankings are effective when preferences are relatively homogeneous, while\npersonalized recommendations become critical as heterogeneity increases. In\ncontrast, in supply constrained settings, revealing just the common term, as\ndone by public rankings, provides limited benefit since the total common value\navailable is limited by capacity constraints, whereas personalized\nrecommendations, by revealing both common and idiosyncratic terms,\nsignificantly enhance welfare by enabling agents to match with items they\nidiosyncratically value highly. These results illustrate the interplay between\nsupply constraints and preference heterogeneity in determining the\neffectiveness of information provisioning tools, offering insights for their\ndesign and deployment in diverse settings.\n","authors":["Omar Besbes","Yash Kanoria","Akshit Kumar"],"pdf_url":"https://arxiv.org/pdf/2506.03369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18460v2","updated":"2025-06-03T17:47:36Z","published":"2025-02-25T18:59:07Z","title":"DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers","summary":"  Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.\n","authors":["Xueguang Ma","Xi Victoria Lin","Barlas Oguz","Jimmy Lin","Wen-tau Yih","Xilun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.18460v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2506.03035v1","updated":"2025-06-03T16:18:45Z","published":"2025-06-03T16:18:45Z","title":"Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning","summary":"  Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.\n","authors":["Pierre Lepagnol","Sahar Ghannay","Thomas Gerald","Christophe Servan","Sophie Rosset"],"pdf_url":"https://arxiv.org/pdf/2506.03035v1.pdf","comment":"Conference paper accepted to INTERSPEECH 2025"},{"id":"http://arxiv.org/abs/2411.06256v4","updated":"2025-06-03T14:58:07Z","published":"2024-11-09T19:07:58Z","title":"Annotative Indexing","summary":"  This paper introduces annotative indexing, a novel framework that unifies and\ngeneralizes traditional inverted indexes, column stores, object stores, and\ngraph databases. As a result, annotative indexing can provide the underlying\nindexing framework for databases that support retrieval augmented generation,\nknowledge graphs, entity retrieval, semi-structured data, and ranked retrieval.\nWhile we primarily focus on human language data in the form of text, annotative\nindexing is sufficiently general to support a range of other datatypes, and we\nprovide examples of SQL-like queries over a JSON store that includes numbers\nand dates. Taking advantage of the flexibility of annotative indexing, we also\ndemonstrate a fully dynamic annotative index incorporating support for ACID\nproperties of transactions with hundreds of multiple concurrent readers and\nwriters.\n","authors":["Charles L. A. Clarke"],"pdf_url":"https://arxiv.org/pdf/2411.06256v4.pdf","comment":"Code at https://github.com/claclark/Cottontail"},{"id":"http://arxiv.org/abs/2506.02924v1","updated":"2025-06-03T14:25:12Z","published":"2025-06-03T14:25:12Z","title":"INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and\n  Prompt-Based Approaches to Depression Symptom Identification","summary":"  In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.\n","authors":["Diogo A. P. Nunes","Eugénio Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2506.02924v1.pdf","comment":"12 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2505.12574v4","updated":"2025-06-03T14:13:57Z","published":"2025-05-18T23:22:53Z","title":"PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.\n","authors":["Liuji Chen","Xiaofang Yang","Yuanzhuo Lu","Jinghao Zhang","Xin Sun","Qiang Liu","Shu Wu","Jing Dong","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12574v4.pdf","comment":"Project page: https://poison-arena.github.io/"},{"id":"http://arxiv.org/abs/2505.20322v2","updated":"2025-06-03T13:40:17Z","published":"2025-05-23T17:59:18Z","title":"Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms","summary":"  Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control.\n","authors":["Mengru Wang","Ziwen Xu","Shengyu Mao","Shumin Deng","Zhaopeng Tu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.20322v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2506.02872v1","updated":"2025-06-03T13:37:44Z","published":"2025-06-03T13:37:44Z","title":"Token and Span Classification for Entity Recognition in French\n  Historical Encyclopedias","summary":"  Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.\n","authors":["Ludovic Moncla","Hédi Zeghidi"],"pdf_url":"https://arxiv.org/pdf/2506.02872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23908v2","updated":"2025-06-03T13:32:45Z","published":"2025-05-29T18:02:16Z","title":"Transforming Podcast Preview Generation: From Expert Models to LLM-Based\n  Systems","summary":"  Discovering and evaluating long-form talk content such as videos and podcasts\nposes a significant challenge for users, as it requires a considerable time\ninvestment. Previews offer a practical solution by providing concise snippets\nthat showcase key moments of the content, enabling users to make more informed\nand confident choices. We propose an LLM-based approach for generating podcast\nepisode previews and deploy the solution at scale, serving hundreds of\nthousands of podcast previews in a real-world application. Comprehensive\noffline evaluations and online A/B testing demonstrate that LLM-generated\npreviews consistently outperform a strong baseline built on top of various ML\nexpert models, showcasing a significant reduction in the need for meticulous\nfeature engineering. The offline results indicate notable enhancements in\nunderstandability, contextual clarity, and interest level, and the online A/B\ntest shows a 4.6% increase in user engagement with preview content, along with\na 5x boost in processing efficiency, offering a more streamlined and performant\nsolution compared to the strong baseline of feature-engineered expert models.\n","authors":["Winstead Zhu","Ann Clifton","Azin Ghazimatin","Edgar Tanaka","Edward Ronan"],"pdf_url":"https://arxiv.org/pdf/2505.23908v2.pdf","comment":"9 pages, 2 figures, accepted at ACL 2025 Industry Track"},{"id":"http://arxiv.org/abs/2506.02839v1","updated":"2025-06-03T13:08:17Z","published":"2025-06-03T13:08:17Z","title":"DeepShop: A Benchmark for Deep Research Shopping Agents","summary":"  Web agents for online shopping have shown great promise in automating user\ninteractions across e-commerce platforms. Benchmarks for assessing such agents\ndo not reflect the complexity of real-world shopping scenarios, as they often\nconsist of overly simple queries with deterministic paths, such as \"Find iPhone\n15.\" Real shopping scenarios are inherently more layered, involving\nmulti-dimensional product attributes, search filters, and user-specific sorting\npreferences. To address this gap, we introduce DeepShop, a benchmark designed\nto evaluate web agents in complex and realistic online shopping environments.\nDeepShop comprises three key components. (1) Query diversity evolution:\nStarting from real user queries, we generate diverse queries across five\npopular online shopping domains. (2) Query complexity evolution: We further\nevolve these queries to increase complexity, considering product attributes,\nsearch filters, and sorting preferences, and classify them into three levels:\neasy, medium, and hard, based on the number of evolutions. (3) Fine-grained and\nholistic evaluation: We propose an automated evaluation framework that assesses\nagent performance in terms of fine-grained aspects (product attributes, search\nfilters, and sorting preferences) and reports the overall success rate through\nholistic evaluation. We conduct a systematic evaluation of retrieval-augmented\ngeneration (RAG) methods, web agents, and deep research systems. Results show\nthat RAG struggles with complex queries due to its lack of web interaction,\nwhile other methods face significant challenges with filters and sorting\npreferences, leading to low overall success rates. We also perform\ncross-category, complexity-based evaluations and error analyses to support the\nadvancement of deep research shopping agents.\n","authors":["Yougang Lyu","Xiaoyu Zhang","Lingyong Yan","Maarten de Rijke","Zhaochun Ren","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2506.02839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02834v1","updated":"2025-06-03T13:04:00Z","published":"2025-06-03T13:04:00Z","title":"Combining social relations and interaction data in Recommender System\n  with Graph Convolution Collaborative Filtering","summary":"  A recommender system is an important subject in the field of data mining,\nwhere the item rating information from users is exploited and processed to make\nsuitable recommendations with all other users. The recommender system creates\nconvenience for e-commerce users and stimulates the consumption of items that\nare suitable for users. In addition to e-commerce, a recommender system is also\nused to provide recommendations on books to read, movies to watch, courses to\ntake or websites to visit. Similarity between users is an important impact for\nrecommendation, which could be calculated from the data of past user ratings of\nthe item by methods of collaborative filtering, matrix factorization or\nsingular vector decomposition. In the development of graph data mining\ntechniques, the relationships between users and items can be represented by\nmatrices from which collaborative filtering could be done with the larger\ndatabase, more accurate and faster in calculation. All these data can be\nrepresented graphically and mined by today's highly developed graph neural\nnetwork models. On the other hand, users' social friendship data also influence\nconsumption habits because recommendations from friends will be considered more\ncarefully than information sources. However, combining a user's friend\ninfluence and the similarity between users whose similar shopping habits is\nchallenging. Because the information is noisy and it affects each particular\ndata set in different ways. In this study, we present the input data processing\nmethod to remove outliers which are single reviews or users with little\ninteraction with the items; the next proposed model will combine the social\nrelationship data and the similarity in the rating history of users to improve\nthe accuracy and recall of the recommender system.\n","authors":["Tin T. Tran","Vaclav Snasel","Loc Tan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2506.02834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02784v1","updated":"2025-06-03T12:11:34Z","published":"2025-06-03T12:11:34Z","title":"UTCS: Effective Unsupervised Temporal Community Search with Pre-training\n  of Temporal Dynamics and Subgraph Knowledge","summary":"  In many real-world applications, the evolving relationships between entities\ncan be modeled as temporal graphs, where each edge has a timestamp representing\nthe interaction time.\n  As a fundamental problem in graph analysis, {\\it community search (CS)} in\ntemporal graphs has received growing attention but exhibits two major\nlimitations: (1) Traditional methods typically require predefined subgraph\nstructures, which are not always known in advance. (2) Learning-based methods\nstruggle to capture temporal interaction information. To fill this research\ngap, in this paper, we propose an effective \\textbf{U}nsupervised\n\\textbf{T}emporal \\textbf{C}ommunity \\textbf{S}earch with pre-training of\ntemporal dynamics and subgraph knowledge model (\\textbf{\\model}).\n\\model~contains two key stages: offline pre-training and online search. In the\nfirst stage, we introduce multiple learning objectives to facilitate the\npre-training process in the unsupervised learning setting. In the second stage,\nwe identify a candidate subgraph and compute community scores using the\npre-trained node representations and a novel scoring mechanism to determine the\nfinal community members. Experiments on five real-world datasets demonstrate\nthe effectiveness.\n","authors":["Yue Zhang","Yankai Chen","Yingli Zhou","Yucan Guo","Xiaolin Han","Chenhao Ma"],"pdf_url":"https://arxiv.org/pdf/2506.02784v1.pdf","comment":"Accepted by SIGIR'25 short paper track"},{"id":"http://arxiv.org/abs/2506.02750v1","updated":"2025-06-03T11:11:43Z","published":"2025-06-03T11:11:43Z","title":"Learning Binarized Representations with Pseudo-positive Sample\n  Enhancement for Efficient Graph Collaborative Filtering","summary":"  Learning vectorized embeddings is fundamental to many recommender systems for\nuser-item matching. To enable efficient online inference, representation\nbinarization, which embeds latent features into compact binary sequences, has\nrecently shown significant promise in optimizing both memory usage and\ncomputational overhead. However, existing approaches primarily focus on\nnumerical quantization, neglecting the associated information loss, which often\nresults in noticeable performance degradation. To address these issues, we\nstudy the problem of graph representation binarization for efficient\ncollaborative filtering. Our findings indicate that explicitly mitigating\ninformation loss at various stages of embedding binarization has a significant\npositive impact on performance. Building on these insights, we propose an\nenhanced framework, BiGeaR++, which specifically leverages supervisory signals\nfrom pseudo-positive samples, incorporating both real item data and latent\nembedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a\nfine-grained inference distillation mechanism and an effective embedding sample\nsynthesis approach. Empirical evaluations across five real-world datasets\ndemonstrate that the new designs in BiGeaR++ work seamlessly well with other\nmodules, delivering substantial improvements of around 1%-10% over BiGeaR and\nthus achieving state-of-the-art performance compared to the competing methods.\nOur implementation is available at https://github.com/QueYork/BiGeaR-SS.\n","authors":["Yankai Chen","Yue Que","Xinni Zhang","Chen Ma","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2506.02750v1.pdf","comment":"Accepted by TOIS"},{"id":"http://arxiv.org/abs/2505.11582v2","updated":"2025-06-03T09:18:51Z","published":"2025-05-16T17:06:35Z","title":"Comparing Lexical and Semantic Vector Search Methods When Classifying\n  Medical Documents","summary":"  Classification is a common AI problem, and vector search is a typical\nsolution. This transforms a given body of text into a numerical representation,\nknown as an embedding, and modern improvements to vector search focus on\noptimising speed and predictive accuracy. This is often achieved through neural\nmethods that aim to learn language semantics. However, our results suggest that\nthese are not always the best solution. Our task was to classify\nrigidly-structured medical documents according to their content, and we found\nthat using off-the-shelf semantic vector search produced slightly worse\npredictive accuracy than creating a bespoke lexical vector search model, and\nthat it required significantly more time to execute. These findings suggest\nthat traditional methods deserve to be contenders in the information retrieval\ntoolkit, despite the prevalence and success of neural models.\n","authors":["Lee Harris"],"pdf_url":"https://arxiv.org/pdf/2505.11582v2.pdf","comment":"This project was funded by a UKRI grant, number: 10048265"},{"id":"http://arxiv.org/abs/2501.03835v4","updated":"2025-06-03T09:02:22Z","published":"2025-01-07T14:45:30Z","title":"TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification","summary":"  Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR.\n","authors":["Yindu Su","Huike Zou","Lin Sun","Ting Zhang","Haiyang Yang","Liyu Chen","David Lo","Qingheng Zhang","Shuguang Han","Jufeng Chen"],"pdf_url":"https://arxiv.org/pdf/2501.03835v4.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2502.09304v2","updated":"2025-06-03T08:45:42Z","published":"2025-02-13T13:16:16Z","title":"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG","summary":"  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate 13\nsolutions on three real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%.\n","authors":["Yiqian Huang","Shiqi Zhang","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.09304v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00622v2","updated":"2025-06-03T08:31:06Z","published":"2025-05-31T16:20:14Z","title":"Improving Dialogue State Tracking through Combinatorial Search for\n  In-Context Examples","summary":"  In dialogue state tracking (DST), in-context learning comprises a retriever\nthat selects labeled dialogues as in-context examples and a DST model that uses\nthese examples to infer the dialogue state of the query dialogue. Existing\nmethods for constructing training data for retrievers suffer from three key\nlimitations: (1) the synergistic effect of examples is not considered, (2) the\nlinguistic characteristics of the query are not sufficiently factored in, and\n(3) scoring is not directly optimized for DST performance. Consequently, the\nretriever can fail to retrieve examples that would substantially improve DST\nperformance. To address these issues, we present CombiSearch, a method that\nscores effective in-context examples based on their combinatorial impact on DST\nperformance. Our evaluation on MultiWOZ shows that retrievers trained with\nCombiSearch surpass state-of-the-art models, achieving a 20x gain in data\nefficiency and generalizing well to the SGD dataset. Moreover, CombiSearch\nattains a 12% absolute improvement in the upper bound DST performance over\ntraditional approaches when no retrieval errors are assumed. This significantly\nincreases the headroom for practical DST performance while demonstrating that\nexisting methods rely on suboptimal data for retriever training.\n","authors":["Haesung Pyun","Yoonah Park","Yohan Jo"],"pdf_url":"https://arxiv.org/pdf/2506.00622v2.pdf","comment":"This paper has been accepted for publication at ACL 2025"},{"id":"http://arxiv.org/abs/2506.02589v1","updated":"2025-06-03T08:11:16Z","published":"2025-06-03T08:11:16Z","title":"Evaluating Named Entity Recognition Models for Russian Cultural News\n  Texts: From BERT to LLM","summary":"  This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.\n","authors":["Maria Levchenko"],"pdf_url":"https://arxiv.org/pdf/2506.02589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23809v2","updated":"2025-06-03T07:39:21Z","published":"2025-05-27T08:40:11Z","title":"LLM-Driven E-Commerce Marketing Content Optimization: Balancing\n  Creativity and Conversion","summary":"  As e-commerce competition intensifies, balancing creative content with\nconversion effectiveness becomes critical. Leveraging LLMs' language generation\ncapabilities, we propose a framework that integrates prompt engineering,\nmulti-objective fine-tuning, and post-processing to generate marketing copy\nthat is both engaging and conversion-driven. Our fine-tuning method combines\nsentiment adjustment, diversity enhancement, and CTA embedding. Through offline\nevaluations and online A/B tests across categories, our approach achieves a\n12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content\nnovelty. This provides a practical solution for automated copy generation and\nsuggests paths for future multimodal, real-time personalization.\n","authors":["Haowei Yang","Haotian Lyu","Tianle Zhang","Dingzhou Wang","Yushang Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.23809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01705v2","updated":"2025-06-03T07:35:59Z","published":"2025-06-02T14:11:21Z","title":"SPOT-Trip: Dual-Preference Driven Out-of-Town Trip Recommendation","summary":"  Out-of-town trip recommendation aims to generate a sequence of Points of\nInterest (POIs) for users traveling from their hometowns to previously\nunvisited regions based on personalized itineraries, e.g., origin, destination,\nand trip duration. Modeling the complex user preferences--which often exhibit a\ntwo-fold nature of static and dynamic interests--is critical for effective\nrecommendations. However, the sparsity of out-of-town check-in data presents\nsignificant challenges in capturing such user preferences. Meanwhile, existing\nmethods often conflate the static and dynamic preferences, resulting in\nsuboptimal performance. In this paper, we for the first time systematically\nstudy the problem of out-of-town trip recommendation. A novel framework\nSPOT-Trip is proposed to explicitly learns the dual static-dynamic user\npreferences. Specifically, to handle scarce data, we construct a POI attribute\nknowledge graph to enrich the semantic modeling of users' hometown and\nout-of-town check-ins, enabling the static preference modeling through\nattribute relation-aware aggregation. Then, we employ neural ordinary\ndifferential equations (ODEs) to capture the continuous evolution of latent\ndynamic user preferences and innovatively combine a temporal point process to\ndescribe the instantaneous probability of each preference behavior. Further, a\nstatic-dynamic fusion module is proposed to merge the learned static and\ndynamic user preferences. Extensive experiments on real data offer insight into\nthe effectiveness of the proposed solutions, showing that SPOT-Trip achieves\nperformance improvement by up to 17.01%.\n","authors":["Yinghui Liu","Hao Miao","Guojiang Shen","Yan Zhao","Xiangjie Kong","Ivan Lee"],"pdf_url":"https://arxiv.org/pdf/2506.01705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02527v1","updated":"2025-06-03T07:05:49Z","published":"2025-06-03T07:05:49Z","title":"Multilingual Information Retrieval with a Monolingual Knowledge Base","summary":"  Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.\n","authors":["Yingying Zhuang","Aman Gupta","Anurag Beniwal"],"pdf_url":"https://arxiv.org/pdf/2506.02527v1.pdf","comment":"6 pages, accepted at GENNEXT@SIGIR25"},{"id":"http://arxiv.org/abs/2502.18017v2","updated":"2025-06-03T05:34:30Z","published":"2025-02-25T09:26:12Z","title":"ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\n  Iterative Reasoning Agents","summary":"  Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark. The code is available at\nhttps://github.com/Alibaba-NLP/ViDoRAG.\n","authors":["Qiuchen Wang","Ruixue Ding","Zehui Chen","Weiqi Wu","Shihang Wang","Pengjun Xie","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09891v2","updated":"2025-06-03T03:38:31Z","published":"2025-02-14T03:28:36Z","title":"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has proven effective in integrating\nexternal knowledge into large language models (LLMs) for solving\nquestion-answer (QA) tasks. The state-of-the-art RAG approaches often use the\ngraph data as the external data since they capture the rich semantic\ninformation and link relationships between entities. However, existing\ngraph-based RAG approaches cannot accurately identify the relevant information\nfrom the graph and also consume large numbers of tokens in the online retrieval\nprocess. To address these issues, we introduce a novel graph-based RAG\napproach, called Attributed Community-based Hierarchical RAG (ArchRAG), by\naugmenting the question using attributed communities, and also introducing a\nnovel LLM-based hierarchical clustering method. To retrieve the most relevant\ninformation from the graph for the question, we build a novel hierarchical\nindex structure for the attributed communities and develop an effective online\nretrieval method. Experimental results demonstrate that ArchRAG outperforms\nexisting methods in both accuracy and token cost. Moreover, ArchRAG has been\nsuccessfully applied to domain knowledge QA in Huawei Cloud Computing.\n","authors":["Shu Wang","Yixiang Fang","Yingli Zhou","Xilin Liu","Yuchi Ma"],"pdf_url":"https://arxiv.org/pdf/2502.09891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05031v2","updated":"2025-06-03T02:55:09Z","published":"2025-05-08T08:06:34Z","title":"LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving\n  Cloud-Device Collaboration","summary":"  Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Applied-Machine-Learning-Lab/LSRP.\n","authors":["Yingyi Zhang","Pengyue Jia","Xianneng Li","Derong Xu","Maolin Wang","Yichao Wang","Zhaocheng Du","Huifeng Guo","Yong Liu","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.05031v2.pdf","comment":"Accepted at KDD'25"},{"id":"http://arxiv.org/abs/2505.16133v4","updated":"2025-06-03T02:49:46Z","published":"2025-05-22T02:22:11Z","title":"HASH-RAG: Bridging Deep Hashing with Retriever for Efficient, Fine\n  Retrieval and Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) encounters efficiency challenges when\nscaling to massive knowledge bases while preserving contextual relevance. We\npropose Hash-RAG, a framework that integrates deep hashing techniques with\nsystematic optimizations to address these limitations. Our queries directly\nlearn binary hash codes from knowledgebase code, eliminating intermediate\nfeature extraction steps, and significantly reducing storage and computational\noverhead. Building upon this hash-based efficient retrieval framework, we\nestablish the foundation for fine-grained chunking. Consequently, we design a\nPrompt-Guided Chunk-to-Context (PGCC) module that leverages retrieved\nhash-indexed propositions and their original document segments through prompt\nengineering to enhance the LLM's contextual awareness. Experimental evaluations\non NQ, TriviaQA, and HotpotQA datasets demonstrate that our approach achieves a\n90% reduction in retrieval time compared to conventional methods while\nmaintaining considerate recall performance. Additionally, The proposed system\noutperforms retrieval/non-retrieval baselines by 1.4-4.3% in EM scores.\n","authors":["Jinyu Guo","Xunlei Chen","Qiyang Xia","Zhaokun Wang","Jie Ou","Libo Qin","Shunyu Yao","Wenhong Tian"],"pdf_url":"https://arxiv.org/pdf/2505.16133v4.pdf","comment":"Accepted at Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2506.02368v1","updated":"2025-06-03T02:08:55Z","published":"2025-06-03T02:08:55Z","title":"NextQuill: Causal Preference Modeling for Enhancing LLM Personalization","summary":"  Personalizing large language models (LLMs) for individual users has become\nincreasingly important as they are progressively integrated into real-world\napplications to support users' daily lives. However, existing personalization\napproaches often fail to distinguish which components of model predictions and\ntraining data truly reflect user preferences, leading to superficial\npersonalization alignment. In this paper, we introduce NextQuill, a novel LLM\npersonalization alignment framework grounded in causal preference modeling. We\napproach personalization from a causal perspective, treating both model\npredictions and ground-truth data generation as outcomes influenced by user\npreferences, along with other factors. We define the true preference effect as\nthe causal impact of user history (which reflects preferences) on each token\nprediction or data generation instance, estimated through causal intervention\ntechniques. Building on this insight, NextQuill introduces two complementary\nalignment strategies: (1) aligning model-internal causal preference effects on\npredictions with those reflected in ground-truth data, rather than\nindiscriminately fitting predictions, and (2) focusing on fitting\npreference-bearing tokens identified via ground-truth data preference effects,\nrather than treating all tokens uniformly. By integrating these strategies,\nNextQuill shifts the alignment process toward learning from causal preference\neffects, facilitating more effective and personalized adaptation. Experiments\nacross multiple personalization benchmarks demonstrate that NextQuill\nsignificantly improves personalization quality, offering a principled, causal\nfoundation for LLM personalization. Our codes are available on\nhttps://github.com/juntaoyou/NextQuill.\n","authors":["Xiaoyan Zhao","Juntao You","Yang Zhang","Wenjie Wang","Hong Cheng","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.02368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00261v2","updated":"2025-06-03T02:07:40Z","published":"2025-05-30T21:50:29Z","title":"GPR: Empowering Generation with Graph-Pretrained Retriever","summary":"  Graph retrieval-augmented generation (GRAG) places high demands on\ngraph-specific retrievers. However, existing retrievers often rely on language\nmodels pretrained on plain text, limiting their effectiveness due to domain\nmisalignment and structure ignorance. To address these challenges, we propose\nGPR, a graph-based retriever pretrained directly on knowledge graphs. GPR\naligns natural language questions with relevant subgraphs through LLM-guided\ngraph augmentation and employs a structure-aware objective to learn\nfine-grained retrieval strategies. Experiments on two datasets, three LLM\nbackbones, and five baselines show that GPR consistently improves both\nretrieval quality and downstream generation, demonstrating its effectiveness as\na robust retrieval solution for GRAG.\n","authors":["Xiaochen Wang","Zongyu Wu","Yuan Zhong","Xiang Zhang","Suhang Wang","Fenglong Ma"],"pdf_url":"https://arxiv.org/pdf/2506.00261v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.03378v1","updated":"2025-06-03T20:37:23Z","published":"2025-06-03T20:37:23Z","title":"SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through\n  Audio-Visual Alignment with Cascaded Cross-Transformer","summary":"  As video-sharing platforms have grown over the past decade, child viewership\nhas surged, increasing the need for precise detection of harmful content like\nviolence or explicit scenes. Malicious users exploit moderation systems by\nembedding unsafe content in minimal frames to evade detection. While prior\nresearch has focused on visual cues and advanced such fine-grained detection,\naudio features remain underexplored. In this study, we embed audio cues with\nvisual for fine-grained child harmful content detection and introduce SNIFR, a\nnovel framework for effective alignment. SNIFR employs a transformer encoder\nfor intra-modality interaction, followed by a cascaded cross-transformer for\ninter-modality alignment. Our approach achieves superior performance over\nunimodal and baseline fusion methods, setting a new state-of-the-art.\n","authors":["Orchid Chetia Phukan","Mohd Mujtaba Akhtar"," Girish","Swarup Ranjan Behera","Abu Osama Siddiqui","Sarthak Jain","Priyabrata Mallick","Jaya Sai Kiran Patibandla","Pailla Balakrishna Reddy","Arun Balaji Buduru","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2506.03378v1.pdf","comment":"Accepted to INTERSPEECH 2025"},{"id":"http://arxiv.org/abs/2506.03364v1","updated":"2025-06-03T20:16:41Z","published":"2025-06-03T20:16:41Z","title":"Towards Source Attribution of Singing Voice Deepfake with Multimodal\n  Foundation Models","summary":"  In this work, we introduce the task of singing voice deepfake source\nattribution (SVDSA). We hypothesize that multimodal foundation models (MMFMs)\nsuch as ImageBind, LanguageBind will be most effective for SVDSA as they are\nbetter equipped for capturing subtle source-specific characteristics-such as\nunique timbre, pitch manipulation, or synthesis artifacts of each singing voice\ndeepfake source due to their cross-modality pre-training. Our experiments with\nMMFMs, speech foundation models and music foundation models verify the\nhypothesis that MMFMs are the most effective for SVDSA. Furthermore, inspired\nfrom related research, we also explore fusion of foundation models (FMs) for\nimproved SVDSA. To this end, we propose a novel framework, COFFE which employs\nChernoff Distance as novel loss function for effective fusion of FMs. Through\nCOFFE with the symphony of MMFMs, we attain the topmost performance in\ncomparison to all the individual FMs and baseline fusion methods.\n","authors":["Orchid Chetia Phukan"," Girish","Mohd Mujtaba Akhtar","Swarup Ranjan Behera","Priyabrata Mallick","Pailla Balakrishna Reddy","Arun Balaji Buduru","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2506.03364v1.pdf","comment":"Accepted to INTERSPEECH 2025"},{"id":"http://arxiv.org/abs/2506.03150v1","updated":"2025-06-03T17:59:52Z","published":"2025-06-03T17:59:52Z","title":"IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation","summary":"  Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page\n","authors":["Yuanze Lin","Yi-Wen Chen","Yi-Hsuan Tsai","Ronald Clark","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2506.03150v1.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2506.03144v1","updated":"2025-06-03T17:59:14Z","published":"2025-06-03T17:59:14Z","title":"MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query","summary":"  Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.\n","authors":["Wei Chow","Yuan Gao","Linfeng Li","Xian Wang","Qi Xu","Hang Song","Lingdong Kong","Ran Zhou","Yi Zeng","Yidong Cai","Botian Jiang","Shilin Xu","Jiajun Zhang","Minghui Qiu","Xiangtai Li","Tianshu Yang","Siliang Tang","Juncheng Li"],"pdf_url":"https://arxiv.org/pdf/2506.03144v1.pdf","comment":"Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/"},{"id":"http://arxiv.org/abs/2410.03869v2","updated":"2025-06-03T17:32:00Z","published":"2024-10-04T19:04:43Z","title":"Chain-of-Jailbreak Attack for Image Generation Models via Editing Step\n  by Step","summary":"  Text-based image generation models, such as Stable Diffusion and DALL-E 3,\nhold significant potential in content creation and publishing workflows, making\nthem the focus in recent years. Despite their remarkable capability to generate\ndiverse and vivid images, considerable efforts are being made to prevent the\ngeneration of harmful content, such as abusive, violent, or pornographic\nmaterial. To assess the safety of existing models, we introduce a novel\njailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises\nimage generation models through a step-by-step editing process. Specifically,\nfor malicious queries that cannot bypass the safeguards with a single prompt,\nwe intentionally decompose the query into multiple sub-queries. The image\ngeneration models are then prompted to generate and iteratively edit images\nbased on these sub-queries. To evaluate the effectiveness of our CoJ attack\nmethod, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine\nsafety scenarios, three types of editing operations, and three editing\nelements. Experiments on four widely-used image generation services provided by\nGPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack\nmethod can successfully bypass the safeguards of models for over 60% cases,\nwhich significantly outperforms other jailbreaking methods (i.e., 14%).\nFurther, to enhance these models' safety against our CoJ attack method, we also\npropose an effective prompting-based method, Think Twice Prompting, that can\nsuccessfully defend over 95% of CoJ attack. We release our dataset and code to\nfacilitate the AI safety research.\n","authors":["Wenxuan Wang","Kuiyi Gao","Youliang Yuan","Jen-tse Huang","Qiuzhi Liu","Shuai Wang","Wenxiang Jiao","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2410.03869v2.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.11184v2","updated":"2025-06-03T17:27:16Z","published":"2025-02-16T16:12:40Z","title":"Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs","summary":"  Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.\n","authors":["Wenxuan Wang","Xiaoyuan Liu","Kuiyi Gao","Jen-tse Huang","Youliang Yuan","Pinjia He","Shuai Wang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2502.11184v2.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2506.02997v1","updated":"2025-06-03T15:31:16Z","published":"2025-06-03T15:31:16Z","title":"Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich\n  Representation","summary":"  Controllable TTS models with natural language prompts often lack the ability\nfor fine-grained control and face a scarcity of high-quality data. We propose a\ntwo-stage style-controllable TTS system with language models, utilizing a\nquantized masked-autoencoded style-rich representation as an intermediary. In\nthe first stage, an autoregressive transformer is used for the conditional\ngeneration of these style-rich tokens from text and control signals. The second\nstage generates codec tokens from both text and sampled style-rich tokens.\nExperiments show that training the first-stage model on extensive datasets\nenhances the content robustness of the two-stage model as well as control\ncapabilities over multiple attributes. By selectively combining discrete labels\nand speaker embeddings, we explore fully controlling the speaker's timbre and\nother stylistic information, and adjusting attributes like emotion for a\nspecified speaker. Audio samples are available at\nhttps://style-ar-tts.github.io.\n","authors":["Yongqi Wang","Chunlei Zhang","Hangting Chen","Zhou Zhao","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.02997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04523v2","updated":"2025-06-03T11:54:01Z","published":"2024-03-07T14:25:03Z","title":"T-TAME: Trainable Attention Mechanism for Explaining Convolutional\n  Networks and Vision Transformers","summary":"  The development and adoption of Vision Transformers and other deep-learning\narchitectures for image classification tasks has been rapid. However, the\n\"black box\" nature of neural networks is a barrier to adoption in applications\nwhere explainability is essential. While some techniques for generating\nexplanations have been proposed, primarily for Convolutional Neural Networks,\nadapting such techniques to the new paradigm of Vision Transformers is\nnon-trivial. This paper presents T-TAME, Transformer-compatible Trainable\nAttention Mechanism for Explanations, a general methodology for explaining deep\nneural networks used in image classification tasks. The proposed architecture\nand training technique can be easily applied to any convolutional or Vision\nTransformer-like neural network, using a streamlined training approach. After\ntraining, explanation maps can be computed in a single forward pass; these\nexplanation maps are comparable to or outperform the outputs of computationally\nexpensive perturbation-based explainability techniques, achieving SOTA\nperformance. We apply T-TAME to three popular deep learning classifier\narchitectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet\ndataset, and we demonstrate improvements over existing state-of-the-art\nexplainability methods. A detailed analysis of the results and an ablation\nstudy provide insights into how the T-TAME design choices affect the quality of\nthe generated explanation maps.\n","authors":["Mariano V. Ntrougkas","Nikolaos Gkalelis","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2403.04523v2.pdf","comment":"Accepted"},{"id":"http://arxiv.org/abs/2506.02574v1","updated":"2025-06-03T07:55:16Z","published":"2025-06-03T07:55:16Z","title":"Dynamic mapping from static labels: remote sensing dynamic sample\n  generation with temporal-spectral embedding","summary":"  Accurate remote sensing geographic mapping depends heavily on representative\nand timely sample data. However, rapid changes in land surface dynamics\nnecessitate frequent updates, quickly rendering previously collected samples\nobsolete and imposing significant labor demands for continuous manual updates.\nIn this study, we aim to address this problem by dynamic sample generation\nusing existing single-date static labeled samples. We introduce TasGen, a\ntwo-stage automated framework to automatically generate dynamic samples,\ndesigned to simultaneously model spectral and temporal dependencies in\ntime-series remote sensing imagery via temporal-spectral embedding, capturing\nland surface changes without additional manual annotations.\n","authors":["Shuai Yuan","Shuang Chen","Tianwu Lin","Jie Wang","Peng Gong"],"pdf_url":"https://arxiv.org/pdf/2506.02574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20833v2","updated":"2025-06-03T07:52:18Z","published":"2024-12-30T09:58:27Z","title":"Inclusion 2024 Global Multimedia Deepfake Detection Challenge: Towards\n  Multi-dimensional Face Forgery Detection","summary":"  In this paper, we present the Global Multimedia Deepfake Detection held\nconcurrently with the Inclusion 2024. Our Multimedia Deepfake Detection aims to\ndetect automatic image and audio-video manipulations including but not limited\nto editing, synthesis, generation, Photoshop,etc. Our challenge has attracted\n1500 teams from all over the world, with about 5000 valid result submission\ncounts. We invite the top 20 teams to present their solutions to the challenge,\nfrom which the top 3 teams are awarded prizes in the grand finale. In this\npaper, we present the solutions from the top 3 teams of the two tracks, to\nboost the research work in the field of image and audio-video forgery\ndetection. The methodologies developed through the challenge will contribute to\nthe development of next-generation deepfake detection systems and we encourage\nparticipants to open source their methods.\n","authors":["Yi Zhang","Weize Gao","Changtao Miao","Man Luo","Jianshu Li","Wenzhong Deng","Zhe Li","Bingyu Hu","Weibin Yao","Yunfeng Diao","Wenbo Zhou","Tao Gong","Qi Chu"],"pdf_url":"https://arxiv.org/pdf/2412.20833v2.pdf","comment":"Inclusion 2024 Global Multimedia Deepfake Detection Competition Top\n  Team Technical Report"},{"id":"http://arxiv.org/abs/2506.02414v1","updated":"2025-06-03T04:00:53Z","published":"2025-06-03T04:00:53Z","title":"StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech\n  Generation in Voice Conversion","summary":"  Voice Conversion (VC) modifies speech to match a target speaker while\npreserving linguistic content. Traditional methods usually extract speaker\ninformation directly from speech while neglecting the explicit utilization of\nlinguistic content. Since VC fundamentally involves disentangling speaker\nidentity from linguistic content, leveraging structured semantic features could\nenhance conversion performance. However, previous attempts to incorporate\nsemantic features into VC have shown limited effectiveness, motivating the\nintegration of explicit text modeling. We propose StarVC, a unified\nautoregressive VC framework that first predicts text tokens before synthesizing\nacoustic features. The experiments demonstrate that StarVC outperforms\nconventional VC methods in preserving both linguistic content (i.e., WER and\nCER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found\nat: https://thuhcsi.github.io/StarVC/.\n","authors":["Fengjin Li","Jie Wang","Yadong Niu","Yongqing Wang","Meng Meng","Jian Luan","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2506.02414v1.pdf","comment":"5 pages, 2 figures, Accepted by Interspeech 2025, Demo:\n  https://thuhcsi.github.io/StarVC/"},{"id":"http://arxiv.org/abs/2506.02401v1","updated":"2025-06-03T03:40:39Z","published":"2025-06-03T03:40:39Z","title":"Trusted Fake Audio Detection Based on Dirichlet Distribution","summary":"  With the continuous development of deep learning-based speech conversion and\nspeech synthesis technologies, the cybersecurity problem posed by fake audio\nhas become increasingly serious. Previously proposed models for defending\nagainst fake audio have attained remarkable performance. However, they all fall\nshort in modeling the trustworthiness of the decisions made by the models\nthemselves. Based on this, we put forward a plausible fake audio detection\napproach based on the Dirichlet distribution with the aim of enhancing the\nreliability of fake audio detection. Specifically, we first generate evidence\nthrough a neural network. Uncertainty is then modeled using the Dirichlet\ndistribution. By modeling the belief distribution with the parameters of the\nDirichlet distribution, an estimate of uncertainty can be obtained for each\ndecision. Finally, the predicted probabilities and corresponding uncertainty\nestimates are combined to form the final opinion. On the ASVspoof series\ndataset (i.e., ASVspoof 2019 LA, ASVspoof 2021 LA, and DF), we conduct a number\nof comparison experiments to verify the excellent performance of the proposed\nmodel in terms of accuracy, robustness, and trustworthiness.\n","authors":["Chi Ding","Junxiao Xue","Cong Wang","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.02401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02380v1","updated":"2025-06-03T02:32:35Z","published":"2025-06-03T02:32:35Z","title":"EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for\n  Real-World 3DGS Scenes in VR","summary":"  3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.\n","authors":["Zihao Ding","Cheng-Tse Lee","Mufeng Zhu","Tao Guan","Yuan-Chun Sun","Cheng-Hsin Hsu","Yao Liu"],"pdf_url":"https://arxiv.org/pdf/2506.02380v1.pdf","comment":null}]},"2025-06-02T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.16311v2","updated":"2025-06-02T22:52:12Z","published":"2024-12-20T19:49:12Z","title":"HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational\n  Knowledge Bases","summary":"  Given a semi-structured knowledge base (SKB), where text documents are\ninterconnected by relations, how can we effectively retrieve relevant\ninformation to answer user questions? Retrieval-Augmented Generation (RAG)\nretrieves documents to assist large language models (LLMs) in question\nanswering; while Graph RAG (GRAG) uses structured knowledge bases as its\nknowledge source. However, many questions require both textual and relational\ninformation from SKB - referred to as \"hybrid\" questions - which complicates\nthe retrieval process and underscores the need for a hybrid retrieval method\nthat leverages both information. In this paper, through our empirical analysis,\nwe identify key insights that show why existing methods may struggle with\nhybrid question answering (HQA) over SKB. Based on these insights, we propose\nHybGRAG for HQA consisting of a retriever bank and a critic module, with the\nfollowing advantages: (1) Agentic, it automatically refines the output by\nincorporating feedback from the critic module, (2) Adaptive, it solves hybrid\nquestions requiring both textual and relational information with the retriever\nbank, (3) Interpretable, it justifies decision making with intuitive refinement\npath, and (4) Effective, it surpasses all baselines on HQA benchmarks. In\nexperiments on the STaRK benchmark, HybGRAG achieves significant performance\ngains, with an average relative improvement in Hit@1 of 51%.\n","authors":["Meng-Chieh Lee","Qi Zhu","Costas Mavromatis","Zhen Han","Soji Adeshina","Vassilis N. Ioannidis","Huzefa Rangwala","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2412.16311v2.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2503.05037v2","updated":"2025-06-02T22:19:43Z","published":"2025-03-06T23:23:13Z","title":"Collapse of Dense Retrievers: Short, Early, and Literal Biases\n  Outranking Factual Evidence","summary":"  Dense retrieval models are commonly used in Information Retrieval (IR)\napplications, such as Retrieval-Augmented Generation (RAG). Since they often\nserve as the first step in these systems, their robustness is critical to avoid\ndownstream failures. In this work, we repurpose a relation extraction dataset\n(e.g., Re-DocRED) to design controlled experiments that quantify the impact of\nheuristic biases, such as a preference for shorter documents, on retrievers\nlike Dragon+ and Contriever. We uncover major vulnerabilities, showing\nretrievers favor shorter documents, early positions, repeated entities, and\nliteral matches, all while ignoring the answer's presence! Notably, when\nmultiple biases combine, models exhibit catastrophic performance degradation,\nselecting the answer-containing document in less than 10% of cases over a\nsynthetic biased document without the answer. Furthermore, we show that these\nbiases have direct consequences for downstream applications like RAG, where\nretrieval-preferred documents can mislead LLMs, resulting in a 34% performance\ndrop than providing no documents at all.\nhttps://huggingface.co/datasets/mohsenfayyaz/ColDeR\n","authors":["Mohsen Fayyaz","Ali Modarressi","Hinrich Schuetze","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2503.05037v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.02291v1","updated":"2025-06-02T22:04:06Z","published":"2025-06-02T22:04:06Z","title":"Entity Image and Mixed-Modal Image Retrieval Datasets","summary":"  Despite advances in multimodal learning, challenging benchmarks for\nmixed-modal image retrieval that combines visual and textual information are\nlacking. This paper introduces a novel benchmark to rigorously evaluate image\nretrieval that demands deep cross-modal contextual understanding. We present\ntwo new datasets: the Entity Image Dataset (EI), providing canonical images for\nWikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived\nfrom the WIT dataset. The MMIR benchmark features two challenging query types\nrequiring models to ground textual descriptions in the context of provided\nvisual entities: single entity-image queries (one entity image with descriptive\ntext) and multi-entity-image queries (multiple entity images with relational\ntext). We empirically validate the benchmark's utility as both a training\ncorpus and an evaluation set for mixed-modal retrieval. The quality of both\ndatasets is further affirmed through crowd-sourced human annotations. The\ndatasets are accessible through the GitHub page:\nhttps://github.com/google-research-datasets/wit-retrieval.\n","authors":["Cristian-Ioan Blaga","Paul Suganthan","Sahil Dua","Krishna Srinivasan","Enrique Alfonseca","Peter Dornbach","Tom Duerig","Imed Zitouni","Zhe Dong"],"pdf_url":"https://arxiv.org/pdf/2506.02291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02267v1","updated":"2025-06-02T21:15:20Z","published":"2025-06-02T21:15:20Z","title":"TransAct V2: Lifelong User Action Sequence Modeling on Pinterest\n  Recommendation","summary":"  Modeling user action sequences has become a popular focus in industrial\nrecommendation system research, particularly for Click-Through Rate (CTR)\nprediction tasks. However, industry-scale CTR models often rely on short user\nsequences, limiting their ability to capture long-term behavior. Additionally,\nthese models typically lack an integrated action-prediction task within a\npoint-wise ranking framework, reducing their predictive power. They also rarely\naddress the infrastructure challenges involved in efficiently serving\nlarge-scale sequential models. In this paper, we introduce TransAct V2, a\nproduction model for Pinterest's Homefeed ranking system, featuring three key\ninnovations: (1) leveraging very long user sequences to improve CTR\npredictions, (2) integrating a Next Action Loss function for enhanced user\naction forecasting, and (3) employing scalable, low-latency deployment\nsolutions tailored to handle the computational demands of extended user action\nsequences.\n","authors":["Xue Xia","Saurabh Vishwas Joshi","Kousik Rajesh","Kangnan Li","Yangyi Lu","Nikil Pancha","Dhruvil Deven Badani","Jiajing Xu","Pong Eksombatchai"],"pdf_url":"https://arxiv.org/pdf/2506.02267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02261v1","updated":"2025-06-02T21:09:29Z","published":"2025-06-02T21:09:29Z","title":"Towards Human-like Preference Profiling in Sequential Recommendation","summary":"  Sequential recommendation systems aspire to profile users by interpreting\ntheir interaction histories, echoing how humans make decisions by weighing\nexperience, relative preference strength, and situational relevance. Yet,\nexisting large language model (LLM)-based recommenders often fall short of\nmimicking the flexible, context-aware decision strategies humans exhibit,\nneglecting the structured, dynamic, and context-aware mechanisms fundamental to\nhuman behaviors. To bridge this gap, we propose RecPO, a preference\noptimization framework that models structured feedback and contextual delay to\nemulate human-like prioritization in sequential recommendation RecPO exploits\nadaptive reward margins based on inferred preference hierarchies and temporal\nsignals, enabling the model to favor immediately relevant items and to\ndistinguish between varying degrees of preference and aversion. Extensive\nexperiments across five real-world datasets demonstrate that RecPO not only\nyields performance gains over state-of-the-art baselines, but also mirrors key\ncharacteristics of human decision-making: favoring timely satisfaction,\nmaintaining coherent preferences, and exercising discernment under shifting\ncontexts.\n","authors":["Zhongyu Ouyang","Qianlong Wen","Chunhui Zhang","Yanfang Ye","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2506.02261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02160v1","updated":"2025-06-02T18:43:37Z","published":"2025-06-02T18:43:37Z","title":"A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE)\n  Using Embeddings and Clustering","summary":"  This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability.\n","authors":["Madan Krishnamurthy","Daniel Korn","Melissa A Haendel","Christopher J Mungall","Anne E Thessen"],"pdf_url":"https://arxiv.org/pdf/2506.02160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01910v1","updated":"2025-06-02T17:31:42Z","published":"2025-06-02T17:31:42Z","title":"GLoSS: Generative Language Models with Semantic Search for Sequential\n  Recommendation","summary":"  We propose Generative Low-rank language model with Semantic Search (GLoSS), a\ngenerative recommendation framework that combines large language models with\ndense retrieval for sequential recommendation. Unlike prior methods such as\nGPT4Rec, which rely on lexical matching via BM25, GLoSS uses semantic search to\nretrieve relevant items beyond lexical matching. For query generation, we\nemploy 4-bit quantized LlaMA-3 models fine-tuned with low-rank adaptation\n(LoRA), enabling efficient training and inference on modest hardware. We\nevaluate GLoSS on three real-world Amazon review datasets: Beauty, Toys, and\nSports, and find that it achieves state-of-the-art performance. Compared to\ntraditional ID-based baselines, GLoSS improves Recall@5 by 33.3%, 52.8%, and\n15.2%, and NDCG@5 by 30.0%, 42.6%, and 16.1%, respectively. It also outperforms\nLLM-based recommenders such as P5, GPT4Rec, LlamaRec and E4SRec with Recall@5\ngains of 4.3%, 22.8%, and 29.5%. Additionally, user segment evaluations show\nthat GLoSS performs particularly well for cold-start users in the Amazon Toys\nand Sports datasets, and benefits from longer user histories in Amazon Beauty\ndataset, demonstrating robustness across different levels of interaction\nlengths.\n","authors":["Krishna Acharya","Aleksandr V. Petrov","Juba Ziani"],"pdf_url":"https://arxiv.org/pdf/2506.01910v1.pdf","comment":"Our code and model checkpoints are publicly available\n  at:https://github.com/krishnacharya/GLoSS"},{"id":"http://arxiv.org/abs/2506.01903v1","updated":"2025-06-02T17:24:30Z","published":"2025-06-02T17:24:30Z","title":"Getting almost all the bits from a quantum random access code","summary":"  A quantum random access code (QRAC) is a map $x\\mapsto\\rho_x$ that encodes\n$n$-bit strings $x$ into $m$-qubit quantum states $\\rho_x$, in a way that\nallows us to recover any one bit of $x$ with success probability $\\geq p$. The\nmeasurement on $\\rho_x$ that is used to recover, say, $x_1$ may destroy all the\ninformation about the other bits; this is in fact what happens in the\nwell-known QRAC that encodes $n=2$ bits into $m=1$ qubits. Does this generalize\nto large $n$, i.e., could there exist QRACs that are so \"obfuscated\" that one\ncannot get much more than one bit out of them? Here we show that this is not\nthe case: for every QRAC there exists a measurement that (with high\nprobability) recovers the full $n$-bit string $x$ up to small Hamming distance,\neven for the worst-case $x$.\n","authors":["Han-Hsuan Lin","Ronald de Wolf"],"pdf_url":"https://arxiv.org/pdf/2506.01903v1.pdf","comment":"14 pages LaTeX"},{"id":"http://arxiv.org/abs/2502.08826v3","updated":"2025-06-02T17:15:08Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v3.pdf","comment":"GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey"},{"id":"http://arxiv.org/abs/2506.01877v1","updated":"2025-06-02T17:06:35Z","published":"2025-06-02T17:06:35Z","title":"When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting\n  Out-of-Distribution Corpora Using GradNormIR","summary":"  Dense retrievers encode texts into embeddings to efficiently retrieve\nrelevant documents from large databases in response to user queries. However,\nreal-world corpora continually evolve, leading to a shift from the original\ntraining distribution of the retriever. Without timely updates or retraining,\nindexing newly emerging documents can degrade retrieval performance for future\nqueries. Thus, identifying when a dense retriever requires an update is\ncritical for maintaining robust retrieval systems. In this paper, we propose a\nnovel task of predicting whether a corpus is out-of-distribution (OOD) relative\nto a dense retriever before indexing. Addressing this task allows us to\nproactively manage retriever updates, preventing potential retrieval failures.\nWe introduce GradNormIR, an unsupervised approach that leverages gradient norms\nto detect OOD corpora effectively. Experiments on the BEIR benchmark\ndemonstrate that GradNormIR enables timely updates of dense retrievers in\nevolving document collections, significantly enhancing retrieval robustness and\nefficiency.\n","authors":["Dayoon Ko","Jinyoung Kim","Sohyeon Kim","Jinhyuk Kim","Jaehoon Lee","Seonghak Song","Minyoung Lee","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2506.01877v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.01829v1","updated":"2025-06-02T16:15:34Z","published":"2025-06-02T16:15:34Z","title":"CiteEval: Principle-Driven Citation Evaluation for Source Attribution","summary":"  Citation quality is crucial in information-seeking systems, directly\ninfluencing trust and the effectiveness of information access. Current\nevaluation frameworks, both human and automatic, mainly rely on Natural\nLanguage Inference (NLI) to assess binary or ternary supportiveness from cited\nsources, which we argue is a suboptimal proxy for citation evaluation. In this\nwork we introduce CiteEval, a citation evaluation framework driven by\nprinciples focusing on fine-grained citation assessment within a broad context,\nencompassing not only the cited sources but the full retrieval context, user\nquery, and generated text. Guided by the proposed framework, we construct\nCiteBench, a multi-domain benchmark with high-quality human annotations on\ncitation quality. To enable efficient evaluation, we further develop\nCiteEval-Auto, a suite of model-based metrics that exhibit strong correlation\nwith human judgments. Experiments across diverse systems demonstrate\nCiteEval-Auto's superior ability to capture the multifaceted nature of\ncitations compared to existing metrics, offering a principled and scalable\napproach to evaluate and improve model-generated citations.\n","authors":["Yumo Xu","Peng Qi","Jifan Chen","Kunlun Liu","Rujun Han","Lan Liu","Bonan Min","Vittorio Castelli","Arshit Gupta","Zhiguo Wang"],"pdf_url":"https://arxiv.org/pdf/2506.01829v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2412.00639v2","updated":"2025-06-02T15:22:19Z","published":"2024-12-01T01:36:41Z","title":"Needle: A Generative AI-Powered Multi-modal Database for Answering\n  Complex Natural Language Queries","summary":"  Multi-modal datasets, like those involving images, often miss the detailed\ndescriptions that properly capture the rich information encoded in each item.\nThis makes answering complex natural language queries a major challenge in this\ndomain. In particular, unlike the traditional nearest neighbor search, where\nthe tuples and the query are represented as points in a single metric space,\nthese settings involve queries and tuples embedded in fundamentally different\nspaces, making the traditional query answering methods inapplicable. Existing\nliterature addresses this challenge for image datasets through vector\nrepresentations jointly trained on natural language and images. This technique,\nhowever, underperforms for complex queries due to various reasons.\n  This paper takes a step towards addressing this challenge by introducing a\nGenerative-based Monte Carlo method that utilizes foundation models to generate\nsynthetic samples that capture the complexity of the natural language query and\nrepresent it in the same metric space as the multi-modal data.\n  Following this method, we propose Needle, a database for image data\nretrieval. Instead of relying on contrastive learning or metadata-searching\napproaches, our system is based on synthetic data generation to capture the\ncomplexities of natural language queries. Our system is open-source and ready\nfor deployment, designed to be easily adopted by researchers and developers.\nThe comprehensive experiments on various benchmark datasets verify that this\nsystem significantly outperforms state-of-the-art text-to-image retrieval\nmethods in the literature. Any foundation model and embedder can be easily\nintegrated into Needle to improve the performance, piggybacking on the\nadvancements in these technologies.\n","authors":["Mahdi Erfanian","Mohsen Dehghankar","Abolfazl Asudeh"],"pdf_url":"https://arxiv.org/pdf/2412.00639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19755v2","updated":"2025-06-02T13:46:57Z","published":"2025-05-26T09:33:54Z","title":"EGA-V1: Unifying Online Advertising with End-to-End Learning","summary":"  Modern industrial advertising systems commonly employ Multi-stage Cascading\nArchitectures (MCA) to balance computational efficiency with ranking accuracy.\nHowever, this approach presents two fundamental challenges: (1) performance\ninconsistencies arising from divergent optimization targets and capability\ndifferences between stages, and (2) failure to account for advertisement\nexternalities - the complex interactions between candidate ads during ranking.\nThese limitations ultimately compromise system effectiveness and reduce\nplatform profitability. In this paper, we present EGA-V1, an end-to-end\ngenerative architecture that unifies online advertising ranking as one model.\nEGA-V1 replaces cascaded stages with a single model to directly generate\noptimal ad sequences from the full candidate ad corpus in location-based\nservices (LBS). The primary challenges associated with this approach stem from\nhigh costs of feature processing and computational bottlenecks in modeling\nexternalities of large-scale candidate pools. To address these challenges,\nEGA-V1 introduces an algorithm and engine co-designed hybrid feature service to\ndecouple user and ad feature processing, reducing latency while preserving\nexpressiveness. To efficiently extract intra- and cross-sequence mutual\ninformation, we propose RecFormer with an innovative cluster-attention\nmechanism as its core architectural component. Furthermore, we propose a\nbi-stage training strategy that integrates pre-training with reinforcement\nlearning-based post-training to meet sophisticated platform and advertising\nobjectives. Extensive offline evaluations on public benchmarks and large-scale\nonline A/B testing on industrial advertising platform have demonstrated the\nsuperior performance of EGA-V1 over state-of-the-art MCAs.\n","authors":["Junyan Qiu","Ze Wang","Fan Zhang","Zuowu Zheng","Jile Zhu","Jiangke Fan","Teng Zhang","Haitao Wang","Yongkang Wang","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2505.19755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01673v1","updated":"2025-06-02T13:42:46Z","published":"2025-06-02T13:42:46Z","title":"GRAM: Generative Recommendation via Semantic-aware Multi-granular Late\n  Fusion","summary":"  Generative recommendation is an emerging paradigm that leverages the\nextensive knowledge of large language models by formulating recommendations\ninto a text-to-text generation task. However, existing studies face two key\nlimitations in (i) incorporating implicit item relationships and (ii) utilizing\nrich yet lengthy item information. To address these challenges, we propose a\nGenerative Recommender via semantic-Aware Multi-granular late fusion (GRAM),\nintroducing two synergistic innovations. First, we design semantic-to-lexical\ntranslation to encode implicit hierarchical and collaborative item\nrelationships into the vocabulary space of LLMs. Second, we present\nmulti-granular late fusion to integrate rich semantics efficiently with minimal\ninformation loss. It employs separate encoders for multi-granular prompts,\ndelaying the fusion until the decoding stage. Experiments on four benchmark\ndatasets show that GRAM outperforms eight state-of-the-art generative\nrecommendation models, achieving significant improvements of 11.5-16.0% in\nRecall@5 and 5.3-13.6% in NDCG@5. The source code is available at\nhttps://github.com/skleee/GRAM.\n","authors":["Sunkyung Lee","Minjin Choi","Eunseong Choi","Hye-young Kim","Jongwuk Lee"],"pdf_url":"https://arxiv.org/pdf/2506.01673v1.pdf","comment":"ACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2506.01668v1","updated":"2025-06-02T13:38:45Z","published":"2025-06-02T13:38:45Z","title":"Small Stickers, Big Meanings: A Multilingual Sticker Semantic\n  Understanding Dataset with a Gamified Approach","summary":"  Stickers, though small, are a highly condensed form of visual expression,\nubiquitous across messaging platforms and embraced by diverse cultures,\ngenders, and age groups. Despite their popularity, sticker retrieval remains an\nunderexplored task due to the significant human effort and subjectivity\ninvolved in constructing high-quality sticker query datasets. Although large\nlanguage models (LLMs) excel at general NLP tasks, they falter when confronted\nwith the nuanced, intangible, and highly specific nature of sticker query\ngeneration.\n  To address this challenge, we propose a threefold solution. First, we\nintroduce Sticktionary, a gamified annotation framework designed to gather\ndiverse, high-quality, and contextually resonant sticker queries. Second, we\npresent StickerQueries, a multilingual sticker query dataset containing 1,115\nEnglish and 615 Chinese queries, annotated by over 60 contributors across 60+\nhours. Lastly, Through extensive quantitative and qualitative evaluation, we\ndemonstrate that our approach significantly enhances query generation quality,\nretrieval accuracy, and semantic understanding in the sticker domain. To\nsupport future research, we publicly release our multilingual dataset along\nwith two fine-tuned query generation models.\n","authors":["Heng Er Metilda Chee","Jiayin Wang","Zhiqiang Guo","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.01668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01659v1","updated":"2025-06-02T13:30:39Z","published":"2025-06-02T13:30:39Z","title":"Engram Memory Encoding and Retrieval: A Neurocomputational Perspective","summary":"  Despite substantial research into the biological basis of memory, the precise\nmechanisms by which experiences are encoded, stored, and retrieved in the brain\nremain incompletely understood. A growing body of evidence supports the engram\ntheory, which posits that sparse populations of neurons undergo lasting\nphysical and biochemical changes to support long-term memory. Yet, a\ncomprehensive computational framework that integrates biological findings with\nmechanistic models remains elusive. This work synthesizes insights from\ncellular neuroscience and computational modeling to address key challenges in\nengram research: how engram neurons are identified and manipulated; how\nsynaptic plasticity mechanisms contribute to stable memory traces; and how\nsparsity promotes efficient, interference-resistant representations. Relevant\ncomputational approaches -- such as sparse regularization, engram gating, and\nbiologically inspired architectures like Sparse Distributed Memory and spiking\nneural networks -- are also examined. Together, these findings suggest that\nmemory efficiency, capacity, and stability emerge from the interaction of\nplasticity and sparsity constraints. By integrating neurobiological and\ncomputational perspectives, this paper provides a comprehensive theoretical\nfoundation for engram research and proposes a roadmap for future inquiry into\nthe mechanisms underlying memory, with implications for the diagnosis and\ntreatment of memory-related disorders.\n","authors":["Daniel Szelogowski"],"pdf_url":"https://arxiv.org/pdf/2506.01659v1.pdf","comment":"18 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.21801v2","updated":"2025-06-02T13:26:30Z","published":"2024-10-29T07:13:47Z","title":"PerSRV: Personalized Sticker Retrieval with Vision-Language Model","summary":"  Instant Messaging is a popular means for daily communication, allowing users\nto send text and stickers. As the saying goes, \"a picture is worth a thousand\nwords\", so developing an effective sticker retrieval technique is crucial for\nenhancing user experience. However, existing sticker retrieval methods rely on\nlabeled data to interpret stickers, and general-purpose Vision-Language Models\n(VLMs) often struggle to capture the unique semantics of stickers.\nAdditionally, relevant-based sticker retrieval methods lack personalization,\ncreating a gap between diverse user expectations and retrieval results. To\naddress these, we propose the Personalized Sticker Retrieval with\nVision-Language Model framework, namely PerSRV, structured into offline\ncalculations and online processing modules. The online retrieval part follows\nthe paradigm of relevant recall and personalized ranking, supported by the\noffline pre-calculation parts, which are sticker semantic understanding,\nutility evaluation and personalization modules. Firstly, for sticker-level\nsemantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate\nhuman-like sticker semantics, complemented by textual content extracted from\nfigures and historical interaction queries. Secondly, we investigate three\ncrowd-sourcing metrics for sticker utility evaluation. Thirdly, we cluster\nstyle centroids based on users' historical interactions to achieve personal\npreference modeling. Finally, we evaluate our proposed PerSRV method on a\npublic sticker retrieval dataset from WeChat, containing 543,098 candidates and\n12,568 interactions. Experimental results show that PerSRV significantly\noutperforms existing methods in multi-modal sticker retrieval. Additionally,\nour fine-tuned VLM delivers notable improvements in sticker semantic\nunderstandings.\n","authors":["Heng Er Metilda Chee","Jiayin Wang","Zhiqiang Guo","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21801v2.pdf","comment":"Accepted at WWW '25"},{"id":"http://arxiv.org/abs/2503.05315v2","updated":"2025-06-02T12:19:34Z","published":"2025-03-07T10:50:45Z","title":"LoRACode: LoRA Adapters for Code Embeddings","summary":"  Code embeddings are essential for semantic code search; however, current\napproaches often struggle to capture the precise syntactic and contextual\nnuances inherent in code. Open-source models such as CodeBERT and UniXcoder\nexhibit limitations in scalability and efficiency, while high-performing\nproprietary systems impose substantial computational costs. We introduce a\nparameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to\nconstruct task-specific adapters for code retrieval. Our approach reduces the\nnumber of trainable parameters to less than two percent of the base model,\nenabling rapid fine-tuning on extensive code corpora (2 million samples in 25\nminutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in\nMean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code\nsearch tasks across multiple programming languages. Distinction in task-wise\nand language-wise adaptation helps explore the sensitivity of code retrieval\nfor syntactical and linguistic variations. To foster research in this area, we\nmake our code and pre-trained models publicly available.\n","authors":["Saumya Chaturvedi","Aman Chadha","Laurent Bindschaedler"],"pdf_url":"https://arxiv.org/pdf/2503.05315v2.pdf","comment":"Accepted at the Deep Learning for Code (DL4C) Workshop at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.12499v4","updated":"2025-06-02T10:17:05Z","published":"2025-05-18T17:18:06Z","title":"Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video\n  Retrieval","summary":"  Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.\n","authors":["Jian Xiao","Zijie Song","Jialong Hu","Hao Cheng","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.12499v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01488v1","updated":"2025-06-02T09:46:59Z","published":"2025-06-02T09:46:59Z","title":"Argument-Centric Causal Intervention Method for Mitigating Bias in\n  Cross-Document Event Coreference Resolution","summary":"  Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in\nnatural language processing (NLP) that seeks to determine whether event\nmentions across multiple documents refer to the same real-world occurrence.\nHowever, current CD-ECR approaches predominantly rely on trigger features\nwithin input mention pairs, which induce spurious correlations between\nsurface-level lexical features and coreference relationships, impairing the\noverall performance of the models. To address this issue, we propose a novel\ncross-document event coreference resolution method based on Argument-Centric\nCausal Intervention (ACCI). Specifically, we construct a structural causal\ngraph to uncover confounding dependencies between lexical triggers and\ncoreference labels, and introduce backdoor-adjusted interventions to isolate\nthe true causal effect of argument semantics. To further mitigate spurious\ncorrelations, ACCI integrates a counterfactual reasoning module that quantifies\nthe causal influence of trigger word perturbations, and an argument-aware\nenhancement module to promote greater sensitivity to semantically grounded\ninformation. In contrast to prior methods that depend on costly data\naugmentation or heuristic-based filtering, ACCI enables effective debiasing in\na unified end-to-end framework without altering the underlying training\nprocedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of\n88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The\nimplementation and materials are available at https://github.com/era211/ACCI.\n","authors":["Long Yao","Wenzhong Yang","Yabo Yin","Fuyuan Wei","Hongzhen Lv","Jiaren Peng","Liejun Wang","Xiaoming Tao"],"pdf_url":"https://arxiv.org/pdf/2506.01488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01451v1","updated":"2025-06-02T09:08:38Z","published":"2025-06-02T09:08:38Z","title":"Building Entity Association Mining Framework for Knowledge Discovery","summary":"  Extracting useful signals or pattern to support important business decisions\nfor example analyzing investment product traction and discovering customer\npreference, risk monitoring etc. from unstructured text is a challenging task.\nCapturing interaction of entities or concepts and association mining is a\ncrucial component in text mining, enabling information extraction and reasoning\nover and knowledge discovery from text. Furthermore, it can be used to enrich\nor filter knowledge graphs to guide exploration processes, descriptive\nanalytics and uncover hidden stories in the text. In this paper, we introduce a\ndomain independent pipeline i.e., generalized framework to enable document\nfiltering, entity extraction using various sources (or techniques) as plug-ins\nand association mining to build any text mining business use-case and\nquantitatively define a scoring metric for ranking purpose. The proposed\nframework has three major components a) Document filtering: filtering\ndocuments/text of interest from massive amount of texts b) Configurable entity\nextraction pipeline: include entity extraction techniques i.e., i) DBpedia\nSpotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or\ndictionary) based c) Association Relationship Mining: To generates\nco-occurrence graph to analyse potential relationships among entities,\nconcepts. Further, co-occurrence count based frequency statistics provide a\nholistic window to observe association trends or buzz rate in specific business\ncontext. The paper demonstrates the usage of framework as fundamental building\nbox in two financial use-cases namely brand product discovery and vendor risk\nmonitoring. We aim that such framework will remove duplicated effort, minimize\nthe development effort, and encourage reusability and rapid prototyping in\nassociation mining business applications for institutions.\n","authors":["Anshika Rawal","Abhijeet Kumar","Mridul Mishra"],"pdf_url":"https://arxiv.org/pdf/2506.01451v1.pdf","comment":"Presented at Business Analytics and Intelligence Conference, IIM\n  Bengaluru"},{"id":"http://arxiv.org/abs/2410.13248v2","updated":"2025-06-02T08:41:09Z","published":"2024-10-17T06:15:00Z","title":"Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation","summary":"  Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. The datasets and benchmark\nimplementation are available at: https://github.com/jchanxtarov/sent_xrec.\n","authors":["Ryotaro Shimizu","Takashi Wada","Yu Wang","Johannes Kruse","Sean O'Brien","Sai HtaungKham","Linxin Song","Yuya Yoshikawa","Yuki Saito","Fugee Tsung","Masayuki Goto","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13248v2.pdf","comment":"This manuscript has been accepted for presentation at The Web\n  Conference (WWW) 2025"},{"id":"http://arxiv.org/abs/2506.01375v1","updated":"2025-06-02T07:04:16Z","published":"2025-06-02T07:04:16Z","title":"Generative Next POI Recommendation with Semantic ID","summary":"  Point-of-interest (POI) recommendation systems aim to predict the next\ndestinations of user based on their preferences and historical check-ins.\nExisting generative POI recommendation methods usually employ random numeric\nIDs for POIs, limiting the ability to model semantic relationships between\nsimilar locations. In this paper, we propose Generative Next POI Recommendation\nwith Semantic ID (GNPR-SID), an LLM-based POI recommendation model with a novel\nsemantic POI ID (SID) representation method that enhances the semantic\nunderstanding of POI modeling. There are two key components in our GNPR-SID:\n(1) a Semantic ID Construction module that generates semantically rich POI IDs\nbased on semantic and collaborative features, and (2) a Generative POI\nRecommendation module that fine-tunes LLMs to predict the next POI using these\nsemantic IDs. By incorporating user interaction patterns and POI semantic\nfeatures into the semantic ID generation, our method improves the\nrecommendation accuracy and generalization of the model. To construct\nsemantically related SIDs, we propose a POI quantization method based on\nresidual quantized variational autoencoder, which maps POIs into a discrete\nsemantic space. We also propose a diversity loss to ensure that SIDs are\nuniformly distributed across the semantic space. Extensive experiments on three\nbenchmark datasets demonstrate that GNPR-SID substantially outperforms\nstate-of-the-art methods, achieving up to 16% improvement in recommendation\naccuracy.\n","authors":["Dongsheng Wang","Yuxi Huang","Shen Gao","Yifan Wang","Chengrui Huang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2506.01375v1.pdf","comment":"11 pages, 4 figures, the paper has been accepted by KDD 2025"},{"id":"http://arxiv.org/abs/2506.01361v1","updated":"2025-06-02T06:34:11Z","published":"2025-06-02T06:34:11Z","title":"TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal\n  Discovery","summary":"  Robust causal discovery in time series datasets depends on reliable benchmark\ndatasets with known ground-truth causal relationships. However, such datasets\nremain scarce, and existing synthetic alternatives often overlook critical\ntemporal properties inherent in real-world data, including nonstationarity\ndriven by trends and seasonality, irregular sampling intervals, and the\npresence of unobserved confounders. To address these challenges, we introduce\nTimeGraph, a comprehensive suite of synthetic time-series benchmark datasets\nthat systematically incorporates both linear and nonlinear dependencies while\nmodeling key temporal characteristics such as trends, seasonal effects, and\nheterogeneous noise patterns. Each dataset is accompanied by a fully specified\ncausal graph featuring varying densities and diverse noise distributions and is\nprovided in two versions: one including unobserved confounders and one without,\nthereby offering extensive coverage of real-world complexity while preserving\nmethodological neutrality. We further demonstrate the utility of TimeGraph\nthrough systematic evaluations of state-of-the-art causal discovery algorithms\nincluding PCMCI+, LPCMCI, and FGES across a diverse array of configurations and\nmetrics. Our experiments reveal significant variations in algorithmic\nperformance under realistic temporal conditions, underscoring the need for\nrobust synthetic benchmarks in the fair and transparent assessment of causal\ndiscovery methods. The complete TimeGraph suite, including dataset generation\nscripts, evaluation metrics, and recommended experimental protocols, is freely\navailable to facilitate reproducible research and foster community-driven\nadvancements in time-series causal discovery.\n","authors":["Muhammad Hasan Ferdous","Emam Hossain","Md Osman Gani"],"pdf_url":"https://arxiv.org/pdf/2506.01361v1.pdf","comment":"11 pages, 4 figures, accepted at KDD 2025 (Datasets and Benchmarks\n  Track)"},{"id":"http://arxiv.org/abs/2505.07155v2","updated":"2025-06-02T05:57:53Z","published":"2025-05-12T00:15:02Z","title":"Reassessing Large Language Model Boolean Query Generation for Systematic\n  Reviews","summary":"  Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain.\n","authors":["Shuai Wang","Harrisen Scells","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2505.07155v2.pdf","comment":"Accepted in SIGIR-2025"},{"id":"http://arxiv.org/abs/2506.01308v1","updated":"2025-06-02T04:36:13Z","published":"2025-06-02T04:36:13Z","title":"A Platform for Investigating Public Health Content with Efficient\n  Concern Classification","summary":"  A recent rise in online content expressing concerns with public health\ninitiatives has contributed to already stalled uptake of preemptive measures\nglobally. Future public health efforts must attempt to understand such content,\nwhat concerns it may raise among readers, and how to effectively respond to it.\nTo this end, we present ConcernScope, a platform that uses a teacher-student\nframework for knowledge transfer between large language models and light-weight\nclassifiers to quickly and effectively identify the health concerns raised in a\ntext corpus. The platform allows uploading massive files directly,\nautomatically scraping specific URLs, and direct text editing. ConcernScope is\nbuilt on top of a taxonomy of public health concerns. Intended for public\nhealth officials, we demonstrate several applications of this platform: guided\ndata exploration to find useful examples of common concerns found in online\ncommunity datasets, identification of trends in concerns through an example\ntime series analysis of 186,000 samples, and finding trends in topic frequency\nbefore and after significant events.\n","authors":["Christopher Li","Rickard Stureborg","Bhuwan Dhingra","Jun Yang"],"pdf_url":"https://arxiv.org/pdf/2506.01308v1.pdf","comment":"19 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.20368v2","updated":"2025-06-02T01:12:15Z","published":"2025-05-26T11:08:23Z","title":"Hierarchical Retrieval with Evidence Curation for Open-Domain Financial\n  Question Answering on Standardized Documents","summary":"  Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC.\n","authors":["Jaeyoung Choe","Jihoon Kim","Woohwan Jung"],"pdf_url":"https://arxiv.org/pdf/2505.20368v2.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2505.24584v2","updated":"2025-06-02T01:08:24Z","published":"2025-05-30T13:32:00Z","title":"AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams","summary":"  Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.\n","authors":["Sakhinana Sagar Srinivas","Shivam Gupta","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2505.24584v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.12562v3","updated":"2025-06-02T21:55:31Z","published":"2025-02-18T05:57:35Z","title":"SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings","summary":"  Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.\n","authors":["Weikai Lu","Hao Peng","Huiping Zhuang","Cen Chen","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12562v3.pdf","comment":"Accepted in ACL 2025 Main Track"},{"id":"http://arxiv.org/abs/2502.04328v3","updated":"2025-06-02T19:33:24Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal Language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts, pushing the frontiers of\nthe omni-modal language model to a large extent. We conduct a comprehensive\nexploration of architectural design, data curation, and training strategies\nessential for building a robust omni-modal model. Ola incorporates advanced\nvisual understanding and audio recognition capabilities through several\ncritical and effective improvements over mainstream baselines. Moreover, we\nrethink inter-modal relationships during omni-modal training, emphasizing\ncross-modal alignment with video as a central bridge, and propose a progressive\ntraining pipeline that begins with the most distinct modalities and gradually\nmoves towards closer modality alignment. Extensive experiments demonstrate that\nOla surpasses existing open omni-modal LLMs across all modalities while\nachieving highly competitive performance compared to state-of-the-art\nspecialized models of similar sizes. We aim to make Ola a fully open omni-modal\nunderstanding solution to advance future research in this emerging field. Model\nweights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01850v1","updated":"2025-06-02T16:38:50Z","published":"2025-06-02T16:38:50Z","title":"MoDA: Modulation Adapter for Fine-Grained Visual Grounding in\n  Instructional MLLMs","summary":"  Recently, Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive performance on instruction-following tasks by integrating pretrained\nvisual encoders with large language models (LLMs). However, existing approaches\noften struggle to ground fine-grained visual concepts in complex scenes. In\nthis paper, we propose MoDA (Modulation Adapter), a lightweight yet effective\nmodule designed to refine pre-aligned visual features through\ninstruction-guided modulation. Our approach follows the standard LLaVA training\nprotocol, consisting of a two-stage process: (1) aligning image features to the\nLLMs input space via a frozen vision encoder and adapter layers, and (2)\nrefining those features using the MoDA adapter during the instructional tuning\nstage. MoDA employs a Transformer-based cross-attention mechanism to generate a\nmodulation mask over the aligned visual tokens, thereby emphasizing\nsemantically relevant embedding dimensions based on the language instruction.\nThe modulated features are then passed to the LLM for autoregressive language\ngeneration. Our experimental evaluation shows that MoDA improves visual\ngrounding and generates more contextually appropriate responses, demonstrating\nits effectiveness as a general-purpose enhancement for image-based MLLMs.\n","authors":["Wayner Barrios","Andrés Villa","Juan León Alcázar","SouYoung Jin","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2506.01850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01822v1","updated":"2025-06-02T16:04:25Z","published":"2025-06-02T16:04:25Z","title":"GSCodec Studio: A Modular Framework for Gaussian Splat Compression","summary":"  3D Gaussian Splatting and its extension to 4D dynamic scenes enable\nphotorealistic, real-time rendering from real-world captures, positioning\nGaussian Splats (GS) as a promising format for next-generation immersive media.\nHowever, their high storage requirements pose significant challenges for\npractical use in sharing, transmission, and storage. Despite various studies\nexploring GS compression from different perspectives, these efforts remain\nscattered across separate repositories, complicating benchmarking and the\nintegration of best practices. To address this gap, we present GSCodec Studio,\na unified and modular framework for GS reconstruction, compression, and\nrendering. The framework incorporates a diverse set of 3D/4D GS reconstruction\nmethods and GS compression techniques as modular components, facilitating\nflexible combinations and comprehensive comparisons. By integrating best\npractices from community research and our own explorations, GSCodec Studio\nsupports the development of compact representation and compression solutions\nfor static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,\nachieving competitive rate-distortion performance in static and dynamic GS\ncompression. The code for our framework is publicly available at\nhttps://github.com/JasonLSC/GSCodec_Studio , to advance the research on\nGaussian Splats compression.\n","authors":["Sicheng Li","Chengzhen Wu","Hao Li","Xiang Gao","Yiyi Liao","Lu Yu"],"pdf_url":"https://arxiv.org/pdf/2506.01822v1.pdf","comment":"Repository of the project: https://github.com/JasonLSC/GSCodec_Studio"},{"id":"http://arxiv.org/abs/2503.00071v2","updated":"2025-06-02T14:52:36Z","published":"2025-02-27T17:28:12Z","title":"I see what you mean: Co-Speech Gestures for Reference Resolution in\n  Multimodal Dialogue","summary":"  In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction.\n","authors":["Esam Ghaleb","Bulat Khaertdinov","Aslı Özyürek","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2503.00071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01668v1","updated":"2025-06-02T13:38:45Z","published":"2025-06-02T13:38:45Z","title":"Small Stickers, Big Meanings: A Multilingual Sticker Semantic\n  Understanding Dataset with a Gamified Approach","summary":"  Stickers, though small, are a highly condensed form of visual expression,\nubiquitous across messaging platforms and embraced by diverse cultures,\ngenders, and age groups. Despite their popularity, sticker retrieval remains an\nunderexplored task due to the significant human effort and subjectivity\ninvolved in constructing high-quality sticker query datasets. Although large\nlanguage models (LLMs) excel at general NLP tasks, they falter when confronted\nwith the nuanced, intangible, and highly specific nature of sticker query\ngeneration.\n  To address this challenge, we propose a threefold solution. First, we\nintroduce Sticktionary, a gamified annotation framework designed to gather\ndiverse, high-quality, and contextually resonant sticker queries. Second, we\npresent StickerQueries, a multilingual sticker query dataset containing 1,115\nEnglish and 615 Chinese queries, annotated by over 60 contributors across 60+\nhours. Lastly, Through extensive quantitative and qualitative evaluation, we\ndemonstrate that our approach significantly enhances query generation quality,\nretrieval accuracy, and semantic understanding in the sticker domain. To\nsupport future research, we publicly release our multilingual dataset along\nwith two fine-tuned query generation models.\n","authors":["Heng Er Metilda Chee","Jiayin Wang","Zhiqiang Guo","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.01668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02640v2","updated":"2025-06-02T12:10:47Z","published":"2025-04-03T14:36:08Z","title":"RoSMM: A Robust and Secure Multi-Modal Watermarking Framework for\n  Diffusion Models","summary":"  Current image watermarking technologies are predominantly categorized into\ntext watermarking techniques and image steganography; however, few methods can\nsimultaneously handle text and image-based watermark data, which limits their\napplicability in complex digital environments. This paper introduces an\ninnovative multi-modal watermarking approach, drawing on the concept of vector\ndiscretization in encoder-based vector quantization. By constructing adjacency\nmatrices, the proposed method enables the transformation of text watermarks\ninto robust image-based representations, providing a novel multi-modal\nwatermarking paradigm for image generation applications. Additionally, this\nstudy presents a newly designed image restoration module to mitigate image\ndegradation caused by transmission losses and various noise interferences,\nthereby ensuring the reliability and integrity of the watermark. Experimental\nresults validate the robustness of the method under multiple noise attacks,\nproviding a secure, scalable, and efficient solution for digital image\ncopyright protection.\n","authors":["ZhongLi Fang","Yu Xie","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2504.02640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02083v1","updated":"2025-06-02T10:59:31Z","published":"2025-06-02T10:59:31Z","title":"LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned\n  Cross-Attention","summary":"  Speaker recognition models face challenges in multi-lingual settings due to\nthe entanglement of linguistic information within speaker embeddings. The\noverlap between vocal traits such as accent, vocal anatomy, and a language's\nphonetic structure complicates separating linguistic and speaker information.\nDisentangling these components can significantly improve speaker recognition\naccuracy. To this end, we propose a novel disentanglement learning strategy\nthat integrates joint learning through prefix-tuned cross-attention. This\napproach is particularly effective when speakers switch between languages.\nExperimental results show the model generalizes across monolingual and\nmulti-lingual settings, including unseen languages. Notably, the proposed model\nimproves the equal error rate across multiple datasets, highlighting its\nability to separate language information from speaker embeddings and enhance\nrecognition in diverse linguistic conditions.\n","authors":["Aditya Srinivas Menon","Raj Prakash Gohil","Kumud Tripathi","Pankaj Wasnik"],"pdf_url":"https://arxiv.org/pdf/2506.02083v1.pdf","comment":"Accepted at Interspeech 2025, Netherlands"},{"id":"http://arxiv.org/abs/2505.12499v4","updated":"2025-06-02T10:17:05Z","published":"2025-05-18T17:18:06Z","title":"Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video\n  Retrieval","summary":"  Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.\n","authors":["Jian Xiao","Zijie Song","Jialong Hu","Hao Cheng","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.12499v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01482v1","updated":"2025-06-02T09:42:36Z","published":"2025-06-02T09:42:36Z","title":"Automatic Stage Lighting Control: Is it a Rule-Driven Process or\n  Generative Task?","summary":"  Stage lighting plays an essential role in live music performances,\ninfluencing the engaging experience of both musicians and audiences. Given the\nhigh costs associated with hiring or training professional lighting engineers,\nAutomatic Stage Lighting Control (ASLC) has gained increasing attention.\nHowever, most existing approaches only classify music into limited categories\nand map them to predefined light patterns, resulting in formulaic and\nmonotonous outcomes that lack rationality. To address this issue, this paper\npresents an end-to-end solution that directly learns from experienced lighting\nengineers -- Skip-BART. To the best of our knowledge, this is the first work to\nconceptualize ASLC as a generative task rather than merely a classification\nproblem. Our method modifies the BART model to take audio music as input and\nproduce light hue and value (intensity) as output, incorporating a novel skip\nconnection mechanism to enhance the relationship between music and light within\nthe frame grid.We validate our method through both quantitative analysis and an\nhuman evaluation, demonstrating that Skip-BART outperforms conventional\nrule-based methods across all evaluation metrics and shows only a limited gap\ncompared to real lighting engineers.Specifically, our method yields a p-value\nof 0.72 in a statistical comparison based on human evaluations with human\nlighting engineers, suggesting that the proposed approach closely matches human\nlighting engineering performance. To support further research, we have made our\nself-collected dataset, code, and trained model parameters available at\nhttps://github.com/RS2002/Skip-BART .\n","authors":["Zijian Zhao","Dian Jin","Zijing Zhou","Xiaoyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.01482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01478v1","updated":"2025-06-02T09:36:08Z","published":"2025-06-02T09:36:08Z","title":"MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic\n  Drug-Drug Interactions","summary":"  Understanding the interaction between different drugs (drug-drug interaction\nor DDI) is critical for ensuring patient safety and optimizing therapeutic\noutcomes. Existing DDI datasets primarily focus on textual information,\noverlooking multimodal data that reflect complex drug mechanisms. In this\npaper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for\nUnderstanding pharmacodynamic Drug-drug Interactions, and (2) benchmark\nlearning methods to study it. In brief, MUDI provides a comprehensive\nmultimodal representation of drugs by combining pharmacological text, chemical\nformulas, molecular structure graphs, and images across 310,532 annotated drug\npairs labeled as Synergism, Antagonism, or New Effect. Crucially, to\neffectively evaluate machine-learning based generalization, MUDI consists of\nunseen drug pairs in the test set. We evaluate benchmark models using both late\nfusion voting and intermediate fusion strategies. All data, annotations,\nevaluation scripts, and baselines are released under an open research license.\n","authors":["Tung-Lam Ngo","Ba-Hoang Tran","Duy-Cat Can","Trung-Hieu Do","Oliver Y. Chén","Hoang-Quynh Le"],"pdf_url":"https://arxiv.org/pdf/2506.01478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12636v5","updated":"2025-06-02T08:14:09Z","published":"2023-08-24T08:22:21Z","title":"Exploring Transferability of Multimodal Adversarial Samples for\n  Vision-Language Pre-training Models with Contrastive Learning","summary":"  The integration of visual and textual data in Vision-Language Pre-training\n(VLP) models is crucial for enhancing vision-language understanding. However,\nthe adversarial robustness of these models, especially in the alignment of\nimage-text features, has not yet been sufficiently explored. In this paper, we\nintroduce a novel gradient-based multimodal adversarial attack method,\nunderpinned by contrastive learning, to improve the transferability of\nmultimodal adversarial samples in VLP models. This method concurrently\ngenerates adversarial texts and images within imperceptive perturbation,\nemploying both image-text and intra-modal contrastive loss. We evaluate the\neffectiveness of our approach on image-text retrieval and visual entailment\ntasks, using publicly available datasets in a black-box setting. Extensive\nexperiments indicate a significant advancement over existing single-modal\ntransfer-based adversarial attack methods and current multimodal adversarial\nattack approaches.\n","authors":["Youze Wang","Wenbo Hu","Yinpeng Dong","Hanwang Zhang","Hang Su","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2308.12636v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01319v1","updated":"2025-06-02T05:02:03Z","published":"2025-06-02T05:02:03Z","title":"Learning Sparsity for Effective and Efficient Music Performance Question\n  Answering","summary":"  Music performances, characterized by dense and continuous audio as well as\nseamless audio-visual integration, present unique challenges for multimodal\nscene understanding and reasoning. Recent Music Performance Audio-Visual\nQuestion Answering (Music AVQA) datasets have been proposed to reflect these\nchallenges, highlighting the continued need for more effective integration of\naudio-visual representations in complex question answering. However, existing\nMusic AVQA methods often rely on dense and unoptimized representations, leading\nto inefficiencies in the isolation of key information, the reduction of\nredundancy, and the prioritization of critical samples. To address these\nchallenges, we introduce Sparsify, a sparse learning framework specifically\ndesigned for Music AVQA. It integrates three sparsification strategies into an\nend-to-end pipeline and achieves state-of-the-art performance on the Music AVQA\ndatasets. In addition, it reduces training time by 28.32% compared to its fully\ntrained dense counterpart while maintaining accuracy, demonstrating clear\nefficiency gains. To further improve data efficiency, we propose a key-subset\nselection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0\ntraining data and retains 70-80% of full-data performance across models.\n","authors":["Xingjian Diao","Tianzhen Yang","Chunhui Zhang","Weiyi Wu","Ming Cheng","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2506.01319v1.pdf","comment":"Accepted to the main conference of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)"},{"id":"http://arxiv.org/abs/2402.00045v6","updated":"2025-06-02T02:47:24Z","published":"2024-01-22T15:08:19Z","title":"Detecting Multimedia Generated by Large AI Models: A Survey","summary":"  The rapid advancement of Large AI Models (LAIMs), particularly diffusion\nmodels and large language models, has marked a new era where AI-generated\nmultimedia is increasingly integrated into various aspects of daily life.\nAlthough beneficial in numerous fields, this content presents significant\nrisks, including potential misuse, societal disruptions, and ethical concerns.\nConsequently, detecting multimedia generated by LAIMs has become crucial, with\na marked rise in related research. Despite this, there remains a notable gap in\nsystematic surveys that focus specifically on detecting LAIM-generated\nmultimedia. Addressing this, we provide the first survey to comprehensively\ncover existing research on detecting multimedia (such as text, images, videos,\naudio, and multimodal content) created by LAIMs. Specifically, we introduce a\nnovel taxonomy for detection methods, categorized by media modality, and\naligned with two perspectives: pure detection (aiming to enhance detection\nperformance) and beyond detection (adding attributes like generalizability,\nrobustness, and interpretability to detectors). Additionally, we have presented\na brief overview of generation mechanisms, public datasets, online detection\ntools, and evaluation metrics to provide a valuable resource for researchers\nand practitioners in this field. Most importantly, we offer a focused analysis\nfrom a social media perspective to highlight their broader societal impact.\nFurthermore, we identify current challenges in detection and propose directions\nfor future research that address unexplored, ongoing, and emerging issues in\ndetecting multimedia generated by LAIMs. Our aim for this survey is to fill an\nacademic gap and contribute to global AI security efforts, helping to ensure\nthe integrity of information in the digital realm. The project link is\nhttps://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.\n","authors":["Li Lin","Neeraj Gupta","Yue Zhang","Hainan Ren","Chun-Hao Liu","Feng Ding","Xin Wang","Xin Li","Luisa Verdoliva","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2402.00045v6.pdf","comment":null}]},"2025-06-01T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.22877v2","updated":"2025-06-01T21:44:34Z","published":"2025-03-28T21:07:43Z","title":"Understanding Inequality of LLM Fact-Checking over Geographic Regions\n  with Agent and Retrieval models","summary":"  Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.\n","authors":["Bruno Coelho","Shujaat Mirza","Yuyuan Cui","Christina Pöpper","Damon McCoy"],"pdf_url":"https://arxiv.org/pdf/2503.22877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09795v3","updated":"2025-06-01T20:17:37Z","published":"2025-05-14T20:45:29Z","title":"Beyond Pairwise Learning-To-Rank At Airbnb","summary":"  There are three fundamental asks from a ranking algorithm: it should scale to\nhandle a large number of items, sort items accurately by their utility, and\nimpose a total order on the items for logical consistency. But here's the\ncatch-no algorithm can achieve all three at the same time. We call this\nlimitation the SAT theorem for ranking algorithms. Given the dilemma, how can\nwe design a practical system that meets user needs? Our current work at Airbnb\nprovides an answer, with a working solution deployed at scale. We start with\npairwise learning-to-rank (LTR) models-the bedrock of search ranking tech\nstacks today. They scale linearly with the number of items ranked and perform\nstrongly on metrics like NDCG by learning from pairwise comparisons. They are\nat a sweet spot of performance vs. cost, making them an ideal choice for\nseveral industrial applications. However, they have a drawback-by ignoring\ninteractions between items, they compromise on accuracy. To improve accuracy,\nwe create a \"true\" pairwise LTR model-one that captures interactions between\nitems during pairwise comparisons. But accuracy comes at the expense of\nscalability and total order, and we discuss strategies to counter these\nchallenges. For greater accuracy, we take each item in the search result, and\ncompare it against the rest of the items along two dimensions: (1) Superiority:\nHow strongly do searchers prefer the given item over the remaining ones? (2)\nSimilarity: How similar is the given item to all the other items? This forms\nthe basis of our \"all-pairwise\" LTR framework, which factors in interactions\nacross all items at once. Looking at items on the search result page all\ntogether-superiority and similarity combined-gives us a deeper understanding of\nwhat searchers truly want. We quantify the resulting improvements in searcher\nexperience through offline and online experiments at Airbnb.\n","authors":["Malay Haldar","Daochen Zha","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2505.09795v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22238v2","updated":"2025-06-01T19:48:42Z","published":"2025-05-28T11:12:57Z","title":"Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval","summary":"  We present Yambda-5B, a large-scale open dataset sourced from the Yandex\nMusic streaming platform. Yambda-5B contains 4.79 billion user-item\ninteractions from 1 million users across 9.39 million tracks. The dataset\nincludes two primary types of interactions: implicit feedback (listening\nevents) and explicit feedback (likes, dislikes, unlikes and undislikes). In\naddition, we provide audio embeddings for most tracks, generated by a\nconvolutional neural network trained on audio spectrograms. A key\ndistinguishing feature of Yambda-5B is the inclusion of the is_organic flag,\nwhich separates organic user actions from recommendation-driven events. This\ndistinction is critical for developing and evaluating machine learning\nalgorithms, as Yandex Music relies on recommender systems to personalize track\nselection for users. To support rigorous benchmarking, we introduce an\nevaluation protocol based on a Global Temporal Split, allowing recommendation\nalgorithms to be assessed in conditions that closely mirror real-world use. We\nreport benchmark results for standard baselines (ItemKNN, iALS) and advanced\nmodels (SANSA, SASRec) using a variety of evaluation metrics. By releasing\nYambda-5B to the community, we aim to provide a readily accessible,\nindustrial-scale resource to advance research, foster innovation, and promote\nreproducible results in recommender systems.\n","authors":["A. Ploshkin","V. Tytskiy","A. Pismenny","V. Baikalov","E. Taychinov","A. Permiakov","D. Burlakov","E. Krofto","N. Savushkin"],"pdf_url":"https://arxiv.org/pdf/2505.22238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01063v1","updated":"2025-06-01T16:05:00Z","published":"2025-06-01T16:05:00Z","title":"AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative\n  Contracts","summary":"  Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) are\nreshaping how AI systems extract and organize information from unstructured\ntext. A key challenge is designing AI methods that can incrementally extract,\nstructure, and validate information while preserving hierarchical and\ncontextual relationships. We introduce CDMizer, a template-driven, LLM, and\nRAG-based framework for structured text transformation. By leveraging\ndepth-based retrieval and hierarchical generation, CDMizer ensures a\ncontrolled, modular process that aligns generated outputs with predefined\nschema. Its template-driven approach guarantees syntactic correctness, schema\nadherence, and improved scalability, addressing key limitations of direct\ngeneration methods. Additionally, we propose an LLM-powered evaluation\nframework to assess the completeness and accuracy of structured\nrepresentations. Demonstrated in the transformation of Over-the-Counter (OTC)\nfinancial derivative contracts into the Common Domain Model (CDM), CDMizer\nestablishes a scalable foundation for AI-driven document understanding,\nstructured synthesis, and automated validation in broader contexts.\n","authors":["Maruf Ahmed Mridul","Ian Sloyan","Aparna Gupta","Oshani Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2506.01063v1.pdf","comment":"8 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.18458v3","updated":"2025-06-01T16:00:34Z","published":"2025-05-24T01:57:12Z","title":"A Survey of LLM $\\times$ DATA","summary":"  The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.\n","authors":["Xuanhe Zhou","Junxuan He","Wei Zhou","Haodong Chen","Zirui Tang","Haoyu Zhao","Xin Tong","Guoliang Li","Youmin Chen","Jun Zhou","Zhaojun Sun","Binyuan Hui","Shuo Wang","Conghui He","Zhiyuan Liu","Jingren Zhou","Fan Wu"],"pdf_url":"https://arxiv.org/pdf/2505.18458v3.pdf","comment":"Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm"},{"id":"http://arxiv.org/abs/2307.00438v3","updated":"2025-06-01T15:40:04Z","published":"2023-07-01T23:20:38Z","title":"Towards Resource-Efficient Streaming of Large-Scale Medical Image\n  Datasets for Deep Learning","summary":"  Large-scale medical imaging datasets have accelerated deep learning (DL) for\nmedical image analysis. However, the large scale of these datasets poses a\nchallenge for researchers, resulting in increased storage and bandwidth\nrequirements for hosting and accessing them. Since different researchers have\ndifferent use cases and require different resolutions or formats for DL, it is\nneither feasible to anticipate every researcher's needs nor practical to store\ndata in multiple resolutions and formats. To that end, we propose the Medical\nImage Streaming Toolkit (MIST), a format-agnostic database that enables\nstreaming of medical images at different resolutions and formats from a single\nhigh-resolution copy. We evaluated MIST across eight popular, large-scale\nmedical imaging datasets spanning different body parts, modalities, and\nformats. Our results showed that our framework reduced the storage and\nbandwidth requirements for hosting and downloading datasets without impacting\nimage quality. We demonstrate that MIST addresses the challenges posed by\nlarge-scale medical imaging datasets by building a data-efficient and\nformat-agnostic database to meet the diverse needs of researchers and reduce\nbarriers to DL research in medical imaging.\n","authors":["Pranav Kulkarni","Adway Kanhere","Eliot Siegel","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2307.00438v3.pdf","comment":"17 pages, 4 figures, 10 tables, accepted to MIDL'25"},{"id":"http://arxiv.org/abs/2506.02058v1","updated":"2025-06-01T15:32:44Z","published":"2025-06-01T15:32:44Z","title":"Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?","summary":"  Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.\n","authors":["Xiang Li","Jiayi Xin","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2506.02058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17549v3","updated":"2025-06-01T14:54:02Z","published":"2025-05-23T06:55:02Z","title":"EGA-V2: An End-to-end Generative Framework for Industrial Advertising","summary":"  Traditional online industrial advertising systems suffer from the limitations\nof multi-stage cascaded architectures, which often discard high-potential\ncandidates prematurely and distribute decision logic across disconnected\nmodules. While recent generative recommendation approaches provide end-to-end\nsolutions, they fail to address critical advertising requirements of key\ncomponents for real-world deployment, such as explicit bidding, creative\nselection, ad allocation, and payment computation. To bridge this gap, we\nintroduce End-to-End Generative Advertising (EGA-V2), the first unified\nframework that holistically models user interests, point-of-interest (POI) and\ncreative generation, ad allocation, and payment optimization within a single\ngenerative model. Our approach employs hierarchical tokenization and\nmulti-token prediction to jointly generate POI recommendations and ad\ncreatives, while a permutation-aware reward model and token-level bidding\nstrategy ensure alignment with both user experiences and advertiser objectives.\nAdditionally, we decouple allocation from payment using a differentiable\nex-post regret minimization mechanism, guaranteeing approximate incentive\ncompatibility at the POI level. Through extensive offline evaluations we\ndemonstrate that EGA-V2 significantly outperforms traditional cascaded systems\nin both performance and practicality. Our results highlight its potential as a\npioneering fully generative advertising solution, paving the way for\nnext-generation industrial ad systems.\n","authors":["Zuowu Zheng","Ze Wang","Fan Yang","Jiangke Fan","Teng Zhang","Yongkang Wang","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2505.17549v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01136v2","updated":"2025-06-01T12:49:01Z","published":"2025-05-02T09:25:41Z","title":"Descriptor: C++ Self-Admitted Technical Debt Dataset (CppSATD)","summary":"  In software development, technical debt (TD) refers to suboptimal\nimplementation choices made by the developers to meet urgent deadlines and\nlimited resources, posing challenges for future maintenance. Self-Admitted\nTechnical Debt (SATD) is a sub-type of TD, representing specific TD instances\n``openly admitted'' by the developers and often expressed through source code\ncomments. Previous research on SATD has focused predominantly on the Java\nprogramming language, revealing a significant gap in cross-language SATD. Such\na narrow focus limits the generalizability of existing findings as well as SATD\ndetection techniques across multiple programming languages. Our work addresses\nsuch limitation by introducing CppSATD, a dedicated C++ SATD dataset,\ncomprising over 531,000 annotated comments and their source code contexts. Our\ndataset can serve as a foundation for future studies that aim to develop SATD\ndetection methods in C++, generalize the existing findings to other languages,\nor contribute novel insights to cross-language SATD research.\n","authors":["Phuoc Pham","Murali Sridharan","Matteo Esposito","Valentina Lenarduzzi"],"pdf_url":"https://arxiv.org/pdf/2505.01136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00983v1","updated":"2025-06-01T12:30:58Z","published":"2025-06-01T12:30:58Z","title":"Bridging the Gap: From Ad-hoc to Proactive Search in Conversations","summary":"  Proactive search in conversations (PSC) aims to reduce user effort in\nformulating explicit queries by proactively retrieving useful relevant\ninformation given conversational context. Previous work in PSC either directly\nuses this context as input to off-the-shelf ad-hoc retrievers or further\nfine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on\nshort and concise queries, while the PSC input is longer and noisier. This\ninput mismatch between ad-hoc search and PSC limits retrieval quality. While\nfine-tuning on PSC data helps, its benefits remain constrained by this input\ngap. In this work, we propose Conv2Query, a novel conversation-to-query\nframework that adapts ad-hoc retrievers to PSC by bridging the input gap\nbetween ad-hoc search and PSC. Conv2Query maps conversational context into\nad-hoc queries, which can either be used as input for off-the-shelf ad-hoc\nretrievers or for further fine-tuning on PSC data. Extensive experiments on two\nPSC datasets show that Conv2Query significantly improves ad-hoc retrievers'\nperformance, both when used directly and after fine-tuning on PSC.\n","authors":["Chuan Meng","Francesco Tonolini","Fengran Mo","Nikolaos Aletras","Emine Yilmaz","Gabriella Kazai"],"pdf_url":"https://arxiv.org/pdf/2506.00983v1.pdf","comment":"Accepted as a full paper at SIGIR 2025"},{"id":"http://arxiv.org/abs/2412.19302v3","updated":"2025-06-01T12:04:04Z","published":"2024-12-26T17:51:54Z","title":"RecLM: Recommendation Instruction Tuning","summary":"  Modern recommender systems aim to deeply understand users' complex\npreferences through their past interactions. While deep collaborative filtering\napproaches using Graph Neural Networks (GNNs) excel at capturing user-item\nrelationships, their effectiveness is limited when handling sparse data or\nzero-shot scenarios, primarily due to constraints in ID-based embedding\nfunctions. To address these challenges, we propose a model-agnostic\nrecommendation instruction-tuning paradigm that seamlessly integrates large\nlanguage models with collaborative filtering. Our proposed\n$\\underline{Rec}$ommendation $\\underline{L}$anguage $\\underline{M}$odel (RecLM)\nenhances the capture of user preference diversity through a carefully designed\nreinforcement learning reward function that facilitates self-augmentation of\nlanguage models. Comprehensive evaluations demonstrate significant advantages\nof our approach across various settings, and its plug-and-play compatibility\nwith state-of-the-art recommender systems results in notable performance\nenhancements. The implementation of our RecLM framework is publicly available\nat: https://github.com/HKUDS/RecLM.\n","authors":["Yangqin Jiang","Yuhao Yang","Lianghao Xia","Da Luo","Kangyi Lin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.19302v3.pdf","comment":"This paper is accepted by ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2506.00954v1","updated":"2025-06-01T10:56:18Z","published":"2025-06-01T10:56:18Z","title":"AliBoost: Ecological Boosting Framework in Alibaba Platform","summary":"  Maintaining a healthy ecosystem in billion-scale online platforms is\nchallenging, as users naturally gravitate toward popular items, leaving cold\nand less-explored items behind. This ''rich-get-richer'' phenomenon hinders the\ngrowth of potentially valuable cold items and harms the platform's ecosystem.\nExisting cold-start models primarily focus on improving initial recommendation\nperformance for cold items but fail to address users' natural preference for\npopular content. In this paper, we introduce AliBoost, Alibaba's ecological\nboosting framework, designed to complement user-oriented natural\nrecommendations and foster a healthier ecosystem. AliBoost incorporates a\ntiered boosting structure and boosting principles to ensure high-potential\nitems quickly gain exposure while minimizing disruption to low-potential items.\nTo achieve this, we propose the Stacking Fine-Tuning Cold Predictor to enhance\nthe foundation CTR model's performance on cold items for accurate CTR and\npotential prediction. AliBoost then employs an Item-oriented Bidding Boosting\nmechanism to deliver cold items to the most suitable users while balancing\nboosting speed with user-personalized preferences. Over the past six months,\nAliBoost has been deployed across Alibaba's mainstream platforms, successfully\ncold-starting over a billion new items and increasing both clicks and GMV of\ncold items by over 60% within 180 days. Extensive online analysis and A/B\ntesting demonstrate the effectiveness of AliBoost in addressing ecological\nchallenges, offering new insights into the design of billion-scale recommender\nsystems.\n","authors":["Qijie Shen","Yuanchen Bei","Zihong Huang","Jialin Zhu","Keqin Xu","Boya Du","Jiawei Tang","Yuning Jiang","Feiran Huang","Xiao Huang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2506.00954v1.pdf","comment":"12 pages, 5 figures, accepted by KDD2025"},{"id":"http://arxiv.org/abs/2409.05806v4","updated":"2025-06-01T08:49:57Z","published":"2024-09-09T17:11:51Z","title":"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs","summary":"  Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Jizhan Fang","Tianhe Lu","Yunzhi Yao","Ziyan Jiang","Xin Xu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05806v4.pdf","comment":"ACL 2025; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2502.20317v4","updated":"2025-06-01T05:30:58Z","published":"2025-02-27T17:42:52Z","title":"Mixture of Structural-and-Textual Retrieval over Text-rich Graph\n  Knowledge Bases","summary":"  Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for\nanswering queries by providing textual and structural knowledge. However,\ncurrent retrieval methods often retrieve these two types of knowledge in\nisolation without considering their mutual reinforcement and some hybrid\nmethods even bypass structural retrieval entirely after neighboring\naggregation. To fill in this gap, we propose a Mixture of\nStructural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge\nvia a Planning-Reasoning-Organizing framework. In the Planning stage, MoR\ngenerates textual planning graphs delineating the logic for answering queries.\nFollowing planning graphs, in the Reasoning stage, MoR interweaves structural\ntraversal and textual matching to obtain candidates from TG-KBs. In the\nOrganizing stage, MoR further reranks fetched candidates based on their\nstructural trajectory. Extensive experiments demonstrate the superiority of MoR\nin harmonizing structural and textual retrieval with insights, including uneven\nretrieving performance across different query logics and the benefits of\nintegrating structural trajectories for candidate reranking. Our code is\navailable at https://github.com/Yoega/MoR.\n","authors":["Yongjia Lei","Haoyu Han","Ryan A. Rossi","Franck Dernoncourt","Nedim Lipka","Mahantesh M Halappanavar","Jiliang Tang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.20317v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00828v1","updated":"2025-06-01T04:23:06Z","published":"2025-06-01T04:23:06Z","title":"Breaker: Removing Shortcut Cues with User Clustering for Single-slot\n  Recommendation System","summary":"  In a single-slot recommendation system, users are only exposed to one item at\na time, and the system cannot collect user feedback on multiple items\nsimultaneously. Therefore, only pointwise modeling solutions can be adopted,\nfocusing solely on modeling the likelihood of clicks or conversions for items\nby users to learn user-item preferences, without the ability to capture the\nranking information among different items directly. However, since user-side\ninformation is often much more abundant than item-side information, the model\ncan quickly learn the differences in user intrinsic tendencies, which are\nindependent of the items they are exposed to. This can cause these intrinsic\ntendencies to become a shortcut bias for the model, leading to insufficient\nmining of the most concerned user-item preferences. To solve this challenge, we\nintroduce the Breaker model. Breaker integrates an auxiliary task of user\nrepresentation clustering with a multi-tower structure for cluster-specific\npreference modeling. By clustering user representations, we ensure that users\nwithin each cluster exhibit similar characteristics, which increases the\ncomplexity of the pointwise recommendation task on the user side. This forces\nthe multi-tower structure with cluster-driven parameter learning to better\nmodel user-item preferences, ultimately eliminating shortcut biases related to\nuser intrinsic tendencies. In terms of training, we propose a delayed parameter\nupdate mechanism to enhance training stability and convergence, enabling\nend-to-end joint training of the auxiliary clustering and classification tasks.\nBoth offline and online experiments demonstrate that our method surpasses the\nbaselines. It has already been deployed and is actively serving tens of\nmillions of users daily on Meituan, one of the most popular e-commerce\nplatforms for services.\n","authors":["Chao Wang","Yue Zheng","Yujing Zhang","Yan Feng","Zhe Wang","Xiaowei Shi","An You","Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2506.00828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19189v2","updated":"2025-06-01T01:41:09Z","published":"2025-05-25T15:31:52Z","title":"POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval","summary":"  Although Multi-Vector Retrieval (MVR) has achieved the state of the art on\nmany information retrieval (IR) tasks, its performance highly depends on how to\ndecompose queries into smaller pieces, say phrases or tokens. However,\noptimizing query decomposition for MVR performance is not end-to-end\ndifferentiable. Even worse, jointly solving this problem and training the\ndownstream retrieval-based systems, say RAG systems could be highly\ninefficient. To overcome these challenges, we propose Performance-Oriented\nQuery Decomposer (POQD), a novel query decomposition framework for MVR. POQD\nleverages one LLM for query decomposition and searches the optimal prompt with\nan LLM-based optimizer. We further propose an end-to-end training algorithm to\nalternatively optimize the prompt for query decomposition and the downstream\nmodels. This algorithm can achieve superior MVR performance at a reasonable\ntraining cost as our theoretical analysis suggests. POQD can be integrated\nseamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented\nGeneration (RAG) systems. Extensive empirical studies on representative\nRAG-based QA tasks show that POQD outperforms existing query decomposition\nstrategies in both retrieval performance and end-to-end QA accuracy. POQD is\navailable at https://github.com/PKU-SDS-lab/POQD-ICML25.\n","authors":["Yaoyang Liu","Junlin Li","Yinjun Wu","Zhen Chen"],"pdf_url":"https://arxiv.org/pdf/2505.19189v2.pdf","comment":"Published in ICML 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.01211v1","updated":"2025-06-01T23:13:46Z","published":"2025-06-01T23:13:46Z","title":"Iola Walker: A Mobile Footfall Detection System for Music Composition","summary":"  This project is the first of several experiments composing music that changes\nin response to biosignals. The system is dubbed \"iola walker\" in reference to a\ncommon polyrhythm, the hemiola. A listener goes for a walk, and the Iola Walker\napp detects their walking pace. Iola Walker picks up footfalls using a\nfoot-mounted accelerometer, processing the signals in real time using a\nrecurrent neural network in an Android app. The Android app outputs a MIDI\nevent for each footfall. The iola walker player, which might be a VST running\nin a DAW, plays the version of the next music passage with underlying\npolyrhythms closest to the listener's walking pace.\n  This paper documents the process of training the model to detect the\nfootfalls in real time. The model is trained on accelerometer data from an\nMbient Labs foot-mounted IMU at 200~Hz, with the ground truth for footfalls\nannotated by pressing the volume-up button on the Android device when the foot\nhits the ground. To collect training data, I walked around my neighborhood\nclicking the volume-up button each time my foot hit the ground. Several methods\nwere tried for detecting footfalls in real time from sensor data, including\nones based on digital signal processing techniques and traditional machine\nlearning techniques.\n","authors":["Will James"],"pdf_url":"https://arxiv.org/pdf/2506.01211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01109v1","updated":"2025-06-01T18:19:47Z","published":"2025-06-01T18:19:47Z","title":"CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic\n  Gaussian Splatting","summary":"  Accurate fruit counting in real-world agricultural environments is a\nlongstanding challenge due to visual occlusions, semantic ambiguity, and the\nhigh computational demands of 3D reconstruction. Existing methods based on\nneural radiance fields suffer from low inference speed, limited generalization,\nand lack support for open-set semantic control. This paper presents\nFruitLangGS, a real-time 3D fruit counting framework that addresses these\nlimitations through spatial reconstruction, semantic embedding, and\nlanguage-guided instance estimation. FruitLangGS first reconstructs\norchard-scale scenes using an adaptive Gaussian splatting pipeline with\nradius-aware pruning and tile-based rasterization for efficient rendering. To\nenable semantic control, each Gaussian encodes a compressed CLIP-aligned\nlanguage embedding, forming a compact and queryable 3D representation. At\ninference time, prompt-based semantic filtering is applied directly in 3D\nspace, without relying on image-space segmentation or view-level fusion. The\nselected Gaussians are then converted into dense point clouds via\ndistribution-aware sampling and clustered to estimate fruit counts.\nExperimental results on real orchard data demonstrate that FruitLangGS achieves\nhigher rendering speed, semantic flexibility, and counting accuracy compared to\nprior approaches, offering a new perspective for language-driven, real-time\nneural rendering across open-world scenarios.\n","authors":["Fengze Li","Yangle Liu","Jieming Ma","Hai-Ning Liang","Yaochun Shen","Huangxiang Li","Zhijing Wu"],"pdf_url":"https://arxiv.org/pdf/2506.01109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13772v3","updated":"2025-06-01T16:57:27Z","published":"2025-01-23T15:51:38Z","title":"Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak\n  Threats for Large Audio Language Models","summary":"  Large Language Models (LLMs) demonstrate impressive zero-shot performance\nacross a wide range of natural language processing tasks. Integrating various\nmodality encoders further expands their capabilities, giving rise to Multimodal\nLarge Language Models (MLLMs) that process not only text but also visual and\nauditory modality inputs. However, these advanced capabilities may also pose\nsignificant security risks, as models can be exploited to generate harmful or\ninappropriate content through jailbreak attack. While prior work has\nextensively explored how manipulating textual or visual modality inputs can\ncircumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific\nJailbreak on Large Audio-Language Models (LALMs) remains largely underexplored.\nTo address this gap, we introduce \\textbf{Jailbreak-AudioBench}, which consists\nof the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox\nsupports not only text-to-audio conversion but also various editing techniques\nfor injecting audio hidden semantics. The curated Dataset provides diverse\nexplicit and implicit jailbreak audio examples in both original and edited\nforms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and\nestablish the most comprehensive Jailbreak benchmark to date for audio\nmodality. Finally, Jailbreak-AudioBench establishes a foundation for advancing\nfuture research on LALMs safety alignment by enabling the in-depth exposure of\nmore powerful jailbreak threats, such as query-based audio editing, and by\nfacilitating the development of effective defense mechanisms.\n","authors":["Hao Cheng","Erjia Xiao","Jing Shao","Yichi Wang","Le Yang","Chao Shen","Philip Torr","Jindong Gu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13772v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00974v1","updated":"2025-06-01T11:58:25Z","published":"2025-06-01T11:58:25Z","title":"Camera Trajectory Generation: A Comprehensive Survey of Methods,\n  Metrics, and Future Directions","summary":"  Camera trajectory generation is a cornerstone in computer graphics, robotics,\nvirtual reality, and cinematography, enabling seamless and adaptive camera\nmovements that enhance visual storytelling and immersive experiences. Despite\nits growing prominence, the field lacks a systematic and unified survey that\nconsolidates essential knowledge and advancements in this domain. This paper\naddresses this gap by providing the first comprehensive review of the field,\ncovering from foundational definitions to advanced methodologies. We introduce\nthe different approaches to camera representation and present an in-depth\nreview of available camera trajectory generation models, starting with\nrule-based approaches and progressing through optimization-based techniques,\nmachine learning advancements, and hybrid methods that integrate multiple\nstrategies. Additionally, we gather and analyze the metrics and datasets\ncommonly used for evaluating camera trajectory systems, offering insights into\nhow these tools measure performance, aesthetic quality, and practical\napplicability. Finally, we highlight existing limitations, critical gaps in\ncurrent research, and promising opportunities for investment and innovation in\nthe field. This paper not only serves as a foundational resource for\nresearchers entering the field but also paves the way for advancing adaptive,\nefficient, and creative camera trajectory systems across diverse applications.\n","authors":["Zahra Dehghanian","Pouya Ardekhani","Amir Vahedi","Hamid Beigy","Hamid R. Rabiee"],"pdf_url":"https://arxiv.org/pdf/2506.00974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20756v2","updated":"2025-06-01T08:52:54Z","published":"2025-03-26T17:45:29Z","title":"ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems","summary":"  Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Chenxi Wang","Jizhan Fang","Xiang Chen","Bozhong Tian","Ziwen Xu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20756v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.00868v1","updated":"2025-06-01T07:17:16Z","published":"2025-06-01T07:17:16Z","title":"Multiverse Through Deepfakes: The MultiFakeVerse Dataset of\n  Person-Centric Visual and Conceptual Manipulations","summary":"  The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.\n","authors":["Parul Gupta","Shreya Ghosh","Tom Gedeon","Thanh-Toan Do","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2506.00868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00854v1","updated":"2025-06-01T06:26:32Z","published":"2025-06-01T06:26:32Z","title":"EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG","summary":"  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n","authors":["Jacky Tai-Yu Lu","Jung Chiang","Chi-Sheng Chen","Anna Nai-Yun Tung","Hsiang Wei Hu","Yuan Chiao Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.00854v1.pdf","comment":null}]},"2025-05-31T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.05710v2","updated":"2025-05-31T23:34:02Z","published":"2024-12-07T17:51:31Z","title":"PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic\n  Languages with Example Selection from Related Example Banks","summary":"  Large Language Models (LLMs) have recently demonstrated impressive few-shot\nlearning capabilities through in-context learning (ICL). However, ICL\nperformance is highly dependent on the choice of few-shot demonstrations,\nmaking the selection of the most optimal examples a persistent research\nchallenge. This issue is further amplified in low-resource Indic languages,\nwhere the scarcity of ground-truth data complicates the selection process. In\nthis work, we propose PromptRefine, a novel Alternating Minimization approach\nfor example selection that improves ICL performance on low-resource Indic\nlanguages. PromptRefine leverages auxiliary example banks from related\nhigh-resource Indic languages and employs multi-task learning techniques to\nalign language-specific retrievers, enabling effective cross-language\nretrieval. Additionally, we incorporate diversity in the selected examples to\nenhance generalization and reduce bias. Through comprehensive evaluations on\nfour text generation tasks -- Cross-Lingual Question Answering, Multilingual\nQuestion Answering, Machine Translation, and Cross-Lingual Summarization using\nstate-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and\nQwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms\nexisting frameworks for retrieving examples.\n","authors":["Soumya Suvra Ghosal","Soumyabrata Pal","Koyel Mukherjee","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2412.05710v2.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2506.00723v1","updated":"2025-05-31T21:49:17Z","published":"2025-05-31T21:49:17Z","title":"Pitfalls in Evaluating Language Model Forecasters","summary":"  Large language models (LLMs) have recently been applied to forecasting tasks,\nwith some works claiming these systems match or exceed human performance. In\nthis paper, we argue that, as a community, we should be careful about such\nconclusions as evaluating LLM forecasters presents unique challenges. We\nidentify two broad categories of issues: (1) difficulty in trusting evaluation\nresults due to many forms of temporal leakage, and (2) difficulty in\nextrapolating from evaluation performance to real-world forecasting. Through\nsystematic analysis and concrete examples from prior work, we demonstrate how\nevaluation flaws can raise concerns about current and future performance\nclaims. We argue that more rigorous evaluation methodologies are needed to\nconfidently assess the forecasting abilities of LLMs.\n","authors":["Daniel Paleka","Shashwat Goel","Jonas Geiping","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2506.00723v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.00491v1","updated":"2025-05-31T09:57:07Z","published":"2025-05-31T09:57:07Z","title":"Optimizing Question Semantic Space for Dynamic Retrieval-Augmented\n  Multi-hop Question Answering","summary":"  Retrieval-augmented generation (RAG) is usually integrated into large\nlanguage models (LLMs) to mitigate hallucinations and knowledge obsolescence.\nWhereas,conventional one-step retrieve-and-read methods are insufficient for\nmulti-hop question answering, facing challenges of retrieval semantic\nmismatching and the high cost in handling interdependent subquestions. In this\npaper, we propose Optimizing Question Semantic Space for Dynamic\nRetrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of\nthree key modules: (1) the Question Decomposition Module (QDM), which\ndecomposes multi-hop questions into fine-grained subquestions; (2) the\nSubquestion Dependency Optimizer Module (SDOM), which models the interdependent\nrelations of subquestions for better understanding; and (3) the Dynamic Passage\nRetrieval Module (DPRM), which aligns subquestions with relevant passages by\noptimizing the semantic embeddings. Experimental results across various\nbenchmarks demonstrate that Q-DREAM significantly outperforms existing RAG\nmethods, achieving state-of-the-art performance in both in-domain and\nout-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency\nwhile maintaining high accuracy compared with recent baselines.\n","authors":["Linhao Ye","Lang Yu","Zhikai Lei","Qin Chen","Jie Zhou","Liang He"],"pdf_url":"https://arxiv.org/pdf/2506.00491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20243v2","updated":"2025-05-31T09:34:42Z","published":"2025-05-26T17:21:26Z","title":"It's High Time: A Survey of Temporal Information Retrieval and Question\n  Answering","summary":"  Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization.\n","authors":["Bhawna Piryani","Abdelrahman Abdallah","Jamshid Mozafari","Avishek Anand","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2505.20243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04796v2","updated":"2025-05-31T09:02:25Z","published":"2025-03-02T11:33:22Z","title":"Optimizing Multi-Hop Document Retrieval Through Intermediate\n  Representations","summary":"  Retrieval-augmented generation (RAG) encounters challenges when addressing\ncomplex queries, particularly multi-hop questions. While several methods tackle\nmulti-hop queries by iteratively generating internal queries and retrieving\nexternal documents, these approaches are computationally expensive. In this\npaper, we identify a three-stage information processing pattern in LLMs during\nlayer-by-layer reasoning, consisting of extraction, processing, and subsequent\nextraction steps. This observation suggests that the representations in\nintermediate layers contain richer information compared to those in other\nlayers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike\nprior methods that focus on generating new internal queries, L-RAG leverages\nintermediate representations from the middle layers, which capture next-hop\ninformation, to retrieve external knowledge. L-RAG achieves performance\ncomparable to multi-step approaches while maintaining inference overhead\nsimilar to that of standard RAG. Experimental results show that L-RAG\noutperforms existing RAG methods on open-domain multi-hop question-answering\ndatasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is\navailable in https://anonymous.4open.science/r/L-RAG-ADD5/\n","authors":["Jiaen Lin","Jingyu Liu","Yingbo Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04796v2.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.00450v1","updated":"2025-05-31T08:09:54Z","published":"2025-05-31T08:09:54Z","title":"DV365: Extremely Long User History Modeling at Instagram","summary":"  Long user history is highly valuable signal for recommendation systems, but\neffectively incorporating it often comes with high cost in terms of data center\npower consumption and GPU. In this work, we chose offline embedding over\nend-to-end sequence length optimization methods to enable extremely long user\nsequence modeling as a cost-effective solution, and propose a new user\nembedding learning strategy, multi-slicing and summarization, that generates\nhighly generalizable user representation of user's long-term stable interest.\nHistory length we encoded in this embedding is up to 70,000 and on average\n40,000. This embedding, named as DV365, is proven highly incremental on top of\nadvanced attentive user sequence models deployed in Instagram. Produced by a\nsingle upstream foundational model, it is launched in 15 different models\nacross Instagram and Threads with significant impact, and has been production\nbattle-proven for >1 year since our first launch.\n","authors":["Wenhan Lyu","Devashish Tyagi","Yihang Yang","Ziwei Li","Ajay Somani","Karthikeyan Shanmugasundaram","Nikola Andrejevic","Ferdi Adeputra","Curtis Zeng","Arun K. Singh","Maxime Ransan","Sagar Jain"],"pdf_url":"https://arxiv.org/pdf/2506.00450v1.pdf","comment":"SIGKDD 2025 accepted"},{"id":"http://arxiv.org/abs/2502.19163v2","updated":"2025-05-31T07:53:48Z","published":"2025-02-26T14:17:56Z","title":"TestNUC: Enhancing Test-Time Computing Approaches and Scaling through\n  Neighboring Unlabeled Data Consistency","summary":"  Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC.\n","authors":["Henry Peng Zou","Zhengyao Gu","Yue Zhou","Yankai Chen","Weizhi Zhang","Liancheng Fang","Yibo Wang","Yangning Li","Kay Liu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.19163v2.pdf","comment":"Accepted by ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2506.00441v1","updated":"2025-05-31T07:46:42Z","published":"2025-05-31T07:46:42Z","title":"K-order Ranking Preference Optimization for Large Language Models","summary":"  To adapt large language models (LLMs) to ranking tasks, existing list-wise\nmethods, represented by list-wise Direct Preference Optimization (DPO), focus\non optimizing partial-order or full-order list ranking consistency for LLMs to\nenhance their ranking abilities. However, we argue that optimizing top-K\nranking consistency could be more appropriate for real-world applications.\nThere are two main reasons: (1) users are typically concerned with only the\ntop-K results, making top-K ranking more important, and (2) tail items often\nlack precise feedback, making top-K ranking more reliable. Based on this, we\npropose K-order Ranking Preference Optimization (KPO) by extending the DPO's\nPlackett-Luce model to accommodate top-K rankings. Additionally, recognizing\nthat the number of important items can vary across queries, we extend KPO to\ndynamically determine appropriate K for different samples and introduce a\ncurriculum learning strategy to boost training efficiency. Extensive\nexperiments demonstrate the effectiveness of KPO, highlighting its high sample\nefficiency and robustness to noise. The code is available at\nhttps://github.com/Lanyu0303/KPO.\n","authors":["Shihao Cai","Chongming Gao","Yang Zhang","Wentao Shi","Jizhi Zhang","Keqin Bao","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2506.00441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02614v2","updated":"2025-05-31T03:25:45Z","published":"2025-03-04T13:34:19Z","title":"Personalized Generation In Large Model Era: A Survey","summary":"  In the era of large models, content generation is gradually shifting to\nPersonalized Generation (PGen), tailoring content to individual preferences and\nneeds. This paper presents the first comprehensive survey on PGen,\ninvestigating existing research in this rapidly growing field. We conceptualize\nPGen from a unified perspective, systematically formalizing its key components,\ncore objectives, and abstract workflows. Based on this unified perspective, we\npropose a multi-level taxonomy, offering an in-depth review of technical\nadvancements, commonly used datasets, and evaluation metrics across multiple\nmodalities, personalized contexts, and tasks. Moreover, we envision the\npotential applications of PGen and highlight open challenges and promising\ndirections for future exploration. By bridging PGen research across multiple\nmodalities, this survey serves as a valuable resource for fostering knowledge\nsharing and interdisciplinary collaboration, ultimately contributing to a more\npersonalized digital landscape.\n","authors":["Yiyan Xu","Jinghao Zhang","Alireza Salemi","Xinting Hu","Wenjie Wang","Fuli Feng","Hamed Zamani","Xiangnan He","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2503.02614v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2506.00363v1","updated":"2025-05-31T03:06:09Z","published":"2025-05-31T03:06:09Z","title":"Adapting General-Purpose Embedding Models to Private Datasets Using\n  Keyword-based Retrieval","summary":"  Text embedding models play a cornerstone role in AI applications, such as\nretrieval-augmented generation (RAG). While general-purpose text embedding\nmodels demonstrate strong performance on generic retrieval benchmarks, their\neffectiveness diminishes when applied to private datasets (e.g.,\ncompany-specific proprietary data), which often contain specialized terminology\nand lingo. In this work, we introduce BMEmbed, a novel method for adapting\ngeneral-purpose text embedding models to private datasets. By leveraging the\nwell-established keyword-based retrieval technique (BM25), we construct\nsupervisory signals from the ranking of keyword-based retrieval results to\nfacilitate model adaptation. We evaluate BMEmbed across a range of domains,\ndatasets, and models, showing consistent improvements in retrieval performance.\nMoreover, we provide empirical insights into how BM25-based signals contribute\nto improving embeddings by fostering alignment and uniformity, highlighting the\nvalue of this approach in adapting models to domain-specific data. We release\nthe source code available at https://github.com/BaileyWei/BMEmbed for the\nresearch community.\n","authors":["Yubai Wei","Jiale Han","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2506.00363v1.pdf","comment":"Link: https://github.com/BaileyWei/BMEmbed"},{"id":"http://arxiv.org/abs/2501.14956v2","updated":"2025-05-31T01:44:04Z","published":"2025-01-24T22:44:22Z","title":"ExPerT: Effective and Explainable Evaluation of Personalized Long-Form\n  Text Generation","summary":"  Evaluating personalized text generated by large language models (LLMs) is\nchallenging, as only the LLM user, i.e., prompt author, can reliably assess the\noutput, but re-engaging the same individuals across studies is infeasible. This\npaper addresses the challenge of evaluating personalized text generation by\nintroducing ExPerT, an explainable reference-based evaluation framework. ExPerT\nleverages an LLM to extract atomic aspects and their evidence from the\ngenerated and reference texts, match the aspects, and evaluate their alignment\nbased on content and writing style -- two key attributes in personalized text\ngeneration. Additionally, ExPerT generates detailed, fine-grained explanations\nfor every step of the evaluation process, enhancing transparency and\ninterpretability. Our experiments demonstrate that ExPerT achieves a 7.2%\nrelative improvement in alignment with human judgments compared to the\nstate-of-the-art text generation evaluation methods. Furthermore, human\nevaluators rated the usability of ExPerT's explanations at 4.7 out of 5,\nhighlighting its effectiveness in making evaluation decisions more\ninterpretable.\n","authors":["Alireza Salemi","Julian Killingback","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2501.14956v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.00667v1","updated":"2025-05-31T18:37:21Z","published":"2025-05-31T18:37:21Z","title":"Scene Detection Policies and Keyframe Extraction Strategies for\n  Large-Scale Video Analysis","summary":"  Robust scene segmentation and keyframe extraction are essential preprocessing\nsteps in video understanding pipelines, supporting tasks such as indexing,\nsummarization, and semantic retrieval. However, existing methods often lack\ngeneralizability across diverse video types and durations. We present a\nunified, adaptive framework for automatic scene detection and keyframe\nselection that handles formats ranging from short-form media to long-form\nfilms, archival content, and surveillance footage. Our system dynamically\nselects segmentation policies based on video length: adaptive thresholding for\nshort videos, hybrid strategies for mid-length ones, and interval-based\nsplitting for extended recordings. This ensures consistent granularity and\nefficient processing across domains. For keyframe selection, we employ a\nlightweight module that scores sampled frames using a composite metric of\nsharpness, luminance, and temporal spread, avoiding complex saliency models\nwhile ensuring visual relevance. Designed for high-throughput workflows, the\nsystem is deployed in a commercial video analysis platform and has processed\ncontent from media, education, research, and security domains. It offers a\nscalable and interpretable solution suitable for downstream applications such\nas UI previews, embedding pipelines, and content filtering. We discuss\npractical implementation details and outline future enhancements, including\naudio-aware segmentation and reinforcement-learned frame scoring.\n","authors":["Vasilii Korolkov"],"pdf_url":"https://arxiv.org/pdf/2506.00667v1.pdf","comment":"24 pages, 8 figures, submitted as a preprint. ArXiv preprint only,\n  not submitted to a journal yet"},{"id":"http://arxiv.org/abs/2505.17543v2","updated":"2025-05-31T16:03:00Z","published":"2025-05-23T06:47:06Z","title":"MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance\n  Generation","summary":"  Music-driven 3D dance generation has attracted increasing attention in recent\nyears, with promising applications in choreography, virtual reality, and\ncreative content creation. Previous research has generated promising realistic\ndance movement from audio signals. However, traditional methods underutilize\ngenre conditioning, often treating it as auxiliary modifiers rather than core\nsemantic drivers. This oversight compromises music-motion synchronization and\ndisrupts dance genre continuity, particularly during complex rhythmic\ntransitions, thereby leading to visually unsatisfactory effects. To address the\nchallenge, we propose MEGADance, a novel architecture for music-driven 3D dance\ngeneration. By decoupling choreographic consistency into dance generality and\ngenre specificity, MEGADance demonstrates significant dance quality and strong\ngenre controllability. It consists of two stages: (1) High-Fidelity Dance\nQuantization Stage (HFDQ), which encodes dance motions into a latent\nrepresentation by Finite Scalar Quantization (FSQ) and reconstructs them with\nkinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage\n(GADG), which maps music into the latent representation by synergistic\nutilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid\nbackbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate\nthe state-of-the-art performance of MEGADance both qualitatively and\nquantitatively. Code will be released upon acceptance.\n","authors":["Kaixing Yang","Xulong Tang","Ziqiao Peng","Yuxuan Hu","Jun He","Hongyan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.17543v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2505.14222"},{"id":"http://arxiv.org/abs/2505.20011v2","updated":"2025-05-31T15:01:45Z","published":"2025-05-26T14:00:39Z","title":"The Many Challenges of Human-Like Agents in Virtual Game Environments","summary":"  Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players.\n","authors":["Maciej Swiechowski","Dominik Slezak"],"pdf_url":"https://arxiv.org/pdf/2505.20011v2.pdf","comment":"In proceedings of the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23,\n  Detroit, Michigan, USA"},{"id":"http://arxiv.org/abs/2505.17534v2","updated":"2025-05-31T13:51:12Z","published":"2025-05-23T06:41:07Z","title":"Co-Reinforcement Learning for Unified Multimodal Understanding and\n  Generation","summary":"  This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce CoRL, a co-reinforcement learning framework comprising a unified RL\nstage for joint optimization and a refined RL stage for task-specific\nenhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves\naverage improvements of 7% on three text-to-image generation datasets and 23%\non nine multimodal understanding benchmarks. These results demonstrate the\neffectiveness of CoRL and highlight the substantial benefit of reinforcement\nlearning in facilitating cross-task synergy and optimization for ULMs. Code is\navailable at https://github.com/mm-vl/ULM-R1.\n","authors":["Jingjing Jiang","Chongjie Si","Jun Luo","Hanwang Zhang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2505.17534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00562v1","updated":"2025-05-31T13:39:45Z","published":"2025-05-31T13:39:45Z","title":"SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with\n  Diffusion Models","summary":"  Diffusion models have recently enabled precise and photorealistic facial\nediting across a wide range of semantic attributes. Beyond single-step\nmodifications, a growing class of applications now demands the ability to\nanalyze and track sequences of progressive edits, such as stepwise changes to\nhair, makeup, or accessories. However, sequential editing introduces\nsignificant challenges in edit attribution and detection robustness, further\ncomplicated by the lack of large-scale, finely annotated benchmarks tailored\nexplicitly for this task. We introduce SEED, a large-scale Sequentially Edited\nfacE Dataset constructed via state-of-the-art diffusion models. SEED contains\nover 90,000 facial images with one to four sequential attribute modifications,\ngenerated using diverse diffusion-based editing pipelines (LEdits, SDXL, SD3).\nEach image is annotated with detailed edit sequences, attribute masks, and\nprompts, facilitating research on sequential edit tracking, visual provenance\nanalysis, and manipulation robustness assessment. To benchmark this task, we\npropose FAITH, a frequency-aware transformer-based model that incorporates\nhigh-frequency cues to enhance sensitivity to subtle sequential changes.\nComprehensive experiments, including systematic comparisons of multiple\nfrequency-domain methods, demonstrate the effectiveness of FAITH and the unique\nchallenges posed by SEED. SEED offers a challenging and flexible resource for\nstudying progressive diffusion-based edits at scale. Dataset and code will be\npublicly released at: https://github.com/Zeus1037/SEED.\n","authors":["Yule Zhu","Ping Liu","Zhedong Zheng","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2506.00562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11515v2","updated":"2025-05-31T09:09:03Z","published":"2025-04-15T14:26:12Z","title":"Graph-Driven Multimodal Feature Learning Framework for Apparent\n  Personality Assessment","summary":"  Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance.\n","authors":["Kangsheng Wang","Chengwei Ye","Huanzhen Zhang","Linuo Xu","Shuyan Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11515v2.pdf","comment":"The article contains serious scientific errors and cannot be\n  corrected by updating the preprint"}]},"2025-05-30T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.00314v1","updated":"2025-05-30T23:54:13Z","published":"2025-05-30T23:54:13Z","title":"FACE: A Fine-grained Reference Free Evaluator for Conversational\n  Recommender Systems","summary":"  A systematic, reliable, and low-cost evaluation of Conversational Recommender\nSystems (CRSs) remains an open challenge. Existing automatic CRS evaluation\nmethods are proven insufficient for evaluating the dynamic nature of\nrecommendation conversations. This work proposes FACE: a Fine-grained,\nAspect-based Conversation Evaluation method that provides evaluation scores for\ndiverse turn and dialogue level qualities of recommendation conversations. FACE\nis reference-free and shows strong correlation with human judgments, achieving\nsystem correlation of 0.9 and turn/dialogue-level of 0.5, outperforming\nstate-of-the-art CRS evaluation methods by a large margin. Additionally, unlike\nexisting LLM-based methods that provide single uninterpretable scores, FACE\nprovides insights into the system performance and enables identifying and\nlocating problems within conversations.\n","authors":["Hideaki Joko","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2506.00314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00238v1","updated":"2025-05-30T21:15:11Z","published":"2025-05-30T21:15:11Z","title":"ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer\n  Mapping for Natural Disaster Damage Assessment","summary":"  Natural disasters usually affect vast areas and devastate infrastructures.\nPerforming a timely and efficient response is crucial to minimize the impact on\naffected communities, and data-driven approaches are the best choice. Visual\nquestion answering (VQA) models help management teams to achieve in-depth\nunderstanding of damages. However, recently published models do not possess the\nability to answer open-ended questions and only select the best answer among a\npredefined list of answers. If we want to ask questions with new additional\npossible answers that do not exist in the predefined list, the model needs to\nbe fin-tuned/retrained on a new collected and annotated dataset, which is a\ntime-consuming procedure. In recent years, large-scale Vision-Language Models\n(VLMs) have earned significant attention. These models are trained on extensive\ndatasets and demonstrate strong performance on both unimodal and multimodal\nvision/language downstream tasks, often without the need for fine-tuning. In\nthis paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and\ninvestigate the performance of on post-disaster FloodNet dataset. Since the\nproposed method takes advantage of zero-shot learning, it can be applied on new\ndatasets without fine-tuning. In addition, ZeShot-VQA is able to process and\ngenerate answers that has been not seen during the training procedure, which\ndemonstrates its flexibility.\n","authors":["Ehsan Karimi","Maryam Rahnemoonfar"],"pdf_url":"https://arxiv.org/pdf/2506.00238v1.pdf","comment":"Accepted by the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2506.00220v1","updated":"2025-05-30T20:48:32Z","published":"2025-05-30T20:48:32Z","title":"Curate, Connect, Inquire: A System for Findable Accessible Interoperable\n  and Reusable (FAIR) Human-Robot Centered Datasets","summary":"  The rapid growth of AI in robotics has amplified the need for high-quality,\nreusable datasets, particularly in human-robot interaction (HRI) and\nAI-embedded robotics. While more robotics datasets are being created, the\nlandscape of open data in the field is uneven. This is due to a lack of\ncuration standards and consistent publication practices, which makes it\ndifficult to discover, access, and reuse robotics data. To address these\nchallenges, this paper presents a curation and access system with two main\ncontributions: (1) a structured methodology to curate, publish, and integrate\nFAIR (Findable, Accessible, Interoperable, Reusable) human-centered robotics\ndatasets; and (2) a ChatGPT-powered conversational interface trained with the\ncurated datasets metadata and documentation to enable exploration, comparison\nrobotics datasets and data retrieval using natural language. Developed based on\npractical experience curating datasets from robotics labs within Texas Robotics\nat the University of Texas at Austin, the system demonstrates the value of\nstandardized curation and persistent publication of robotics data. The system's\nevaluation suggests that access and understandability of human-robotics data\nare significantly improved. This work directly aligns with the goals of the\nHCRL @ ICRA 2025 workshop and represents a step towards more human-centered\naccess to data for embodied AI.\n","authors":["Xingru Zhou","Sadanand Modak","Yao-Cheng Chan","Zhiyun Deng","Luis Sentis","Maria Esteva"],"pdf_url":"https://arxiv.org/pdf/2506.00220v1.pdf","comment":"7 pages (excluding references), 8 pages (including references); 5\n  figures; accepted to the ICRA 2025 Workshop on Human-Centered Robot Learning\n  in the Era of Big Data and Large Models"},{"id":"http://arxiv.org/abs/2506.00203v1","updated":"2025-05-30T20:14:17Z","published":"2025-05-30T20:14:17Z","title":"The World As Large Language Models See It: Exploring the reliability of\n  LLMs in representing geographical features","summary":"  As large language models (LLMs) continue to evolve, questions about their\ntrustworthiness in delivering factual information have become increasingly\nimportant. This concern also applies to their ability to accurately represent\nthe geographic world. With recent advancements in this field, it is relevant to\nconsider whether and to what extent LLMs' representations of the geographical\nworld can be trusted. This study evaluates the performance of GPT-4o and Gemini\n2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and\nreverse geocoding. In the geocoding task, both models exhibited systematic and\nrandom errors in estimating the coordinates of St. Anne's Column in Innsbruck,\nAustria, with GPT-4o showing greater deviations and Gemini 2.0 Flash\ndemonstrating more precision but a significant systematic offset. For elevation\nestimation, both models tended to underestimate elevations across Austria,\nthough they captured overall topographical trends, and Gemini 2.0 Flash\nperformed better in eastern regions. The reverse geocoding task, which involved\nidentifying Austrian federal states from coordinates, revealed that Gemini 2.0\nFlash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating\nbetter consistency across regions. Despite these findings, neither model\nachieved an accurate reconstruction of Austria's federal states, highlighting\npersistent misclassifications. The study concludes that while LLMs can\napproximate geographic information, their accuracy and reliability are\ninconsistent, underscoring the need for fine-tuning with geographical\ninformation to enhance their utility in GIScience and Geoinformatics.\n","authors":["Omid Reza Abbasi","Franz Welscher","Georg Weinberger","Johannes Scholz"],"pdf_url":"https://arxiv.org/pdf/2506.00203v1.pdf","comment":"9 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2506.03187v1","updated":"2025-05-30T19:29:18Z","published":"2025-05-30T19:29:18Z","title":"Comparing Retrieval Strategies to Capture Interdisciplinary Scientific\n  Research: A Bibliometric Evaluation of the Integration of Neuroscience and\n  Computer Science","summary":"  Interdisciplinary scientific research is increasingly important in knowledge\nproduction, funding policies, and academic discussions on scholarly\ncommunication. While many studies focus on interdisciplinary corpora defined a\npriori - usually through keyword-based searches within assumed\ninterdisciplinary domains - few explore interdisciplinarity as an emergent\nintersection between two distinct fields. Thus, methodological proposals for\nbuilding databases at the intersection of two fields of knowledge are scarce.\nThe goal of this article is to develop and compare different strategies for\ndefining an interdisciplinary corpus between two bodies of knowledge. As a case\nstudy, we focus on the intersection between neuroscience and computer science.\nTo this end, we develop and compare four retrieval strategies, two of them\nbased on keywords and two based on citation and reference patterns. Our results\nshow that keyword-based strategies provide both better precision and recall.\nWhile we focus on comparing strategies for the study of the intersection\nbetween the fields of neuroscience and computer science, this proposed\nmethodological reflection is applicable to a wide range of interdisciplinary\ndomains.\n","authors":["Malena Mendez Isla","Agustin Mauro","Diego Kozlowski"],"pdf_url":"https://arxiv.org/pdf/2506.03187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00388v4","updated":"2025-05-30T18:40:25Z","published":"2023-11-01T09:25:21Z","title":"Towards Automatic Sampling of User Behaviors for Sequential Recommender\n  Systems","summary":"  Sequential recommender systems (SRS) have gained increasing popularity due to\ntheir remarkable proficiency in capturing dynamic user preferences. In the\ncurrent setup of SRS, a common configuration is to uniformly consider each\nhistorical behavior as a positive interaction. However, this setting has the\npotential to yield sub-optimal performance as each individual item often have a\ndifferent impact on shaping the user's interests. Hence, in this paper, we\npropose a novel automatic sampling framework for sequential recommendation,\nnamed AutoSAM, to non-uniformly treat historical behaviors. Specifically,\nAutoSAM extends the conventional SRS framework by integrating an extra sampler\nto intelligently discern the skew distribution of the raw input, and then\nsample informative sub-sets to build more generalizable SRS. To tackle the\nchallenges posed by non differentiable sampling actions and to introduce\nmultiple decision factors for sampling, we further design a novel reinforcement\nlearning based method to guide the training of the sampler. Furthermore, we\ntheoretically devise multi-objective sampling rewards including \\textit{Future\nPrediction} and \\textit{Sequence Perplexity}, and then optimize the whole\nframework in an end-to-end manner by combining the policy gradient. We conduct\nextensive experiments on benchmark recommendation models and four real-world\ndatasets. The experimental results demonstrate the effectiveness of the\nproposed AutoSAM.\n","authors":["Hao Zhang","Mingyue Cheng","Zhiding Liu","Junzhe Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.00388v4.pdf","comment":"IJCAI 2025"},{"id":"http://arxiv.org/abs/2506.00137v1","updated":"2025-05-30T18:16:03Z","published":"2025-05-30T18:16:03Z","title":"LaMP-QA: A Benchmark for Personalized Long-form Question Answering","summary":"  Personalization is essential for question answering systems that are\nuser-centric. Despite its importance, personalization in answer generation has\nbeen relatively underexplored. This is mainly due to lack of resources for\ntraining and evaluating personalized question answering systems. We address\nthis gap by introducing LaMP-QA -- a benchmark designed for evaluating\npersonalized long-form answer generation. The benchmark covers questions from\nthree major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal\nDevelopment, and (3) Society & Culture, encompassing over 45 subcategories in\ntotal. To assess the quality and potential impact of the LaMP-QA benchmark for\npersonalized question answering, we conduct comprehensive human and automatic\nevaluations, to compare multiple evaluation strategies for evaluating generated\npersonalized responses and measure their alignment with human preferences.\nFurthermore, we benchmark a number of non-personalized and personalized\napproaches based on open-source and proprietary large language models (LLMs).\nOur results show that incorporating the personalized context provided leads to\nperformance improvements of up to 39%. The benchmark is publicly released to\nsupport future research in this area.\n","authors":["Alireza Salemi","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2506.00137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02746v2","updated":"2025-05-30T17:39:41Z","published":"2025-05-05T15:56:25Z","title":"Using Knowledge Graphs to harvest datasets for efficient CLIP model\n  training","summary":"  Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time.\n","authors":["Simon Ging","Sebastian Walter","Jelena Bratulić","Johannes Dienert","Hannah Bast","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2505.02746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00107v1","updated":"2025-05-30T16:57:17Z","published":"2025-05-30T16:57:17Z","title":"Gated Multimodal Graph Learning for Personalized Recommendation","summary":"  Multimodal recommendation has emerged as a promising solution to alleviate\nthe cold-start and sparsity problems in collaborative filtering by\nincorporating rich content information, such as product images and textual\ndescriptions. However, effectively integrating heterogeneous modalities into a\nunified recommendation framework remains a challenge. Existing approaches often\nrely on fixed fusion strategies or complex architectures , which may fail to\nadapt to modality quality variance or introduce unnecessary computational\noverhead.\n  In this work, we propose RLMultimodalRec, a lightweight and modular\nrecommendation framework that combines graph-based user modeling with adaptive\nmultimodal item encoding. The model employs a gated fusion module to\ndynamically balance the contribution of visual and textual modalities, enabling\nfine-grained and content-aware item representations. Meanwhile, a two-layer\nLightGCN encoder captures high-order collaborative signals by propagating\nembeddings over the user-item interaction graph without relying on nonlinear\ntransformations.\n  We evaluate our model on a real-world dataset from the Amazon product domain.\nExperimental results demonstrate that RLMultimodalRec consistently outperforms\nseveral competitive baselines, including collaborative filtering, visual-aware,\nand multimodal GNN-based methods. The proposed approach achieves significant\nimprovements in top-K recommendation metrics while maintaining scalability and\ninterpretability, making it suitable for practical deployment.\n","authors":["Sibei Liu","Yuanzhe Zhang","Xiang Li","Yunbo Liu","Chengwei Feng","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2506.00107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24782v1","updated":"2025-05-30T16:43:28Z","published":"2025-05-30T16:43:28Z","title":"Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings","summary":"  A limitation of modern document retrieval embedding methods is that they\ntypically encode passages (chunks) from the same documents independently, often\noverlooking crucial contextual information from the rest of the document that\ncould greatly improve individual chunk representations.\n  In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a\nbenchmark designed to evaluate retrieval models on their ability to leverage\ndocument-wide context. Our results show that state-of-the-art embedding models\nstruggle in retrieval scenarios where context is required. To address this\nlimitation, we propose InSeNT (In-sequence Negative Training), a novel\ncontrastive post-training approach which combined with late chunking pooling\nenhances contextual representation learning while preserving computational\nefficiency. Our method significantly improves retrieval quality on ConTEB\nwithout sacrificing base model performance. We further find chunks embedded\nwith our method are more robust to suboptimal chunking strategies and larger\nretrieval corpus sizes. We open-source all artifacts at\nhttps://github.com/illuin-tech/contextual-embeddings.\n","authors":["Max Conti","Manuel Faysse","Gautier Viaud","Antoine Bosselut","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2505.24782v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.24754v1","updated":"2025-05-30T16:16:22Z","published":"2025-05-30T16:16:22Z","title":"Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding\n  based on Guided Space Transformation","summary":"  In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text. Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction. To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation. Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized. Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation. We conduct extensive\nexperiments on three instruction-awareness downstream tasks across nine\nreal-world datasets, demonstrating that GSTransform improves\ninstruction-following text embedding quality over state-of-the-art methods\nwhile achieving dramatic speedups of 6~300x in real-time processing on\nlarge-scale datasets. The source code is available at\nhttps://github.com/YingchaojieFeng/GSTransform.\n","authors":["Yingchaojie Feng","Yiqun Sun","Yandong Sun","Minfeng Zhu","Qiang Huang","Anthony K. H. Tung","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2505.24754v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2501.02772v2","updated":"2025-05-30T15:15:19Z","published":"2025-01-06T05:29:00Z","title":"GeAR: Generation Augmented Retrieval","summary":"  Document retrieval techniques are essential for developing large-scale\ninformation systems. The common approach involves using a bi-encoder to compute\nthe semantic similarity between a query and documents. However, the scalar\nsimilarity often fail to reflect enough information, hindering the\ninterpretation of retrieval results. In addition, this process primarily\nfocuses on global semantics, overlooking the finer-grained semantic\nrelationships between the query and the document's content. In this paper, we\nintroduce a novel method, $\\textbf{Ge}$neration $\\textbf{A}$ugmented\n$\\textbf{R}$etrieval ($\\textbf{GeAR}$), which not only improves the global\ndocument-query similarity through contrastive learning, but also integrates\nwell-designed fusion and decoding modules. This enables GeAR to generate\nrelevant context within the documents based on a given query, facilitating\nlearning to retrieve local fine-grained information. Furthermore, when used as\na retriever, GeAR does not incur any additional computational cost over\nbi-encoders. GeAR exhibits competitive retrieval performance across diverse\nscenarios and tasks. Moreover, qualitative analysis and the results generated\nby GeAR provide novel insights into the interpretation of retrieval results.\nThe code, data, and models will be released at\n\\href{https://github.com/microsoft/LMOps}{https://github.com/microsoft/LMOps}.\n","authors":["Haoyu Liu","Shaohan Huang","Jianfeng Liu","Yuefeng Zhan","Hao Sun","Weiwei Deng","Feng Sun","Furu Wei","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.02772v2.pdf","comment":"In ACL 2025"},{"id":"http://arxiv.org/abs/2505.23449v2","updated":"2025-05-30T11:34:43Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08713v5","updated":"2025-05-30T09:31:15Z","published":"2024-08-16T12:51:52Z","title":"Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction","summary":"  Modeling high-order feature interactions is crucial for click-through rate\n(CTR) prediction, and traditional approaches often predefine a maximum\ninteraction order and rely on exhaustive enumeration of feature combinations up\nto this predefined order. This framework heavily relies on prior domain\nknowledge to define interaction scope and entails high computational costs from\nenumeration. Conventional CTR models face a trade-off between improving\nrepresentation through complex high-order feature interactions and reducing\ncomputational inefficiencies associated with these processes. To address this\ndual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse\nEfficient Interaction Network (KarSein). Drawing inspiration from the learnable\nactivation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages\nthis mechanism to adaptively transform low-order basic features into high-order\nfeature interactions, offering a novel approach to feature interaction\nmodeling. KarSein extends the capabilities of KAN by introducing a more\nefficient architecture that significantly reduces computational costs while\naccommodating two-dimensional embedding vectors as feature inputs. Furthermore,\nit overcomes the limitation of KAN's its inability to spontaneously capture\nmultiplicative relationships among features.\n  Extensive experiments highlight the superiority of KarSein, demonstrating its\nability to surpass not only the vanilla implementation of KAN in CTR predictio\nbut also other baseline methods. Remarkably, KarSein achieves exceptional\npredictive accuracy while maintaining a highly compact parameter size and\nminimal computational overhead. As the first attempt to apply KAN in the CTR\ndomain, this work introduces KarSein as a novel solution for modeling complex\nfeature interactions, underscoring its transformative potential in advancing\nCTR prediction task.\n","authors":["Yunxiao Shi","Wujiang Xu","Haimin Zhang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2408.08713v5.pdf","comment":"draft paper"},{"id":"http://arxiv.org/abs/2303.16750v2","updated":"2025-05-30T08:34:07Z","published":"2023-03-23T16:15:03Z","title":"A Gold Standard Dataset for the Reviewer Assignment Problem","summary":"  Many peer-review venues are using algorithms to assign submissions to\nreviewers. The crux of such automated approaches is the notion of the\n\"similarity score\" -- a numerical estimate of the expertise of a reviewer in\nreviewing a paper -- and many algorithms have been proposed to compute these\nscores. However, these algorithms have not been subjected to a principled\ncomparison, making it difficult for stakeholders to choose the algorithm in an\nevidence-based manner. The key challenge in comparing existing algorithms and\ndeveloping better algorithms is the lack of publicly available gold-standard\ndata. We address this challenge by collecting a novel dataset of similarity\nscores that we release to the research community. Our dataset consists of 477\nself-reported expertise scores provided by 58 researchers who evaluated their\nexpertise in reviewing papers they have read previously.\n  Using our dataset, we compare several widely used similarity algorithms and\noffer key insights. First, all algorithms exhibit significant error, with\nmisranking rates between 12%-30% in easier cases and 36%-43% in harder ones.\nSecond, most specialized algorithms are designed to work with titles and\nabstracts of papers, and in this regime the SPECTER2 algorithm performs best.\nInterestingly, classical TF-IDF matches SPECTER2 in accuracy when given access\nto full submission texts. In contrast, off-the-shelf LLMs lag behind\nspecialized approaches.\n","authors":["Ivan Stelmakh","John Wieting","Sarina Xi","Graham Neubig","Nihar B. Shah"],"pdf_url":"https://arxiv.org/pdf/2303.16750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20995v2","updated":"2025-05-30T08:19:35Z","published":"2025-02-28T12:32:53Z","title":"The RAG Paradox: A Black-Box Attack Exploiting Unintentional\n  Vulnerabilities in Retrieval-Augmented Generation Systems","summary":"  With the growing adoption of retrieval-augmented generation (RAG) systems,\nvarious attack methods have been proposed to degrade their performance.\nHowever, most existing approaches rely on unrealistic assumptions in which\nexternal attackers have access to internal components such as the retriever. To\naddress this issue, we introduce a realistic black-box attack based on the RAG\nparadox, a structural vulnerability arising from the system's effort to enhance\ntrust by revealing both the retrieved documents and their sources to users.\nThis transparency enables attackers to observe which sources are used and how\ninformation is phrased, allowing them to craft poisoned documents that are more\nlikely to be retrieved and upload them to the identified sources. Moreover, as\nRAG systems directly provide retrieved content to users, these documents must\nnot only be retrievable but also appear natural and credible to maintain user\nconfidence in the search results. Unlike prior work that focuses solely on\nimproving document retrievability, our attack method explicitly considers both\nretrievability and user trust in the retrieved content. Both offline and online\nexperiments demonstrate that our method significantly degrades system\nperformance without internal access, while generating natural-looking poisoned\ndocuments.\n","authors":["Chanwoo Choi","Jinsoo Kim","Sukmin Cho","Soyeong Jeong","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2502.20995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08922v2","updated":"2025-05-30T08:09:29Z","published":"2024-12-12T04:13:09Z","title":"Nested Hash Layer: A Plug-and-play Module for Multiple-length Hash Code\n  Learning","summary":"  Deep supervised hashing is essential for efficient storage and search in\nlarge-scale image retrieval. Traditional deep supervised hashing models\ngenerate single-length hash codes, but this creates a trade-off between\nefficiency and effectiveness for different code lengths. To find the optimal\nlength for a task, multiple models must be trained, increasing time and\ncomputation. Furthermore, relationships between hash codes of different lengths\nare often ignored. To address these issues, we propose the Nested Hash Layer\n(NHL), a plug-and-play module for deep supervised hashing models. NHL generates\nhash codes of multiple lengths simultaneously in a nested structure. To resolve\noptimization conflicts from multiple learning objectives, we introduce a\ndominance-aware dynamic weighting strategy to adjust gradients. Additionally,\nwe propose a long-short cascade self-distillation method, where long hash codes\nguide the learning of shorter ones, improving overall code quality. Experiments\nindicate that the NHL achieves an overall training speed improvement of\napproximately 5 to 8 times across various deep supervised hashing models and\nenhances the average performance of these models by about 3.4%.\n","authors":["Liyang He","Yuren Zhang","Rui Li","Zhenya Huang","Runze Wu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.08922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11471v4","updated":"2025-05-30T07:59:38Z","published":"2025-02-17T06:02:59Z","title":"GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.\n","authors":["Kangyang Luo","Yuzhuo Bai","Cheng Gao","Shuzheng Si","Yingli Shen","Zhu Liu","Zhitong Wang","Cunliang Kong","Wenhao Li","Yufei Huang","Ye Tian","Xuantang Xiong","Lei Han","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11471v4.pdf","comment":"Accepted by ACL2025(Findings)"},{"id":"http://arxiv.org/abs/2505.24294v1","updated":"2025-05-30T07:12:02Z","published":"2025-05-30T07:12:02Z","title":"A Novel Discrete Memristor-Coupled Heterogeneous Dual-Neuron Model and\n  Its Application in Multi-Scenario Image Encryption","summary":"  Simulating brain functions using neural networks is an important area of\nresearch. Recently, discrete memristor-coupled neurons have attracted\nsignificant attention, as memristors effectively mimic synaptic behavior, which\nis essential for learning and memory. This highlights the biological relevance\nof such models. This study introduces a discrete memristive heterogeneous\ndual-neuron network (MHDNN). The stability of the MHDNN is analyzed with\nrespect to initial conditions and a range of neuronal parameters. Numerical\nsimulations demonstrate complex dynamical behaviors. Various neuronal firing\npatterns are investigated under different coupling strengths, and\nsynchronization phenomena between neurons are explored. The MHDNN is\nimplemented and validated on the STM32 hardware platform. An image encryption\nalgorithm based on the MHDNN is proposed, along with two hardware platforms\ntailored for multi-scenario police image encryption. These solutions enable\nreal-time and secure transmission of police data in complex environments,\nreducing hacking risks and enhancing system security.\n","authors":["Yi Zou","Mengjiao Wang","Xinan Zhang","Herbert Ho-Ching Iu"],"pdf_url":"https://arxiv.org/pdf/2505.24294v1.pdf","comment":"IEEE INTERNET OF THINGS JOURNAL(IOTJ)"},{"id":"http://arxiv.org/abs/2505.24279v1","updated":"2025-05-30T06:57:27Z","published":"2025-05-30T06:57:27Z","title":"On the Scaling of Robustness and Effectiveness in Dense Retrieval","summary":"  Robustness and Effectiveness are critical aspects of developing dense\nretrieval models for real-world applications. It is known that there is a\ntrade-off between the two. Recent work has addressed scaling laws of\neffectiveness in dense retrieval, revealing a power-law relationship between\neffectiveness and the size of models and data. Does robustness follow scaling\nlaws too? If so, can scaling improve both robustness and effectiveness\ntogether, or do they remain locked in a trade-off?\n  To answer these questions, we conduct a comprehensive experimental study. We\nfind that:(i) Robustness, including out-of-distribution and adversarial\nrobustness, also follows a scaling law.(ii) Robustness and effectiveness\nexhibit different scaling patterns, leading to significant resource costs when\njointly improving both. Given these findings, we shift to the third factor that\naffects model performance, namely the optimization strategy, beyond the model\nsize and data size. We find that: (i) By fitting different optimization\nstrategies, the joint performance of robustness and effectiveness traces out a\nPareto frontier. (ii) When the optimization strategy strays from Pareto\nefficiency, the joint performance scales in a sub-optimal direction. (iii) By\nadjusting the optimization weights to fit the Pareto efficiency, we can achieve\nPareto training, where the scaling of joint performance becomes most efficient.\nEven without requiring additional resources, Pareto training is comparable to\nthe performance of scaling resources several times under optimization\nstrategies that overly prioritize either robustness or effectiveness. Finally,\nwe demonstrate that our findings can help deploy dense retrieval models in\nreal-world applications that scale efficiently and are balanced for robustness\nand effectiveness.\n","authors":["Yu-An Liu","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.24279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24274v1","updated":"2025-05-30T06:49:39Z","published":"2025-05-30T06:49:39Z","title":"MGS3: A Multi-Granularity Self-Supervised Code Search Framework","summary":"  In the pursuit of enhancing software reusability and developer productivity,\ncode search has emerged as a key area, aimed at retrieving code snippets\nrelevant to functionalities based on natural language queries. Despite\nsignificant progress in self-supervised code pre-training utilizing the vast\namount of code data in repositories, existing methods have primarily focused on\nleveraging contrastive learning to align natural language with function-level\ncode snippets. These studies have overlooked the abundance of fine-grained\n(such as block-level and statement-level) code snippets prevalent within the\nfunction-level code snippets, which results in suboptimal performance across\nall levels of granularity. To address this problem, we first construct a\nmulti-granularity code search dataset called MGCodeSearchNet, which contains\n536K+ pairs of natural language and code snippets. Subsequently, we introduce a\nnovel Multi-Granularity Self-Supervised contrastive learning code Search\nframework (MGS$^{3}$}). First, MGS$^{3}$ features a Hierarchical\nMulti-Granularity Representation module (HMGR), which leverages syntactic\nstructural relationships for hierarchical representation and aggregates\nfine-grained information into coarser-grained representations. Then, during the\ncontrastive learning phase, we endeavor to construct positive samples of the\nsame granularity for fine-grained code, and introduce in-function negative\nsamples for fine-grained code. Finally, we conduct extensive experiments on\ncode search benchmarks across various granularities, demonstrating that the\nframework exhibits outstanding performance in code search tasks of multiple\ngranularities. These experiments also showcase its model-agnostic nature and\ncompatibility with existing pre-trained code representation models.\n","authors":["Rui Li","Junfeng Kang","Qi Liu","Liyang He","Zheng Zhang","Yunhao Sha","Linbo Zhu","Zhenya Huang"],"pdf_url":"https://arxiv.org/pdf/2505.24274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13703v2","updated":"2025-05-30T06:44:32Z","published":"2025-04-18T14:03:40Z","title":"Consensus-aware Contrastive Learning for Group Recommendation","summary":"  Group recommendation aims to provide personalized item suggestions to a group\nof users by reflecting their collective preferences. A fundamental challenge in\nthis task is deriving a consensus that adequately represents the diverse\ninterests of individual group members. Despite advancements made by deep\nlearning-based models, existing approaches still struggle in two main areas:\n(1) Capturing consensus in small-group settings, which are more prevalent in\nreal-world applications, and (2) Balancing individual preferences with overall\ngroup performance, particularly in hypergraph-based methods that tend to\nemphasize group accuracy at the expense of personalization. To address these\nchallenges, we introduce a Consensus-aware Contrastive Learning for Group\nRecommendation (CoCoRec) that models group consensus through contrastive\nlearning. CoCoRec utilizes a transformer encoder to jointly learn user and\ngroup representations, enabling richer modeling of intra-group dynamics.\nAdditionally, the contrastive objective helps reduce overfitting from\nhigh-frequency user interactions, leading to more robust and representative\ngroup embeddings. Experiments conducted on four benchmark datasets show that\nCoCoRec consistently outperforms state-of-the-art baselines in both individual\nand group recommendation scenarios, highlighting the effectiveness of\nconsensus-aware contrastive learning in group recommendation tasks.\n","authors":["Soyoung Kim","Dongjun Lee","Jaekwang Kim"],"pdf_url":"https://arxiv.org/pdf/2504.13703v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.24251v1","updated":"2025-05-30T06:16:30Z","published":"2025-05-30T06:16:30Z","title":"Proactive Guidance of Multi-Turn Conversation in Industrial Search","summary":"  The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation.\n","authors":["Xiaoyu Li","Xiao Li","Li Gao","Yiding Liu","Xiaoyang Wang","Shuaiqiang Wang","Junfeng Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2505.24251v1.pdf","comment":"ACL'25 (Industry)"},{"id":"http://arxiv.org/abs/2505.19253v2","updated":"2025-05-30T05:20:27Z","published":"2025-05-25T18:16:13Z","title":"DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research","summary":"  Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.\n","authors":["João Coelho","Jingjie Ning","Jingyuan He","Kangrui Mao","Abhijay Paladugu","Pranav Setlur","Jiahe Jin","Jamie Callan","João Magalhães","Bruno Martins","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.19253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13230v3","updated":"2025-05-30T04:54:10Z","published":"2024-10-17T05:33:50Z","title":"Starbucks-v2: Improved Training for 2D Matryoshka Embeddings","summary":"  2D Matryoshka training enables a single embedding model to generate\nsub-network representations across different layers and embedding dimensions,\noffering adaptability to diverse computational and task constraints. However,\nits effectiveness remains well below that of individually trained models of\nequivalent sizes. To address this, we propose Starbucks, a new training\nstrategy for Matryoshka-style embedding models that combines structured\nfine-tuning with masked autoencoder (MAE) pre-training. During fine-tuning, we\ncompute the loss over a fixed set of layer-dimension pairs, from small to\nlarge, which significantly improves performance over randomly sampled\nsub-networks and matches that of separately trained models. Our MAE-based\npre-training further enhances the representation quality of sub-networks,\nproviding a stronger backbone for downstream tasks. Experiments on both\nin-domain (semantic similarity and passage retrieval) and out-of-domain (BEIR)\nbenchmarks show that Starbucks consistently outperforms 2D Matryoshka models\nand matches or exceeds the performance of individually trained models, while\nmaintaining high efficiency and adaptability. Ablation studies confirm our loss\ndesign choices, the impact of SMAE pre-training and demonstrate the\napplicability of Starbucks across backbones. We further show that depth- and\nwidth-wise Starbucks variants capture complementary information, and that their\nhybridization yields additional performance gains with minimal latency overhead\ndue to parallelization. Code available at https://github.com/ielab/Starbucks\n","authors":["Shengyao Zhuang","Shuai Wang","Fabio Zheng","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2410.13230v3.pdf","comment":"Updated Version of Starbucks, add (1) Generalisation to E5 model (2)\n  Out-of-domain zero-shot effectiveness (3) Propose Depth-wise Starbucks and\n  Hybrid-Starbucks"},{"id":"http://arxiv.org/abs/2409.17275v2","updated":"2025-05-30T04:01:35Z","published":"2024-09-12T02:43:40Z","title":"On the Vulnerability of Applying Retrieval-Augmented Generation within\n  Knowledge-Intensive Application Domains","summary":"  Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases.\n","authors":["Xun Xian","Ganghua Wang","Xuan Bi","Jayanth Srinivasa","Ashish Kundu","Charles Fleming","Mingyi Hong","Jie Ding"],"pdf_url":"https://arxiv.org/pdf/2409.17275v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.24172v1","updated":"2025-05-30T03:32:26Z","published":"2025-05-30T03:32:26Z","title":"Heterogeneous Graph Masked Contrastive Learning for Robust\n  Recommendation","summary":"  Heterogeneous graph neural networks (HGNNs) have demonstrated their\nsuperiority in exploiting auxiliary information for recommendation tasks.\nHowever, graphs constructed using meta-paths in HGNNs are usually too dense and\ncontain a large number of noise edges. The propagation mechanism of HGNNs\npropagates even small amounts of noise in a graph to distant neighboring nodes,\nthereby affecting numerous node embeddings. To address this limitation, we\nintroduce a novel model, named Masked Contrastive Learning (MCL), to enhance\nrecommendation robustness to noise. MCL employs a random masking strategy to\naugment the graph via meta-paths, reducing node sensitivity to specific\nneighbors and bolstering embedding robustness. Furthermore, MCL employs\ncontrastive cross-view on a Heterogeneous Information Network (HIN) from two\nperspectives: one-hop neighbors and meta-path neighbors. This approach acquires\nembeddings capturing both local and high-order structures simultaneously for\nrecommendation. Empirical evaluations on three real-world datasets confirm the\nsuperiority of our approach over existing recommendation methods.\n","authors":["Lei Sang","Yu Wang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.24172v1.pdf","comment":"12 pages, 7 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.24518v1","updated":"2025-05-30T12:30:04Z","published":"2025-05-30T12:30:04Z","title":"ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis\n  Optimization for Speech Multi-Metric Estimation","summary":"  Speech signal analysis poses significant challenges, particularly in tasks\nsuch as speech quality evaluation and profiling, where the goal is to predict\nmultiple perceptual and objective metrics. For instance, metrics like PESQ\n(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective\nIntelligibility), and MOS (Mean Opinion Score) each capture different aspects\nof speech quality. However, these metrics often have different scales,\nassumptions, and dependencies, making joint estimation non-trivial. To address\nthese issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based\nHypothesis Optimization), a chain-based, versatile evaluation system for speech\nassessment grounded in autoregressive dependency modeling. ARECHO is\ndistinguished by three key innovations: (1) a comprehensive speech information\ntokenization pipeline; (2) a dynamic classifier chain that explicitly captures\ninter-metric dependencies; and (3) a two-step confidence-oriented decoding\nalgorithm that enhances inference reliability. Experiments demonstrate that\nARECHO significantly outperforms the baseline framework across diverse\nevaluation scenarios, including enhanced speech analysis, speech generation\nevaluation, and noisy speech evaluation. Furthermore, its dynamic dependency\nmodeling improves interpretability by capturing inter-metric relationships.\n","authors":["Jiatong Shi","Yifan Cheng","Bo-Hao Su","Hye-jin Shim","Jinchuan Tian","Samuele Cornell","Yiwen Zhao","Siddhant Arora","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2505.24518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06685v2","updated":"2025-05-30T12:27:30Z","published":"2025-05-10T16:15:26Z","title":"Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General\n  Vision-Language Understanding","summary":"  Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://github.com/24DavidHuang/Emotion-Qwen.\n","authors":["Dawei Huang","Qing Li","Chuan Yan","Zebang Cheng","Yurong Huang","Xiang Li","Bin Li","Xiaohui Wang","Zheng Lian","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2505.06685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23449v2","updated":"2025-05-30T11:34:43Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20630v2","updated":"2025-05-30T10:18:08Z","published":"2025-04-29T10:56:44Z","title":"ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting","summary":"  Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo.\n","authors":["Yu Zhang","Wenxiang Guo","Changhao Pan","Zhiyuan Zhu","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.20630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24253v1","updated":"2025-05-30T06:19:47Z","published":"2025-05-30T06:19:47Z","title":"Interactive Video Generation via Domain Adaptation","summary":"  Text-conditioned diffusion models have emerged as powerful tools for\nhigh-quality video generation. However, enabling Interactive Video Generation\n(IVG), where users control motion elements such as object trajectory, remains\nchallenging. Recent training-free approaches introduce attention masking to\nguide trajectory, but this often degrades perceptual quality. We identify two\nkey failure modes in these methods, both of which we interpret as domain shift\nproblems, and propose solutions inspired by domain adaptation. First, we\nattribute the perceptual degradation to internal covariate shift induced by\nattention masking, as pretrained models are not trained to handle masked\nattention. To address this, we propose mask normalization, a pre-normalization\nlayer designed to mitigate this shift via distribution matching. Second, we\naddress initialization gap, where the randomly sampled initial noise does not\nalign with IVG conditioning, by introducing a temporal intrinsic diffusion\nprior that enforces spatio-temporal consistency at each denoising step.\nExtensive qualitative and quantitative evaluations demonstrate that mask\nnormalization and temporal intrinsic denoising improve both perceptual quality\nand trajectory control over the existing state-of-the-art IVG techniques.\n","authors":["Ishaan Rawal","Suryansh Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.24253v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2505.23018v2","updated":"2025-05-30T05:09:13Z","published":"2025-05-29T02:56:08Z","title":"EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich\n  Annotations","summary":"  In recent years, emotion recognition plays a critical role in applications\nsuch as human-computer interaction, mental health monitoring, and sentiment\nanalysis. While datasets for emotion analysis in languages such as English have\nproliferated, there remains a pressing need for high-quality, comprehensive\ndatasets tailored to the unique linguistic, cultural, and multimodal\ncharacteristics of Chinese. In this work, we propose \\textbf{EmotionTalk}, an\ninteractive Chinese multimodal emotion dataset with rich annotations. This\ndataset provides multimodal information from 19 actors participating in dyadic\nconversational settings, incorporating acoustic, visual, and textual\nmodalities. It includes 23.6 hours of speech (19,250 utterances), annotations\nfor 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,\nfear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,\nneutral, weakly positive, and positive) and 4-dimensional speech captions\n(speaker, speaking style, emotion and overall). The dataset is well-suited for\nresearch on unimodal and multimodal emotion recognition, missing modality\nchallenges, and speech captioning tasks. To our knowledge, it represents the\nfirst high-quality and versatile Chinese dialogue multimodal emotion dataset,\nwhich is a valuable contribution to research on cross-cultural emotion analysis\nand recognition. Additionally, we conduct experiments on EmotionTalk to\ndemonstrate the effectiveness and quality of the dataset. It will be\nopen-source and freely available for all academic purposes. The dataset and\ncodes will be made available at: https://github.com/NKU-HLT/EmotionTalk.\n","authors":["Haoqin Sun","Xuechen Wang","Jinghua Zhao","Shiwan Zhao","Jiaming Zhou","Hui Wang","Jiabei He","Aobo Kong","Xi Yang","Yequan Wang","Yonghua Lin","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2505.23018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24176v1","updated":"2025-05-30T03:36:32Z","published":"2025-05-30T03:36:32Z","title":"ISMAF: Intrinsic-Social Modality Alignment and Fusion for Multimodal\n  Rumor Detection","summary":"  The rapid dissemination of rumors on social media highlights the urgent need\nfor automatic detection methods to safeguard societal trust and stability.\nWhile existing multimodal rumor detection models primarily emphasize capturing\nconsistency between intrinsic modalities (e.g., news text and images), they\noften overlook the intricate interplay between intrinsic and social modalities.\nThis limitation hampers the ability to fully capture nuanced relationships that\nare crucial for a comprehensive understanding. Additionally, current methods\nstruggle with effectively fusing social context with textual and visual\ninformation, resulting in fragmented interpretations. To address these\nchallenges, this paper proposes a novel Intrinsic-Social Modality Alignment and\nFusion (ISMAF) framework for multimodal rumor detection. ISMAF first employs a\ncross-modal consistency alignment strategy to align complex interactions\nbetween intrinsic and social modalities. It then leverages a mutual learning\napproach to facilitate collaborative refinement and integration of\ncomplementary information across modalities. Finally, an adaptive fusion\nmechanism is incorporated to dynamically adjust the contribution of each\nmodality, tackling the complexities of three-modality fusion. Extensive\nexperiments on both English and Chinese real-world multimedia datasets\ndemonstrate that ISMAF consistently outperforms state-of-the-art models.\n","authors":["Zihao Yu","Xiang Li","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.24176v1.pdf","comment":null}]}}